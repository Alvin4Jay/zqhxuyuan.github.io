	<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Apache Spark入门 | 任何忧伤,都抵不过世界的美丽</title>
  <meta name="author" content="zqhxuyuan">
  
  <meta name="description" content="http://spark.apache.org/docs/latest/quick-start.html
Spark Shell➜  spark-1.4.0-bin-hadoop2.6  bin/spark-shell
1234567891011121314Welcome to      ____ ">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Apache Spark入门"/>
  <meta property="og:site_name" content="任何忧伤,都抵不过世界的美丽"/>

  
  

  <!-- toc -->
  <link rel="stylesheet" href="/libs/tocify/jquery.tocify.css" media="screen" type="text/css">

  <!-- <link rel="stylesheet" href="/libs/bs/css/bootstrap.min.css" media="screen" type="text/css"> -->
  <link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap/3.3.4/css/bootstrap.min.css" media="screen" type="text/css">

  <!-- material design -->
	<!-- <link rel="stylesheet" href="/libs/bs-material/css/ripples.min.css" media="screen" type="text/css"> -->
  <link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap-material/0.3.0/css/ripples.min.css" media="screen" type="text/css">
  <!-- <link rel="stylesheet" href="/libs/bs-material/css/material.min.css" media="screen" type="text/css"> -->
	<link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap-material/0.3.0/css/material.min.css" media="screen" type="text/css">

  <link rel="stylesheet" href="/css/highlight.light.css" media="screen" type="text/css">

  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">

  <!-- 百度统计 -->
  

  <!-- 谷歌统计 -->
  

  <script src="//apps.bdimg.com/libs/jquery/2.0.3/jquery.min.js"></script>
	<script>window.jQuery || document.write('<script src="/libs/jquery-2.0.3.min.js" type="text/javascript"><\/script>')</script>

</head>

 	<body>
	  <nav class="navbar navbar-default">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">菜单</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">任何忧伤,都抵不过世界的美丽</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
            <ul class="nav navbar-nav navbar-right">
                
                <li>
                    <a href="/" title="">
                    <i class="fa fa-home"></i>首页
                    </a>
                </li>
                
                <li>
                    <a href="/archives" title="">
                    <i class="fa fa-list"></i>存档
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>

	  <div class="container" >
	    <div class="row">
	<div class="col-md-8">
	  <!-- index -->
	  
			<h1>Apache Spark入门</h1>
			
			<div>
				<i class="fa fa-clock-o"></i>
				<span class="post-time">2015-11-29 20:51:45</span>
			</div>
			
	  

		<div class="content">
			<!-- index -->
		  
					<p><a href="http://spark.apache.org/docs/latest/quick-start.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/quick-start.html</a></p>
<h2 id="Spark_Shell">Spark Shell</h2><p>➜  spark-1.4.0-bin-hadoop2.6  <code>bin/spark-shell</code></p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  '_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   <span class="keyword">version</span> 1.4.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using <span class="keyword">Scala</span> <span class="keyword">version</span> 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_25)</span><br><span class="line">15/06/28 10:36:07 INFO ui.SparkUI: Started SparkUI at http:<span class="comment">//127.0.0.1:4040</span></span><br><span class="line">15/06/28 10:36:07 INFO repl.SparkILoop: Created spark context..</span><br><span class="line">Spark context available <span class="keyword">as</span> <span class="keyword">sc</span>.</span><br><span class="line">15/06/28 10:36:08 INFO hive.HiveContext: Initializing execution hive, <span class="keyword">version</span> 0.13.1</span><br><span class="line">15/06/28 10:36:23 INFO repl.SparkILoop: Created sql context (with Hive support)..</span><br><span class="line">SQL context available <span class="keyword">as</span> sqlContext.</span><br></pre></td></tr></table></figure>
<h2 id="Basic_RDD_Operation">Basic RDD Operation</h2><p>第一个例子: 统计一个文本文件的单词数量.<br>调用sc的textFile(fileName)会生成一个MapPartitionsRDD  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textFile = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"README.md"</span>)</span></span></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: <span class="function"><span class="title">ensureFreeSpace</span><span class="params">(<span class="number">63424</span>)</span></span> called with curMem=<span class="number">0</span>, maxMem=<span class="number">278019440</span></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: Block broadcast_0 stored as values <span class="keyword">in</span> memory (estimated size <span class="number">61.9</span> KB, free <span class="number">265.1</span> MB)</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: <span class="function"><span class="title">ensureFreeSpace</span><span class="params">(<span class="number">20061</span>)</span></span> called with curMem=<span class="number">63424</span>, maxMem=<span class="number">278019440</span></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: Block broadcast_0_piece0 stored as bytes <span class="keyword">in</span> memory (estimated size <span class="number">19.6</span> KB, free <span class="number">265.1</span> MB)</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.BlockManagerInfo</span>: Added broadcast_0_piece0 <span class="keyword">in</span> memory on localhost:<span class="number">58638</span> (size: <span class="number">19.6</span> KB, free: <span class="number">265.1</span> MB)</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO spark<span class="class">.SparkContext</span>: Created broadcast <span class="number">0</span> from textFile at &lt;console&gt;:<span class="number">21</span></span><br><span class="line">textFile: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br></pre></td></tr></table></figure>
<p>调用上面生成的textFile RDD的count()会触发一个Action.  </p>
<figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.count()</span><br><span class="line">java.net.ConnectException: Call <span class="keyword">From</span> hadoop/<span class="number">127.0</span>.<span class="number">0.1</span> <span class="keyword">to</span> localhost:<span class="number">9000</span> failed <span class="keyword">on</span> connection exception: java.net.ConnectException: 拒绝连接; <span class="keyword">For</span> more details see:  http:<span class="comment">//wiki.apache.org/hadoop/ConnectionRefused</span></span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native <span class="function"><span class="keyword">Method</span>)</span><br><span class="line">    ...</span><br><span class="line"><span class="title">Caused</span> <span class="title">by</span>:</span> java.net.ConnectException: 拒绝连接</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.checkConnect(Native <span class="function"><span class="keyword">Method</span>)</span><br><span class="line">	...</span></span><br></pre></td></tr></table></figure>
<p>由于本机已经安装了Hadoop,使用的是伪分布式模式,所以Spark会读取Hadoop的配置信息.<br>我们这里先不启动Hadoop,使用本地模式,要手动添加file:///并使用绝对路径读取文本文件  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile</span><br><span class="line">res1: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br></pre></td></tr></table></figure>
<p>重新构造读取本地文本文件的textFile RDD</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textFile = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"file:///home/hadoop/soft/spark-1.4.0-bin-hadoop2.6/README.md"</span>)</span></span></span><br><span class="line">textFile: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">3</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br></pre></td></tr></table></figure>
<p>触发RDD的Action: count  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.count()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">44</span>:<span class="number">07</span> INFO scheduler.DAGScheduler: Job <span class="number">0</span> finished: count at &lt;console&gt;:<span class="number">24</span>, took <span class="number">0.275609</span> s</span><br><span class="line">res2: Long = <span class="number">98</span></span><br></pre></td></tr></table></figure>
<p>又一个Action RDD : 输出文本文件的第一行  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.first()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">44</span>:<span class="number">27</span> INFO scheduler.DAGScheduler: Job <span class="number">1</span> finished: first at &lt;console&gt;:<span class="number">24</span>, took <span class="number">0.017917</span> s</span><br><span class="line">res3: String = <span class="preprocessor"># Apache Spark</span></span><br></pre></td></tr></table></figure>
<h2 id="More_RDD_Operations">More RDD Operations</h2><p>1.统计包含了Spark这个单词一共有几行</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val linesWithSpark = textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>)</span><br><span class="line">linesWithSpark: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">4</span>] at <span class="attribute">filter</span> at &lt;console&gt;:<span class="number">23</span></span><br><span class="line">scala&gt; textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>).<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<p>2.文本文件中长度最长的那一行,它一共有多少个单词</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.<span class="function"><span class="title">map</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>.size).<span class="function"><span class="title">reduce</span><span class="params">((a, b)</span></span> =&gt; <span class="keyword">if</span> (<span class="tag">a</span> &gt; b) <span class="tag">a</span> <span class="keyword">else</span> b)</span><br></pre></td></tr></table></figure>
<p>3.MapReduce WordCount</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val wordCounts = textFile.<span class="function"><span class="title">flatMap</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>).<span class="function"><span class="title">map</span><span class="params">(word =&gt; (word, <span class="number">1</span>)</span></span>).<span class="function"><span class="title">reduceByKey</span><span class="params">((a, b)</span></span> =&gt; <span class="tag">a</span> + b)</span><br><span class="line">wordCounts: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[(String, Int)] = ShuffledRDD[<span class="number">9</span>] at reduceByKey at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt;  wordCounts.<span class="function"><span class="title">collect</span><span class="params">()</span></span></span><br><span class="line">res6: Array[(String, Int)] = <span class="function"><span class="title">Array</span><span class="params">((package,<span class="number">1</span>)</span></span>, (this,<span class="number">1</span>), (Version<span class="string">"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (cluster.,1), (its,1), ([run,1), (general,2), (have,1), (pre-built,1), (locally.,1), (locally,2), (changed,1), (sc.parallelize(1,1), (only,1), (several,1), (This,2), (basic,1), (Configuration,1), (learning,,1), (documentation,3), (YARN,,1), (graph,1), (Hive,2), (first,1), (["</span>Specifying,<span class="number">1</span>), (<span class="string">"yarn-client"</span>,<span class="number">1</span>), (page](http:<span class="comment">//spark.apache.org/documentation.html),1), ([params]`.,1), (application,1), ([project,2), (prefer,1), (SparkPi,2), (&lt;http://spark.apache.org/&gt;,1), (engine,1), (version,1), (file,1), (documentation,,1), (MASTER,1), (example,3), (distribution.,1), (are,1), (params,1), (scala&gt;,1), (systems.,1...</span></span><br></pre></td></tr></table></figure>
<p>4.Cache</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  linesWithSpark.cache()</span><br><span class="line">res7: linesWithSpark.type = MapPartitionsRDD[<span class="number">4</span>] at filter at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">47</span>:<span class="number">11</span> INFO scheduler.DAGScheduler: Job <span class="number">5</span> finished: count at &lt;console&gt;:<span class="number">26</span>, took <span class="number">0.054036</span> s</span><br><span class="line">res8: Long = <span class="number">19</span></span><br><span class="line"></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">47</span>:<span class="number">14</span> INFO scheduler.DAGScheduler: Job <span class="number">6</span> finished: count at &lt;console&gt;:<span class="number">26</span>, took <span class="number">0.016638</span> s</span><br><span class="line">res9: Long = <span class="number">19</span></span><br></pre></td></tr></table></figure>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">val textFile = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"file:///home/hadoop/soft/spark-1.4.0-bin-hadoop2.6/README.md"</span>)</span></span></span><br><span class="line">〇 textFile.<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br><span class="line">① textFile.<span class="function"><span class="title">first</span><span class="params">()</span></span></span><br><span class="line">val linesWithSpark = textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>)</span><br><span class="line">② textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>).<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br><span class="line">textFile.<span class="function"><span class="title">map</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>.size).<span class="function"><span class="title">reduce</span><span class="params">((a, b)</span></span> =&gt; <span class="keyword">if</span> (<span class="tag">a</span> &gt; b) <span class="tag">a</span> <span class="keyword">else</span> b)</span><br><span class="line">③ val wordCounts = textFile.<span class="function"><span class="title">flatMap</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>).<span class="function"><span class="title">map</span><span class="params">(word =&gt; (word, <span class="number">1</span>)</span></span>).<span class="function"><span class="title">reduceByKey</span><span class="params">((a, b)</span></span> =&gt; <span class="tag">a</span> + b)</span><br><span class="line">④ wordCounts.<span class="function"><span class="title">collect</span><span class="params">()</span></span></span><br><span class="line">linesWithSpark.<span class="function"><span class="title">cache</span><span class="params">()</span></span></span><br><span class="line">⑤ linesWithSpark.<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br><span class="line">⑥ linesWithSpark.<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br><span class="line">⑦ linesWithSpark.<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<h2 id="Spark_Shell_UI">Spark Shell UI</h2><p><a href="http://127.0.0.1:4040" target="_blank" rel="external">http://127.0.0.1:4040</a></p>
<h3 id="Jobs,_Stages,_Storage">Jobs, Stages, Storage</h3><p>Jobs: 上面每个Action RDD编号对应了下图中的Job Id.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark1.png" alt=""></p>
<p>Stages: 上面有8个Job, 但是Stages多了一个. 其实是④的<code>collect</code>有两个stage  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark2.png" alt=""></p>
<p>Storage: 在Cache的时候才有</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark3.png" alt=""></p>
<h3 id="查看Stage">查看Stage</h3><p>在Jobs中点击Job Id=4的collect RDD(输出WordCount的结果). 在下方的列表中可以看到有2个Stages<br>仔细观察列表的最后面两列, 分别是Shuffle Read和Shuffle Write.<br>其中map会进行Shuffle Write, collect会进行Shuffle Read</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark4.png" alt=""></p>
<p>点击Stage Id=4的map. 它的DAG可视化图和上面的概览图的左侧是一样的</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark5.png" alt=""></p>
<p>Spark的WebUI还提供了一个EventTime,可以很清楚地看到每个阶段消耗的时间</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark7.png" alt=""></p>
<p>回退,点击Stage Id=5的collect</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark6.png" alt=""></p>
<hr>
<h2 id="Spark_Standalone_集群安装">Spark Standalone 集群安装</h2><p>准备工作:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>master无密码ssh到slaves(将master的pub追加到所有slaves的authorized_keys)</span><br><span class="line"><span class="number">2.</span>关闭所有节点的防火墙(chkconfig iptables off)</span><br><span class="line"><span class="number">3.</span>安装scala-<span class="number">2.10</span>,并设置~/.bashrc</span><br></pre></td></tr></table></figure>
<p>cd $SPARK_HOME<br>vi conf/spark-env.sh  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> JAVA_HOME=/usr/java/jdk1<span class="number">.7</span><span class="number">.0</span>_51</span><br><span class="line"><span class="keyword">export</span> SCALA_HOME=/usr/install/scala-<span class="number">2.10</span><span class="number">.5</span></span><br><span class="line"><span class="keyword">export</span> HADOOP_HOME=/usr/install/hadoop</span><br><span class="line"><span class="keyword">export</span> HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_IP=dp0652</span><br><span class="line"><span class="keyword">export</span> MASTER=spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="preprocessor">#export SPARK_LOCAL_IP=dp0652</span></span><br><span class="line"><span class="keyword">export</span> SPARK_LOCAL_DIRS=/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_WEBUI_PORT=<span class="number">8082</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_PORT=<span class="number">7077</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_CORES=<span class="number">1</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_INSTANCES=<span class="number">1</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_MEMORY=<span class="number">8</span>g</span><br></pre></td></tr></table></figure>
<p>vi conf/slaves</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">dp0652</span></span><br><span class="line">dp0653</span><br><span class="line">dp0655</span><br><span class="line">dp0656</span><br><span class="line">dp0657</span><br></pre></td></tr></table></figure>
<p>将spark目录分发到集群的其他节点</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0653:/usr/install</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0655:/usr/install</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0656:/usr/install</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0657:/usr/install</span><br></pre></td></tr></table></figure>
<p>由于集群中dp0652和dp0653的内存比较大, 我们修改了这两个节点的spark-env.sh  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> SPARK_WORKER_INSTANCES=<span class="number">2</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_MEMORY=<span class="number">20</span>g</span><br></pre></td></tr></table></figure>
<p>启动集群, 在master上启动即可.  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>]$ sbin/start-all<span class="class">.sh</span></span><br><span class="line">starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.master</span><span class="class">.Master</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.master</span><span class="class">.Master-1-dp0652</span><span class="class">.out</span></span><br><span class="line">dp0656: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0656</span><span class="class">.out</span></span><br><span class="line">dp0655: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0655</span><span class="class">.out</span></span><br><span class="line">dp0657: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0657</span><span class="class">.out</span></span><br><span class="line">dp0652: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0652</span><span class="class">.out</span></span><br><span class="line">dp0653: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0653</span><span class="class">.out</span></span><br><span class="line">dp0652: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-2-dp0652</span><span class="class">.out</span></span><br><span class="line">dp0653: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-2-dp0653</span><span class="class">.out</span></span><br></pre></td></tr></table></figure>
<p>在master和slaves上查看Spark进程</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ jps -lm</span><br><span class="line"><span class="number">40708</span> org.apache.spark.deploy.master.Master --ip dp0652 --port <span class="number">7077</span> --webui-port <span class="number">8082</span></span><br><span class="line"><span class="number">41095</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8082</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="number">40926</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ ssh dp0653</span><br><span class="line">Last login: Thu Jul  <span class="number">2</span> <span class="number">09</span>:<span class="number">07</span>:<span class="number">17</span> <span class="number">2015</span> from <span class="number">192.168</span><span class="number">.6</span><span class="number">.140</span></span><br><span class="line">[qihuang.zheng@dp0653 ~]$ jps -lm</span><br><span class="line"><span class="number">27153</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8082</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="number">27029</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line">[qihuang.zheng@dp0653 ~]$ <span class="built_in">exit</span></span><br><span class="line">logout</span><br><span class="line">Connection to dp0653 closed.</span><br><span class="line">[qihuang.zheng@dp0652 logs]$ ssh dp0655</span><br><span class="line">Last login: Thu Jul  <span class="number">2</span> <span class="number">08</span>:<span class="number">55</span>:<span class="number">05</span> <span class="number">2015</span> from <span class="number">192.168</span><span class="number">.6</span><span class="number">.140</span></span><br><span class="line">[qihuang.zheng@dp0655 ~]$ jps -lm</span><br><span class="line"><span class="number">8766</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line">[qihuang.zheng@dp0655 ~]$</span><br></pre></td></tr></table></figure>
<p>在master上查看web ui: <a href="http://dp0652:8082/" target="_blank" rel="external">http://dp0652:8082/</a></p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark8.png" alt=""></p>
<h3 id="遇到一些问题">遇到一些问题</h3><p><strong>1.如果配置了SPARK_LOCAL_IP, 但是并没有在slaves上修改为自己的IP,则会报错:</strong>  </p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">15/07/02 09:04:08 <span class="keyword">ERROR</span> netty.NettyTransport: failed to bind to /192.168.6.52:0, shutting down Netty transport</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.<span class="keyword">net</span>.BindException: Failed to bind to: /192.168.6.52:0: Service 'sparkWorker' failed after 16 retries!</span><br><span class="line">        at org.jboss.netty.<span class="keyword">bootstrap</span>.ServerBootstrap.bind(ServerBootstrap.java:272)</span><br><span class="line">        at akka.remote.transport.netty.NettyTransport$<span class="label">$anonfun</span><span class="label">$listen</span><span class="label">$1</span>.apply(NettyTransport.<span class="keyword">scala</span>:393)</span><br><span class="line">        at akka.remote.transport.netty.NettyTransport$<span class="label">$anonfun</span><span class="label">$listen</span><span class="label">$1</span>.apply(NettyTransport.<span class="keyword">scala</span>:389)</span><br><span class="line">        at <span class="keyword">scala</span>.util.Success$<span class="label">$anonfun</span><span class="label">$map</span><span class="label">$1</span>.apply(Try.<span class="keyword">scala</span>:206)</span><br><span class="line">        at <span class="keyword">scala</span>.util.Try$.apply(Try.<span class="keyword">scala</span>:161)</span><br><span class="line">        at <span class="keyword">scala</span>.util.Success.map(Try.<span class="keyword">scala</span>:206)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.Future$<span class="label">$anonfun</span><span class="label">$map</span><span class="label">$1</span>.apply(Future.<span class="keyword">scala</span>:235)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.Future$<span class="label">$anonfun</span><span class="label">$map</span><span class="label">$1</span>.apply(Future.<span class="keyword">scala</span>:235)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.impl.CallbackRunnable.<span class="keyword">run</span>(Promise.<span class="keyword">scala</span>:32)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.processBatch<span class="label">$1</span>(BatchingExecutor.<span class="keyword">scala</span>:67)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.apply<span class="label">$mcV</span><span class="label">$sp</span>(BatchingExecutor.<span class="keyword">scala</span>:82)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.apply(BatchingExecutor.<span class="keyword">scala</span>:59)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.apply(BatchingExecutor.<span class="keyword">scala</span>:59)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.BlockContext$.withBlockContext(BlockContext.<span class="keyword">scala</span>:72)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>.<span class="keyword">run</span>(BatchingExecutor.<span class="keyword">scala</span>:58)</span><br><span class="line">        at akka.dispatch.TaskInvocation.<span class="keyword">run</span>(AbstractDispatcher.<span class="keyword">scala</span>:41)</span><br><span class="line">        at akka.dispatch.ForkJoinExecutorConfigurator<span class="label">$AkkaForkJoinTask</span>.exec(AbstractDispatcher.<span class="keyword">scala</span>:393)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinPool<span class="label">$WorkQueue</span>.runTask(ForkJoinPool.java:1339)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinWorkerThread.<span class="keyword">run</span>(ForkJoinWorkerThread.java:107)</span><br><span class="line">15/07/02 09:04:09 INFO remote.RemoteActorRefProvider<span class="label">$RemotingTerminator</span>: Shutting down remote daemon.</span><br><span class="line">15/07/02 09:04:09 INFO remote.RemoteActorRefProvider<span class="label">$RemotingTerminator</span>: Remote daemon shut down; proceeding with flushing remote transports.</span><br><span class="line">15/07/02 09:04:09 INFO util.Utils: Shutdown hook called</span><br></pre></td></tr></table></figure>
<p>原因分析: SPARK_LOCAL_IP指的是本机IP地址,因此分发到集群的不同节点上,都要到各自的节点修改为自己的IP地址.<br>如果集群节点比较多,则比较麻烦, 可以用SPARK_LOCAL_DIRS代替.</p>
<p><strong>2.如果没有配置export MASTER, 在worker上会报错:</strong>  </p>
<figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">40</span>:<span class="number">51</span> INFO worker.Worker: Retrying connection <span class="keyword">to</span> master (attempt <span class="preprocessor"># 12)</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">40</span>:<span class="number">51</span> INFO worker.Worker: Connecting <span class="keyword">to</span> master akka.tcp://sparkMaster<span class="constant">@dp0652</span>:<span class="number">7077</span>/user/Master...</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">40</span>:<span class="number">51</span> WARN Remoting: Tried <span class="keyword">to</span> associate <span class="keyword">with</span> unreachable remote address [akka.tcp://sparkMaster<span class="constant">@dp0652</span>:<span class="number">7077</span>].</span><br><span class="line">Address is <span class="built_in">now</span> gated <span class="keyword">for</span> <span class="number">5000</span> ms, all messages <span class="keyword">to</span> this address will be delivered <span class="keyword">to</span> dead letters.</span><br><span class="line">Reason: 拒绝连接: dp0652/<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">7077</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">41</span>:<span class="number">23</span> ERROR worker.Worker: RECEIVED SIGNAL <span class="number">15</span>: SIGTERM</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">41</span>:<span class="number">23</span> INFO util.Utils: <span class="built_in">Shutdown</span> hook called</span><br></pre></td></tr></table></figure>
<p>导致的后果是虽然slaves上都启动了Worker进程(使用jps查看),但是在Master上并没有看到workers. 这时候应该查看Master上的日志.<br>master上启动成功显示的日志是spark@dp0652:7077. 而上面却显示的是sparkMaster@dp0652:7077. 所以应该手动export MASTER  </p>
<p><strong>3.最后成功启动集群, 在Master上的日志:</strong>  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Spark <span class="string">Command:</span> <span class="regexp">/usr/</span>java<span class="regexp">/jdk1.7.0_51/</span>bin<span class="regexp">/java -cp /</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/sbin/</span>..<span class="regexp">/conf/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/spark-assembly-1.4.0-hadoop2.6.0.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-rdbms-<span class="number">3.2</span><span class="number">.9</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/datanucleus-core-3.2.10.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-api-jdo-<span class="number">3.2</span><span class="number">.6</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span> -Xms512m -Xmx512m -<span class="string">XX:</span>MaxPermSize=<span class="number">128</span>m org.apache.spark.deploy.master.Master --ip dp0652 --port <span class="number">7077</span> --webui-port <span class="number">8082</span></span><br><span class="line">========================================</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">49</span> INFO master.<span class="string">Master:</span> Registered signal handlers <span class="keyword">for</span> [TERM, HUP, INT]</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> WARN util.<span class="string">NativeCodeLoader:</span> Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> INFO spark.<span class="string">SecurityManager:</span> Changing view acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> INFO spark.<span class="string">SecurityManager:</span> Changing modify acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> INFO spark.<span class="string">SecurityManager:</span> <span class="string">SecurityManager:</span> authentication disabled; ui acls disabled; users with view <span class="string">permissions:</span> Set(qihuang.zheng); users with modify <span class="string">permissions:</span> Set(qihuang.zheng)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO slf4j.<span class="string">Slf4jLogger:</span> Slf4jLogger started</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO <span class="string">Remoting:</span> Starting remoting</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO <span class="string">Remoting:</span> Remoting started; listening on <span class="string">addresses :</span>[akka.<span class="string">tcp:</span><span class="comment">//sparkMaster@dp0652:7077]</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'sparkMaster'</span> on port <span class="number">7077.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">Server:</span> jetty-<span class="number">8.</span>y.z-SNAPSHOT</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">AbstractConnector:</span> Started SelectChannelConnector<span class="annotation">@dp</span><span class="number">0652:</span><span class="number">6066</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO util.<span class="string">Utils:</span> Successfully started service on port <span class="number">6066.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO rest.<span class="string">StandaloneRestServer:</span> Started REST server <span class="keyword">for</span> submitting applications on port <span class="number">6066</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO master.<span class="string">Master:</span> Starting Spark master at <span class="string">spark:</span><span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO master.<span class="string">Master:</span> Running Spark version <span class="number">1.4</span><span class="number">.0</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">Server:</span> jetty-<span class="number">8.</span>y.z-SNAPSHOT</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">AbstractConnector:</span> Started SelectChannelConnector@<span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">8082</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'MasterUI'</span> on port <span class="number">8082.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO ui.<span class="string">MasterWebUI:</span> Started MasterWebUI at <span class="string">http:</span><span class="comment">//192.168.6.52:8082</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO master.<span class="string">Master:</span> I have been elected leader! New <span class="string">state:</span> ALIVE</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">35398</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">60106</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">50995</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">55994</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">34020</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">56</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">55912</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">56</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">35846</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br></pre></td></tr></table></figure>
<p><strong>在53的其中一个Worker上的日志:</strong>  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Spark <span class="string">Command:</span> <span class="regexp">/usr/</span>java<span class="regexp">/jdk1.7.0_51/</span>bin<span class="regexp">/java -cp /</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/sbin/</span>..<span class="regexp">/conf/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/spark-assembly-1.4.0-hadoop2.6.0.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-api-jdo-<span class="number">3.2</span><span class="number">.6</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/datanucleus-core-3.2.10.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-rdbms-<span class="number">3.2</span><span class="number">.9</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span> -Xms512m -Xmx512m -<span class="string">XX:</span>MaxPermSize=<span class="number">128</span>m org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> <span class="string">spark:</span><span class="comment">//dp0652:7077</span></span><br><span class="line">========================================</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO worker.<span class="string">Worker:</span> Registered signal handlers <span class="keyword">for</span> [TERM, HUP, INT]</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> WARN util.<span class="string">NativeCodeLoader:</span> Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO spark.<span class="string">SecurityManager:</span> Changing view acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO spark.<span class="string">SecurityManager:</span> Changing modify acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO spark.<span class="string">SecurityManager:</span> <span class="string">SecurityManager:</span> authentication disabled; ui acls disabled; users with view <span class="string">permissions:</span> Set(qihuang.zheng); users with modify <span class="string">permissions:</span> Set(qihuang.zheng)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">53</span> INFO slf4j.<span class="string">Slf4jLogger:</span> Slf4jLogger started</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">53</span> INFO <span class="string">Remoting:</span> Starting remoting</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO <span class="string">Remoting:</span> Remoting started; listening on <span class="string">addresses :</span>[akka.<span class="string">tcp:</span><span class="comment">//sparkWorker@192.168.6.53:55994]</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'sparkWorker'</span> on port <span class="number">55994.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Starting Spark worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">55994</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Running Spark version <span class="number">1.4</span><span class="number">.0</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Spark <span class="string">home:</span> <span class="regexp">/usr/</span>install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO server.<span class="string">Server:</span> jetty-<span class="number">8.</span>y.z-SNAPSHOT</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO server.<span class="string">AbstractConnector:</span> Started SelectChannelConnector@<span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">8081</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'WorkerUI'</span> on port <span class="number">8081.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO ui.<span class="string">WorkerWebUI:</span> Started WorkerWebUI at <span class="string">http:</span><span class="comment">//192.168.6.53:8081</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Connecting to master akka.<span class="string">tcp:</span><span class="comment">//sparkMaster@dp0652:7077/user/Master...</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Successfully registered with master <span class="string">spark:</span><span class="comment">//dp0652:7077</span></span><br></pre></td></tr></table></figure>
<h3 id="spark-shell_&amp;_spark-submit">spark-shell &amp; spark-submit</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --master spark:<span class="comment">//dp0652:7077 --executor-memory 4g</span></span><br><span class="line"></span><br><span class="line">bin/spark-submit --master spark:<span class="comment">//dp0652:7077 \</span></span><br><span class="line">  --<span class="keyword">class</span> org.apache.spark.examples.SparkPi \</span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">2</span> \</span><br><span class="line">  lib/spark-examples-<span class="number">1.4</span><span class="number">.0</span>-hadoop2<span class="number">.6</span><span class="number">.0</span>.jar <span class="number">1000</span></span><br></pre></td></tr></table></figure>

			  
		</div>

		<!-- pagination -->
	  

		
  


	</div>
	<div class="col-md-4">
		
			
				<div id="sidebar">
	 			
	 					
<div class="widget">
  <h4>最新文章</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2015/11/30/2015-11-10-Cassandra-Client/"  > <i class="mdi-editor-insert-drive-file"></i>Cassandra Client查询优化</a>
      </li>
    
      <li>
        <a href="/2015/11/30/2015-07-26-drill-fragment-execute/"  > <i class="mdi-editor-insert-drive-file"></i>Apache Drill源码阅读(6) execute</a>
      </li>
    
      <li>
        <a href="/2015/11/30/2015-07-15-drill-fragments/"  > <i class="mdi-editor-insert-drive-file"></i>Apache Drill源码阅读(5) Fragment</a>
      </li>
    
      <li>
        <a href="/2015/11/30/2015-07-14-drill-logical/"  > <i class="mdi-editor-insert-drive-file"></i>Apache Drill源码阅读(4) 逻辑计划</a>
      </li>
    
      <li>
        <a href="/2015/11/30/2015-07-13-drill-phyplan/"  > <i class="mdi-editor-insert-drive-file"></i>Apache Drill源码阅读(3) 物理计划</a>
      </li>
    
  </ul>
</div>


	 				
	 					
<div class="widget">
	<h4>链接</h4>
	<ul class="blogroll list-unstyled">
	
		<li> <a href="http://www.github.com/zqhxuyuan" title="同性交友社区" target="_blank"> <i class="mdi-action-launch"></i> GitHub</a></li>
	
		<li> <a href="http://www.weibo.com/xuyuantree" title="" target="_blank"> <i class="mdi-action-launch"></i> Weibo</a></li>
	
	</ul>
</div>


	 				
	 			</div>
			
		
	</div>

</div>


			<footer>
				

<p>
  由 <a href="https://hexo.io">hexo</a> 强力驱动 | 搭载 <a href="https://github.com/wayou/hexo-theme-material">material</a> 主题
</p>
<p>
  &copy; 2015 <a href="http://github.com/zqhxuyuan"> zqhxuyuan </a>
</p>
<a id="gotop" href="#" title="back to top"><i class="mdi-hardware-keyboard-arrow-up"></i></a>

			</footer>
	  </div>

		<!-- <script src="/libs/bs/js/bootstrap.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap/3.3.4/js/bootstrap.min.js"></script>
		<script>(typeof $().modal == 'function')|| document.write('<script src="/libs/bs/js/bootstrap.min.js" type="text/javascript"><\/script>')</script>

		<!-- material design -->
		<!-- <script src="/libs/bs-material/js/ripples.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap-material/0.3.0/js/ripples.min.js"></script>
		<!-- <script src="/libs/bs-material/js/material.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap-material/0.3.0/js/material.min.js"></script>
		<!-- toc -->
		<!-- <script src="/libs/tocify/jquery-ui.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/jqueryui/1.10.4/jquery-ui.min.js"></script>
		<script src="/libs/tocify/jquery.tocify.custom.js"></script>

		<script src="/js/main.js"></script>

	</body>
</html>
