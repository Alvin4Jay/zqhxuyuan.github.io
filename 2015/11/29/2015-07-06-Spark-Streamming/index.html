	<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Spark-Stramming入门 | 任何忧伤,都抵不过世界的美丽</title>
  <meta name="author" content="zqhxuyuan">
  
  <meta name="description" content="NetworkWordCount1.本机运行时,修改spark-env.sh(这一步不是必须的)
1234567export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Homeexport SCALA_H">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Spark-Stramming入门"/>
  <meta property="og:site_name" content="任何忧伤,都抵不过世界的美丽"/>

  
  

  <!-- toc -->
  <link rel="stylesheet" href="/libs/tocify/jquery.tocify.css" media="screen" type="text/css">

  <!-- <link rel="stylesheet" href="/libs/bs/css/bootstrap.min.css" media="screen" type="text/css"> -->
  <link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap/3.3.4/css/bootstrap.min.css" media="screen" type="text/css">

  <!-- material design -->
	<!-- <link rel="stylesheet" href="/libs/bs-material/css/ripples.min.css" media="screen" type="text/css"> -->
  <link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap-material/0.3.0/css/ripples.min.css" media="screen" type="text/css">
  <!-- <link rel="stylesheet" href="/libs/bs-material/css/material.min.css" media="screen" type="text/css"> -->
	<link rel="stylesheet" href="//apps.bdimg.com/libs/bootstrap-material/0.3.0/css/material.min.css" media="screen" type="text/css">

  <link rel="stylesheet" href="/css/highlight.light.css" media="screen" type="text/css">

  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">

  <!-- 百度统计 -->
  

  <!-- 谷歌统计 -->
  

  <script src="//apps.bdimg.com/libs/jquery/2.0.3/jquery.min.js"></script>
	<script>window.jQuery || document.write('<script src="/libs/jquery-2.0.3.min.js" type="text/javascript"><\/script>')</script>

</head>

 	<body>
	  <nav class="navbar navbar-default">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">菜单</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">任何忧伤,都抵不过世界的美丽</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
            <ul class="nav navbar-nav navbar-right">
                
                <li>
                    <a href="/" title="">
                    <i class="fa fa-home"></i>首页
                    </a>
                </li>
                
                <li>
                    <a href="/archives" title="">
                    <i class="fa fa-list"></i>存档
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>

	  <div class="container" >
	    <div class="row">
	<div class="col-md-8">
	  <!-- index -->
	  
			<h1>Spark-Stramming入门</h1>
			
			<div>
				<i class="fa fa-clock-o"></i>
				<span class="post-time">2015-11-29 20:52:01</span>
			</div>
			
	  

		<div class="content">
			<!-- index -->
		  
					<h2 id="NetworkWordCount">NetworkWordCount</h2><p>1.本机运行时,修改spark-env.sh(这一步不是必须的)</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1<span class="number">.7</span><span class="number">.0</span>_79.jdk/Contents/Home</span><br><span class="line"><span class="keyword">export</span> SCALA_HOME=/Users/zhengqh/Soft/scala-<span class="number">2.10</span><span class="number">.5</span></span><br><span class="line"><span class="keyword">export</span> HADOOP_HOME=/Users/zhengqh/Soft/cdh542/hadoop-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.4</span><span class="number">.2</span></span><br><span class="line"><span class="keyword">export</span> HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_IP=localhost</span><br><span class="line"><span class="preprocessor">#export MASTER=spark:<span class="comment">//localhost:7077</span></span></span><br><span class="line"><span class="keyword">export</span> SPARK_LOCAL_IP=localhost</span><br></pre></td></tr></table></figure>
<p>2.开启netcat数据服务器</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~  nc -lk <span class="number">9999</span></span><br><span class="line">hello world hello spark hello spark hello world</span><br></pre></td></tr></table></figure>
<p>3.运行spark-streamming示例,当在nc终端输入text时,spark-streamming会实时统计过去一秒的wordcount</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>  bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost <span class="number">9999</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">36</span> INFO dstream.SocketReceiver: Connected to localhost:<span class="number">9999</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">37</span> INFO scheduler.ReceiverTracker: Registered receiver <span class="keyword">for</span> stream <span class="number">0</span> from <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">50018</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">37</span> INFO scheduler.JobScheduler: Added jobs <span class="keyword">for</span> time <span class="number">1436143117000</span> ms</span><br><span class="line">...</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">37</span> INFO scheduler.DAGScheduler: Job <span class="number">2</span> finished: print at NetworkWordCount.scala:<span class="number">55</span>, took <span class="number">0.066998</span> s</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">1436143116000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">37</span> INFO scheduler.JobScheduler: Finished job streaming job <span class="number">1436143116000</span> ms<span class="number">.0</span> from job <span class="built_in">set</span> of time <span class="number">1436143116000</span> ms</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">37</span> INFO scheduler.JobScheduler: Total delay: <span class="number">1.204</span> s <span class="keyword">for</span> time <span class="number">1436143116000</span> ms (execution: <span class="number">1.118</span> s)</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">40</span> INFO scheduler.DAGScheduler: Job <span class="number">10</span> finished: print at NetworkWordCount.scala:<span class="number">55</span>, took <span class="number">0.048230</span> s</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">1436143120000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(spark,<span class="number">2</span>)</span><br><span class="line">(hello,<span class="number">4</span>)</span><br><span class="line">(world,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">40</span> INFO scheduler.JobScheduler: Finished job streaming job <span class="number">1436143120000</span> ms<span class="number">.0</span> from job <span class="built_in">set</span> of time <span class="number">1436143120000</span> ms</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">40</span> INFO rdd.ShuffledRDD: Removing RDD <span class="number">16</span> from persistence <span class="built_in">list</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">40</span> INFO scheduler.JobScheduler: Total delay: <span class="number">0.356</span> s <span class="keyword">for</span> time <span class="number">1436143120000</span> ms (execution: <span class="number">0.335</span> s)</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">40</span> INFO rdd.MapPartitionsRDD: Removing RDD <span class="number">15</span> from persistence <span class="built_in">list</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">40</span> INFO storage.BlockManager: Removing RDD <span class="number">16</span></span><br></pre></td></tr></table></figure>
<p>4.在<localhost:4040 streaming="">可以观察streaming的统计信息, 其中在InputRate向上凸出的是产生数据的速度  </localhost:4040></p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss1.png" alt=""></p>
<p>5.IDEA本地运行</p>
<p>给SparkConf添加.setMaster(), 在<code>Program Arguments</code>中添加<code>localhost 9999 local[2]</code>, 然后运行</p>
<p>否则会报错:<code>org.apache.spark.SparkException: A master URL must be set in your configuration</code>  </p>
<p>example-code: spark-streamming实时读取socker流,统计过去5秒的wordcount</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create the context with a 1 second batch size</span></span><br><span class="line">val sparkConf = new <span class="function"><span class="title">SparkConf</span><span class="params">()</span></span>.<span class="function"><span class="title">setAppName</span><span class="params">(<span class="string">"NetworkWordCount"</span>)</span></span>.<span class="function"><span class="title">setMaster</span><span class="params">(args(<span class="number">2</span>)</span></span>)</span><br><span class="line">val ssc = new <span class="function"><span class="title">StreamingContext</span><span class="params">(sparkConf, Seconds(<span class="number">5</span>)</span></span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a socket stream on target ip:port and count the</span></span><br><span class="line"><span class="comment">// words in input stream of \n delimited text (eg. generated by 'nc')</span></span><br><span class="line"><span class="comment">// Note that no duplication in storage level only for running locally.</span></span><br><span class="line"><span class="comment">// Replication necessary in distributed scenario for fault tolerance.</span></span><br><span class="line">val lines = ssc.<span class="function"><span class="title">socketTextStream</span><span class="params">(args(<span class="number">0</span>)</span></span>, <span class="function"><span class="title">args</span><span class="params">(<span class="number">1</span>)</span></span><span class="class">.toInt</span>, StorageLevel.MEMORY_AND_DISK_SER)</span><br><span class="line">val words = lines.<span class="function"><span class="title">flatMap</span><span class="params">(_.split(<span class="string">" "</span>)</span></span>)</span><br><span class="line">val wordCounts = words.<span class="function"><span class="title">map</span><span class="params">(x =&gt; (x, <span class="number">1</span>)</span></span>).<span class="function"><span class="title">reduceByKey</span><span class="params">(_ + _)</span></span></span><br><span class="line">wordCounts.<span class="function"><span class="title">print</span><span class="params">()</span></span></span><br><span class="line">ssc.<span class="function"><span class="title">start</span><span class="params">()</span></span></span><br><span class="line">ssc.<span class="function"><span class="title">awaitTermination</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<p>6.打包本地运行</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>  bin/spark-submit --class com<span class="class">.tongdun</span><span class="class">.bigdata</span><span class="class">.spark</span><span class="class">.intro</span><span class="class">.NetworkWordCount</span> --jars /Users/zhengqh/IdeaProjects/bigdata/out/artifacts/spark_intro/spark-intro<span class="class">.jar</span> /Users/zhengqh/IdeaProjects/bigdata/out/artifacts/spark_intro/spark-intro<span class="class">.jar</span> localhost <span class="number">9999</span> <span class="string">"local[2]"</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>PS: –jars xx.jar可以省略</p>
</blockquote>
<p>7.打包集群运行. 程序的最后一个参数可以指定为local[2]或者spark://dp0652:7077</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>  bin/spark-submit --class com<span class="class">.tongdun</span><span class="class">.bigdata</span><span class="class">.spark</span><span class="class">.intro</span><span class="class">.NetworkWordCount</span> /home/qihuang.zheng/spark-intro<span class="class">.jar</span> localhost <span class="number">9999</span> <span class="string">"local[2]"</span></span><br><span class="line"></span><br><span class="line">➜  spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>  bin/spark-submit --class com<span class="class">.tongdun</span><span class="class">.bigdata</span><span class="class">.spark</span><span class="class">.intro</span><span class="class">.NetworkWordCount</span> /home/qihuang.zheng/spark-intro<span class="class">.jar</span> localhost <span class="number">9999</span> spark:<span class="comment">//dp0652:7077</span></span><br></pre></td></tr></table></figure>
<p>8.一般在集群中运行,我们使用–master而不是在代码中设置setMaster(),所以把代码中的setMaster去掉重新打包</p>
<p>先在本地实验, 👌 👉 不指定–master可以成功运行</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --class com<span class="class">.tongdun</span><span class="class">.bigdata</span><span class="class">.spark</span><span class="class">.intro</span><span class="class">.NetworkWordCount</span> /Users/zhengqh/spark-intro<span class="class">.jar</span> localhost <span class="number">9999</span></span><br></pre></td></tr></table></figure>
<p>然后集群实验, 👌 👉 指定–master或不指定都可以成功运行</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --class com<span class="class">.tongdun</span><span class="class">.bigdata</span><span class="class">.spark</span><span class="class">.intro</span><span class="class">.NetworkWordCount</span> /home/qihuang.zheng/spark-intro<span class="class">.jar</span> <span class="number">192.168</span>.<span class="number">6.52</span> <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">bin/spark-submit --master spark:<span class="comment">//dp0652:7077 --class com.tongdun.bigdata.spark.intro.NetworkWordCount /home/qihuang.zheng/spark-intro.jar 192.168.6.52 9999</span></span><br></pre></td></tr></table></figure>
<p><strong>关于本地和集群的运行方式</strong></p>
<p>下面是在本地和集群中运行NetworkWordCount几种方式的结果, ✅表示能正常统计,🙅表示没显示结果.</p>
<p>localhost(没有更改spark-env.sh)</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">✅ bin/run-example org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">🙅 bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master <span class="string">"local[2]"</span> --jars lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">🙅 bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master local\[<span class="number">2</span>\] --jars lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">✅ bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master <span class="string">"local[*]"</span> --jars lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> localhost <span class="number">9999</span></span><br></pre></td></tr></table></figure>
<p>cluster:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">🙅 bin/run-example org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">✅ bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master <span class="string">"local[2]"</span>  lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">✅ bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master local\[<span class="number">2</span>\] lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">✅ bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master <span class="string">"local[*]"</span> lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">🙅 bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master spark:<span class="comment">//dp0652:7077 lib/spark-examples-1.4.0-hadoop2.6.0.jar localhost 9999</span></span><br></pre></td></tr></table></figure>
<h2 id="Kafka-SparkStreamming">Kafka-SparkStreamming</h2><p>如果是本地IDEA运行, 分别启动zookeeper,kafka,然后运行KafkaWordCountProducer,KafkaWordCount.<br>下面是在集群中的运行步骤:  </p>
<p>1.首先在kafka中创建一个topic:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0656 kafka]$ bin/kafka-topics.sh --zookeeper localhost:<span class="number">2181</span> --create --topic kafka-spark-test --replication-factor <span class="number">2</span> --partitions <span class="number">2</span></span><br><span class="line">Created topic <span class="string">"kafka-spark-test"</span>.</span><br><span class="line">[qihuang.zheng@dp0656 kafka]$ bin/kafka-topics.sh --zookeeper localhost:<span class="number">2181</span> --<span class="built_in">list</span></span><br><span class="line">kafka-spark-test</span><br><span class="line">[qihuang.zheng@dp0656 kafka]$ bin/kafka-topics.sh --zookeeper localhost:<span class="number">2181</span> --describe kafka-spark-test</span><br><span class="line">Topic:kafka-spark-test	PartitionCount:<span class="number">2</span>	ReplicationFactor:<span class="number">2</span>	Configs:</span><br><span class="line">	Topic: kafka-spark-test	Partition: <span class="number">0</span>	Leader: <span class="number">0</span>	Replicas: <span class="number">0</span>,<span class="number">1</span>	Isr: <span class="number">0</span>,<span class="number">1</span></span><br><span class="line">	Topic: kafka-spark-test	Partition: <span class="number">1</span>	Leader: <span class="number">1</span>	Replicas: <span class="number">1</span>,<span class="number">2</span>	Isr: <span class="number">1</span>,<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>2.运行kafka生产者模拟程序: 下面模拟了往kafka-spark-test队列中每秒发送2000个消息,每条消息的长度是70个数字.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ bin/spark-submit --<span class="keyword">class</span> org.apache.spark.examples.streaming.KafkaWordCountProducer /home/qihuang.zheng/spark-intro.jar <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">9092</span> kafka-spark-test <span class="number">2000</span> <span class="number">70</span></span><br><span class="line">Exception in thread <span class="string">"main"</span> java.lang.NoClassDefFoundError: org/apache/kafka/clients/producer/KafkaProducer</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>这是因为我们打包的方式没有把依赖包放进来. 用依赖包的方式就可以正常地在控制台输出kafka的模拟消息:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ bin/spark-submit --<span class="keyword">class</span> org.apache.spark.examples.streaming.KafkaWordCountProducer /home/qihuang.zheng/spark-intro-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">9092</span> kafka-spark-test <span class="number">2000</span> <span class="number">70</span></span><br></pre></td></tr></table></figure>
<p>3.运行KafkaWordCount实时流分析程序</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ bin/spark-submit --<span class="keyword">class</span> org.apache.spark.examples.streaming.KafkaWordCount /home/qihuang.zheng/spark-intro-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">2181</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">2181</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">2181</span> my-consumer-group kafka-spark-test <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>报错: 没有找到Kafka-SparkStreamming的相关jar包,因为我们在pom.xml中把<code>spark-streaming-kafka_2.10-1.4.0</code>也设置为了provided  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java<span class="class">.lang</span><span class="class">.NoClassDefFoundError</span>: org/apache/spark/streaming/kafka/KafkaUtils$</span><br><span class="line">	at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.KafkaWordCount</span>$.<span class="function"><span class="title">main</span><span class="params">(KafkaWordCount.scala:<span class="number">59</span>)</span></span></span><br><span class="line">	at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.KafkaWordCount</span><span class="class">.main</span>(KafkaWordCount.scala)</span><br></pre></td></tr></table></figure>
<p>当然也可以把<code>spark-streaming-kafka_2.10-1.4.0</code>的scope去掉,然后重新编译. 不过还有一种办法:<br>下载<code>spark-streaming-kafka_2.10-1.4.0.jar</code>,在spark-submit中添加–jars选项把<code>spark-streaming-kafka_2.10-1.4.0.jar</code>加进来  </p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>]$ bin/spark-submit --<span class="keyword">master</span> <span class="title">spark</span>://dp0652:<span class="number">7077</span> --class org.apache.spark.examples.streaming.KafkaWordCount --jars /home/qihuang.zheng/spark-streaming-kafka_2.<span class="number">10</span>-<span class="number">1.4</span>.<span class="number">0</span>.jar /home/qihuang.zheng/spark-intro-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar <span class="number">192.168</span>.<span class="number">6.55</span>:<span class="number">2181</span>,<span class="number">192.168</span>.<span class="number">6.56</span>:<span class="number">2181</span>,<span class="number">192.168</span>.<span class="number">6.57</span>:<span class="number">2181</span> my-consumer-<span class="keyword">group</span> <span class="title">kafka-spark-test</span> <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>4.KafkaWordCount的输出日志:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">09</span>:<span class="number">40</span>:<span class="number">11</span> WARN BlockManager: Block input-<span class="number">0</span>-<span class="number">1436233210800</span> replicated to only <span class="number">0</span> peer(s) instead of <span class="number">1</span> peers</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">09</span>:<span class="number">40</span>:<span class="number">11</span> WARN BlockManager: Block input-<span class="number">0</span>-<span class="number">1436233211000</span> replicated to only <span class="number">0</span> peer(s) instead of <span class="number">1</span> peers</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">09</span>:<span class="number">40</span>:<span class="number">12</span> WARN BlockManager: Block input-<span class="number">0</span>-<span class="number">1436233212000</span> replicated to only <span class="number">0</span> peer(s) instead of <span class="number">1</span> peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">1436233212000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(<span class="number">4</span>,<span class="number">139354</span>)</span><br><span class="line">(<span class="number">8</span>,<span class="number">140225</span>)</span><br><span class="line">(<span class="number">6</span>,<span class="number">140124</span>)</span><br><span class="line">(<span class="number">0</span>,<span class="number">140515</span>)</span><br><span class="line">(<span class="number">2</span>,<span class="number">139827</span>)</span><br><span class="line">(<span class="number">7</span>,<span class="number">140151</span>)</span><br><span class="line">(<span class="number">5</span>,<span class="number">139425</span>)</span><br><span class="line">(<span class="number">9</span>,<span class="number">140219</span>)</span><br><span class="line">(<span class="number">3</span>,<span class="number">140248</span>)</span><br><span class="line">(<span class="number">1</span>,<span class="number">139912</span>)</span><br></pre></td></tr></table></figure>
<p>5.Streaming-WebUI</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss2.png" alt=""></p>
<p>因为每秒2000条消息,每隔2秒统计一次,所以每次job的输入大小InputSize大概为2000*2=4000</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss3.png" alt=""></p>
<h2 id="关于打包">关于打包</h2><h3 id="IDEA打包">IDEA打包</h3><p>Project Structures | Artifacts | + | Jar | Empty | 填写jar包名称: spark-intro |<br>在Available Elements中选择项目名称下的’spark-intro’ compile output | 双击 | 就会到左侧的jar包下<br>Build | Build Artifacts | 选择刚刚填写的Artifact: spark-intro | Rebuild</p>
<h3 id="Maven打包">Maven打包</h3><p>因为是maven工程, 所以可以在工程下直接mvn package. 但是注意如果不配置maven的插件.则不会把依赖包打进去.<br>如果要把依赖包添加进去, 则要添加<code>maven-assembly-plugin</code>插件:  </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="title">descriptorRef</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">descriptorRefs</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">archive</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">manifest</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">mainClass</span>&gt;</span><span class="tag">&lt;/<span class="title">mainClass</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">manifest</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">archive</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="title">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="title">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="title">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>注意: 对于spark的相关jar依赖设置scope=provided</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-core_$&#123;scala.bin.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-streaming_$&#123;scala.bin.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>然后在工程的根目录执行<code>mvn package</code>, 编译成功后会在target下生成2个文件.一个是有依赖的,一个是没有任何依赖的.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-intro git:(master) ✗ ll target</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  staff    <span class="number">16</span>M  <span class="number">7</span>  <span class="number">7</span> <span class="number">09</span>:<span class="number">22</span> spark-intro-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  staff   <span class="number">466</span>K  <span class="number">7</span>  <span class="number">7</span> <span class="number">09</span>:<span class="number">22</span> spark-intro-<span class="number">1.0</span>-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>
<p>我们把有依赖的jar包拷贝到spark集群中运行</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  <span class="tag">target</span> <span class="tag">git</span><span class="pseudo">:(master)</span> ✗ <span class="tag">scp</span> <span class="tag">spark-intro-1</span><span class="class">.0-SNAPSHOT-jar-with-dependencies</span><span class="class">.jar</span> <span class="tag">qihuang</span><span class="class">.zheng</span><span class="at_rule">@<span class="keyword">192.168.6.52:~/</span></span></span><br></pre></td></tr></table></figure>
<h2 id="Kafka-SparkStreamming-Redis">Kafka-SparkStreamming-Redis</h2><p>在192.168.6.52上启动redis, 注意要以后台方式启动, 并且用admin用户, 否则shutdown时会报错:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/<span class="operator"><span class="keyword">install</span>/redis-<span class="number">3.0</span><span class="number">.2</span></span><br><span class="line">sudo -u <span class="keyword">admin</span> src/redis-<span class="keyword">server</span> &amp;</span></span><br></pre></td></tr></table></figure>
<p>启动kafka生产者模拟程序</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --<span class="keyword">class</span> com.tongdun.bigdata.spark.intro.KafkaEventProducer /home/qihuang.zheng/spark-intro-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">9092</span></span><br></pre></td></tr></table></figure>
<p>启动用户点击次数实时流统计,最终写到Redis中</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark://dp<span class="number">0652:7077</span> --class com.tongdun.bigdata.spark.intro.UserClickCountAnalytics --jars /home/qihuang.zheng/spark-streaming-kafka_<span class="number">2.10-1.4</span>.0.jar /home/qihuang.zheng/spark-intro-1.0-SNAPSHOT-jar-with-dependencies.jar spark://dp<span class="number">0652:7077</span> <span class="number">192.168.6.55</span>:<span class="number">9092,192.168</span>.<span class="number">6.56:9092</span>,<span class="number">192.168.6.57</span>:9092</span><br><span class="line"></span><br><span class="line">bin/spark-submit --master spark://dp<span class="number">0652:7077</span> --class com.tongdun.bigdata.spark.intro.UserClickCountAnalytics2 --jars /home/qihuang.zheng/spark-streaming-kafka_<span class="number">2.10-1.4</span>.0.jar /home/qihuang.zheng/spark-intro-1.0-SNAPSHOT-jar-with-dependencies.jar spark://dp<span class="number">0652:7077</span> user_events <span class="number">192.168.6.55</span>:<span class="number">9092,192.168</span>.<span class="number">6.56:9092</span>,<span class="number">192.168.6.57</span>:<span class="number">9092 192.168</span>.6.52</span><br></pre></td></tr></table></figure>
<p>使用客户端在本机中验证数据:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">src/redis-cli -h <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span></span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">6379</span>&gt; select <span class="number">1</span></span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">6379</span>[<span class="number">1</span>]&gt; HGETALL app::users::click</span><br><span class="line"> <span class="number">1</span>) <span class="string">"d7f141563005d1b5d0d3dd30138f3f62"</span></span><br><span class="line"> <span class="number">2</span>) <span class="string">"1891"</span></span><br><span class="line"> <span class="number">3</span>) <span class="string">"97edfc08311c70143401745a03a50706"</span></span><br><span class="line"> <span class="number">4</span>) <span class="string">"1814"</span></span><br><span class="line"> <span class="number">5</span>) <span class="string">"4A4D769EB9679C054DE81B973ED5D768"</span></span><br><span class="line"> <span class="number">6</span>) <span class="string">"1845"</span></span><br><span class="line"> <span class="number">7</span>) <span class="string">"a95f22eabc4fd4b580c011a3161a9d9d"</span></span><br><span class="line"> <span class="number">8</span>) <span class="string">"1835"</span></span><br><span class="line"> <span class="number">9</span>) <span class="string">"8dfeb5aaafc027d89349ac9a20b3930f"</span></span><br><span class="line"><span class="number">10</span>) <span class="string">"1821"</span></span><br><span class="line"><span class="number">11</span>) <span class="string">"6b67c8c700427dee7552f81f3228c927"</span></span><br><span class="line"><span class="number">12</span>) <span class="string">"1842"</span></span><br><span class="line"><span class="number">13</span>) <span class="string">"011BBF43B89BFBF266C865DF0397AA71"</span></span><br><span class="line"><span class="number">14</span>) <span class="string">"1819"</span></span><br><span class="line"><span class="number">15</span>) <span class="string">"f2a8474bf7bd94f0aabbd4cdd2c06dcf"</span></span><br><span class="line"><span class="number">16</span>) <span class="string">"1909"</span></span><br><span class="line"><span class="number">17</span>) <span class="string">"c8ee90aade1671a21336c721512b817a"</span></span><br><span class="line"><span class="number">18</span>) <span class="string">"1843"</span></span><br><span class="line"><span class="number">19</span>) <span class="string">"068b746ed4620d25e26055a9f804385f"</span></span><br><span class="line"><span class="number">20</span>) <span class="string">"1955"</span></span><br></pre></td></tr></table></figure>
<p>假设我们同时启动了两个UserClickCountAnalytics进程, 则第二个会进入等待状态. 只有当第一个进程杀掉后, 第二个进程才会开始运行.  </p>
<p>下图是处于等待的进程,除了InputRate有图像,其他三个都没有</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss4.png" alt=""></p>
<p>可以看到status为queued</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss5.png" alt=""></p>
<p>在等待进程的那个终端也可以看到如下输出</p>
<figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">[</span>Stage 0:&gt;                                                         <span class="function"> (</span>0 + 0<span class="function">)</span> / 2]15/07/07 12:05:25 WARN TaskSchedulerImpl: Initial job has<span class="instruction"> not </span>accepted any resources;<span class="instruction"> check </span>your cluster UI to ensure that workers are registered<span class="instruction"> and </span>have sufficient resources</span><br><span class="line">15/07/07 12:05:40 WARN TaskSchedulerImpl: Initial job has<span class="instruction"> not </span>accepted any resources;<span class="instruction"> check </span>your cluster UI to ensure that workers are registered<span class="instruction"> and </span>have sufficient resources</span><br><span class="line">15/07/07 12:05:55 WARN TaskSchedulerImpl: Initial job has<span class="instruction"> not </span>accepted any resources;<span class="instruction"> check </span>your cluster UI to ensure that workers are registered<span class="instruction"> and </span>have sufficient resources</span><br></pre></td></tr></table></figure>
<p>第一个正在运行的进程</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss6.png" alt=""></p>
<p>当我们把正在运行的杀掉, 这时候等待运行的就会接着执行</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss7.png" alt=""></p>

			  
		</div>

		<!-- pagination -->
	  

		
  


	</div>
	<div class="col-md-4">
		
			
				<div id="sidebar">
	 			
	 					
<div class="widget">
  <h4>最新文章</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2015/11/30/2015-11-24-StreamCQL-task/"  > <i class="mdi-editor-insert-drive-file"></i>StreamCQL源码阅读(1)_提交任务</a>
      </li>
    
      <li>
        <a href="/2015/11/29/2015-07-05-Spark-SQL/"  > <i class="mdi-editor-insert-drive-file"></i>Spark SQL入门</a>
      </li>
    
      <li>
        <a href="/2015/11/29/2015-06-20-ElasticSearch/"  > <i class="mdi-editor-insert-drive-file"></i>ElasticSearch入门</a>
      </li>
    
      <li>
        <a href="/2015/11/29/2015-07-07-Spark-HA-YARN/"  > <i class="mdi-editor-insert-drive-file"></i>Spark HA和YARN模式</a>
      </li>
    
      <li>
        <a href="/2015/11/29/2015-07-06-Spark-Streamming/"  > <i class="mdi-editor-insert-drive-file"></i>Spark-Stramming入门</a>
      </li>
    
  </ul>
</div>


	 				
	 					
<div class="widget">
	<h4>链接</h4>
	<ul class="blogroll list-unstyled">
	
		<li> <a href="http://www.github.com/zqhxuyuan" title="同性交友社区" target="_blank"> <i class="mdi-action-launch"></i> GitHub</a></li>
	
		<li> <a href="http://www.weibo.com/xuyuantree" title="" target="_blank"> <i class="mdi-action-launch"></i> Weibo</a></li>
	
	</ul>
</div>


	 				
	 			</div>
			
		
	</div>

</div>


			<footer>
				

<p>
  由 <a href="https://hexo.io">hexo</a> 强力驱动 | 搭载 <a href="https://github.com/wayou/hexo-theme-material">material</a> 主题
</p>
<p>
  &copy; 2015 <a href="http://github.com/zqhxuyuan"> zqhxuyuan </a>
</p>
<a id="gotop" href="#" title="back to top"><i class="mdi-hardware-keyboard-arrow-up"></i></a>

			</footer>
	  </div>

		<!-- <script src="/libs/bs/js/bootstrap.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap/3.3.4/js/bootstrap.min.js"></script>
		<script>(typeof $().modal == 'function')|| document.write('<script src="/libs/bs/js/bootstrap.min.js" type="text/javascript"><\/script>')</script>

		<!-- material design -->
		<!-- <script src="/libs/bs-material/js/ripples.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap-material/0.3.0/js/ripples.min.js"></script>
		<!-- <script src="/libs/bs-material/js/material.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/bootstrap-material/0.3.0/js/material.min.js"></script>
		<!-- toc -->
		<!-- <script src="/libs/tocify/jquery-ui.min.js"></script> -->
		<script src="//apps.bdimg.com/libs/jqueryui/1.10.4/jquery-ui.min.js"></script>
		<script src="/libs/tocify/jquery.tocify.custom.js"></script>

		<script src="/js/main.js"></script>

	</body>
</html>
