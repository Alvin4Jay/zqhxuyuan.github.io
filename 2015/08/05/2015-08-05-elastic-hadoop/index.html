<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>ES导入数据到HDFS | zqhxuyuan</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="使用Spark从ElasticSearch导入数据到HDFS的一些坑.  elasticsearch-hadoop 支持Hadoop和Spark，利用elasticsearch-hadoop项目，spark可以较为轻松的从elasticsearch中读取数据。">
<meta property="og:type" content="article">
<meta property="og:title" content="ES导入数据到HDFS">
<meta property="og:url" content="http://github.com/zqhxuyuan/2015/08/05/2015-08-05-elastic-hadoop/index.html">
<meta property="og:site_name" content="zqhxuyuan">
<meta property="og:description" content="使用Spark从ElasticSearch导入数据到HDFS的一些坑.  elasticsearch-hadoop 支持Hadoop和Spark，利用elasticsearch-hadoop项目，spark可以较为轻松的从elasticsearch中读取数据。">
<meta property="og:image" content="http://img.blog.csdn.net/20150924082457899">
<meta property="og:updated_time" content="2015-12-19T13:25:36.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ES导入数据到HDFS">
<meta name="twitter:description" content="使用Spark从ElasticSearch导入数据到HDFS的一些坑.  elasticsearch-hadoop 支持Hadoop和Spark，利用elasticsearch-hadoop项目，spark可以较为轻松的从elasticsearch中读取数据。">
  
    <link rel="alternative" href="/atom.xml" title="zqhxuyuan" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://avatars1.githubusercontent.com/u/1088525?v=3&amp;s=180" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">任何忧伤,都抵不过世界的美丽</a></h1>
		</hgroup>

		
				


		
			<div id="switch-btn" class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
						<li>关于我</li>
						
					</ul>
				</div>
			</div>
		

		<div id="switch-area" class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives/">归档</a></li>
				        
							<li><a href="/tags/">标签</a></li>
				        
							<li><a href="/about/">关于</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<ul class="social">
							
								<li id="新浪微博"><a class="新浪微博" target="_blank" href="http://weibo.com/xuyuantree" title="新浪微博"></a></li>
					        
								<li id="GitHub"><a class="GitHub" target="_blank" href="http://github.com/zqhxuyuan" title="GitHub"></a></li>
					        
						</ul>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/apex/" style="font-size: 10px;">apex</a> <a href="/tags/cassandra/" style="font-size: 20px;">cassandra</a> <a href="/tags/drill/" style="font-size: 18.33px;">drill</a> <a href="/tags/druid/" style="font-size: 13.33px;">druid</a> <a href="/tags/elasticsearch/" style="font-size: 10px;">elasticsearch</a> <a href="/tags/geode/" style="font-size: 10px;">geode</a> <a href="/tags/hbase/" style="font-size: 15px;">hbase</a> <a href="/tags/ignite/" style="font-size: 10px;">ignite</a> <a href="/tags/spark/" style="font-size: 13.33px;">spark</a> <a href="/tags/storm/" style="font-size: 16.67px;">storm</a> <a href="/tags/work/" style="font-size: 11.67px;">work</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">BIG(DATA)</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">任何忧伤,都抵不过世界的美丽</a></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<a href="/" class="profilepic">
				<img lazy-src="https://avatars1.githubusercontent.com/u/1088525?v=3&amp;s=180" class="js-avatar">
			</a>
			<hgroup>
			  <h1 class="header-author"><a href="/" title="回到主页">任何忧伤,都抵不过世界的美丽</a></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives/">归档</a></li>
		        
					<li><a href="/tags/">标签</a></li>
		        
					<li><a href="/about/">关于</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
						<ul class="social">
							
								<li id="新浪微博"><a class="新浪微博" target="_blank" href="http://weibo.com/xuyuantree" title="新浪微博"></a></li>
					        
								<li id="GitHub"><a class="GitHub" target="_blank" href="http://github.com/zqhxuyuan" title="GitHub"></a></li>
					        
						</ul>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-2015-08-05-elastic-hadoop" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/08/05/2015-08-05-elastic-hadoop/" class="article-date">
  	<time datetime="2015-08-04T16:00:00.000Z" itemprop="datePublished">2015-08-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      ES导入数据到HDFS
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/work/">work</a>
	</div>


        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/work/">work</a></li></ul>
	</div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <p>使用Spark从ElasticSearch导入数据到HDFS的一些坑.  elasticsearch-hadoop 支持Hadoop和Spark，利用elasticsearch-hadoop项目，spark可以较为轻松的从elasticsearch中读取数据。</p>
<a id="more"></a>
<h2 id="项目要求">项目要求</h2><p>存储于elasticsearch中的数据是以json格式存储，由于历史的原因，其格式不统一。<br>现在要求将这些json串组织成统一的格式，树深最大不超过3.<br>导出的数据以parquet格式存储到hdfs中，同时建立外部的hive表。 </p>
<figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE <span class="keyword">EXTERNAL</span> TABLE activity(</span><br><span class="line">  sequence_id` <span class="keyword">string</span>,</span><br><span class="line">  occur_time bigint,</span><br><span class="line">  activity_map map&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">  ...</span><br><span class="line">partitioned by(indice <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">stored</span> as parquet</span><br><span class="line">location <span class="string">'/user/lab/activity'</span>;</span><br></pre></td></tr></table></figure>
<h2 id="测试正常读取ES">测试正常读取ES</h2><p>下载elasticsearch-hadoop.jar,放到$SPARK_HOME/lib目录,或者在运行的时候,使用–jars添加第三方依赖包.  </p>
<blockquote>
<p>注意: 由于下载的包并不是assembly,所以其他依赖的包也要手动加入.  </p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --jars lib/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,lib/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,lib/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,lib/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,\</span><br><span class="line">  lib/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.0</span>.Beta4.jar --conf spark.es.nodes=<span class="number">192.168</span><span class="number">.47</span><span class="number">.11</span>:<span class="number">9200</span> --master local[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">/usr/install/spark-<span class="number">1.4</span><span class="number">.1</span>-bin-hadoop2<span class="number">.4</span>/bin/spark-shell \</span><br><span class="line">  --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">  --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">30</span> --driver-memory <span class="number">8</span>g \</span><br><span class="line">  --conf spark.es.nodes=<span class="number">192.168</span><span class="number">.47</span><span class="number">.155</span>:<span class="number">9200</span> \</span><br><span class="line">  --conf spark.es.field.read.empty.as.null=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>参数说明: 1) spark.es.nodes 指定elasticsearch的地址<br>2) –jars必须在同一行内, 不能用\分开  </p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  repository  cp org/json4s/json4s-ast_2<span class="number">.10</span>/<span class="number">3.2</span><span class="number">.11</span>/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar ~/lib</span><br><span class="line">➜  repository  cp org/json4s/json4s-core_2<span class="number">.10</span>/<span class="number">3.2</span><span class="number">.11</span>/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar ~/lib</span><br><span class="line">➜  repository  cp org/json4s/json4s-jackson_2<span class="number">.10</span>/<span class="number">3.2</span><span class="number">.11</span>/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar ~/lib</span><br><span class="line">➜  repository  cp org/json4s/json4s-native_2<span class="number">.10</span>/<span class="number">3.2</span><span class="number">.11</span>/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar ~/lib</span><br><span class="line">➜  repository  cp org/elasticsearch/elasticsearch-hadoop/<span class="number">2.1</span><span class="number">.1</span>/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar ~/lib</span><br></pre></td></tr></table></figure>
<p>在spark-shell中, 只需要import需要的jar包:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import org<span class="class">.elasticsearch</span><span class="class">.spark</span>._</span><br><span class="line">import org<span class="class">.json4s</span><span class="class">.NoTypeHints</span></span><br><span class="line">import org<span class="class">.json4s</span><span class="class">.native</span><span class="class">.Serialization</span></span><br><span class="line">import org<span class="class">.json4s</span><span class="class">.native</span><span class="class">.Serialization</span>._</span><br><span class="line"></span><br><span class="line">import sqlContext<span class="class">.implicits</span>._</span><br></pre></td></tr></table></figure>
<p>测试连接ES,并读取ES数据:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.<span class="function"><span class="title">esRDD</span><span class="params">(s<span class="string">"$exportDB/activity"</span>)</span></span></span><br><span class="line">rdd1.first</span><br></pre></td></tr></table></figure>
<h3 id="对应用打assembly包">对应用打assembly包</h3><p>打包需要把所有的依赖包打包下来: <code>TODO: 只把需要的jar包打进去, 像spark等直接使用provided方式. 给pom.xml添加插件</code>  </p>
<figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">&lt;plugin&gt;</span></span><br><span class="line">    <span class="variable">&lt;artifactId&gt;</span>maven-assembly-plugin<span class="variable">&lt;/artifactId&gt;</span></span><br><span class="line">    <span class="variable">&lt;configuration&gt;</span></span><br><span class="line">        <span class="variable">&lt;descriptorRefs&gt;</span></span><br><span class="line">            <span class="variable">&lt;descriptorRef&gt;</span>jar-with-dependencies<span class="variable">&lt;/descriptorRef&gt;</span></span><br><span class="line">        <span class="variable">&lt;/descriptorRefs&gt;</span></span><br><span class="line">        <span class="variable">&lt;archive&gt;</span></span><br><span class="line">            <span class="variable">&lt;manifest&gt;</span></span><br><span class="line">                <span class="variable">&lt;mainClass&gt;</span><span class="variable">&lt;/mainClass&gt;</span></span><br><span class="line">            <span class="variable">&lt;/manifest&gt;</span></span><br><span class="line">        <span class="variable">&lt;/archive&gt;</span></span><br><span class="line">    <span class="variable">&lt;/configuration&gt;</span></span><br><span class="line">    <span class="variable">&lt;executions&gt;</span></span><br><span class="line">        <span class="variable">&lt;execution&gt;</span></span><br><span class="line">            <span class="variable">&lt;id&gt;</span>make-assembly<span class="variable">&lt;/id&gt;</span></span><br><span class="line">            <span class="variable">&lt;phase&gt;</span>package<span class="variable">&lt;/phase&gt;</span></span><br><span class="line">            <span class="variable">&lt;goals&gt;</span></span><br><span class="line">                <span class="variable">&lt;goal&gt;</span>single<span class="variable">&lt;/goal&gt;</span></span><br><span class="line">            <span class="variable">&lt;/goals&gt;</span></span><br><span class="line">        <span class="variable">&lt;/execution&gt;</span></span><br><span class="line">    <span class="variable">&lt;/executions&gt;</span></span><br><span class="line"><span class="variable">&lt;/plugin&gt;</span></span><br><span class="line"></span><br><span class="line">cd tongdun-app</span><br><span class="line">rm -rf target</span><br><span class="line">mvn package -D<span class="keyword">skip</span>Tests</span><br><span class="line">cd target</span><br><span class="line">scp tongdun-app-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar <span class="number">192.168</span>.<span class="number">47.211</span>:~/</span><br></pre></td></tr></table></figure>
<p>这样的坏处是打包时间有点长, 文件有点大,传输到跳板机上时间也很慢.</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark:<span class="comment">//192.168.47.213:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --executor-<span class="keyword">memory</span> 8g --<span class="keyword">total</span>-executor-cores 24 --driver-<span class="keyword">memory</span> 8g \</span><br><span class="line">  --<span class="keyword">class</span> cn.fraudmetrix.dataplatform.<span class="keyword">lab</span>.ES2HDFSExporter \</span><br><span class="line">  /home/qihuang.zheng/tongdun-<span class="keyword">app</span>-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>
<h3 id="应用不打依赖包">应用不打依赖包</h3><p>一种改进的方式是: 打成jar包, 但是没有把依赖包打进去, spark-submit的时候使用–jars.<br>这样就不需要上面的maven-assembly-plugin了. 添加–jars如果jar文件很多,可以用脚本的方式:<br><a href="http://stackoverflow.com/questions/24855368/spark-throws-classnotfoundexception-when-using-jars-option" target="_blank" rel="external">http://stackoverflow.com/questions/24855368/spark-throws-classnotfoundexception-when-using-jars-option</a></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">/usr/install/hadoop/bin/hadoop fs -rmr /user/tongdun/activity_hist/year=<span class="number">2015</span>/month=<span class="number">9</span>/day=<span class="number">17</span></span><br><span class="line"></span><br><span class="line">cd /usr/install/spark-<span class="number">1.4</span><span class="number">.1</span>-bin-hadoop2<span class="number">.4</span></span><br><span class="line"></span><br><span class="line">nohup bin/spark-submit --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">30</span> --driver-memory <span class="number">12</span>g \</span><br><span class="line">  --<span class="keyword">class</span> cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter \</span><br><span class="line">  --conf spark.es.nodes=<span class="number">192.168</span><span class="number">.47</span><span class="number">.155</span>:<span class="number">9200</span> --conf spark.es.field.read.empty.as.null=<span class="literal">true</span> \</span><br><span class="line">  --conf spark.speculation=<span class="literal">false</span> \</span><br><span class="line">  /home/qihuang.zheng/tongdun-app-<span class="number">1.0</span>-SNAPSHOT.jar <span class="number">20150917</span> &amp;</span><br><span class="line"></span><br><span class="line">配置信息写死在代码里.</span><br><span class="line">nohup bin/spark-submit --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">30</span> --driver-memory <span class="number">12</span>g \</span><br><span class="line">  --<span class="keyword">class</span> cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter \</span><br><span class="line">  /home/qihuang.zheng/tongdun-app-<span class="number">1.0</span>-SNAPSHOT.jar <span class="number">20150917</span> &amp;</span><br></pre></td></tr></table></figure>
<h2 id="ES2HDFS_IN_ACTION">ES2HDFS IN ACTION</h2><h3 id="连接不到ES">连接不到ES</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">org<span class="class">.elasticsearch</span><span class="class">.hadoop</span><span class="class">.rest</span><span class="class">.EsHadoopTransportException</span>: java<span class="class">.net</span><span class="class">.NoRouteToHostException</span>: 没有到主机的路由</span><br><span class="line">发现这个错误不是连接ES的问题, 是因为当时Spark的master正在迁移导致的. 重启下spark-shell即可.</span><br></pre></td></tr></table></figure>
<h3 id="es的空值">es的空值</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">org<span class="class">.elasticsearch</span><span class="class">.hadoop</span><span class="class">.EsHadoopIllegalArgumentException</span>:</span><br><span class="line">Index [forseti-<span class="number">201509</span>*/activity] missing and settings [es<span class="class">.field</span><span class="class">.read</span><span class="class">.empty</span><span class="class">.as</span><span class="class">.null</span>] is set to false</span><br><span class="line">添加--conf spark<span class="class">.es</span><span class="class">.field</span><span class="class">.read</span><span class="class">.empty</span><span class="class">.as</span><span class="class">.null</span>=true</span><br></pre></td></tr></table></figure>
<h3 id="任务运行较慢,开启了推测执行">任务运行较慢,开启了推测执行</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">56</span> INFO TaskSetManager: Finished task <span class="number">10.0</span> in stage <span class="number">0.0</span> (TID <span class="number">10</span>) in <span class="number">6388</span> ms on <span class="number">192.168</span><span class="number">.47</span><span class="number">.212</span> (<span class="number">14</span>/<span class="number">17</span>)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">58</span> INFO TaskSetManager: Marking task <span class="number">5</span> in stage <span class="number">0.0</span> (on <span class="number">192.168</span><span class="number">.47</span><span class="number">.217</span>) as speculatable because it ran more than <span class="number">8751</span> ms</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">58</span> INFO TaskSetManager: Starting task <span class="number">5.1</span> in stage <span class="number">0.0</span> (TID <span class="number">17</span>, <span class="number">192.168</span><span class="number">.47</span><span class="number">.212</span>, ANY, <span class="number">30707</span> bytes)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">58</span> INFO TaskSetManager: Marking task <span class="number">16</span> in stage <span class="number">0.0</span> (on <span class="number">192.168</span><span class="number">.47</span><span class="number">.223</span>) as speculatable because it ran more than <span class="number">8751</span> ms</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">58</span> INFO TaskSetManager: Marking task <span class="number">9</span> in stage <span class="number">0.0</span> (on <span class="number">192.168</span><span class="number">.47</span><span class="number">.216</span>) as speculatable because it ran more than <span class="number">8751</span> ms</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">58</span> INFO TaskSetManager: Starting task <span class="number">9.1</span> in stage <span class="number">0.0</span> (TID <span class="number">18</span>, <span class="number">192.168</span><span class="number">.47</span><span class="number">.223</span>, ANY, <span class="number">30706</span> bytes)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">58</span> INFO TaskSetManager: Starting task <span class="number">16.1</span> in stage <span class="number">0.0</span> (TID <span class="number">19</span>, <span class="number">192.168</span><span class="number">.47</span><span class="number">.217</span>, ANY, <span class="number">30707</span> bytes)</span><br><span class="line"></span><br><span class="line">在<span class="number">14</span>个Task的时候报错:</span><br><span class="line"></span><br><span class="line">Caused by: org.apache.spark.SparkException: A master URL must be <span class="built_in">set</span> in your configuration</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">00</span>:<span class="number">02</span> WARN TaskSetManager: Lost task <span class="number">9.4</span> in stage <span class="number">0.0</span> (TID <span class="number">26</span>, <span class="number">192.168</span><span class="number">.47</span><span class="number">.212</span>):</span><br><span class="line">  org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException):</span><br><span class="line">  No lease on /user/tongdun/activity_hist/year=<span class="number">2015</span>/month=<span class="number">9</span>/day=<span class="number">17</span>/_temporary/<span class="number">0</span>/_temporary/attempt_201509231800_0000_m_000009_4/part-r-<span class="number">00009</span>-<span class="number">04</span>acba62-<span class="number">4</span>ec8-<span class="number">4212</span>-<span class="number">8185</span>-<span class="number">0999</span>ae3078e1.gz.parquet:</span><br><span class="line">  File does not exist. Holder DFSClient_attempt_201509231759_0000_m_000012_0_2042692825_48 does not have any open files.</span><br><span class="line"></span><br><span class="line">在代码中手动指定--master:</span><br><span class="line">Caused by: java.util.NoSuchElementException: spark.es.nodes</span><br><span class="line">    at org.apache.spark.SparkConf$$anonfun$get$<span class="number">1.</span>apply(SparkConf.scala:<span class="number">172</span>)</span><br><span class="line">    at org.apache.spark.SparkConf$$anonfun$get$<span class="number">1.</span>apply(SparkConf.scala:<span class="number">172</span>)</span><br><span class="line">    at scala.Option.getOrElse(Option.scala:<span class="number">120</span>)</span><br><span class="line">    at org.apache.spark.SparkConf.get(SparkConf.scala:<span class="number">172</span>)</span><br><span class="line">    at cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter$.&lt;init&gt;(ES2HDFSExporter.scala:<span class="number">24</span>)</span><br><span class="line">    at cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter$.&lt;clinit&gt;(ES2HDFSExporter.scala)</span><br><span class="line"></span><br><span class="line">在代码中再次手动设置spark.es.nodes:</span><br><span class="line">nohup bin/spark-submit --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">  --executor-memory <span class="number">8</span>g --total-executor-cores <span class="number">24</span> --driver-memory <span class="number">12</span>g \</span><br><span class="line">  --<span class="keyword">class</span> cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter \</span><br><span class="line">  /home/qihuang.zheng/tongdun-app-<span class="number">1.0</span>-SNAPSHOT.jar <span class="number">20150917</span> <span class="number">1</span> <span class="number">50</span> &amp;</span><br></pre></td></tr></table></figure>
<p>没有报错, 但是因为任务运行较慢,开启了推测机制, 会在后台突然启动了多个应用. 停掉spark-submit进程后,所有ES2HDFS也都停掉了.  </p>
<p><img src="http://img.blog.csdn.net/20150924082457899" alt="spark-speculation"></p>
<p>关闭推测执行: spark.speculation=false. 但是实际上推测执行默认就是关闭的了!!</p>
<h3 id="数据文件找不到??">数据文件找不到??</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">24</span> <span class="number">12</span>:<span class="number">01</span>:<span class="number">49</span> WARN DFSClient: DFSOutputStream ResponseProcessor exception  <span class="keyword">for</span> block BP-<span class="number">1007412381</span>-<span class="number">192.168</span><span class="number">.47</span><span class="number">.213</span>-<span class="number">1425269150800</span>:blk_1075697042_1956779</span><br><span class="line">java.io.EOFException: Premature EOF: no length prefix available</span><br><span class="line">    at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:<span class="number">1987</span>)</span><br><span class="line">    at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:<span class="number">176</span>)</span><br><span class="line">    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:<span class="number">796</span>)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">24</span> <span class="number">12</span>:<span class="number">01</span>:<span class="number">49</span> WARN DFSClient: Error Recovery <span class="keyword">for</span> block BP-<span class="number">1007412381</span>-<span class="number">192.168</span><span class="number">.47</span><span class="number">.213</span>-<span class="number">1425269150800</span>:blk_1075697042_1956779</span><br><span class="line">in pipeline <span class="number">192.168</span><span class="number">.47</span><span class="number">.211</span>:<span class="number">50010</span>, <span class="number">192.168</span><span class="number">.47</span><span class="number">.218</span>:<span class="number">50010</span>, <span class="number">192.168</span><span class="number">.47</span><span class="number">.208</span>:<span class="number">50010</span>: bad datanode <span class="number">192.168</span><span class="number">.47</span><span class="number">.211</span>:<span class="number">50010</span></span><br></pre></td></tr></table></figure>
<h3 id="ES没有暴露_count">ES没有暴露_count</h3><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="atom">val</span> <span class="atom">esRDD</span> = <span class="atom">sc</span>.<span class="atom">esRDD</span>(<span class="atom">s</span><span class="string">"$exportDB/activity"</span>, <span class="atom">query</span>) //<span class="name">RDD</span>[(<span class="name">String</span>, <span class="name">Map</span>[<span class="name">String</span>, <span class="name">AnyRef</span>])]</span><br><span class="line"><span class="atom">println</span>(<span class="string">"COUNT:"</span>+<span class="atom">esRDD</span>.<span class="atom">count</span>())</span><br><span class="line"></span><br><span class="line"><span class="atom">org</span>.<span class="atom">elasticsearch</span>.<span class="atom">hadoop</span>.<span class="atom">rest</span>.<span class="name">EsHadoopInvalidRequest</span>: [<span class="name">GET</span>] <span class="atom">on</span> [<span class="atom">forseti</span>-<span class="number">201509</span>*/<span class="atom">activity</span>/<span class="name">_count</span>] <span class="atom">failed</span>; <span class="atom">server</span>[<span class="atom">null</span>] <span class="atom">returned</span> [<span class="number">404</span>|<span class="name">Not</span> <span class="name">Found</span>:]</span><br></pre></td></tr></table></figure>
<h3 id="减少查询粒度">减少查询粒度</h3><p>将query的查询时间改成一个小时. 发现任务也是停留在14/17, 所以跟ES的数据量大小没有关系(因为一个小时的数据量是很小的,而且是凌晨1点的).</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">02</span> INFO <span class="string">TaskSchedulerImpl:</span> Adding task set <span class="number">1.0</span> with <span class="number">17</span> tasks</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">02</span> INFO <span class="string">TaskSetManager:</span> Starting task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">1</span>, spark047242, ANY, <span class="number">30647</span> bytes)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">02</span> INFO <span class="string">TaskSetManager:</span> Starting task <span class="number">1.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">2</span>, spark047217, ANY, <span class="number">30647</span> bytes)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">02</span> INFO <span class="string">TaskSetManager:</span> Starting task <span class="number">2.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">3</span>, spark047217, ANY, <span class="number">30647</span> bytes)</span><br><span class="line">....</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">02</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047242:</span><span class="number">45400</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047241:</span><span class="number">33811</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047223:</span><span class="number">60695</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047243:</span><span class="number">53908</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Removed broadcast_0_piece0 on <span class="number">192.168</span><span class="number">.47</span><span class="number">.211</span>:<span class="number">59995</span> <span class="keyword">in</span> memory (<span class="string">size:</span> <span class="number">987.0</span> B, <span class="string">free:</span> <span class="number">4.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047243:</span><span class="number">50652</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Removed broadcast_0_piece0 on <span class="string">spark047242:</span><span class="number">45400</span> <span class="keyword">in</span> memory (<span class="string">size:</span> <span class="number">987.0</span> B, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047217:</span><span class="number">51802</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047216:</span><span class="number">38219</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047216:</span><span class="number">45353</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line">....</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">06</span> INFO <span class="string">TaskSetManager:</span> Finished task <span class="number">8.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">9</span>) <span class="keyword">in</span> <span class="number">4162</span> ms on spark047241 (<span class="number">1</span>/<span class="number">17</span>)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">07</span> INFO <span class="string">TaskSetManager:</span> Finished task <span class="number">10.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">11</span>) <span class="keyword">in</span> <span class="number">4831</span> ms on spark047216 (<span class="number">2</span>/<span class="number">17</span>)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">08</span> INFO <span class="string">TaskSetManager:</span> Finished task <span class="number">14.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">15</span>) <span class="keyword">in</span> <span class="number">6181</span> ms on spark047244 (<span class="number">3</span>/<span class="number">17</span>)</span><br><span class="line">....</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">16</span> INFO <span class="string">TaskSetManager:</span> Finished task <span class="number">2.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">3</span>) <span class="keyword">in</span> <span class="number">14051</span> ms on spark047217 (<span class="number">14</span>/<span class="number">17</span>)</span><br><span class="line">这里开始没有反应了....</span><br></pre></td></tr></table></figure>
<h3 id="使用DF读取ES数据">使用DF读取ES数据</h3><p>由于REST服务无法读取具体的index, 报错:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot find mapping <span class="keyword">for</span> forseti-<span class="number">201509</span>*/activity - one is required before <span class="keyword">using</span> Spark SQL</span><br><span class="line"></span><br><span class="line">curl <span class="number">192.168</span><span class="number">.47</span><span class="number">.155</span>:<span class="number">9201</span>/_cat/indices/forseti-<span class="number">20150909</span>: <span class="number">404</span> Not Found</span><br><span class="line"></span><br><span class="line">https:<span class="comment">//github.com/elastic/elasticsearch-hadoop/blob/master/spark/sql-12/src/main/scala/org/elasticsearch/spark/sql/SchemaUtils.scala</span></span><br><span class="line"></span><br><span class="line">可以看到下面是使用REST服务查询index, 如果不存在, 则报上面的错误!</span><br><span class="line">val repo = <span class="keyword">new</span> RestRepository(cfg)</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (repo.indexExists(<span class="literal">true</span>)) &#123;</span><br><span class="line">  ....</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> EsHadoopIllegalArgumentException(s<span class="string">"Cannot find mapping for $&#123;cfg.getResourceRead&#125; - one is required before using Spark SQL"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">后来发现是连错了端口, 应该使用<span class="number">9200</span>, 而不是<span class="number">9201</span>!😢</span><br></pre></td></tr></table></figure>
<p>DataFrame的结构见es_spark_mapping.jspn:</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- activity: struct (nullable = <span class="keyword">true</span>)</span><br><span class="line">|    |-- accountAddress: string (nullable = <span class="keyword">true</span>)</span><br><span class="line">|    |-- accountAddressCity: string (nullable = <span class="keyword">true</span>)</span><br><span class="line">|-- audit: struct (nullable = <span class="keyword">true</span>)</span><br><span class="line">|    |-- needAudit: <span class="keyword">boolean</span> (nullable = <span class="keyword">true</span>)</span><br><span class="line">|    |-- operateResult: string (nullable = <span class="keyword">true</span>)</span><br></pre></td></tr></table></figure>
<h3 id="ElastichSearch-Hive">ElastichSearch-Hive</h3><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">add jar /home/qihuang<span class="built_in">.</span>zheng/elasticsearch<span class="attribute">-hadoop</span>-<span class="number">2.1</span><span class="number">.1</span><span class="built_in">.</span>jar;</span><br><span class="line"></span><br><span class="line">CREATE EXTERNAL TABLE activity_20150917 (</span><br><span class="line">    activity <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    audit    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    browser    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    device    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    event_result    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    geo    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    policy    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    policyString2    <span class="built_in">string</span></span><br><span class="line">) STORED <span class="keyword">BY</span> <span class="string">'org.elasticsearch.hadoop.hive.EsStorageHandler'</span></span><br><span class="line">TBLPROPERTIES(</span><br><span class="line">    <span class="string">'es.resource'</span> = <span class="string">'forseti-201509*/activity'</span>,</span><br><span class="line">    <span class="string">'es.nodes'</span> = <span class="string">'192.168.47.155'</span>,</span><br><span class="line">    <span class="string">'es.port'</span> = <span class="string">'9200'</span>,</span><br><span class="line">    <span class="string">'es.query'</span> = <span class="string">'?q=_id:1442419200*'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">hive&gt; <span class="keyword">select</span> * from activity_20150917;</span><br><span class="line">OK</span><br><span class="line">Failed <span class="keyword">with</span> exception java<span class="built_in">.</span>io<span class="built_in">.</span>IOException:org<span class="built_in">.</span>apache<span class="built_in">.</span>hadoop<span class="built_in">.</span>hive<span class="built_in">.</span>ql<span class="built_in">.</span>metadata<span class="built_in">.</span>HiveException:</span><br><span class="line">java<span class="built_in">.</span>lang<span class="built_in">.</span>ClassCastException: org<span class="built_in">.</span>apache<span class="built_in">.</span>hadoop<span class="built_in">.</span>io<span class="built_in">.</span>BooleanWritable can<span class="subst">not</span> be cast <span class="keyword">to</span> org<span class="built_in">.</span>apache<span class="built_in">.</span>hadoop<span class="built_in">.</span>io<span class="built_in">.</span>Text</span><br><span class="line">Time taken: <span class="number">0.711</span> seconds</span><br><span class="line">可见建表的时候指定为<span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;并不合适,因为ES的value不一定是<span class="built_in">string</span><span class="subst">!</span><span class="subst">!</span></span><br><span class="line"></span><br><span class="line">注意不能指定: stored as parquet location <span class="string">'/user/qihuang.zheng/20150926'</span></span><br><span class="line">因为数据源还是在ES中, 每次查询表的时候还是会去查询ES<span class="built_in">. </span>而不能保存到HDFS中<span class="built_in">. </span>这种方式对于ES2HDFS也不适用<span class="built_in">.</span><br><span class="line"></span>这种方式相当于提供了类似SQL的方式查询ES的数据, 查询SQL的任务放在了hive中去做<span class="built_in">. </span>不过底层还是查的ES<span class="subst">!</span> PASS</span><br></pre></td></tr></table></figure>
<h3 id="Hive_Load_Map_Data">Hive Load Map Data</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> activity_20150917 (</span><br><span class="line">    activity <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    <span class="keyword">audit</span>    <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    browser    <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    device    <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    event_result    <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    geo    <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    <span class="keyword">policy</span>    <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    policyString2    <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span></span><br><span class="line"><span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span>;</span></span><br><span class="line"></span><br><span class="line">字段之间适用\t分隔符, 即多个map之间用\t;  Map的key,value适用:组成.</span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> <span class="keyword">LOCAL</span> INPATH <span class="string">'/home/qihuang.zheng/activity_20150917.txt'</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> activity_20150917;</span></span><br></pre></td></tr></table></figure>
<h3 id="ES_dump_json">ES dump json</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">30分钟的数据</span><br><span class="line">date -j -f <span class="string">"%Y-%m-%d %H:%M:%S"</span> <span class="string">"2015-09-17 00:30:00"</span> <span class="string">"+%s"</span></span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"query"</span>: &#123;<span class="string">"range"</span>: &#123;<span class="string">"activity.eventOccurTime"</span>: &#123;<span class="string">"from"</span> : 1442419200000,<span class="string">"to"</span> : 1442421000000&#125;&#125;&#125;&#125;</span><br><span class="line"></span><br><span class="line">这里没有指定是哪张表, 那是查询全部表吗?</span><br><span class="line">elasticdump \</span><br><span class="line">  --<span class="keyword">input</span>=http:<span class="comment">//localhost:9200/forseti-201509* \</span></span><br><span class="line">  --output=<span class="keyword">query</span>.json \</span><br><span class="line">  --searchBody '&#123;<span class="string">"query"</span>: &#123;<span class="string">"range"</span>: &#123;<span class="string">"activity.eventOccurTime"</span>: &#123;<span class="string">"from"</span> : 1442419200000,<span class="string">"to"</span> : 1442421000000&#125;&#125;&#125;&#125;'</span><br><span class="line"></span><br><span class="line">curl http:<span class="comment">//localhost:9200/_cat/indices | grep forseti-201509</span></span><br><span class="line"></span><br><span class="line">下面这个有问题, 按照文档里面, 实际上不需要/api/<span class="keyword">search</span>的</span><br><span class="line">elasticdump \</span><br><span class="line">  --<span class="keyword">input</span>=http:<span class="comment">//localhost:9200/api/search \</span></span><br><span class="line">  --<span class="keyword">input</span>-index=forseti-20150916/activity \</span><br><span class="line">  --output=201509170030.json \</span><br><span class="line">  --searchBody '&#123;<span class="string">"query"</span>: &#123;<span class="string">"range"</span>: &#123;<span class="string">"activity.eventOccurTime"</span>: &#123;<span class="string">"from"</span> : 1442419200000,<span class="string">"to"</span> : 1442421000000&#125;&#125;&#125;&#125;'</span><br><span class="line"></span><br><span class="line">指定index和<span class="keyword">type</span>, 注意必须精确地指定index名称, 不能用*吗??</span><br><span class="line">elasticdump \</span><br><span class="line">  --<span class="keyword">input</span>=http:<span class="comment">//localhost:9200 \</span></span><br><span class="line">  --<span class="keyword">input</span>-index=forseti-20150916/activity \</span><br><span class="line">  --output=201509170030.json \</span><br><span class="line">  --searchBody '&#123;<span class="string">"query"</span>: &#123;<span class="string">"range"</span>: &#123;<span class="string">"activity.eventOccurTime"</span>: &#123;<span class="string">"from"</span> : 1442419200000,<span class="string">"to"</span> : 1442421000000&#125;&#125;&#125;&#125;'</span><br><span class="line"></span><br><span class="line">查询30分钟的数据, 耗时44-32=12</span><br><span class="line">Sat, 26 Sep 2015 08:44:56 GMT | got 30 objects from source elasticsearch (offset: 508579)</span><br><span class="line">Sat, 26 Sep 2015 08:44:56 GMT | sent 30 objects to destination <span class="keyword">file</span>, wrote 30</span><br><span class="line">Sat, 26 Sep 2015 08:44:56 GMT | got 0 objects from source elasticsearch (offset: 508609)</span><br><span class="line">Sat, 26 Sep 2015 08:44:56 GMT | sent 0 objects to destination <span class="keyword">file</span>, wrote 0</span><br><span class="line">Sat, 26 Sep 2015 08:44:56 GMT | <span class="keyword">Total</span> Writes: 508609</span><br><span class="line">Sat, 26 Sep 2015 08:44:56 GMT | dump complete</span><br><span class="line"></span><br><span class="line">不过文件大小也太大了吧! 30分钟1G, 1天的数据得有50G! 不过压缩后, 文件只有132M.</span><br><span class="line">-rw-r--r--    1 zhengqh  staff   1.0G  9 26 16:44 201509170030.json</span><br><span class="line">-rw-r--r--    1 zhengqh  staff   132M  9 26 17:04 td_activity_201509170030.json.tar.gz</span><br><span class="line"></span><br><span class="line">记录的格式其中_source才是真正的内容:</span><br><span class="line">[</span><br><span class="line">&#123;<span class="string">"_index"</span>:<span class="string">"forseti-20150916"</span>,<span class="string">"_type"</span>:<span class="string">"activity"</span>,<span class="string">"_id"</span>:<span class="string">"1442419220735-70792643"</span>,<span class="string">"_score"</span>:0,<span class="string">"_source"</span>:&#123;&#125;</span><br><span class="line">&#123;<span class="string">"_index"</span>:<span class="string">"forseti-20150916"</span>,<span class="string">"_type"</span>:<span class="string">"activity"</span>,<span class="string">"_id"</span>:<span class="string">"1442419220735-70792643"</span>,<span class="string">"_score"</span>:0,<span class="string">"_source"</span>:&#123;&#125;</span><br><span class="line">]</span><br><span class="line">所以如果要以这种json文件加载到hive中,还要对json文件处理才行!</span><br></pre></td></tr></table></figure>
<h3 id="Python_Dump_json">Python Dump json</h3><figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> elasticsearch</span><br><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> helpers</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    es = elasticsearch.<span class="type">Elasticsearch</span>(['localhost:<span class="number">9200</span>'])</span><br><span class="line">    query = &#123;</span><br><span class="line">        <span class="string">"query"</span>: &#123;</span><br><span class="line">            <span class="string">"filtered"</span>: &#123;</span><br><span class="line">                <span class="string">"filter"</span>: &#123;</span><br><span class="line">                    <span class="string">"range"</span>: &#123;</span><br><span class="line">                        <span class="string">"activity.eventOccurTime"</span>: &#123;</span><br><span class="line">                            <span class="string">"from"</span>: parse_time('<span class="number">2015</span>-<span class="number">09</span>-<span class="number">17</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>'),</span><br><span class="line">                            <span class="string">"to"</span>: parse_time('<span class="number">2015</span>-<span class="number">09</span>-<span class="number">17</span> <span class="number">00</span>:<span class="number">30</span>:<span class="number">00</span>')</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    r = helpers.scan(es, index='forseti-r-*', doc_type='activity', scroll='<span class="number">1</span>m', query=query, request_timeout=<span class="number">999999</span>)</span><br><span class="line">    f = open(sys.argv[<span class="number">1</span>], 'w')</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> r:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            source = e['_source']</span><br><span class="line">            f.write(json.dumps(source))</span><br><span class="line">        <span class="keyword">except</span> <span class="type">Exception</span> <span class="keyword">as</span> ex:</span><br><span class="line">            print(ex)</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">def parse_time(t):</span><br><span class="line">    <span class="literal">result</span> = datetime.datetime.strptime(t, <span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line">    <span class="literal">result</span> = <span class="type">int</span>((<span class="literal">result</span> - datetime.datetime(<span class="number">1970</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">8</span>)).total_seconds()*<span class="number">1000</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">result</span></span><br><span class="line"></span><br><span class="line">main()</span><br><span class="line"></span><br><span class="line">执行python文件, 传入json参数. 时间太慢, <span class="type">PASS</span></span><br><span class="line">python es_export_seq.py ~/data/<span class="number">20150917</span>.json</span><br></pre></td></tr></table></figure>
<h3 id="Storm-ElasticSearch">Storm-ElasticSearch</h3><p>在scala代码中运行EStorm发现都不能输出! 于是放在storm-elasticsearch工程中, 就可以运行. 但是读取ES会超时.<br>这里我们读取的是47.155的ES,通过端口转发到本地localhost:9200. 这也是elasticsearch-storm的默认ES地址.</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">12019</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  org.elasticsearch.hadoop.util.Version - Elasticsearch Hadoop v2.1.1 <span class="list">[<span class="keyword">a1fc48457b</span>]</span><br><span class="line"><span class="number">12025</span> <span class="list">[<span class="keyword">Thread-17-__system</span>] INFO  backtype.storm.daemon.executor - Preparing bolt __system:<span class="list">(<span class="keyword">-1</span>)</span></span><br><span class="line"><span class="number">12035</span> <span class="list">[<span class="keyword">Thread-17-__system</span>] INFO  backtype.storm.daemon.executor - Prepared bolt __system:<span class="list">(<span class="keyword">-1</span>)</span></span><br><span class="line"><span class="number">13596</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  org.elasticsearch.storm.EsSpout - Reading from <span class="list">[<span class="keyword">forseti-20150916/activity</span>]</span><br><span class="line"><span class="number">13706</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  org.elasticsearch.storm.EsSpout - Discovered mapping &#123;forseti-20150916=<span class="list">[<span class="keyword">mappings=</span><span class="list">[<span class="keyword">activity=</span><span class="list">[<span class="keyword">activity=</span><span class="list">[<span class="keyword">accountAddress=STRING</span>,  event_result=<span class="list">[<span class="keyword">accountLogin=STRING</span>, ], geo=<span class="list">[<span class="keyword">ipAddress=STRING</span>, ipCity=STRING, ], policy=<span class="list">[<span class="keyword">hitRules=</span><span class="list">[<span class="keyword">decision=STRING</span>, ]], policyString2=STRING]]]&#125; for <span class="list">[<span class="keyword">forseti-20150916/activity</span>]</span><br><span class="line"><span class="number">13773</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  backtype.storm.daemon.executor - Opened spout es-spout:<span class="list">(<span class="keyword">2</span>)</span></span><br><span class="line"><span class="number">13785</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  backtype.storm.daemon.executor - Activating spout es-spout:<span class="list">(<span class="keyword">2</span>)</span></span><br><span class="line"><span class="number">71947</span> <span class="list">[<span class="keyword">Thread-19-__acker</span>] INFO  backtype.storm.daemon.executor - Processing received message source: __system:-1, stream: __metrics_tick, id: &#123;&#125;, <span class="list">[<span class="keyword">60</span>]</span><br><span class="line"><span class="number">72067</span> <span class="list">[<span class="keyword">Thread-17-__system</span>] INFO  backtype.storm.daemon.executor - Processing received message source: __system:-1, stream: __metrics_tick, id: &#123;&#125;, <span class="list">[<span class="keyword">60</span>]</span><br><span class="line"><span class="number">72068</span> <span class="list">[<span class="keyword">Thread-15-print</span>] INFO  backtype.storm.daemon.executor - Processing received message source: __system:-1, stream: __metrics_tick, id: &#123;&#125;, <span class="list">[<span class="keyword">60</span>]</span><br><span class="line"><span class="number">72069</span> <span class="list">[<span class="keyword">Thread-19-__acker</span>] INFO  backtype.storm.daemon.task - Emitting: __acker __metrics <span class="list">[#&lt;TaskInfo backtype.storm.metric.api.IMetricsConsumer$TaskInfo@6fef5713&gt; <span class="list">[#&lt;DataPoint <span class="list">[<span class="keyword">__emit-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__process-latency</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__receive</span> = &#123;read_pos=0, write_pos=1, capacity=1024, population=1&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__ack-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__transfer-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__execute-latency</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__fail-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__sendqueue</span> = &#123;read_pos=-1, write_pos=-1, capacity=1024, population=0&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__execute-count</span> = &#123;&#125;]&gt;]]</span><br><span class="line"><span class="number">72069</span> <span class="list">[<span class="keyword">Thread-15-print</span>] INFO  backtype.storm.daemon.task - Emitting: print __metrics <span class="list">[#&lt;TaskInfo backtype.storm.metric.api.IMetricsConsumer$TaskInfo@45690684&gt; <span class="list">[#&lt;DataPoint <span class="list">[<span class="keyword">__emit-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__process-latency</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__receive</span> = &#123;read_pos=0, write_pos=1, capacity=1024, population=1&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__ack-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__transfer-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__execute-latency</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__fail-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__sendqueue</span> = &#123;read_pos=-1, write_pos=-1, capacity=1024, population=0&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__execute-count</span> = &#123;&#125;]&gt;]]</span><br><span class="line"><span class="number">88782</span> <span class="list">[<span class="keyword">Thread-13-es-spout</span>] INFO  org.apache.commons.httpclient.HttpMethodDirector - I/O exception <span class="list">(<span class="keyword">java.net.ConnectException</span>)</span> caught when processing request: Operation timed out</span><br><span class="line"><span class="number">88782</span> <span class="list">[<span class="keyword">Thread-11-es-spout</span>] INFO  org.apache.commons.httpclient.HttpMethodDirector - I/O exception <span class="list">(<span class="keyword">java.net.ConnectException</span>)</span> caught when processing request: Operation timed out</span><br><span class="line"><span class="number">88782</span> <span class="list">[<span class="keyword">Thread-11-es-spout</span>] INFO  org.apache.commons.httpclient.HttpMethodDirector - Retrying request</span><br><span class="line"><span class="number">88782</span> <span class="list">[<span class="keyword">Thread-13-es-spout</span>] INFO  org.apache.commons.httpclient.HttpMethodDirector - Retrying request</span><br><span class="line"><span class="number">89120</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  org.apache.commons.httpclient.HttpMethodDirector - I/O exception <span class="list">(<span class="keyword">java.net.ConnectException</span>)</span> caught when processing request: Operation timed out</span><br><span class="line"><span class="number">89120</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  org.apache.commons.httpclient.HttpMethodDirector - Retrying request</span><br><span class="line"></span><br><span class="line">原因是: 使用localhost代理线上的47.155. 但是查询es会转发到ES的其他节点, 而在本机是无法连接47网段的.</span><br><span class="line"></span><br><span class="line"><span class="number">315605</span> <span class="list">[<span class="keyword">Thread-11-es-spout</span>] ERROR org.elasticsearch.hadoop.rest.NetworkClient - Node <span class="list">[<span class="keyword">Operation</span> timed out] failed <span class="list">(<span class="keyword">192.168.47.82:9200</span>)</span><span class="comment">; selected next node [172.31.238.19:9200]</span></span><br><span class="line"><span class="number">315605</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] ERROR org.elasticsearch.hadoop.rest.NetworkClient - Node <span class="list">[<span class="keyword">Operation</span> timed out] failed <span class="list">(<span class="keyword">172.31.238.26:9200</span>)</span><span class="comment">; selected next node [192.168.47.28:9200]</span></span><br><span class="line"><span class="number">315647</span> <span class="list">[<span class="keyword">Thread-13-es-spout</span>] ERROR org.elasticsearch.hadoop.rest.NetworkClient - Node <span class="list">[<span class="keyword">Operation</span> timed out] failed <span class="list">(<span class="keyword">172.31.238.19:9200</span>)</span><span class="comment">; selected next node [192.168.47.82:9200]</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><br></pre></td></tr></table></figure>
<h3 id="每半个小时处理一次">每半个小时处理一次</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(partition &gt; <span class="number">0</span>)</span><br><span class="line">  activityDF.repartition(partition).write.parquet(s<span class="string">"/user/tongdun/activity_hist/year=$year/month=$month/day=$day"</span>)</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  activityDF.write.parquet(s<span class="string">"/user/tongdun/activity_hist/year=$year/month=$month/day=$day"</span>)</span><br><span class="line"></span><br><span class="line">原先查询一天, 改成半个小时写一次, 最后复制到activity中:</span><br><span class="line"></span><br><span class="line">                   start           end</span><br><span class="line"><span class="number">0</span>:  <span class="number">0</span>min  -- <span class="number">30</span>m   start           start+halfMs</span><br><span class="line"><span class="number">1</span>:  <span class="number">30</span>min -- <span class="number">1</span>h    start+halfMs    start+halfMs*<span class="number">2</span></span><br><span class="line"><span class="number">2</span>:  <span class="number">1</span>h    -- <span class="number">1.5</span>h  start+halfMs*<span class="number">2</span>  start+halfMs*<span class="number">3</span></span><br><span class="line"><span class="number">3</span>:  <span class="number">1.5</span>h  -- <span class="number">2</span>h</span><br><span class="line">..</span><br><span class="line"><span class="number">46</span>: <span class="number">23</span>h   -- <span class="number">23.5</span>h</span><br><span class="line"><span class="number">47</span>: <span class="number">23.5</span>h -- <span class="number">24</span>h</span><br><span class="line"></span><br><span class="line">change the startHour, endHour=startHour+halfMs</span><br><span class="line">startHour = startOfDay + halfMs*hourIndex.  hourIndex: <span class="number">0</span> until <span class="number">48</span></span><br><span class="line"></span><br><span class="line">copy file to hist. If exist, <span class="keyword">delete</span> first scala执行外部shell命令,返回值<span class="number">0</span>表示执行成功,<span class="number">1</span>表示失败</span><br><span class="line"></span><br><span class="line">import scala.sys.process._</span><br><span class="line"></span><br><span class="line">val createDirDemo = <span class="string">"/usr/install/hadoop/bin/hadoop fs -mkdir /user/tongdun/activity_hist/year=2015/month=9/day=17"</span> !</span><br><span class="line">val removeDirDmoe = <span class="string">"/usr/install/hadoop/bin/hadoop fs -rmr /user/tongdun/activity_hist/year=2015/month=9/day=17"</span> !</span><br><span class="line">val testRm = <span class="string">"/usr/install/hadoop/bin/hadoop fs -rmr /user/qihuang.zheng/test/20150917"</span> !</span><br><span class="line">val create = <span class="string">"/usr/install/hadoop/bin/hadoop fs -mkdir /user/qihuang.zheng/test/20150917"</span> !</span><br><span class="line">val cpDemo = <span class="string">"/usr/install/hadoop/bin/hadoop fs -cp /user/qihuang.zheng/test2/20150917/0/*.parquet /user/qihuang.zheng/test/20150917"</span> !</span><br><span class="line"></span><br><span class="line">删除文件夹,创建文件夹,拷贝文件,刷新分区:</span><br><span class="line"></span><br><span class="line"><span class="string">"/usr/install/hadoop/bin/hadoop fs -mkdir /user/tongdun/activity_hist/year=2015/month=9/day=17"</span> !</span><br><span class="line"></span><br><span class="line">(<span class="number">0</span> until <span class="number">48</span>).foreach(halfHourIndex=&gt;&#123;</span><br><span class="line">  s<span class="string">"/usr/install/hadoop/bin/hadoop fs -cp /user/qihuang.zheng/test2/20150917/$halfHourIndex/*.parquet /user/tongdun/activity_hist/year=2015/month=9/day=17"</span> !</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="string">"/usr/install/apache-hive-0.13.1-bin/bin/hive -e 'msck repair table activity'"</span> !</span><br><span class="line"></span><br><span class="line">如果导入的天数比较多, 可以用多个spark-submit多次执行. 每个spark-submit执行不同的导入时间. 传递的参数示例:</span><br><span class="line"></span><br><span class="line"><span class="number">20150917</span></span><br><span class="line"><span class="number">20150819</span> <span class="number">40</span></span><br><span class="line"><span class="number">20150819</span> <span class="number">40</span> <span class="number">20150917</span>,<span class="number">20150924</span>,<span class="number">20150925</span>,<span class="number">20150926</span>        -- 正式</span><br><span class="line"><span class="number">20150819</span> <span class="number">40</span> <span class="number">20150917</span>,<span class="number">20150924</span>,<span class="number">20150925</span>,<span class="number">20150926</span> debug  -- 测试</span><br><span class="line"></span><br><span class="line">nohup bin/spark-submit --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">4</span> --driver-memory <span class="number">4</span>g \</span><br><span class="line">  --<span class="keyword">class</span> cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter \</span><br><span class="line">  /home/qihuang.zheng/tongdun-app-<span class="number">1.0</span>-SNAPSHOT.jar <span class="number">20150819</span> <span class="number">40</span> <span class="number">20150917</span>,<span class="number">20150924</span>,<span class="number">20150925</span>,<span class="number">20150926</span> &amp;</span><br><span class="line"></span><br><span class="line">/usr/install/es2hdfs.sh</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">#!/bin/sh</span></span><br><span class="line">datebeg=<span class="string">"20150819"</span></span><br><span class="line">dateend=<span class="string">"20150927"</span></span><br><span class="line">beg_s=`date -d <span class="string">"$datebeg"</span> +%s`</span><br><span class="line">end_s=`date -d <span class="string">"$dateend"</span> +%s`</span><br><span class="line">excludes=<span class="string">"20150917 20150924 20150925 20150926"</span></span><br><span class="line">cd /usr/install/spark-<span class="number">1.4</span><span class="number">.1</span>-bin-hadoop2<span class="number">.4</span>/</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> [ <span class="string">"$beg_s"</span> -le <span class="string">"$end_s"</span> ];<span class="keyword">do</span></span><br><span class="line">     day=`date -d @$beg_s +<span class="string">"%Y%m%d"</span>`;</span><br><span class="line">     beg_s=$((beg_s+<span class="number">86400</span>));</span><br><span class="line">     flag=<span class="literal">false</span></span><br><span class="line">     <span class="keyword">for</span> item in $excludes</span><br><span class="line">     <span class="keyword">do</span></span><br><span class="line">        <span class="keyword">if</span> [ <span class="string">"$day"</span> == <span class="string">"$item"</span> ]; then</span><br><span class="line">          echo <span class="string">"$day In the list, skip"</span></span><br><span class="line">          flag=<span class="literal">true</span></span><br><span class="line">        fi</span><br><span class="line">     done</span><br><span class="line"></span><br><span class="line">     <span class="keyword">if</span> [ $flag == <span class="literal">false</span> ]; then</span><br><span class="line">       echo $day</span><br><span class="line">       <span class="preprocessor">#do something here....</span></span><br><span class="line"></span><br><span class="line">       bin/spark-submit --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">         --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">         --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">4</span> --driver-memory <span class="number">4</span>g \</span><br><span class="line">         --<span class="keyword">class</span> cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter \</span><br><span class="line">         /home/qihuang.zheng/tongdun-app-<span class="number">1.0</span>-SNAPSHOT.jar $day</span><br><span class="line">     fi</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<h3 id="shell可以,submit不行">shell可以,submit不行</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">测试一天的数据, 使用submit会报空指针错误. 但是同样的代码使用spark-shell则可以成功运行.</span><br><span class="line">bin/spark-submit --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">         --jars /home/qihuang.zheng/json4s-ast_2.<span class="number">10</span>-<span class="number">3.2</span>.<span class="number">11</span><span class="class">.jar</span>,/home/qihuang.zheng/json4s-core_2.<span class="number">10</span>-<span class="number">3.2</span>.<span class="number">11</span><span class="class">.jar</span>,/home/qihuang.zheng/json4s-jackson_2.<span class="number">10</span>-<span class="number">3.2</span>.<span class="number">11</span><span class="class">.jar</span>,/home/qihuang.zheng/json4s-native_2.<span class="number">10</span>-<span class="number">3.2</span>.<span class="number">11</span><span class="class">.jar</span>,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span>.<span class="number">1</span><span class="class">.jar</span> \</span><br><span class="line">         --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">4</span> --driver-memory <span class="number">4</span>g \</span><br><span class="line">         --class cn<span class="class">.fraudmetrix</span><span class="class">.dataplatform</span><span class="class">.lab</span><span class="class">.ES2HDFSExporter</span> \</span><br><span class="line">         /home/qihuang.zheng/tongdun-app-<span class="number">1.0</span>-SNAPSHOT<span class="class">.jar</span> <span class="number">20150927</span></span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">19</span> INFO TaskSchedulerImpl: Adding task set <span class="number">0.0</span> with <span class="number">3</span> tasks</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">19</span> INFO TaskSetManager: Starting task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">0</span>, spark047218, ANY, <span class="number">28706</span> bytes)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">19</span> INFO TaskSetManager: Starting task <span class="number">1.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">1</span>, spark047242, ANY, <span class="number">28705</span> bytes)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">19</span> INFO TaskSetManager: Starting task <span class="number">2.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">2</span>, spark047216, ANY, <span class="number">28705</span> bytes)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">20</span> INFO BlockManagerInfo: Added broadcast_0_piece0 <span class="keyword">in</span> memory on spark047242:<span class="number">58232</span> (size: <span class="number">21.6</span> KB, free: <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">20</span> INFO BlockManagerInfo: Added broadcast_0_piece0 <span class="keyword">in</span> memory on spark047218:<span class="number">59207</span> (size: <span class="number">21.6</span> KB, free: <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">22</span> INFO BlockManagerInfo: Added broadcast_0_piece0 <span class="keyword">in</span> memory on spark047216:<span class="number">48575</span> (size: <span class="number">21.6</span> KB, free: <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">49</span>:<span class="number">45</span> WARN TaskSetManager: Lost task <span class="number">2.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">2</span>, spark047216): java<span class="class">.lang</span><span class="class">.NullPointerException</span></span><br><span class="line">    at parquet<span class="class">.hadoop</span><span class="class">.InternalParquetRecordWriter</span><span class="class">.flushRowGroupToStore</span>(InternalParquetRecordWriter<span class="class">.java</span>:<span class="number">146</span>)</span><br><span class="line">    at parquet<span class="class">.hadoop</span><span class="class">.InternalParquetRecordWriter</span><span class="class">.close</span>(InternalParquetRecordWriter<span class="class">.java</span>:<span class="number">112</span>)</span><br><span class="line">    at parquet<span class="class">.hadoop</span><span class="class">.ParquetRecordWriter</span><span class="class">.close</span>(ParquetRecordWriter<span class="class">.java</span>:<span class="number">73</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.parquet</span><span class="class">.ParquetOutputWriter</span><span class="class">.close</span>(newParquet<span class="class">.scala</span>:<span class="number">88</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.sources</span><span class="class">.DefaultWriterContainer</span><span class="class">.abortTask</span>(commands<span class="class">.scala</span>:<span class="number">491</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.sources</span><span class="class">.InsertIntoHadoopFsRelation</span><span class="class">.org</span><span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$sql</span><span class="variable">$sources</span><span class="variable">$InsertIntoHadoopFsRelation</span>$<span class="variable">$writeRows</span>$<span class="number">1</span>(commands<span class="class">.scala</span>:<span class="number">190</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.sources</span><span class="class">.InsertIntoHadoopFsRelation</span>$<span class="variable">$anonfun</span><span class="variable">$insert</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(commands.scala:<span class="number">160</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.sources</span><span class="class">.InsertIntoHadoopFsRelation</span>$<span class="variable">$anonfun</span><span class="variable">$insert</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(commands.scala:<span class="number">160</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.scheduler</span><span class="class">.ResultTask</span><span class="class">.runTask</span>(ResultTask<span class="class">.scala</span>:<span class="number">63</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.scheduler</span><span class="class">.Task</span><span class="class">.run</span>(Task<span class="class">.scala</span>:<span class="number">70</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.executor</span><span class="class">.Executor</span><span class="variable">$TaskRunner</span>.<span class="function"><span class="title">run</span><span class="params">(Executor.scala:<span class="number">213</span>)</span></span></span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.ThreadPoolExecutor</span><span class="class">.runWorker</span>(ThreadPoolExecutor<span class="class">.java</span>:<span class="number">1145</span>)</span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.ThreadPoolExecutor</span><span class="variable">$Worker</span>.<span class="function"><span class="title">run</span><span class="params">(ThreadPoolExecutor.java:<span class="number">615</span>)</span></span></span><br><span class="line">    at java<span class="class">.lang</span><span class="class">.Thread</span><span class="class">.run</span>(Thread<span class="class">.java</span>:<span class="number">744</span>)</span><br></pre></td></tr></table></figure>
<h3 id="spark-shell+script_file(×)">spark-shell+script file(×)</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/install/ES2HDFS.scala <span class="number">20150917</span> | spark-<span class="number">1.4</span><span class="number">.1</span>-bin-hadoop2<span class="number">.4</span>/bin/spark-shell --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">  --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">4</span> --driver-memory <span class="number">4</span>g \</span><br><span class="line">  --conf spark.es.nodes=<span class="number">192.168</span><span class="number">.47</span><span class="number">.155</span>:<span class="number">9200</span> \</span><br><span class="line">  --conf spark.es.field.read.empty.as.null=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line">spark-shell &lt; ES2HDFSExporter.scala <span class="number">20150927</span></span><br><span class="line"></span><br><span class="line">bin/spark-shell -i ES2HDFSExporter.scala <span class="number">20150927</span></span><br></pre></td></tr></table></figure>
<p>参数怎么传递:</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">http:<span class="comment">//stackoverflow.com/questions/29928999/passing-command-line-arguments-to-spark-shell</span></span><br><span class="line"></span><br><span class="line">./spark-script.<span class="keyword">sh</span> your_file.<span class="keyword">scala</span> first_arg second_arg third_arg</span><br><span class="line"></span><br><span class="line">#!/bin/<span class="keyword">sh</span></span><br><span class="line">scala_file=<span class="label">$1</span></span><br><span class="line">shift 1</span><br><span class="line">arguments=$@</span><br><span class="line">#<span class="keyword">set</span> +o posix  # to enable process substitution when not running <span class="keyword">on</span> bash</span><br><span class="line"><span class="keyword">cd</span> /usr/install/spark-1.4.1-bin-hadoop2.4/bin</span><br><span class="line">spark-<span class="keyword">shell</span>  --jars /home/qihuang.zheng/json4s-ast_2.10-3.2.11.jar,/home/qihuang.zheng/json4s-core_2.10-3.2.11.jar,/home/qihuang.zheng/json4s-jackson_2.10-3.2.11.jar,/home/qihuang.zheng/json4s-native_2.10-3.2.11.jar,/home/qihuang.zheng/elasticsearch-hadoop-2.1.1.jar \</span><br><span class="line">               --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">               --executor-<span class="keyword">memory</span> 4g --<span class="keyword">total</span>-executor-cores 4 --driver-<span class="keyword">memory</span> 4g \</span><br><span class="line">               --<span class="keyword">conf</span> spark.es.nodes=192.168.47.155:9200 \</span><br><span class="line">               --<span class="keyword">conf</span> spark.es.field.<span class="keyword">read</span>.empty.<span class="keyword">as</span>.null=true \</span><br><span class="line">               -i &lt;(echo 'val <span class="keyword">args</span> = <span class="string">"'$arguments'"</span>.<span class="keyword">split</span>(<span class="string">"\\s+"</span>)' ; <span class="keyword">cat</span> <span class="label">$scala_file</span>)</span><br><span class="line"></span><br><span class="line">将es2hdfs.<span class="keyword">sh</span>中间部分换成:</span><br><span class="line">       <span class="keyword">sh</span> spark-script.<span class="keyword">sh</span> /usr/install/ES2HDFS.<span class="keyword">scala</span> <span class="label">$day</span></span><br><span class="line"></span><br><span class="line">./spark-script.<span class="keyword">sh</span>: <span class="keyword">line</span> 16: <span class="keyword">syntax</span> <span class="keyword">error</span> near unexpected <span class="keyword">token</span> `('</span><br><span class="line">./spark-script.<span class="keyword">sh</span>: <span class="keyword">line</span> 16: `               -i &lt;(echo 'val <span class="keyword">args</span> = 20150917' ; <span class="keyword">cat</span> <span class="label">$scala_file</span>)'</span><br></pre></td></tr></table></figure>
<h3 id="JSON解析异常">JSON解析异常</h3><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Caused <span class="keyword">by</span>: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException:</span><br><span class="line">  org.codehaus.jackson.JsonParseException: Illegal unquoted <span class="keyword">character</span> ((CTRL-CHAR, code <span class="number">4</span>)):</span><br><span class="line">  has <span class="built_in">to</span> be escaped <span class="keyword">using</span> <span class="constant">backslash</span> <span class="built_in">to</span> be included <span class="operator">in</span> <span class="keyword">string</span> <span class="built_in">value</span></span><br><span class="line"> <span class="keyword">at</span> [Source: org.apache.commons.httpclient.AutoCloseInputStream@b1c8cbf; <span class="built_in">line</span>: <span class="number">1</span>, column: <span class="number">29762</span>]</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">16</span>:<span class="number">31</span>:<span class="number">44</span> WARN TaskSetManager: Lost task <span class="number">0.0</span> <span class="operator">in</span> stage <span class="number">17.0</span> (TID <span class="number">51</span>, spark047216):</span><br><span class="line">  org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException):</span><br><span class="line">  No lease <span class="command"><span class="keyword">on</span> /<span class="title">user</span>/<span class="title">qihuang</span>.<span class="title">zheng</span>/<span class="title">es_hdfs_back</span>/<span class="title">20150917</span>/<span class="title">17</span>/<span class="title">_temporary</span>/<span class="title">0</span>/<span class="title">_temporary</span>/<span class="title">attempt_201509291628_0017_m_000000_0</span>/</span></span><br><span class="line">  part-r-<span class="number">00000</span>-b701f6b9-f4d2-<span class="number">439</span>b-<span class="number">8</span>fe9-<span class="number">5334331e6</span>e06.gz.parquet: File does <span class="operator">not</span> exist.</span><br><span class="line">  Holder DFSClient_attempt_201509291525_0000_m_000002_0_-<span class="number">1846407483</span>_48 does <span class="operator">not</span> have <span class="keyword">any</span> <span class="built_in">open</span> <span class="built_in">files</span>.</span><br></pre></td></tr></table></figure>
<p>这个错误并不是总是出现.  </p>
<h3 id="关于ES的mapping和Spark的类型解析">关于ES的mapping和Spark的类型解析</h3><p>esRDD读取出来的map: Map[String, Any]. 下面是ES的一个示例.</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="collection">&#123;</span><br><span class="line">    <span class="string">"policyString2"</span>: <span class="string">"&#123;....&#125;"</span></span><br><span class="line">    <span class="string">"geo"</span>: <span class="collection">&#123;</span><br><span class="line">        <span class="string">"ipCountry"</span>: <span class="string">"*"</span>,</span><br><span class="line">        <span class="string">"sequenceId"</span>: <span class="string">"111111111-111111"</span>,</span><br><span class="line">        <span class="string">"ipAddress"</span>: <span class="string">"100.126.1.41"</span></span><br><span class="line">    &#125;</span>,</span><br><span class="line">    <span class="string">"browser"</span>: <span class="collection">&#123;</span><br><span class="line">        <span class="string">"sequenceId"</span>: <span class="string">"111111111-111111"</span></span><br><span class="line">    &#125;</span>,..</span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure>
<p>由于Map的value有String和{}(比如geo的值是一个Map), 所以是Any类型.<br>而读取HDFS中已经存在的parquet文件的schema, 对于map的value类型都是string.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[sequence_id: <span class="built_in">string</span>, occur_time: bigint,</span><br><span class="line">  activity_map: <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">  browser_map: <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">  ......</span><br><span class="line">  year: <span class="keyword">int</span>, month: <span class="keyword">int</span>, day: <span class="keyword">int</span>]</span><br></pre></td></tr></table></figure>
<p>esRDD读取的是ES的mapping策略. 业务系统中activity的age字段为LONG  </p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"age"</span>: &#123;</span><br><span class="line">  <span class="string">"type"</span>: <span class="string">"long"</span></span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure>
<p>对应es_spark_mapping.json中的也是LONG类型:</p>
<figure class="highlight fix"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">age</span>=<span class="string">LONG,</span></span><br></pre></td></tr></table></figure>
<p>如果是使用esDF查询, 类型是long(es_spark_mapping.json):</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">|    |-- age: <span class="keyword">long</span> (nullable = <span class="keyword">true</span>)</span><br></pre></td></tr></table></figure>
<p>所以parsing方法要具体处理每一种类型.<br>TODO: 但是处理每一种类型, parsing方法在大数据量的时候很慢!</p>
<hr>
<p>parsing要做的是: 要将一个大的Map, 拆分成多个小的Map.</p>
<figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="label">geo_map:</span> &#123;</span><br><span class="line">    <span class="string">"ipCountry"</span>: <span class="string">"*"</span>,</span><br><span class="line">    <span class="string">"sequenceId"</span>: <span class="string">"111111111-111111"</span>,</span><br><span class="line">    <span class="string">"ipAddress"</span>: <span class="string">"100.126.1.41"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="label">browser_map:</span> &#123;</span><br><span class="line">    <span class="string">"sequenceId"</span>: <span class="string">"111111111-111111"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最终写成parquet文件的表结构:</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">activityDF.printSchema</span><br><span class="line">     root</span><br><span class="line">     |-- <span class="string">sequence_id:</span> string (nullable = <span class="literal">true</span>)</span><br><span class="line">     |-- <span class="string">occur_time:</span> <span class="typename">long</span> (nullable = <span class="literal">false</span>)</span><br><span class="line">     |-- <span class="string">activity_map:</span> map (nullable = <span class="literal">true</span>)</span><br><span class="line">     |    |-- <span class="string">key:</span> string</span><br><span class="line">     |    |-- <span class="string">value:</span> string (valueContainsNull = <span class="literal">true</span>)</span><br><span class="line">     |-- <span class="string">browser_map:</span> map (nullable = <span class="literal">true</span>)</span><br><span class="line">     |    |-- <span class="string">key:</span> string</span><br><span class="line">     |    |-- <span class="string">value:</span> string (valueContainsNull = <span class="literal">true</span>)</span><br><span class="line">     .....</span><br></pre></td></tr></table></figure>
<p>EOF.</p>

      
    </div>
    
  </div>
  
    
<div class="copyright">
  <p><span>本文标题:</span><a href="/2015/08/05/2015-08-05-elastic-hadoop/">ES导入数据到HDFS</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 任何忧伤,都抵不过世界的美丽 的个人博客">任何忧伤,都抵不过世界的美丽</a></p>
  <p><span>发布时间:</span>2015年08月05日 - 00时00分</p>
  <p><span>最后更新:</span>2015年12月19日 - 21时25分</p>
  <p>
    <span>原始链接:</span><a href="/2015/08/05/2015-08-05-elastic-hadoop/" title="ES导入数据到HDFS">http://github.com/zqhxuyuan/2015/08/05/2015-08-05-elastic-hadoop/</a>
    <span class="btn" data-clipboard-text="原文: http://github.com/zqhxuyuan/2015/08/05/2015-08-05-elastic-hadoop/　　作者: 任何忧伤,都抵不过世界的美丽" title="点击复制文章链接">
        <i class="fa fa-clipboard"></i>
    </span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" title="中国大陆 (CC BY-NC-SA 3.0 CN)">"署名-非商用-相同方式共享 3.0"</a> 转载请保留原文链接及作者。</p>
  <script src="/js/clipboard.min.js"></script>
  <script> var clipboard = new Clipboard('.btn'); </script>
</div>
<style type="text/css">
  .copyright p .btn {
    margin-left: 1em;
  }
  .copyright:hover p .btn::after {
    content: "复制"
  }
  .copyright p .btn:hover {
      color: gray;
      cursor: pointer;
    };
</style>



<nav id="article-nav">
  
    <div id="article-nav-newer" class="article-nav-title">
      <a href="/2015/08/25/2015-08-25-Cassandra-Architecture/">
        Apache Cassandra架构理解
      </a>
    </div>
  
  
    <div id="article-nav-older" class="article-nav-title">
      <a href="/2015/08/01/2015-08-01-ElasticSearch/">
        ElasticSearch入门
      </a>
    </div>
  
</nav>

  
</article>

<!-- 默认显示文章目录，在文章---前输入toc: false关闭目录 -->
<!-- Show TOC and tocButton in default, Hide TOC via putting "toc: false" before "---" at [post].md -->
<div id="toc" class="toc-article">
<strong class="toc-title">文章目录</strong>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#项目要求"><span class="toc-number">1.</span> <span class="toc-text">项目要求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#测试正常读取ES"><span class="toc-number">2.</span> <span class="toc-text">测试正常读取ES</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#对应用打assembly包"><span class="toc-number">2.1.</span> <span class="toc-text">对应用打assembly包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#应用不打依赖包"><span class="toc-number">2.2.</span> <span class="toc-text">应用不打依赖包</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ES2HDFS_IN_ACTION"><span class="toc-number">3.</span> <span class="toc-text">ES2HDFS IN ACTION</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#连接不到ES"><span class="toc-number">3.1.</span> <span class="toc-text">连接不到ES</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#es的空值"><span class="toc-number">3.2.</span> <span class="toc-text">es的空值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#任务运行较慢,开启了推测执行"><span class="toc-number">3.3.</span> <span class="toc-text">任务运行较慢,开启了推测执行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据文件找不到??"><span class="toc-number">3.4.</span> <span class="toc-text">数据文件找不到??</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ES没有暴露_count"><span class="toc-number">3.5.</span> <span class="toc-text">ES没有暴露_count</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#减少查询粒度"><span class="toc-number">3.6.</span> <span class="toc-text">减少查询粒度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用DF读取ES数据"><span class="toc-number">3.7.</span> <span class="toc-text">使用DF读取ES数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ElastichSearch-Hive"><span class="toc-number">3.8.</span> <span class="toc-text">ElastichSearch-Hive</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive_Load_Map_Data"><span class="toc-number">3.9.</span> <span class="toc-text">Hive Load Map Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ES_dump_json"><span class="toc-number">3.10.</span> <span class="toc-text">ES dump json</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Python_Dump_json"><span class="toc-number">3.11.</span> <span class="toc-text">Python Dump json</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Storm-ElasticSearch"><span class="toc-number">3.12.</span> <span class="toc-text">Storm-ElasticSearch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#每半个小时处理一次"><span class="toc-number">3.13.</span> <span class="toc-text">每半个小时处理一次</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#shell可以,submit不行"><span class="toc-number">3.14.</span> <span class="toc-text">shell可以,submit不行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-shell+script_file(×)"><span class="toc-number">3.15.</span> <span class="toc-text">spark-shell+script file(×)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JSON解析异常"><span class="toc-number">3.16.</span> <span class="toc-text">JSON解析异常</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#关于ES的mapping和Spark的类型解析"><span class="toc-number">3.17.</span> <span class="toc-text">关于ES的mapping和Spark的类型解析</span></a></li></ol></li></ol>
</div>
<style type="text/css">
  .left-col .switch-btn {
    display: none;
  }
  .left-col .switch-area {
    display: none;
  }
</style>

<input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">
<script type="text/javascript">
  var toc_button= document.getElementById("tocButton");
  var toc_div= document.getElementById("toc");
  /* Show or hide toc when click on tocButton.
  通过点击设置的按钮显示或者隐藏文章目录.*/
  toc_button.onclick=function(){
  if(toc_div.style.display=="none"){
  toc_div.style.display="block";
  toc_button.value="隐藏目录";
  document.getElementById("switch-btn").style.display="none";
  document.getElementById("switch-area").style.display="none";
  }
  else{
  toc_div.style.display="none";
  toc_button.value="显示目录";
  document.getElementById("switch-btn").style.display="block";
  document.getElementById("switch-area").style.display="block";
  }
  }
</script>


<div class="share">
	<div class="bdsharebuttonbox">
	<a href="#" class="bds_more" data-cmd="more"></a>
	<a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
	<a href="#" class="bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
	<a href="#" class="bds_copy" data-cmd="copy" title="复制网址"></a>
	<a href="#" class="bds_mail" data-cmd="mail" title="通过邮件分享"></a>
	<a href="#" class="bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
	</div>
	<script>
	window._bd_share_config={
		"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
	</script>
</div>







    <style type="text/css">
    #scroll {
      display: none;
    }
    </style>
    <div class="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
    </div>


  
  
    
    <div  class="post-nav-button">
    <a href="/2015/08/25/2015-08-25-Cassandra-Architecture/" title="上一篇: Apache Cassandra架构理解">
    <i class="fa fa-angle-left"></i>
    </a>
    <a href="/2015/08/01/2015-08-01-ElasticSearch/" title="下一篇: ElasticSearch入门">
    <i class="fa fa-angle-right"></i>
    </a>
    </div>
  



    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2015 任何忧伤,都抵不过世界的美丽
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的静态博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减双栏 Hexo 博客主题">Yelee</a> by MOxFIVE
        </div>
    </div>
    <div class="visit">
      <span id="busuanzi_container_site_pv" style='display:none'>
        <span id="site-visit" >本站到访数: 
        <span id="busuanzi_value_site_uv"></span>
        </span>
      </span>
      <span id="busuanzi_container_page_pv" style='display:none'>
        <span id="page-visit">, 本页阅读量: 
        <span id="busuanzi_value_page_pv"></span>
        </span>
      </span>
    </div>
  </div>
</footer>
    </div>
    

<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>

<script>
  var backgroundnum = 5;
  var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));

  $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
</script>




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
<a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
<a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>