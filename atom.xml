<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zqhxuyuan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://github.com/zqhxuyuan/"/>
  <updated>2017-02-25T07:07:06.000Z</updated>
  <id>http://github.com/zqhxuyuan/</id>
  
  <author>
    <name>任何忧伤,都抵不过世界的美丽</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kafka技术内幕</title>
    <link href="http://github.com/zqhxuyuan/2018/01/01/Kafka-Code-Index/"/>
    <id>http://github.com/zqhxuyuan/2018/01/01/Kafka-Code-Index/</id>
    <published>2017-12-31T16:00:00.000Z</published>
    <updated>2017-02-25T07:07:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>《Kafka技术内幕》<br><a id="more"></a></p>
<h2 id="Introduce">Introduce</h2><ul>
<li><a href="http://zqhxuyuan.github.io/2016/01/04/2016-01-04-Kafka-Intro/" target="_blank" rel="external">Kafka介绍</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/05/2016-01-05-Kafka-Unix/" target="_blank" rel="external">使用Unix管道解释Kafka</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/13/2016-01-13-Kafka-Picture/" target="_blank" rel="external">Kafka图文理解</a></li>
</ul>
<h2 id="源码分析汇总">源码分析汇总</h2><p>GitBook入口: <a href="https://www.gitbook.com/book/zqhxuyuan1/kafka/details" target="_blank" rel="external">https://www.gitbook.com/book/zqhxuyuan1/kafka/details</a></p>
<ul>
<li><a href="http://zqhxuyuan.github.io/2016/01/06/2016-01-06-Kafka_Producer/" target="_blank" rel="external">生产者(java)</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/07/2016-01-07-Kafka_Producer-scala/" target="_blank" rel="external">生产者(scala)</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/08/2016-01-08-Kafka_SocketServer/" target="_blank" rel="external">网络层SocketServer</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/10/2016-01-10-Kafka_LogAppend/" target="_blank" rel="external">消息存储到日志文件中</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/14/2016-01-14-Kafka-ISR/" target="_blank" rel="external">Partition的ISR工作机制</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/19/2016-01-19-Kafka-Consumer-scala/" target="_blank" rel="external">消费者初始化(scala)</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/20/2016-01-20-Kafka-Consumer-fetcher/" target="_blank" rel="external">消费者抓取流程</a></li>
</ul>
<h2 id="Kafka_Connect">Kafka Connect</h2><ul>
<li><a href="http://zqhxuyuan.github.io/2016/02/21/Kafka-Connect/" target="_blank" rel="external">使用Kafka Connect构建一个可扩展的ETL管道</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/02/21/Kafka-connect-pipeline/" target="_blank" rel="external">使用Kafka Connect构建大规模低延迟的数据管道</a></li>
</ul>
<h2 id="Kafka_Streams">Kafka Streams</h2><ul>
<li><a href="http://zqhxuyuan.github.io/2016/11/18/Kafka-CQRS-Streams/" target="_blank" rel="external">译：Kafka事件驱动和流处理</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/11/02/Kafka-Streams-cn/" target="_blank" rel="external">Kafka Streams中文翻译</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/10/29/Kafka-Interactive-Query/" target="_blank" rel="external">译：Kafka交互式查询和流处理的统一</a></li>
</ul>
<h2 id="《Kafka技术内幕》目录">《Kafka技术内幕》目录</h2><ul>
<li><a href="http://zqhxuyuan.github.io/2016/05/26/2016-05-13-Kafka-Book-Sample/" target="_blank" rel="external">样章</a>  </li>
</ul>
<ul>
<li>第一章 Kafka入门<ul>
<li>1.1 介绍<ul>
<li>1.1.1 流式数据平台</li>
<li>1.1.2 主要概念</li>
<li>1.1.3 Kafka的设计实现</li>
</ul>
</li>
<li>1.2 快速开始<ul>
<li>1.2.1 单机模式</li>
<li>1.2.2 集群模式</li>
<li>1.2.3 消费组示例</li>
</ul>
</li>
<li>1.3 环境准备<ul>
<li>1.3.1 编译运行</li>
<li>1.3.2 本书导读</li>
</ul>
</li>
</ul>
</li>
<li>第2章 生产者<ul>
<li>2.1 生产者<ul>
<li>2.1.1 同步和异步发送消息</li>
<li>2.1.2 发送线程（Sender）</li>
<li>2.1.3 选择器（Selector）</li>
<li>2.1.4 客户端网络连接对象（NetworkClient）</li>
</ul>
</li>
<li>2.2 旧的生产者（Producer）<ul>
<li>2.2.1 消息分组</li>
<li>2.2.2 为消息选择Partition</li>
<li>2.2.3 发送请求</li>
</ul>
</li>
<li>2.3 服务端网络连接（SocketServer）<ul>
<li>2.3.1 处理器（Processor）</li>
<li>2.3.2 请求通道（RequestChannel）</li>
<li>2.3.3 请求处理器（KafkaRequestHandler）</li>
<li>2.3.4 服务端请求入口（KafkaApis）</li>
</ul>
</li>
<li>2.4 本章总结</li>
</ul>
</li>
<li>第三章：消费者<ul>
<li>3.1 消费者启动和初始化<ul>
<li>3.1.1 ZooKeeper消费者连接器</li>
<li>3.1.2 Zookeeper节点</li>
<li>3.1.3 客户端线程模型</li>
<li>3.1.4 重新初始化消费者</li>
</ul>
</li>
<li>3.2 消费者Rebalancer操作<ul>
<li>3.2.1 Partition的所有权</li>
<li>3.2.2 Assignment的上下文信息</li>
<li>3.2.3 Partition分配算法</li>
<li>3.2.4 为消费者分配Partition</li>
<li>3.2.5 PartitionTopicInfo</li>
<li>3.2.6 更新和关闭Fetcher</li>
</ul>
</li>
<li>3.3 消费者拉取数据<ul>
<li>3.3.1 选择有Leader的Partition</li>
<li>3.3.2 创建拉取线程</li>
<li>3.3.3 将Partition添加到拉取线程</li>
<li>3.3.4 抽象拉取线程的工作过程</li>
<li>3.3.5 消费者拉取线程</li>
</ul>
</li>
<li>3.4 消费者消费消息<ul>
<li>3.4.1 Kafka消息流（KafkaStream）</li>
<li>3.4.2 消费者迭代消费消息</li>
</ul>
</li>
<li>3.5 消费者提交offset<ul>
<li>3.5.1 Zookeeper存储offset</li>
<li>3.5.2 以topic方式存储offset</li>
<li>3.5.3 连接OffsetManager</li>
<li>3.5.4 服务端处理offset请求</li>
</ul>
</li>
<li>3.6 消费者低级API示例<ul>
<li>3.6.1 消息消费主流程</li>
<li>3.6.2 找出Partition的Leader</li>
<li>3.6.3 获取Partition最近读取的offset</li>
<li>3.6.4 发送拉取请求并消费消息</li>
</ul>
</li>
<li>3.7 本章总结</li>
</ul>
</li>
<li>第四章 新消费者<ul>
<li>4.1 新消费者<ul>
<li>4.1.1 订阅和状态</li>
<li>4.1.2 消费者消费消息</li>
<li>4.1.3 RequestFuture</li>
<li>4.1.4 客户端轮询</li>
<li>4.1.5 延迟的任务</li>
</ul>
</li>
<li>4.2 心跳任务<ul>
<li>4.2.1 心跳发送流程</li>
<li>4.2.2 心跳监听器</li>
<li>4.2.3 心跳和协调者的关系</li>
</ul>
</li>
<li>4.3 自动提交任务<ul>
<li>4.3.1 异步提交offset</li>
<li>4.3.2 同步提交Offset</li>
</ul>
</li>
<li>4.4 本章总结</li>
</ul>
</li>
<li>第五章 协调者<ul>
<li>5.1 消费者加入消费组<ul>
<li>5.1.1 发送JoinGroup请求</li>
<li>5.1.2 JoinGroup和SyncGroup</li>
<li>5.1.3 分配Partition</li>
</ul>
</li>
<li>5.2 协调者处理消费者Join请求<ul>
<li>5.2.1 消费者和消费组元数据</li>
<li>5.2.2 协调者处理JoinGroup</li>
</ul>
</li>
<li>5.3 同步消费组<ul>
<li>5.3.1 Leader消费者分配Assignment</li>
<li>5.3.2 协调者处理SyncGroup请求</li>
<li>5.3.3 异常场景分析</li>
</ul>
</li>
<li>5.4 延迟的心跳<ul>
<li>5.4.1 协调者处理心跳</li>
<li>5.4.2 延迟操作完成时返回响应</li>
<li>5.4.3 完成和调度下一次心跳</li>
<li>5.4.4 心跳超时导致消费者失败</li>
<li>5.4.5 Heartbeat和JoinGroup的关系</li>
<li>5.4.6 平衡、状态机、心跳</li>
</ul>
</li>
<li>5.5 离开消费组</li>
<li>5.6 本章总结</li>
</ul>
</li>
<li>第六章 日志存储<ul>
<li>6.1 追加消息流程<ul>
<li>6.1.1 Kafka服务（KafkaApis）</li>
<li>6.1.2 副本管理器（ReplicaManager）</li>
<li>6.1.3 Partition</li>
</ul>
</li>
<li>6.2 Log日志文件<ul>
<li>6.2.1 消息</li>
<li>6.2.2 Segment</li>
<li>6.2.3 读取日志文件</li>
</ul>
</li>
<li>6.3 日志管理类的后台线程<ul>
<li>6.3.1 日志清理</li>
<li>6.3.2 日志压缩</li>
</ul>
</li>
<li>6.4 延迟操作<ul>
<li>6.4.1 创建延迟对象</li>
<li>6.4.2 延迟操作接口</li>
<li>6.4.3 延迟操作的缓存</li>
</ul>
</li>
<li>6.5 本章总结</li>
</ul>
</li>
<li>第七章 控制器<ul>
<li>7.1 Kafka副本原理<ul>
<li>7.1.1 负载均衡</li>
<li>7.1.2 数据同步</li>
<li>7.1.3 故障处理</li>
<li>7.1.4 控制器</li>
</ul>
</li>
<li>7.2 KafkaController<ul>
<li>7.2.1 控制器选举</li>
<li>7.2.2 控制器上下文（ControllerContext）</li>
<li>7.2.3 ZooKeeper监听器</li>
<li>7.2.4 控制器初始化</li>
<li>7.2.5 状态机</li>
<li>7.2.6 管理工作</li>
</ul>
</li>
<li>7.3 Leader和ISR请求<ul>
<li>7.3.1 ReplicaManager处理请求</li>
<li>7.3.2 Partition创建Leader和Follower副本</li>
<li>7.3.3 Leader副本</li>
<li>7.3.4 Follower副本</li>
<li>7.3.5 检查点线程（checkpoint）</li>
<li>7.3.6 LeaderAndIsr请求和协调者</li>
</ul>
</li>
<li>7.4 UpdateMetadata<ul>
<li>7.4.1 共享缓存（MetadataCache）</li>
<li>7.4.2 获取TopicMetadata</li>
</ul>
</li>
<li>7.5 本章总结</li>
</ul>
</li>
<li>第八章 Kafka高级应用<ul>
<li>8.1 消息传递语义（Message Delivery Guarantee）<ul>
<li>8.1.1 生产者的数据可靠性</li>
<li>8.1.2 消费者的消息处理语义</li>
</ul>
</li>
<li>8.2 镜像同步（MirrorMaker）<ul>
<li>8.2.1 单机模拟Mirror Maker</li>
<li>8.2.2 MirrorMaker的生产者和消费者</li>
</ul>
</li>
<li>8.3 Kafka Connect连接器<ul>
<li>8.3.1 架构与模型</li>
<li>8.3.2 单机模式</li>
<li>8.3.3 开发一个简单的Connector</li>
<li>8.3.4 分布式模式</li>
</ul>
</li>
<li>8.4 其他高级特性<ul>
<li>8.4.1 Avro序列化、反序列化</li>
<li>8.4.2 REST服务</li>
</ul>
</li>
</ul>
</li>
<li>第九章 Kafka Streams<ul>
<li>9.1 流处理的拓扑<ul>
<li>9.1.1 数据流和处理节点</li>
<li>9.1.2 构建拓扑</li>
</ul>
</li>
<li>9.2 流处理的线程模型<ul>
<li>9.2.1 流线程（StreamThread）</li>
<li>9.2.2 流任务（StreamTask）</li>
</ul>
</li>
<li>9.3 状态存储<ul>
<li>9.3.1 主要概念</li>
<li>9.3.2 备份任务（StandbyTask）</li>
<li>9.3.3 状态恢复</li>
</ul>
</li>
<li>9.4 Kafka Streams DSL<ul>
<li>9.4.1 KStream和KTable抽象接口</li>
<li>9.4.2 流转换操作</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《Kafka技术内幕》&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>开源大数据ETL工具</title>
    <link href="http://github.com/zqhxuyuan/2017/02/15/2017-02-17-BigData-ETL/"/>
    <id>http://github.com/zqhxuyuan/2017/02/15/2017-02-17-BigData-ETL/</id>
    <published>2017-02-14T16:00:00.000Z</published>
    <updated>2017-02-25T07:13:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>BigData ETL Tools<br><a id="more"></a></p>
<h2 id="datatorrent(apex)">datatorrent(apex)</h2><p>执行<code>./datatorrent-rts-community-3.7.0.bin --help</code>打印帮助项</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 <span class="operator"><span class="keyword">install</span>]$ sudo -u <span class="keyword">admin</span> ./datatorrent-rts-community-<span class="number">3.7</span><span class="number">.0</span>.<span class="keyword">bin</span> \</span><br><span class="line">-B /usr/<span class="keyword">install</span>/datatorrent-rts -<span class="keyword">g</span> <span class="number">9094</span> \</span><br><span class="line">-<span class="keyword">E</span> DT_LOG_DIR=/home/<span class="keyword">admin</span>/datatorrent \</span><br><span class="line">-<span class="keyword">E</span> DT_RUN_DIR=/home/<span class="keyword">admin</span>/run/datatorrent</span><br><span class="line"></span><br><span class="line">Verifying <span class="keyword">archive</span> integrity... All good.</span><br><span class="line">Uncompressing DataTorrent Distribution  <span class="number">100</span>%</span><br><span class="line"></span><br><span class="line">DataTorrent Platform <span class="number">3.7</span><span class="number">.0</span> will be installed <span class="keyword">under</span> /usr/<span class="keyword">install</span>/datatorrent-rts/releases/<span class="number">3.7</span><span class="number">.0</span></span><br><span class="line"></span><br><span class="line">dtGateway can be <span class="keyword">managed</span> <span class="keyword">with</span>: /usr/<span class="keyword">install</span>/datatorrent-rts/releases/<span class="number">3.7</span><span class="number">.0</span>/<span class="keyword">bin</span>/dtgateway [<span class="keyword">start</span>|<span class="keyword">stop</span>|<span class="keyword">status</span>]</span><br><span class="line">DTGateway <span class="keyword">is</span> running <span class="keyword">as</span> pid <span class="number">24571</span> <span class="keyword">and</span> listening <span class="keyword">on</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">9094</span></span><br><span class="line"></span><br><span class="line">Please <span class="keyword">finish</span> the remaining installation steps via DataTorrent Console <span class="keyword">at</span>: <span class="keyword">http</span>://dp0653:<span class="number">9094</span>/</span></span><br></pre></td></tr></table></figure>
<p>创建apex项目，并打包</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">name=salesapp</span><br><span class="line">version=3.5.0</span><br><span class="line"></span><br><span class="line">mvn -B archetype:generate \</span><br><span class="line">  -<span class="ruby"><span class="constant">DarchetypeGroupId</span>=org.apache.apex \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">DarchetypeArtifactId</span>=apex-app-archetype \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">DarchetypeVersion</span>=<span class="variable">$version</span>  \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">DgroupId</span>=com.example \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Dpackage</span>=com.example.<span class="variable">$name</span> \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">DartifactId</span>=<span class="variable">$name</span> \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Dversion</span>=<span class="number">1.0</span>-<span class="constant">SNAPSHOT</span></span></span><br></pre></td></tr></table></figure>
<p>上传到datatorrent平台</p>
<h2 id="StreamSets(https://github-com/streamsets/datacollector)">StreamSets(<a href="https://github.com/streamsets/datacollector">https://github.com/streamsets/datacollector</a>)</h2><h2 id="StreamFlow(https://github-com/lmco/streamflow)">StreamFlow(<a href="https://github.com/lmco/streamflow">https://github.com/lmco/streamflow</a>)</h2><h2 id="CDAP(https://github-com/caskdata/cdap)">CDAP(<a href="https://github.com/caskdata/cdap">https://github.com/caskdata/cdap</a>)</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;BigData ETL Tools&lt;br&gt;
    
    </summary>
    
      <category term="work" scheme="http://github.com/zqhxuyuan/categories/work/"/>
    
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>Daily Work</title>
    <link href="http://github.com/zqhxuyuan/2016/12/31/Daily/"/>
    <id>http://github.com/zqhxuyuan/2016/12/31/Daily/</id>
    <published>2016-12-30T16:00:00.000Z</published>
    <updated>2017-02-21T08:07:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>日常工作常用命令（Linux、Mac）<br><a id="more"></a></p>
<h1 id="Linux_Daily">Linux Daily</h1><h2 id="1-_rpm简单命令">1. rpm简单命令</h2><p>查看已经安装的软件 <figure class="highlight"><figcaption><span>-qa | grep mysql```  </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#24378;&#21147;&#21368;&#36733;&#36719;&#20214; ```rpm -e --nodeps mysql```  &#10;&#23433;&#35013;&#36719;&#20214; ```rpm -ivy xxx.rpm```  &#10;&#10;## 2. yum&#19979;&#36733;&#28304;</span><br></pre></td></tr></table></figure></p>
<p>cd /etc/yum.repos.d<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="blockquote">&gt; 如果是RHEL, 则/etc/yum.repos.d下没有任何源.   </span></span><br><span class="line"><span class="blockquote">&gt; 可以通过rpm -ivh epel*.rpm安装源. 安装后会在/etc/yum.repos.d下生成repl.repo文件.   </span></span><br><span class="line"><span class="blockquote">&gt; 如果是CentOS, 则有CentOS-Base.repo. 在确保虚拟机能够ping通外网, 可以直接通过wget获取文件.   </span></span><br><span class="line"></span><br><span class="line"><span class="header">### RHEL使用163源</span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="title">http:</span>//<span class="attribute">boris05.blog.51cto.com</span>/<span class="attribute">1073705</span>/<span class="attribute">1439865</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">删除redhat原有的yum源 <span class="code">```</span>rpm -aq|grep yum|xargs rpm -e --nodeps</span><br></pre></td></tr></table></figure></p>
<p>下载yum安装文件 163 6.5</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget <span class="symbol">http:</span>/<span class="regexp">/mirrors.163.com/centos</span><span class="regexp">/6.5/os</span><span class="regexp">/x86_64/</span><span class="constant">Packages</span>/yum-<span class="number">3.2</span>.<span class="number">29</span>-<span class="number">40</span>.el6.centos.noarch.rpm </span><br><span class="line">wget <span class="symbol">http:</span>/<span class="regexp">/mirrors.163.com/centos</span><span class="regexp">/6.5/os</span><span class="regexp">/x86_64/</span><span class="constant">Packages</span>/yum-metadata-parser-<span class="number">1.1</span>.<span class="number">2</span>-<span class="number">16</span>.el6.x86_64.rpm</span><br><span class="line">wget <span class="symbol">http:</span>/<span class="regexp">/mirrors.163.com/centos</span><span class="regexp">/6.5/os</span><span class="regexp">/x86_64/</span><span class="constant">Packages</span>/yum-plugin-fastestmirror-<span class="number">1.1</span>.<span class="number">30</span>-<span class="number">14</span>.el6.noarch.rpm</span><br><span class="line">wget <span class="symbol">http:</span>/<span class="regexp">/mirrors.163.com/centos</span><span class="regexp">/6.5/os</span><span class="regexp">/x86_64/</span><span class="constant">Packages</span>/python-iniparse-<span class="number">0.3</span>.<span class="number">1</span>-<span class="number">2.1</span>.el6.noarch.rpm</span><br></pre></td></tr></table></figure>
<p>进行安装yum</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh python<span class="keyword">*</span></span><br><span class="line">rpm -ivh yum<span class="keyword">*</span></span><br></pre></td></tr></table></figure>
<p>更改yum源</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/yum<span class="class">.repos</span><span class="class">.d</span>/</span><br><span class="line">vim /etc/yum<span class="class">.repos</span><span class="class">.d</span>/rhel<span class="class">.repo</span> </span><br><span class="line"></span><br><span class="line">[base]</span><br><span class="line">name=CentOS-<span class="variable">$releasever</span> - Base</span><br><span class="line">baseurl=http:<span class="comment">//mirrors.163.com/centos/6.5/os/$basearch/</span></span><br><span class="line">gpgcheck=<span class="number">1</span></span><br><span class="line">gpgkey=http:<span class="comment">//mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6</span></span><br><span class="line"><span class="id">#released</span> updates</span><br><span class="line">[updates]</span><br><span class="line">name=CentOS-<span class="variable">$releasever</span> - Updates</span><br><span class="line">baseurl=http:<span class="comment">//mirrors.163.com/centos/6.5/updates/$basearch/</span></span><br><span class="line">gpgcheck=<span class="number">1</span></span><br><span class="line">gpgkey=http:<span class="comment">//mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6</span></span><br><span class="line"><span class="id">#packages</span> used/produced <span class="keyword">in</span> the build but not released</span><br><span class="line">#[addons]</span><br><span class="line">#name=CentOS-<span class="variable">$releasever</span> - Addons</span><br><span class="line">#baseurl=http:<span class="comment">//mirrors.163.com/centos/$releasever/addons/$basearch/</span></span><br><span class="line">#gpgcheck=<span class="number">1</span></span><br><span class="line">#gpgkey=http:<span class="comment">//mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6</span></span><br><span class="line"><span class="hexcolor">#add</span>itional packages that may be useful</span><br><span class="line">[extras]</span><br><span class="line">name=CentOS-<span class="variable">$releasever</span> - Extras</span><br><span class="line">baseurl=http:<span class="comment">//mirrors.163.com/centos/6.5/extras/$basearch/</span></span><br><span class="line">gpgcheck=<span class="number">1</span></span><br><span class="line">gpgkey=http:<span class="comment">//mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6</span></span><br><span class="line"><span class="hexcolor">#add</span>itional packages that extend functionality of existing packages</span><br><span class="line">[centosplus]</span><br><span class="line">name=CentOS-<span class="variable">$releasever</span> - Plus</span><br><span class="line">baseurl=http:<span class="comment">//mirrors.163.com/centos/6.5/centosplus/$basearch/</span></span><br><span class="line">gpgcheck=<span class="number">1</span></span><br><span class="line">enabled=<span class="number">0</span></span><br><span class="line">gpgkey=http:<span class="comment">//mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6</span></span><br><span class="line"></span><br><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br><span class="line">yum update</span><br></pre></td></tr></table></figure>
<h3 id="EPEL-7_下载源">EPEL-7 下载源</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># wget http:<span class="comment">//dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-1.noarch.rpm </span></span></span><br><span class="line"><span class="preprocessor"># rpm -ivh epel-release-<span class="number">7</span>-<span class="number">1.</span>noarch.rpm </span></span><br><span class="line"><span class="preprocessor"># rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-<span class="number">7</span></span></span><br></pre></td></tr></table></figure>
<h3 id="EPEL-6_下载源">EPEL-6 下载源</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># wget http:<span class="comment">//dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm   </span></span></span><br><span class="line"><span class="preprocessor"># rpm -ivh epel-release-<span class="number">6</span>-<span class="number">8.</span>noarch.rpm </span></span><br><span class="line"><span class="preprocessor"># rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-<span class="number">6</span></span></span><br></pre></td></tr></table></figure>
<p>注意如果是CentOS, 则最后的rpm –import要修改成CentOS-6 </p>
<h3 id="CentOS使用阿里云源">CentOS使用阿里云源</h3><h3 id="CentOS-163源">CentOS-163源</h3><p>对于CentOS, /etc/yum.repos.d下有CentOS-Base.repo, 可以直接用别的源替换默认的, 或者新增加源. </p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># wget http:<span class="comment">//mirrors.163.com/.help/CentOS6-Base-163.repo </span></span><br><span class="line"></span><br><span class="line">mv /etc/yum.repos.<span class="keyword">d</span>/CentOS-Base.repo /etc/yum.repos.<span class="keyword">d</span>/CentOS-Base.repo.backup</span><br><span class="line">wget -O /etc/yum.repos.<span class="keyword">d</span>/CentOS-Base.repo http:<span class="comment">//mirrors.aliyun.com/repo/Centos-6.repo</span></span><br><span class="line">mv /etc/yum.repos.<span class="keyword">d</span>/epel.repo /etc/yum.repos.<span class="keyword">d</span>/epel.repo.backup</span><br><span class="line">mv /etc/yum.repos.<span class="keyword">d</span>/epel-testing.repo /etc/yum.repos.<span class="keyword">d</span>/epel-testing.repo.backup</span><br><span class="line">wget -O /etc/yum.repos.<span class="keyword">d</span>/epel.repo http:<span class="comment">//mirrors.aliyun.com/repo/epel-6.repo</span></span><br><span class="line">yum makecache</span><br></pre></td></tr></table></figure>
<p>更新源 </p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># yum clean all </span></span><br><span class="line"><span class="preprocessor"># yum makecache </span></span><br><span class="line"><span class="preprocessor"># yum update </span></span><br><span class="line"><span class="preprocessor"># yum repolist</span></span><br></pre></td></tr></table></figure>
<h2 id="3-_安装基本软件">3. 安装基本软件</h2><p><strong>1. 确保能够上网, 并且yum repolist有数据, 比如先下个163的源. 当服务器稳定之后, 可以禁用源(文件后缀改下即可)</strong></p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># yum install wget</span></span><br></pre></td></tr></table></figure>
<p><strong>2.英文环境 </strong></p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># vi ~/.bashrc </span></span><br><span class="line">export LANG=en_US.UTF-<span class="number">8</span></span><br><span class="line"><span class="preprocessor"># source ~/.bashrc </span></span><br><span class="line"><span class="preprocessor"># vi /etc/sysconfig/i18n </span></span><br><span class="line">LANG=<span class="string">"en_US.UTF-8"</span></span><br></pre></td></tr></table></figure>
<p><strong>3. 安装gcc, git等</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># yum group <span class="operator"><span class="keyword">install</span> <span class="string">"Development Tools"</span>     <span class="comment">--&gt; CentOS6要使用yum groupinstall中间没有空格!</span></span><br><span class="line"># yum grouplist </span><br><span class="line">Loaded plugins: fastestmirror, product-<span class="keyword">id</span>, subscription-manager </span><br><span class="line">This <span class="keyword">system</span> <span class="keyword">is</span> <span class="keyword">not</span> registered <span class="keyword">to</span> Red Hat Subscription <span class="keyword">Management</span>. You can <span class="keyword">use</span> subscription-manager <span class="keyword">to</span> <span class="keyword">register</span>. </span><br><span class="line">Setting up <span class="keyword">Group</span> Process </span><br><span class="line">Loading mirror speeds <span class="keyword">from</span> cached hostfile </span><br><span class="line">Installed <span class="keyword">Groups</span>: </span><br><span class="line">   Console internet tools </span><br><span class="line">   Development tools </span><br><span class="line">   <span class="keyword">E</span>-mail <span class="keyword">server</span> </span><br><span class="line">   Perl Support </span><br><span class="line">   <span class="keyword">Security</span> Tools</span></span><br></pre></td></tr></table></figure>
<p>这里已经安装了Development tools, 所以会显示在Installed Groups里.  注意不是yum group list<br>如果没有安装, 会显示在Available Groups里.  上一步的英文环境很重要, 否则如果是中文环境, 你就不知道要安装哪个组了. </p>
<p><strong>4.yum安装MySQL客户端和服务器</strong> </p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># yum info mysql </span><br><span class="line"># yum <span class="keyword">list</span> | <span class="keyword">grep</span> mysql </span><br><span class="line"># yum groupinfo <span class="string">"MySQL Database server"</span> </span><br><span class="line"># yum groupinfo <span class="string">"MySQL Database client"</span> </span><br><span class="line"> Mandatory Package<span class="variable">s:</span> </span><br><span class="line">   mysql </span><br><span class="line"> Default Package<span class="variable">s:</span> </span><br><span class="line">   MySQL-<span class="keyword">python</span> </span><br><span class="line">   mysql-connector-odbc </span><br><span class="line"> Optional Package<span class="variable">s:</span> </span><br><span class="line">   libdbi-dbd-mysql </span><br><span class="line">   mysql-connector-java </span><br><span class="line">   <span class="keyword">perl</span>-DBD-MySQL</span><br></pre></td></tr></table></figure>
<p>当然也可以单独一个一个安装. 同样要注意在/etc/yum.repos.d中要存在163, 或者epel等源. 否则如果没有源, 安装任何软件都没有数据.</p>
<p><a href="http://www.cnblogs.com/xiaoluo501395377/archive/2013/04/07/3003278.html" target="_blank" rel="external">http://www.cnblogs.com/xiaoluo501395377/archive/2013/04/07/3003278.html</a>  </p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># yum install mysql mysql-server mysql-devel </span></span><br><span class="line"><span class="preprocessor"># service mysqld start </span></span><br><span class="line"><span class="preprocessor"># netstat -anpt | grep 3306 </span></span><br><span class="line"><span class="preprocessor"># chkconfig --add mysqld </span></span><br><span class="line"><span class="preprocessor"># chkconfig --list | grep mysqld </span></span><br><span class="line"><span class="preprocessor"># chkconfig mysqld on </span></span><br><span class="line"><span class="preprocessor"># mysqladmin -u root password 'root' </span></span><br><span class="line"><span class="preprocessor"># mysql -u root -p </span></span><br><span class="line">&gt; show databases;</span><br></pre></td></tr></table></figure>
<p><strong>5. nginx</strong></p>
<p>源码安装方式: <a href="http://network810.blog.51cto.com/2212549/1264669" target="_blank" rel="external">http://network810.blog.51cto.com/2212549/1264669</a><br>yum源安装:  nginx默认不在源里. 需要自己去nginx官网下载repo文件. 下载完后可以删除或者备份. </p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="constant">CentOS</span> </span><br><span class="line"><span class="comment"># vi /etc/yum.repos.d/nginx.repo </span></span><br><span class="line">[nginx]</span><br><span class="line">name=nginx repo</span><br><span class="line">baseurl=<span class="symbol">http:</span>/<span class="regexp">/nginx.org/packages</span><span class="regexp">/centos/</span><span class="variable">$releasever</span>/<span class="variable">$basearch</span>/</span><br><span class="line">gpgcheck=<span class="number">0</span></span><br><span class="line">enabled=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="constant">RHEL</span></span><br><span class="line"><span class="comment"># vi /etc/yum.repos.d/nginx.repo </span></span><br><span class="line">[nginx]</span><br><span class="line">name=nginx repo</span><br><span class="line">baseurl=<span class="symbol">http:</span>/<span class="regexp">/nginx.org/packages</span><span class="regexp">/rhel/</span><span class="variable">$releasever</span>/<span class="variable">$basearch</span>/</span><br><span class="line">gpgcheck=<span class="number">0</span></span><br><span class="line">enabled=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># yum install nginx</span></span><br></pre></td></tr></table></figure>
<p>如果服务器开启防火墙, 则要开放80端口 </p>
<figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi /etc/sysconfig/iptables </span></span><br><span class="line">-A INPUT -m <span class="keyword">state</span> --state NEW -m    tcp -p tcp --dport <span class="number">80</span> -j ACCEPT</span><br></pre></td></tr></table></figure>
<p>重启防火墙 </p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># service iptables restart</span></span><br></pre></td></tr></table></figure>
<p>启动nginx方法. 显然第一种方法最快 </p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># service nginx start </span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># cd /usr/local/nginx/sbin </span></span><br><span class="line"><span class="preprocessor"># ./nginx </span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf</span></span><br></pre></td></tr></table></figure>
<p>同mysqld加入到chkconfig的方式开机启动, 也可以把nginx加入开机启动项中.<br>如果提示没有nginx这个service, 参考 <a href="http://www.01happy.com/centos-nginx-shell-chkconfig" target="_blank" rel="external">http://www.01happy.com/centos-nginx-shell-chkconfig</a><br>可以在主机的浏览器上查看, 或者查看端口号80是否开启   </p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># netstat –na|grep 80 </span></span><br><span class="line"><span class="preprocessor"># ps -ef | grep nginx</span></span><br></pre></td></tr></table></figure>
<p>默认配置</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">user</span>  nginx;</span><br><span class="line"><span class="title">worker_processes</span>  <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="title">error_log</span>  /var/log/nginx/error.log <span class="built_in">warn</span>;</span><br><span class="line"><span class="title">pid</span>        /var/run/nginx.pid;</span><br><span class="line"></span><br><span class="line"><span class="title">events</span> &#123;</span><br><span class="line">    <span class="title">worker_connections</span>  <span class="number">1024</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="title">http</span> &#123;</span><br><span class="line">    <span class="title">include</span>       /etc/nginx/mime.types;</span><br><span class="line">    <span class="title">default_type</span>  application/octet-stream;</span><br><span class="line"></span><br><span class="line">    <span class="title">log_format</span>  main  <span class="string">'<span class="variable">$remote_addr</span> - <span class="variable">$remote_user</span> [<span class="variable">$time_local</span>] "<span class="variable">$request</span>" '</span></span><br><span class="line">                      <span class="string">'<span class="variable">$status</span> <span class="variable">$body_bytes_sent</span> "<span class="variable">$http_referer</span>" '</span></span><br><span class="line">                      <span class="string">'"<span class="variable">$http_user_agent</span>" "<span class="variable">$http_x_forwarded_for</span>"'</span>;</span><br><span class="line">    <span class="title">access_log</span>  /var/log/nginx/access.log  main;</span><br><span class="line">    <span class="title">sendfile</span>        <span class="built_in">on</span>;</span><br><span class="line">    <span class="title">keepalive_timeout</span>  <span class="number">65</span>;</span><br><span class="line">    <span class="title">include</span> /etc/nginx/conf.d/<span class="regexp">*.conf</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">user  www www;</span><br><span class="line">worker_processes  <span class="number">2</span>;</span><br><span class="line">error_<span class="built_in">log</span>  logs/error.log;</span><br><span class="line">pid        logs/nginx.pid;</span><br><span class="line">events &#123;</span><br><span class="line">    use epoll;</span><br><span class="line">    worker_connections  <span class="number">2048</span>;</span><br><span class="line">&#125;</span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_<span class="built_in">type</span>  application/octet-stream;</span><br><span class="line">    sendfile        on;</span><br><span class="line">    keepalive_timeout  <span class="number">65</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># gzip压缩功能设置</span></span><br><span class="line">    gzip on;</span><br><span class="line">    gzip_min_length <span class="number">1</span>k;</span><br><span class="line">    gzip_buffers    <span class="number">4</span> <span class="number">16</span>k;</span><br><span class="line">    gzip_http_version <span class="number">1.0</span>;</span><br><span class="line">    gzip_comp_level <span class="number">6</span>;</span><br><span class="line">    gzip_types text/html text/plain text/css text/javascript application/json application/javascript application/x-javascript application/xml;</span><br><span class="line">    gzip_vary on;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># http_proxy 设置</span></span><br><span class="line">    client_max_body_size   <span class="number">10</span>m;</span><br><span class="line">    client_body_buffer_size   <span class="number">128</span>k;</span><br><span class="line">    proxy_connect_timeout   <span class="number">75</span>;</span><br><span class="line">    proxy_send_timeout   <span class="number">75</span>;</span><br><span class="line">    proxy_<span class="built_in">read</span>_timeout   <span class="number">75</span>;</span><br><span class="line">    proxy_buffer_size   <span class="number">4</span>k;</span><br><span class="line">    proxy_buffers   <span class="number">4</span> <span class="number">32</span>k;</span><br><span class="line">    proxy_busy_buffers_size   <span class="number">64</span>k;</span><br><span class="line">    proxy_temp_file_write_size  <span class="number">64</span>k;</span><br><span class="line">    proxy_temp_path   /usr/<span class="built_in">local</span>/nginx/proxy_temp <span class="number">1</span> <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设定负载均衡后台服务器列表 </span></span><br><span class="line">    upstream  backend  &#123; </span><br><span class="line">        server   <span class="number">192.168</span>.<span class="number">10.100</span>:<span class="number">8080</span> max_fails=<span class="number">2</span> fail_timeout=<span class="number">30</span>s ;  </span><br><span class="line">        server   <span class="number">192.168</span>.<span class="number">10.101</span>:<span class="number">8080</span> max_fails=<span class="number">2</span> fail_timeout=<span class="number">30</span>s ;  </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 很重要的虚拟主机配置</span></span><br><span class="line">    server &#123;</span><br><span class="line">        listen       <span class="number">80</span>;</span><br><span class="line">        server_name  itoatest.example.com;</span><br><span class="line">        root   /apps/oaapp;</span><br><span class="line">        charset utf-<span class="number">8</span>;</span><br><span class="line">        access_<span class="built_in">log</span>  logs/host.access.log  main;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#对 / 所有做负载均衡+反向代理</span></span><br><span class="line">        location / &#123;</span><br><span class="line">            root   /apps/oaapp;</span><br><span class="line">            index  index.jsp index.html index.htm;</span><br><span class="line"></span><br><span class="line">            proxy_pass        http://backend;  </span><br><span class="line">            proxy_redirect off;</span><br><span class="line">            <span class="comment"># 后端的Web服务器可以通过X-Forwarded-For获取用户真实IP</span></span><br><span class="line">            proxy_<span class="built_in">set</span>_header  Host  <span class="variable">$host</span>;</span><br><span class="line">            proxy_<span class="built_in">set</span>_header  X-Real-IP  <span class="variable">$remote_addr</span>;  </span><br><span class="line">            proxy_<span class="built_in">set</span>_header  X-Forwarded-For  <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#静态文件，nginx自己处理，不去backend请求tomcat</span></span><br><span class="line">        location  ~* /download/ &#123;  </span><br><span class="line">            root /apps/oa/fs;  </span><br><span class="line">        &#125;</span><br><span class="line">        location ~ .*\.(gif|jpg|jpeg|bmp|png|ico|txt|js|css)$   </span><br><span class="line">        &#123;   </span><br><span class="line">            root /apps/oaapp;   </span><br><span class="line">            expires      <span class="number">7</span>d; </span><br><span class="line">        &#125;</span><br><span class="line">        location /nginx_status &#123;</span><br><span class="line">            stub_status on;</span><br><span class="line">            access_<span class="built_in">log</span> off;</span><br><span class="line">            allow <span class="number">192.168</span>.<span class="number">10.0</span>/<span class="number">24</span>;</span><br><span class="line">            deny all;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location ~ ^/(WEB-INF)/ &#123;   </span><br><span class="line">            deny all;   </span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        error_page   <span class="number">500</span> <span class="number">502</span> <span class="number">503</span> <span class="number">504</span>  /<span class="number">50</span>x.html;</span><br><span class="line">        location = /<span class="number">50</span>x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">## 其它虚拟主机，server 指令开始</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>6. scp文件拷贝</strong></p>
<p>在主机中要拷贝文件到虚拟机中 </p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[主机]<span class="comment"># scp xxx root<span class="doctag">@h</span>101:~/ </span></span><br><span class="line"><span class="symbol">bash:</span> <span class="symbol">scp:</span> command <span class="keyword">not</span> found </span><br><span class="line">lost connection </span><br><span class="line">[虚机]<span class="comment"># yum install openssh-clients</span></span><br></pre></td></tr></table></figure>
<p><strong>7. http虚拟机yum源</strong></p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># yum install -y httpd</span></span><br></pre></td></tr></table></figure>
<p>将主机上的iso文件拷贝到虚拟机中 </p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[主机]</span><span class="comment"># scp **.iso root@h101:~/ </span></span><br><span class="line"><span class="title">[虚机]</span><span class="comment"># mount -o loop CentOS*.iso /var/www/html</span></span><br></pre></td></tr></table></figure>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi /etc/yum.repos.d/http-local.repo </span></span><br><span class="line"><span class="title">[http-local]</span></span><br><span class="line"><span class="setting">name=<span class="value">http-local-<span class="keyword">on</span></span></span></span><br><span class="line"><span class="setting">baseurl=<span class="value">http://<span class="number">192.168</span>.<span class="number">56.101</span>/CentOS_6.<span class="number">5</span>_Final</span></span></span><br><span class="line"><span class="setting">enabled=<span class="value"><span class="number">1</span></span></span></span><br><span class="line"><span class="setting">gpgcheck=<span class="value"><span class="number">1</span></span></span></span><br><span class="line"><span class="setting">gpgkey=<span class="value">file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-<span class="number">6</span></span></span></span><br></pre></td></tr></table></figure>
<p><strong>8. http主机yum源</strong></p>
<p>上面的方式会将iso文件拷贝到虚拟机中, 占用本来空间就不多的虚拟机.<br>可以不用这种方式, 而是在主机中安装httpd服务器(或者nginx), 在虚拟机中直接能访问也可以.   </p>
<p><strong>9. 虚拟机ftp客户端</strong></p>
<p>◇ 主机中开启vsftpd服务, 虚拟机中安装ftp客户端, </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># yum install ftp </span></span><br><span class="line"><span class="preprocessor"># ftp <span class="number">192.168</span><span class="number">.56</span><span class="number">.1</span> </span></span><br><span class="line">Connected to <span class="number">192.168</span><span class="number">.56</span><span class="number">.1</span> (<span class="number">192.168</span><span class="number">.56</span><span class="number">.1</span>). </span><br><span class="line"><span class="number">220</span> (vsFTPd <span class="number">3.0</span><span class="number">.2</span>) </span><br><span class="line">Name (<span class="number">192.168</span><span class="number">.56</span><span class="number">.1</span>:root): anonymous </span><br><span class="line"><span class="number">331</span> Please specify the password. </span><br><span class="line">Password: </span><br><span class="line"><span class="number">230</span> Login successful. </span><br><span class="line">Remote system type is UNIX. </span><br><span class="line">Using binary mode to transfer files. </span><br><span class="line">ftp&gt;ls      ftp服务器当前位置的文件列表</span><br><span class="line">ftp&gt;!Ls     ftp客户端当前位置的文件列表</span><br></pre></td></tr></table></figure>
<p><strong>10. SVN</strong></p>
<p>1). 查看服务器系统是否已经安装SVN</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@datanode01 svn]<span class="preprocessor"># svn --version</span></span><br><span class="line">svn，版本 <span class="number">1.6</span><span class="number">.11</span> (r934486)   编译于 Apr <span class="number">12</span> <span class="number">2012</span>，<span class="number">11</span>:<span class="number">09</span>:<span class="number">11</span></span><br><span class="line">可使用以下的版本库访问模块: </span><br><span class="line">* ra_neon : 通过 WebDAV 协议使用 neon 访问版本库的模块。</span><br><span class="line">  - 处理“http”方案</span><br><span class="line">  - 处理“https”方案</span><br><span class="line">* ra_svn : 使用 svn 网络协议访问版本库的模块。  - 使用 Cyrus SASL 认证</span><br><span class="line">  - 处理“svn”方案</span><br><span class="line">* ra_local : 访问本地磁盘的版本库模块。</span><br><span class="line">  - 处理“file”方案</span><br></pre></td></tr></table></figure>
<p>说明svn已经安装, 并且支持http访问方式. </p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@datanode01 svn]# whereis httpd</span><br><span class="line">httpd: <span class="regexp">/usr/</span>sbin<span class="regexp">/httpd.event /u</span>sr<span class="regexp">/sbin/</span>httpd <span class="regexp">/usr/</span>sbin<span class="regexp">/httpd.worker /</span>etc<span class="regexp">/httpd /u</span>sr<span class="regexp">/lib64/</span>httpd <span class="regexp">/usr/i</span>nclude<span class="regexp">/httpd /u</span>sr<span class="regexp">/share/m</span>an<span class="regexp">/man8/</span>httpd.<span class="number">8</span>.gz</span><br><span class="line">[root@datanode01 svn]# whereis svn</span><br><span class="line">svn: <span class="regexp">/usr/</span>bin<span class="regexp">/svn /u</span>sr<span class="regexp">/share/m</span>an<span class="regexp">/man1/</span>svn.<span class="number">1</span>.gz</span><br><span class="line">[root@datanode01 svn]# whereis svnserve</span><br><span class="line">svnserve: <span class="regexp">/usr/</span>bin<span class="regexp">/svnserve /u</span>sr<span class="regexp">/share/m</span>an<span class="regexp">/man8/</span>svnserve.<span class="number">8</span>.gz</span><br></pre></td></tr></table></figure>
<p>2). 创建版本仓库  </p>
<p>◆ 新建一个目录用于存储SVN所有文件<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">mkdir</span> /<span class="typedef"><span class="keyword">data</span>/data8/svn</span></span><br></pre></td></tr></table></figure></p>
<p>◆ 新建一个版本仓库<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">svnadmin create <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span><span class="keyword">project</span></span><br></pre></td></tr></table></figure></p>
<p>◆ 修改 vi /data/data8/svn/project/conf/passwd 添加用户  </p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[users]</span>  </span><br><span class="line"><span class="comment"># harry = harryssecret</span></span><br><span class="line"><span class="comment"># sally = sallyssecret</span></span><br><span class="line"><span class="setting">admin = <span class="value">admin123</span></span></span><br><span class="line"><span class="setting">zhengqh = <span class="value">zhengqh</span></span></span><br></pre></td></tr></table></figure>
<p>◆ 修改 vi /data/data8/svn/project/conf/authz 修改用户访问策略  </p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[groups]</span><br><span class="line"><span class="preprocessor"># harry_and_sally = harry,sally</span></span><br><span class="line"><span class="preprocessor"># harry_sally_and_joe = harry,sally,&amp;joe</span></span><br><span class="line">group1 = admin,zhengqh</span><br><span class="line"></span><br><span class="line">[/]</span><br><span class="line">@group1 = rw</span><br></pre></td></tr></table></figure>
<p>◆ 修改 vi /data/data8/svn/project/conf/svnserve.conf文件,让用户和策略配置升效.  </p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[general]  </span><br><span class="line">anon-access = none</span><br><span class="line">auth-access = <span class="keyword">write</span></span><br><span class="line">password-db = <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span><span class="keyword">project</span><span class="regexp">/conf/</span>passwd</span><br><span class="line">authz-db = <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span><span class="keyword">project</span><span class="regexp">/conf/</span>authz</span><br></pre></td></tr></table></figure>
<p>◆ 启动服务器<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">svnserve</span> -d -r /<span class="typedef"><span class="keyword">data</span>/data8/svn</span></span><br></pre></td></tr></table></figure></p>
<p>◆ 测试checkout代码库</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~</span><br><span class="line">svn co <span class="string">svn:</span><span class="comment">//172.17.212.69/project</span></span><br></pre></td></tr></table></figure>
<p>显示如下就表示成功了:</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Authentication <span class="string">realm:</span> &lt;<span class="string">svn:</span><span class="comment">//172.17.212.69:3690&gt; e296f93b-eec2-43dd-92b5-cc10ee55c901</span></span><br><span class="line">Password <span class="keyword">for</span> <span class="string">'root'</span>:</span><br><span class="line">Authentication <span class="string">realm:</span> &lt;<span class="string">svn:</span><span class="comment">//172.17.212.69:3690&gt; e296f93b-eec2-43dd-92b5-cc10ee55c901</span></span><br><span class="line"><span class="string">Username:</span> admin</span><br><span class="line">Password <span class="keyword">for</span> <span class="string">'admin'</span>: ***</span><br></pre></td></tr></table></figure>
<p>3). 配置支持使用http访问</p>
<p>◆ 创建svn帐号或修改密码：</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/usr/<span class="keyword">bin/htpasswd </span>-<span class="keyword">b </span>-c /<span class="preprocessor">data</span>/data8/svn/svn-auth-file admin admin123</span><br><span class="line">/usr/<span class="keyword">bin/htpasswd </span>-<span class="keyword">b </span>/<span class="preprocessor">data</span>/data8/svn/svn-auth-file zhengqh zhengqh</span><br></pre></td></tr></table></figure>
<p>-c表示不存在这个文件时创建它. 注意第一行加-c, 后面添加用户不能加-c, 否则会发生覆盖.</p>
<p>◆ 修改svn用户访问策略 </p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /<span class="type">data</span>/data8/svn/svn-<span class="keyword">access</span>-<span class="keyword">file</span></span><br><span class="line">[project:/]</span><br><span class="line">admin = rw</span><br><span class="line">zhengqh = rw</span><br></pre></td></tr></table></figure>
<p>◆ 修改svn目录权限<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod -R <span class="number">777</span> <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span><span class="keyword">project</span></span><br></pre></td></tr></table></figure></p>
<p>◆ 查看svn和httpd依赖的文件:</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">cd</span> /etc/httpd/modules</span><br><span class="line"># <span class="keyword">ls</span> | <span class="keyword">grep</span> svn</span><br><span class="line">mod_authz_svn.<span class="keyword">so</span></span><br><span class="line">mod_dav_svn.<span class="keyword">so</span></span><br></pre></td></tr></table></figure>
<p>◆ 修改 vi /etc/httpd/conf/httpd.conf 增加</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">LoadModule dav_svn_module     <span class="regexp">/etc/</span>httpd<span class="regexp">/modules/</span>mod_dav_svn.so</span><br><span class="line">LoadModule authz_svn_module   <span class="regexp">/etc/</span>httpd<span class="regexp">/modules/</span>mod_authz_svn.so</span><br><span class="line"></span><br><span class="line">&lt;Location /svn&gt;</span><br><span class="line">    DAV svn</span><br><span class="line">    SVNParentPath <span class="regexp">/data/</span>data8/svn</span><br><span class="line">    AuthType Basic</span><br><span class="line">    AuthName <span class="string">"Subversion repository"</span></span><br><span class="line">    AuthUserFile <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span>svn-auth-file</span><br><span class="line">    Require valid-user</span><br><span class="line">    AuthzSVNAccessFile <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span>svn-access-file</span><br><span class="line">&lt;/Location&gt;</span><br></pre></td></tr></table></figure>
<p>◆ 启动apache httpd服务</p>
<p>下面几个命令只要执行其中一个即可(最后一个/usr/local一般用于自定义安装httpd才使用)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># /usr/sbin/apachectl <span class="operator"><span class="keyword">start</span></span><br><span class="line"># /etc/init.<span class="keyword">d</span>/httpd <span class="keyword">start</span></span><br><span class="line"># /usr/<span class="keyword">local</span>/apache2/<span class="keyword">bin</span>/apachectl <span class="keyword">start</span></span></span><br></pre></td></tr></table></figure>
<p>◆ 重启svn服务</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># ps -ef | grep svn</span></span><br><span class="line">root     <span class="number">41348</span>     <span class="number">1</span>  <span class="number">0</span> <span class="number">11</span>:<span class="number">22</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> svnserve -d -r /data/data8/svn</span><br><span class="line">root     <span class="number">41971</span> <span class="number">40812</span>  <span class="number">0</span> <span class="number">11</span>:<span class="number">39</span> pts/<span class="number">2</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep svn</span><br><span class="line"><span class="preprocessor"># kill -<span class="number">9</span>  <span class="number">41348</span></span></span><br><span class="line"><span class="preprocessor"># svnserve -d -r /data/data8/svn</span></span><br></pre></td></tr></table></figure>
<p>4). 用户维护</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span>project<span class="regexp">/conf/</span>passwd</span><br><span class="line">添加用户名 = 密码</span><br><span class="line">vi <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span>project<span class="regexp">/conf/</span>authz</span><br><span class="line">添加用户名到group1中</span><br><span class="line">执行命令:   <span class="regexp">/usr/</span>bin<span class="regexp">/htpasswd -b /</span>data<span class="regexp">/data8/</span>svn/svn-auth-file 用户名 密码</span><br><span class="line">添加用户的访问策略</span><br><span class="line">vi <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span>svn-access-file</span><br><span class="line">用户名 = rw</span><br></pre></td></tr></table></figure>
<h2 id="4-_磁盘">4. 磁盘</h2><h3 id="查看文件大小">查看文件大小</h3><p><code>ll -h</code>只能查看文件的大小. 不能查看文件夹占用的大小. </p>
<p>查看某个目录总的大小</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> <span class="keyword">dir</span></span><br><span class="line">du -<span class="keyword">h</span> ./</span><br><span class="line">du -<span class="keyword">sh</span> *</span><br></pre></td></tr></table></figure>
<p>在最后会列出这个文件夹占用的大小.  或者不用cd, 直接du -h dir</p>
<h3 id="扩容操作">扩容操作</h3><p>查看磁盘使用量: <code>df -mh</code><br>根目录/达到了100% :  /dev/mapper/vg_datanode01-LogVol00</p>
<p>查看卷: vgdisplan:  </p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="comment">Volume</span> <span class="comment">group</span> <span class="literal">-</span><span class="literal">-</span><span class="literal">-</span></span><br><span class="line"><span class="comment">VG</span> <span class="comment">Name</span>    <span class="comment">vg_datanode01</span></span><br><span class="line"></span><br><span class="line"><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">Logical</span> <span class="comment">volume</span> <span class="literal">-</span><span class="literal">-</span><span class="literal">-</span></span><br><span class="line"><span class="comment">LV</span> <span class="comment">Path</span>      <span class="comment">/dev/vg_datanode01/LogVol00</span></span><br></pre></td></tr></table></figure>
<p>扩容: </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lvextend -L +<span class="number">10</span>G  <span class="regexp">/dev/</span>mapper/vg_datanode01-LogVol00</span><br><span class="line">resize2fs <span class="regexp">/dev/</span>mapper/vg_datanode01-LogVol00</span><br></pre></td></tr></table></figure>
<h2 id="5-_系统">5. 系统</h2><h3 id="文件数和进程数">文件数和进程数</h3><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ ulimit -a</span><br><span class="line">core file size          <span class="list">(<span class="keyword">blocks</span>, -c)</span> <span class="number">0</span></span><br><span class="line">data seg size           <span class="list">(<span class="keyword">kbytes</span>, -d)</span> unlimited</span><br><span class="line">scheduling priority             <span class="list">(<span class="keyword">-e</span>)</span> <span class="number">0</span></span><br><span class="line">file size               <span class="list">(<span class="keyword">blocks</span>, -f)</span> unlimited</span><br><span class="line">pending signals                 <span class="list">(<span class="keyword">-i</span>)</span> <span class="number">62700</span></span><br><span class="line">max locked memory       <span class="list">(<span class="keyword">kbytes</span>, -l)</span> <span class="number">64</span></span><br><span class="line">max memory size         <span class="list">(<span class="keyword">kbytes</span>, -m)</span> unlimited</span><br><span class="line">open files                      <span class="list">(<span class="keyword">-n</span>)</span> <span class="number">1024</span></span><br><span class="line">pipe size            <span class="list">(<span class="keyword">512</span> bytes, -p)</span> <span class="number">8</span></span><br><span class="line">POSIX message queues     <span class="list">(<span class="keyword">bytes</span>, -q)</span> <span class="number">819200</span></span><br><span class="line">real-time priority              <span class="list">(<span class="keyword">-r</span>)</span> <span class="number">0</span></span><br><span class="line">stack size              <span class="list">(<span class="keyword">kbytes</span>, -s)</span> <span class="number">10240</span></span><br><span class="line">cpu time               <span class="list">(<span class="keyword">seconds</span>, -t)</span> unlimited</span><br><span class="line">max user processes              <span class="list">(<span class="keyword">-u</span>)</span> <span class="number">1024</span></span><br><span class="line">virtual memory          <span class="list">(<span class="keyword">kbytes</span>, -v)</span> unlimited</span><br><span class="line">file locks                      <span class="list">(<span class="keyword">-x</span>)</span> unlimited</span><br><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ ps -ef | grep ETL</span><br><span class="line">midd     <span class="number">14369</span>     <span class="number">1</span>  <span class="number">0</span> Mar29 ?        <span class="number">00</span>:00:00 ETL_ScheduleCenter     </span><br><span class="line">midd     <span class="number">14370</span> <span class="number">14369</span> <span class="number">99</span> Mar29 ?        <span class="number">5</span><span class="number">-06</span>:14:46 ETL_ScheduleServer     </span><br><span class="line">midd     <span class="number">14442</span> <span class="number">14369</span>  <span class="number">6</span> Mar29 ?        <span class="number">02</span>:33:41 ETL_ServerManger       </span><br><span class="line">midd     <span class="number">41892</span> <span class="number">41839</span>  <span class="number">0</span> <span class="number">10</span>:19 pts/3    <span class="number">00</span>:00:00 grep ETL</span><br><span class="line"></span><br><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ lsof -p <span class="number">14369</span> | wc -l</span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ lsof -p <span class="number">14370</span> | wc -l</span><br><span class="line"><span class="number">928</span></span><br><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ lsof -p <span class="number">14442</span> | wc -l</span><br><span class="line"><span class="number">4169</span></span><br><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ cat /proc/sys/fs/file-max</span><br><span class="line"><span class="number">792049</span></span><br><span class="line"></span><br><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ su - </span><br><span class="line"><span class="list">[<span class="keyword">root@datanode01</span> ~]# vi /etc/security/limits.conf</span><br><span class="line">* soft nofile <span class="number">65536</span> * hard nofile <span class="number">65536</span></span><br><span class="line">添加以上, 其中*表示任何用户.  注意要用root用户执行.</span></span></span></span></span></span></span></span></span><br></pre></td></tr></table></figure>
<p>或者使用root用户添加指定用户的文件数和进程数: </p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">echo <span class="string">'########################for ETL 4.1.0'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd soft nofile 65536'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd hard nofile 65536'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd soft nproc 131072'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd hard nproc 131072'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd soft nofile 65536'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd hard nofile 65536'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd soft nproc 131072'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd hard nproc 131072'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br></pre></td></tr></table></figure>
<p>然后使用midd用户验证ulimit</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ ulimit -a</span><br><span class="line">core file size          <span class="list">(<span class="keyword">blocks</span>, -c)</span> <span class="number">0</span></span><br><span class="line">data seg size           <span class="list">(<span class="keyword">kbytes</span>, -d)</span> unlimited</span><br><span class="line">scheduling priority             <span class="list">(<span class="keyword">-e</span>)</span> <span class="number">0</span></span><br><span class="line">file size               <span class="list">(<span class="keyword">blocks</span>, -f)</span> unlimited</span><br><span class="line">pending signals                 <span class="list">(<span class="keyword">-i</span>)</span> <span class="number">62700</span></span><br><span class="line">max locked memory       <span class="list">(<span class="keyword">kbytes</span>, -l)</span> <span class="number">64</span></span><br><span class="line">max memory size         <span class="list">(<span class="keyword">kbytes</span>, -m)</span> unlimited</span><br><span class="line">open files                      <span class="list">(<span class="keyword">-n</span>)</span> <span class="number">65536</span></span><br><span class="line">pipe size            <span class="list">(<span class="keyword">512</span> bytes, -p)</span> <span class="number">8</span></span><br><span class="line">POSIX message queues     <span class="list">(<span class="keyword">bytes</span>, -q)</span> <span class="number">819200</span></span><br><span class="line">real-time priority              <span class="list">(<span class="keyword">-r</span>)</span> <span class="number">0</span></span><br><span class="line">stack size              <span class="list">(<span class="keyword">kbytes</span>, -s)</span> <span class="number">10240</span></span><br><span class="line">cpu time               <span class="list">(<span class="keyword">seconds</span>, -t)</span> unlimited</span><br><span class="line">max user processes              <span class="list">(<span class="keyword">-u</span>)</span> <span class="number">131072</span></span><br><span class="line">virtual memory          <span class="list">(<span class="keyword">kbytes</span>, -v)</span> unlimited</span><br><span class="line">file locks                      <span class="list">(<span class="keyword">-x</span>)</span> unlimited</span></span><br></pre></td></tr></table></figure>
<p>另外方法：<a href="http://gaozzsoft.iteye.com/blog/1824824" target="_blank" rel="external">http://gaozzsoft.iteye.com/blog/1824824</a> </p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.使用ps -ef |grep java   (java代表你程序，查看你程序进程) 查看你的进程ID，记录ID号，假设进程ID为122.使用：lsof -p 12 | wc -<span class="keyword">l</span>    查看当前进程id为12的 文件操作状况    执行该命令出现文件使用情况为 10523.使用命令：ulimit -a   查看每个用户允许打开的最大文件数    发现系统默认的是<span class="keyword">open</span> files (-<span class="keyword">n</span>) 1024，问题就出现在这里。4.然后执行：ulimit -<span class="keyword">n</span> 4096</span><br><span class="line">     将<span class="keyword">open</span> files (-<span class="keyword">n</span>) 1024 设置成<span class="keyword">open</span> files (-<span class="keyword">n</span>) 4096</span><br><span class="line">这样就增大了用户允许打开的最大文件数</span><br></pre></td></tr></table></figure>
<h3 id="内存">内存</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">free</span><br><span class="line">free -<span class="keyword">m</span>   以MB为单位</span><br><span class="line">free -<span class="keyword">g</span>   以GB为单位</span><br><span class="line"></span><br><span class="line">df</span><br><span class="line">df -<span class="keyword">m</span>   以MB</span><br><span class="line">df -<span class="keyword">h</span>   以人类(human)可读的, 即GB</span><br><span class="line"></span><br><span class="line">pstree -p | wc -<span class="keyword">l</span></span><br></pre></td></tr></table></figure>
<h3 id="用户和权限">用户和权限</h3><p>添加用户, 设置密码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">useradd <span class="operator">-d</span> /home/postgres postgres </span><br><span class="line">passwd postgres</span><br></pre></td></tr></table></figure>
<p>更改读写权限, 地柜目录使用大写的R. (注意scp时用的是小写的r) </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod <span class="number">755</span> file</span><br><span class="line">chmod -R <span class="number">755</span> folder</span><br></pre></td></tr></table></figure>
<p>更改用户名:组</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">chown</span> <span class="tag">hadoop</span><span class="pseudo">:hadoop</span> <span class="tag">-R</span> <span class="tag">folder</span></span><br></pre></td></tr></table></figure>
<h3 id="定时任务cron">定时任务cron</h3><p>1.编写脚本</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi /usr/lib/zookeeper/bin/cron.day </span></span><br><span class="line"><span class="comment">#bin/sh</span></span><br><span class="line">cd /usr/<span class="class"><span class="keyword">lib</span>/<span class="title">zookeeper</span>/<span class="title">bin</span></span></span><br><span class="line">./zkCleanup.sh /opt/hadoop/zookeeper/version-<span class="number">2</span> <span class="number">5</span></span><br><span class="line">echo <span class="string">'clean up end...'</span></span><br></pre></td></tr></table></figure>
<p>2.更改脚本权限</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># chmod <span class="number">755</span> /usr/lib/zookeeper/bin/cron.day</span></span><br></pre></td></tr></table></figure>
<p>3.定时调度策略</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /etc/cron.d</span></span><br><span class="line"><span class="comment"># vi /etc/cron.d/zk.cron </span></span><br><span class="line"><span class="number">0</span> <span class="number">13</span> * * * root run-parts /usr/<span class="class"><span class="keyword">lib</span>/<span class="title">zookeeper</span>/<span class="title">bin</span>/<span class="title">cron</span>.<span class="title">day</span></span></span><br></pre></td></tr></table></figure>
<p>4.导入调度配置</p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># crontab zk.cron</span></span><br></pre></td></tr></table></figure>
<p>5.查看调度列表</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># crontab -l</span></span><br><span class="line"><span class="number">0</span> <span class="number">13</span> * * * root run-parts /usr/<span class="class"><span class="keyword">lib</span>/<span class="title">zookeeper</span>/<span class="title">bin</span>/<span class="title">cron</span>.<span class="title">day</span></span></span><br></pre></td></tr></table></figure>
<p>6.查看是否调度的日志 </p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tail -f /var/log/cron</span></span><br><span class="line"><span class="constant">Apr</span>  <span class="number">1</span> <span class="number">13</span>:<span class="number">00</span>:<span class="number">01</span> namenode02 <span class="constant">CROND</span>[<span class="number">41859</span>]: (root) <span class="constant">CMD</span> (root run-parts /usr/<span class="class"><span class="keyword">lib</span>/<span class="title">zookeeper</span>/<span class="title">bin</span>/<span class="title">cron</span>.<span class="title">day</span>)</span></span><br></pre></td></tr></table></figure>
<p>2.定时任务 <code>sudo -u admin crontab -e</code>:<br>20 11 <em> </em> * /usr/install/sh/activity.sh &gt;&gt; /home/admin/output/cronlogs/do_activity.log 2&gt;&amp;1</p>
<p>3.创建日志重定向文件:<br>sudu -u admin touch /home/admin/output/cronlogs/do_activity.log</p>
<p>4.查看任务运行日志:<br>$ sudo tail -f /var/log/cron<br>Sep  9 11:20:02 spark047214 CROND[9191]: (admin) CMD (/usr/install/sh/activity.sh &gt;&gt; /home/admin/output/cronlogs/do_activity.log 2&gt;&amp;1)</p>
<p>如果没有使用日志重定向, 则默认定时任务输出到mail中:<br>$ sudo -u admin tail -200f /var/spool/mail/admin</p>
<p>其他知识点: </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># vi /etc/crontab</span></span><br><span class="line">SHELL=/bin/bash</span><br><span class="line">PATH=/sbin:/bin:/usr/sbin:/usr/bin</span><br><span class="line">MAILTO=root</span><br><span class="line">HOME=/</span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># For details see man <span class="number">4</span> crontabs</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># Example of job definition:</span></span><br><span class="line"><span class="preprocessor"># .---------------- minute (<span class="number">0</span> - <span class="number">59</span>)</span></span><br><span class="line"><span class="preprocessor"># |  .------------- hour (<span class="number">0</span> - <span class="number">23</span>)</span></span><br><span class="line"><span class="preprocessor"># |  |  .---------- day of month (<span class="number">1</span> - <span class="number">31</span>)</span></span><br><span class="line"><span class="preprocessor"># |  |  |  .------- month (<span class="number">1</span> - <span class="number">12</span>) OR jan,feb,mar,apr ...</span></span><br><span class="line"><span class="preprocessor"># |  |  |  |  .---- day of week (<span class="number">0</span> - <span class="number">6</span>) (Sunday=<span class="number">0</span> or <span class="number">7</span>) OR sun,mon,tue,wed,thu,fri,sat</span></span><br><span class="line"><span class="preprocessor"># |  |  |  |  |</span></span><br><span class="line"><span class="preprocessor"># *  *  *  *  * user-name command to be executed</span></span><br></pre></td></tr></table></figure>
<h2 id="6-_进程">6. 进程</h2><h3 id="进程和端口">进程和端口</h3><p>根据端口号查询进程名字</p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -Pnl +M -i4[i6] <span class="string">| grep 20880</span></span><br></pre></td></tr></table></figure>
<h3 id="top">top</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">按M可以按照Memory排序, 按P按照CPU排序</span><br><span class="line">按u可以选择指定的user, 只显示该用户的进程</span><br><span class="line">top -p $(pidof mongod)    只显示指定的进程</span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                                                                                      </span><br><span class="line"> <span class="number">4735</span> midd      <span class="number">20</span>   <span class="number">0</span> <span class="number">30.1</span>g <span class="number">748</span>m <span class="number">4944</span> S  <span class="number">2.0</span>  <span class="number">9.5</span> <span class="number">102</span>:<span class="number">39.94</span> mongod</span><br></pre></td></tr></table></figure>
<h3 id="telnet">telnet</h3><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="atom">telnet</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span> <span class="number">80</span></span><br><span class="line"><span class="name">Trying</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>...</span><br><span class="line"><span class="name">Connected</span> <span class="atom">to</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>.</span><br><span class="line"><span class="name">Escape</span> <span class="atom">character</span> <span class="atom">is</span> <span class="string">'^]'</span>.</span><br><span class="line">^]             ⬅️ <span class="name">MAC</span>下同时按下<span class="name">Control</span>和]两个键</span><br><span class="line"><span class="atom">telnet</span>&gt; <span class="atom">quit</span>   ⬅️ 出现这个，键入<span class="atom">quit</span>，成功退出</span><br><span class="line"><span class="name">Connection</span> <span class="atom">closed</span>.</span><br></pre></td></tr></table></figure>
<h3 id="kill">kill</h3><p><strong>killall</strong> 命令可以杀死同一个进程的所有子进程.<br>如果用ps -ef | grep 则要一个一个杀.<br>比如ps -ef | grep ETL 显示一共由三个相关进程   </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[midd@datanode01 bin]$ ps -ef | grep ETL</span><br><span class="line">midd      <span class="number">4812</span> <span class="number">41955</span>  <span class="number">0</span> <span class="number">16</span>:<span class="number">01</span> pts/<span class="number">3</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep ETL</span><br><span class="line">midd     <span class="number">42276</span>     <span class="number">1</span>  <span class="number">0</span> <span class="number">15</span>:<span class="number">52</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> ETL_ScheduleCenter     </span><br><span class="line">midd     <span class="number">42277</span> <span class="number">42276</span> <span class="number">99</span> <span class="number">15</span>:<span class="number">52</span> ?        <span class="number">00</span>:<span class="number">15</span>:<span class="number">35</span> ETL_ScheduleServer     </span><br><span class="line">midd     <span class="number">42345</span> <span class="number">42276</span>  <span class="number">7</span> <span class="number">15</span>:<span class="number">53</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">34</span> ETL_ServerManger</span><br></pre></td></tr></table></figure>
<p>而用killall 只需要一行: <code>killall ScheduleCenter</code></p>
<p>批量杀进程</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">process=<span class="string">"cross-partner"</span></span><br><span class="line">ps aux|<span class="keyword">grep</span> <span class="variable">$process</span>|<span class="keyword">grep</span> -v <span class="keyword">grep</span>|awk <span class="string">'&#123;print $2&#125;'</span>|xargs <span class="keyword">kill</span> -<span class="number">9</span></span><br></pre></td></tr></table></figure>
<p>或者更简单的：（类似ssh-&gt;pssh, kill-&gt;pkill）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kill -<span class="number">9</span> $(pgrep amarok)</span><br><span class="line">pkill -<span class="number">9</span> amarok</span><br></pre></td></tr></table></figure>
<p> 1: caccandra047202 (192.168.47.202)<br> 2: caccandra047203 (192.168.47.203)<br> 3: caccandra047204 (192.168.47.204)<br> 4: caccandra047205 (192.168.47.205)<br> 5: caccandra047206 (192.168.47.206)<br> 6: caccandra047221 (192.168.47.221)<br> 7: caccandra047222 (192.168.47.222)<br> 8: caccandra047224 (192.168.47.224)<br> 9: caccandra047225 (192.168.47.225)<br>10: caccandra047226 (192.168.47.226)<br>11: caccandra047227 (192.168.47.227)<br>12: caccandra047228 (192.168.47.228)<br>13: caccandra047229 (192.168.47.229)<br>14: cass048047      (192.168.48.47)<br>15: cass048048      (192.168.48.48)<br>16: cass048049      (192.168.48.49)<br>17: cass048165      (192.168.48.165)<br>18: cass048168      (192.168.48.168)<br>25: cass048175      (192.168.48.175)<br>26: cass048176      (192.168.48.176)<br>27: cass049243      (192.168.49.243)<br>28: cass049244      (192.168.49.244)<br>29: fp-cass048159   (192.168.48.159)<br>30: fp-cass048160   (192.168.48.160)<br>31: fp-cass048161   (192.168.48.161)<br>34: fp-cass048226   (192.168.48.226)<br>35: fp-cass048227   (192.168.48.227)<br>36: fp-cass048228   (192.168.48.228)  </p>
<h3 id="nohup">nohup</h3><p><a href="http://ora12c.blogspot.com/2012/04/how-to-put-scp-in-background.html" target="_blank" rel="external">http://ora12c.blogspot.com/2012/04/how-to-put-scp-in-background.html</a></p>
<p>scp命令需要输入密码, 结合nohup, 而nohup是在后台执行, 因此密码没办法输入.  </p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng<span class="variable">@cass047224</span> ~]<span class="variable">$ </span>nohup scp -l <span class="number">100000</span> -r <span class="number">1447314738524</span> <span class="number">192.168</span>.<span class="number">47.219</span><span class="symbol">:~/snapshot/</span><span class="number">224_1114</span>/ &amp;</span><br><span class="line">[<span class="number">1</span>] <span class="number">16169</span></span><br><span class="line"><span class="symbol">nohup:</span> 忽略输入并把输出追加到<span class="string">"nohup.out"</span></span><br><span class="line">[qihuang.zheng<span class="variable">@cass047224</span> ~]<span class="variable">$ </span>qihuang.zheng<span class="variable">@192</span>.<span class="number">168.47</span>.<span class="number">219</span><span class="string">'s password:</span><br><span class="line"></span><br><span class="line">[1]+  Stopped                 nohup scp -l 100000 -r 1447314738524 192.168.47.219:~/snapshot/224_1114/</span></span><br></pre></td></tr></table></figure>
<p>按这里: <a href="http://unix.stackexchange.com/questions/91065/nohup-sudo-does-not-prompt-for-passwd-and-does-nothing" target="_blank" rel="external">http://unix.stackexchange.com/questions/91065/nohup-sudo-does-not-prompt-for-passwd-and-does-nothing</a><br>和这里: <a href="http://stackoverflow.com/questions/13147861/run-scp-in-background-and-monitor-the-progress" target="_blank" rel="external">http://stackoverflow.com/questions/13147861/run-scp-in-background-and-monitor-the-progress</a><br>不要加&amp;, 可以输入密码, Ctrl+Z暂停任务, bg恢复任务  </p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047224 ~]$ nohup scp -l 100000 -r <span class="number">144731473852</span><span class="number">4 192.168.47</span>.219:~/snapshot/<span class="number">224_1114</span>/</span><br><span class="line">nohup: 忽略输入并把输出追加到"nohup.out"</span><br><span class="line">qihuang.zheng@<span class="number">192.168.47.219</span>'s password:   在这里输入密码, 注意必须等输入密码之后,再暂停任务,还没有出现时,不能暂停!!!</span><br><span class="line">^Z</span><br><span class="line">[2]+  Stopped                 nohup scp -l 100000 -r <span class="number">144731473852</span><span class="number">4 192.168.47</span>.219:~/snapshot/<span class="number">224_1114</span>/</span><br><span class="line">[qihuang.zheng@cass047224 ~]$</span><br><span class="line">[qihuang.zheng@cass047224 ~]$ ps -ef|grep scp</span><br><span class="line">501      <span class="number">16169 11856</span>  0 12:29 pts/0    00:00:00 scp -l 100000 -r <span class="number">144731473852</span><span class="number">4 192.168.47</span>.219:~/snapshot/<span class="number">224_1114</span>/</span><br><span class="line">501      <span class="number">16183 16169</span>  0 12:29 pts/0    00:00:00 /usr/bin/ssh -x -oForwardAgent no -oPermitLocalCommand no -oClearAllForwardings yes <span class="number">192.168.47.219</span> scp -r -t ~/snapshot/<span class="number">224_1114</span>/</span><br><span class="line">501      <span class="number">17492 11856</span>  0 12:33 pts/0    00:00:00 scp -l 100000 -r <span class="number">144731473852</span><span class="number">4 192.168.47</span>.219:~/snapshot/<span class="number">224_1114</span>/</span><br><span class="line">501      <span class="number">17493 17492</span>  0 12:33 pts/0    00:00:00 /usr/bin/ssh -x -oForwardAgent no -oPermitLocalCommand no -oClearAllForwardings yes <span class="number">192.168.47.219</span> scp -r -t ~/snapshot/<span class="number">224_1114</span>/</span><br><span class="line">501      <span class="number">17718 11856</span>  0 12:33 pts/0    00:00:00 grep scp</span><br><span class="line">[qihuang.zheng@cass047224 ~]$ bg</span><br><span class="line">[2]+ nohup scp -l 100000 -r <span class="number">144731473852</span><span class="number">4 192.168.47</span>.219:~/snapshot/<span class="number">224_1114</span>/ &amp;    出现这个表示后台开始运行任务了!!!</span><br><span class="line">[qihuang.zheng@cass047224 ~]$ bg</span><br><span class="line">[1]+ nohup scp -l 100000 -r <span class="number">144731473852</span><span class="number">4 192.168.47</span>.219:~/snapshot/<span class="number">224_1114</span>/ &amp;</span><br></pre></td></tr></table></figure>
<p>如果敲入多次bg, 是不是多次执行?  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047224 ~]$ jobs</span><br><span class="line">[<span class="number">1</span>]+  Stopped                 nohup scp -l <span class="number">100000</span> -r <span class="number">1447314738524</span> <span class="number">192.168</span><span class="number">.47</span><span class="number">.219</span>:~/snapshot/<span class="number">224</span>_1114/</span><br><span class="line">[<span class="number">2</span>]-  Running                 nohup scp -l <span class="number">100000</span> -r <span class="number">1447314738524</span> <span class="number">192.168</span><span class="number">.47</span><span class="number">.219</span>:~/snapshot/<span class="number">224</span>_1114/ &amp;</span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20151114125939707" alt="scp-l"></p>
<p>Linux中&amp;、jobs、fg、bg等命令的使用方法: <a href="http://blog.sina.com.cn/s/blog_673ee2b50100iywr.html" target="_blank" rel="external">http://blog.sina.com.cn/s/blog_673ee2b50100iywr.html</a>  </p>
<h3 id="disown">disown</h3><p>disown 示例1（如果提交命令时已经用“&amp;”将命令放入后台运行，则可以直接使用“disown”）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@pvcent107 build]<span class="preprocessor"># cp -r testLargeFile largeFile &amp;</span></span><br><span class="line">[<span class="number">1</span>] <span class="number">4825</span></span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># jobs</span></span><br><span class="line">[<span class="number">1</span>]+  Running                 cp -i -r testLargeFile largeFile &amp;</span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># disown -h %<span class="number">1</span></span></span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># ps -ef |grep largeFile</span></span><br><span class="line">root      <span class="number">4825</span>   <span class="number">968</span>  <span class="number">1</span> <span class="number">09</span>:<span class="number">46</span> pts/<span class="number">4</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> cp -i -r testLargeFile largeFile</span><br><span class="line">root      <span class="number">4853</span>   <span class="number">968</span>  <span class="number">0</span> <span class="number">09</span>:<span class="number">46</span> pts/<span class="number">4</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep largeFile</span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># logout</span></span><br></pre></td></tr></table></figure>
<p>disown 示例2（如果提交命令时未使用“&amp;”将命令放入后台运行，可使用 CTRL-z 和“bg”将其放入后台，再使用“disown”）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@pvcent107 build]<span class="preprocessor"># cp -r testLargeFile largeFile2</span></span><br><span class="line"></span><br><span class="line">[<span class="number">1</span>]+  Stopped                 cp -i -r testLargeFile largeFile2</span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># bg %<span class="number">1</span></span></span><br><span class="line">[<span class="number">1</span>]+ cp -i -r testLargeFile largeFile2 &amp;</span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># jobs</span></span><br><span class="line">[<span class="number">1</span>]+  Running                 cp -i -r testLargeFile largeFile2 &amp;</span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># disown -h %<span class="number">1</span></span></span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># ps -ef |grep largeFile2</span></span><br><span class="line">root      <span class="number">5790</span>  <span class="number">5577</span>  <span class="number">1</span> <span class="number">10</span>:<span class="number">04</span> pts/<span class="number">3</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> cp -i -r testLargeFile largeFile2</span><br><span class="line">root      <span class="number">5824</span>  <span class="number">5577</span>  <span class="number">0</span> <span class="number">10</span>:<span class="number">05</span> pts/<span class="number">3</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep largeFile2</span><br><span class="line">[root@pvcent107 build]<span class="preprocessor">#</span></span><br></pre></td></tr></table></figure>
<h2 id="7-_文件">7. 文件</h2><h3 id="查看文件编码格式">查看文件编码格式</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># vi .vimrc</span></span><br><span class="line">:<span class="built_in">set</span> fileencoding</span><br><span class="line">  fileencoding=utf8</span><br><span class="line"><span class="built_in">set</span> fileencodings=ucs-bom,utf-<span class="number">8</span>,cp936,gb18030,big5,latin1</span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># vi XXX.file</span></span><br><span class="line">:<span class="built_in">set</span> fencs?</span><br><span class="line">  fileencodings=ucs-bom,utf-<span class="number">8</span>,cp936,gb18030,big5,latin1</span><br><span class="line">:<span class="built_in">set</span> fenc?</span><br><span class="line">  fileencoding=cp936</span><br><span class="line">:<span class="built_in">set</span> enc?</span><br><span class="line">  encoding=utf-<span class="number">8</span></span><br></pre></td></tr></table></figure>
<h3 id="cat文件乱码">cat文件乱码</h3><p>Windows下生成的纯文本文件，其中文编码为GBK，在Ubuntu下显示为乱码，可以使用iconv命令进行转换：</p>
<figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># iconv -f gbk -t utf8 source_file &gt; target_file</span></span><br><span class="line"><span class="label">iconv:</span> 未知 <span class="number">5</span> 处的非法输入序列</span><br></pre></td></tr></table></figure>
<h3 id="GBK转码实践">GBK转码实践</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="shebang">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$#</span>"</span> != <span class="string">"2"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Usage: `basename <span class="variable">$0</span>` dir filter"</span></span><br><span class="line">  <span class="built_in">exit</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">dir=<span class="variable">$1</span></span><br><span class="line">filter=<span class="variable">$2</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$1</span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> `find <span class="variable">$dir</span> -name <span class="string">"<span class="variable">$2</span>"</span>`; <span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$file</span>"</span></span><br><span class="line">  iconv <span class="operator">-f</span> gbk -t utf8 -o <span class="variable">$file</span> <span class="variable">$file</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>执行方式: 第一个参数是目录, 第二个是文件选择</p>
<figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~<span class="regexp">/ftp/GBK2UTF-8_batch.sh ./</span>  M_EP_PD_AQI*</span><br><span class="line">~<span class="regexp">/ftp/GBK2UTF-8_batch.sh ./</span>  M_EP_PH_AQI*</span><br><span class="line">~<span class="regexp">/ftp/GBK2UTF-8_batch.sh ./</span>  M_METE_CITY_PRED*</span><br><span class="line">~<span class="regexp">/ftp/GBK2UTF-8_batch.sh ./</span>  M_METE_WEATHER_LIVE*</span><br></pre></td></tr></table></figure>
<p>执行最后一个, 文件&gt;32kb, 报错:</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4361</span> 总线错误 (core dumped) iconv -c -f gbk -t utf8 -o <span class="variable">$file</span> <span class="variable">$file</span></span><br></pre></td></tr></table></figure>
<p>解决方式: <a href="http://myotragusbalearicus.wordpress.com/2010/03/10/batch-convert-files-to-utf-8/" target="_blank" rel="external">http://myotragusbalearicus.wordpress.com/2010/03/10/batch-convert-files-to-utf-8/</a><br>还是不行: <a href="http://www.path8.net/tn/archives/3448" target="_blank" rel="external">http://www.path8.net/tn/archives/3448</a><br>使用//IGNORE, 成功!  </p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iconv -f gbk//IGNORE -t utf8//IGNORE <span class="variable">$file</span> -o <span class="variable">$file</span>.tmp</span><br></pre></td></tr></table></figure>
<p>注意原始文件必须是和-f对应,如果原始文件是utf8, 要再次转换成utf8, 也会报错.</p>
<p>GBK2UTF8.sh </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="shebang">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$#</span>"</span> != <span class="string">"2"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Usage: `basename <span class="variable">$0</span>` dir filter"</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"sample: ./GBK2UTF8.sh /home/midd/ftp/fz12345/back/2015-03 fz12345_*.txt"</span></span><br><span class="line">  <span class="built_in">exit</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">dir=<span class="variable">$1</span></span><br><span class="line">filter=<span class="variable">$2</span></span><br><span class="line">tmp=<span class="string">'T'</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> `find <span class="variable">$dir</span> -name <span class="string">"<span class="variable">$2</span>"</span>`; <span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$file</span>"</span></span><br><span class="line">  <span class="comment">#iconv -f gbk -t utf8 -o $file $file</span></span><br><span class="line">  <span class="comment">#Notic, the Source File should not utf8 format. or u 'll get error</span></span><br><span class="line">  iconv <span class="operator">-f</span> gbk//IGNORE -t utf8//IGNORE <span class="variable">$file</span> -o <span class="variable">$tmp</span><span class="variable">$file</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h3 id="wordcount计数">wordcount计数</h3><p>计算文件的行数: <code>wc -l file.txt</code><br>要统计单词数量, 加上w选项. L选项表示最长行的长度. 注意这是一整行.不能按照列计算最长长度. </p>
<p>帮助信息: $ wc –help.  如果不知道一个命令, 最好看看–help</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">用法：wc [选项]... [文件]...</span><br><span class="line">　或：wc [选项]... --files0-from=F</span><br><span class="line">  -<span class="ruby">c, --bytes            print the byte counts</span><br><span class="line"></span>  -<span class="ruby">m, --chars            print the character counts</span><br><span class="line"></span>  -<span class="ruby">l, --lines            print the newline counts</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">L</span>, --max-line-length 显示最长行的长度</span><br><span class="line"></span>  -<span class="ruby">w, --words     显示单词计数</span><br><span class="line"></span>      -<span class="ruby">-help    显示此帮助信息并退出</span><br><span class="line"></span>      -<span class="ruby">-version   显示版本信息并退出</span></span><br></pre></td></tr></table></figure>
<h3 id="grep查找">grep查找</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ps</span> -ef | <span class="keyword">grep</span> ETL  查看进程</span><br><span class="line"><span class="keyword">cat</span> fz12345_original.txt | <span class="keyword">grep</span> FZ15032700599 查找一个文件里的字符串</span><br></pre></td></tr></table></figure>
<p>查找目录下的文件里的内容</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cat</span> foder/*.* | <span class="keyword">grep</span> XXX</span><br><span class="line"><span class="keyword">find</span> /etc/ -name <span class="string">"*"</span> | xargs <span class="keyword">grep</span> XXX</span><br><span class="line"></span><br><span class="line"><span class="keyword">find</span> ./ -name <span class="string">"*"</span> | xargs <span class="keyword">grep</span> <span class="number">8080</span></span><br></pre></td></tr></table></figure>
<p>注意: 第一个cat命令无法用于递归子目录, 第二个命令/etc后面必须跟上/, 而且name是*</p>
<h3 id="大文件定位到某一行">大文件定位到某一行</h3><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">'111,112p'</span> <span class="keyword">file</span>.txt</span><br></pre></td></tr></table></figure>
<p>截取文件：</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2016-05-13</span>T00:00   406465</span><br><span class="line"><span class="number">2016-05-14</span>T00:00   <span class="number">1348308</span></span><br><span class="line"></span><br><span class="line">错误的方式： head -<span class="number">1348308</span> gc.log.0 | tail -406465 &gt; tongdun_cassandra_<span class="number">20160513</span>.log</span><br><span class="line">正确的方式： sed -n '<span class="number">406465,134</span>8308p' gc.log.0 &gt; tongdun_cassandra_<span class="number">20160513</span>.log</span><br></pre></td></tr></table></figure>
<h3 id="find文件名">find文件名</h3><p>在指定目录查找文件名: <code>find ~/repo -name *tmp*</code>  </p>
<p>使用管道, xargs表示递归找到的每个值. 如果是文件, 使用rm. 如果是文件夹, 用rm -rf.<br>递归删除svn文件夹:   <code>find SVNFOLDER -name .svn | xargs rm -rf</code><br>递归删除文件:     <code>find ~/repo -name *tmp* | xargs rm</code>  </p>
<p>find -name ‘<em>0456’ -print<br>cat </em> |grep XXX</p>
<h3 id="文件内容替换">文件内容替换</h3><p>\n替换为,  <code>:%s/\n/,/</code></p>
<p>ORACLE类型转换为hive类型:</p>
<figure class="highlight mojolicious"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"></span><span class="perl"><span class="variable">%s</span>/STRING(.*)/STRING/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/VARCHAR2(.*)/STRING/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/CHAR(.*)/STRING/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/DATE,<span class="regexp">/STRING,/</span></span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/date,<span class="regexp">/STRING,/</span></span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/INTEGER/INT/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/NUMBER(..)/BIGINT/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/NUMBER(.)/INT/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/NUMBER(.*)/DOUBLE/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/.<span class="keyword">not</span> null//</span><span class="xml"></span></span><br></pre></td></tr></table></figure>
<h3 id="rename批量修改文件名">rename批量修改文件名</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">rename</span> .XLS .xlsx *.XLS   把文件名=.XLS的替换成.xlsx</span><br><span class="line"><span class="keyword">rename</span> \_linux <span class="string">''</span> *.txt     把文件名=_linux的替换成空, 注意_要用转义, 即去掉文件名包含_linux的</span><br><span class="line"><span class="keyword">rename</span> -Unlicensed- <span class="string">''</span> *.xlsx</span><br><span class="line"><span class="keyword">rename</span> fun2 fun *</span><br><span class="line"></span><br><span class="line"><span class="keyword">rename</span> <span class="keyword">data</span> <span class="keyword">md5</span> *   # <span class="keyword">rename</span> 原文件要替换  替换后  要替换的文件</span></span><br></pre></td></tr></table></figure>
<h3 id="ftp文件夹下载">ftp文件夹下载</h3><p>wget ftp://172.17.227.236/ctos_analyze/data/tmp/<em> –ftp-user=ftpd –ftp-password=ftpd123 -r  
</em>必须要有, 最后的-r表示目录下载</p>
<p>wget -r -l 1 <a href="http://www.baidu.com/dir/" target="_blank" rel="external">http://www.baidu.com/dir/</a>   </p>
<p>文件续传:  <code>wget -c xxx.file</code></p>
<p>下载网站的所有文件： wget -r -l 1 <a href="http://atlarge.ewi.tudelft.nl/graphalytics/" target="_blank" rel="external">http://atlarge.ewi.tudelft.nl/graphalytics/</a></p>
<h3 id="文件按行数分割">文件按行数分割</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ wc -l dispatch2012.csv </span><br><span class="line"><span class="number">231272</span> dispatch2012.csv</span><br><span class="line"></span><br><span class="line">$ split -l <span class="number">60000</span> dispatch2012.csv dispatch2012_new.csv </span><br><span class="line">$ ll</span><br><span class="line">-rw-r--r-- <span class="number">1</span> hadoop hadoop <span class="number">27649615</span>  <span class="number">9</span>月 <span class="number">27</span>  <span class="number">2014</span> dispatch2012.csv</span><br><span class="line">-rw-rw-r-- <span class="number">1</span> hadoop hadoop  <span class="number">7115577</span>  <span class="number">4</span>月 <span class="number">14</span> <span class="number">19</span>:<span class="number">08</span> dispatch2012_new.csvaa</span><br><span class="line">-rw-rw-r-- <span class="number">1</span> hadoop hadoop  <span class="number">7188497</span>  <span class="number">4</span>月 <span class="number">14</span> <span class="number">19</span>:<span class="number">08</span> dispatch2012_new.csvab</span><br><span class="line">-rw-rw-r-- <span class="number">1</span> hadoop hadoop  <span class="number">7208496</span>  <span class="number">4</span>月 <span class="number">14</span> <span class="number">19</span>:<span class="number">08</span> dispatch2012_new.csvac</span><br><span class="line">-rw-rw-r-- <span class="number">1</span> hadoop hadoop  <span class="number">6137045</span>  <span class="number">4</span>月 <span class="number">14</span> <span class="number">19</span>:<span class="number">08</span> dispatch2012_new.csvad</span><br><span class="line">$ wc -l dispatch2012_new.csv*</span><br><span class="line">   <span class="number">60000</span> dispatch2012_new.csvaa</span><br><span class="line">   <span class="number">60000</span> dispatch2012_new.csvab</span><br><span class="line">   <span class="number">60000</span> dispatch2012_new.csvac</span><br><span class="line">   <span class="number">51272</span> dispatch2012_new.csvad</span><br><span class="line">  <span class="number">231272</span> 总用量</span><br></pre></td></tr></table></figure>
<h3 id="nc">nc</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scp复制方式： scp influxdb-<span class="number">0.13</span><span class="number">.0</span>_linux_amd64.tar.gz qihuang.zheng@<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:~/</span><br><span class="line"></span><br><span class="line">用nc需要先在接收端开启一个端口， 然后在发送端把数据发送到接收端的端口</span><br><span class="line"></span><br><span class="line">远程：nc -l <span class="number">1234</span> &gt; influxdb-<span class="number">0.13</span><span class="number">.0</span>_linux_amd64.tar.gz</span><br><span class="line">本地：nc -w  <span class="number">1</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span> <span class="number">1234</span>  &lt; influxdb-<span class="number">0.13</span><span class="number">.0</span>_linux_amd64.tar.gz</span><br><span class="line"></span><br><span class="line">复制文件夹：</span><br><span class="line">scp方式：scp -r influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span> qihuang.zheng@<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:~/</span><br><span class="line"></span><br><span class="line">(注意不要在|之间加空格！默认远程的文件夹和本地的一样)</span><br><span class="line">远程：$ nc -l <span class="number">1234</span>|tar zxvf -</span><br><span class="line">influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/</span><br><span class="line">influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/etc/</span><br><span class="line">influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/usr/</span><br><span class="line">...</span><br><span class="line">influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/etc/influxdb/influxdb.conf</span><br><span class="line"></span><br><span class="line">本地：$ tar -cvzf - influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>|nc <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span> <span class="number">1234</span></span><br><span class="line">a influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span></span><br><span class="line">a influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/etc</span><br><span class="line">a influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/usr</span><br><span class="line">...</span><br><span class="line">a influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/etc/influxdb/influxdb.conf</span><br><span class="line"></span><br><span class="line">tar -cvzf - android_device_session|nc <span class="number">192.168</span><span class="number">.50</span><span class="number">.20</span> <span class="number">1234</span></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nc -l <span class="number">1234</span>|tar xvf -</span><br><span class="line">tar -cvf - android_device_session|nc <span class="number">192.168</span><span class="number">.50</span><span class="number">.20</span> <span class="number">1234</span></span><br></pre></td></tr></table></figure>
<p>nohup 结合 nc报错：  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gzip: stdin: unexpected <span class="operator"><span class="keyword">end</span> <span class="keyword">of</span> <span class="keyword">file</span></span><br><span class="line">tar: <span class="keyword">Child</span> returned <span class="keyword">status</span> <span class="number">1</span></span><br><span class="line">tar: <span class="keyword">Error</span> <span class="keyword">is</span> <span class="keyword">not</span> recoverable: exiting <span class="keyword">now</span></span><br><span class="line"></span><br><span class="line">[<span class="number">1</span>]+  <span class="keyword">Exit</span> <span class="number">2</span>                  nohup nc -<span class="keyword">l</span> <span class="number">1234</span> | tar zxvf -</span></span><br></pre></td></tr></table></figure>
<p>screen:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">screen -S fp_android_device_session_159 nc -l <span class="number">1234</span>|tar xvf -</span><br><span class="line">screen -S fp_android_device_session_159 tar -cvf - android_device_session|nc <span class="number">192.168</span><span class="number">.50</span><span class="number">.20</span> <span class="number">1234</span></span><br></pre></td></tr></table></figure>
<p>disown:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nc <span class="operator">-l</span> <span class="number">1234</span>|tar xvf - &amp;</span><br><span class="line"><span class="built_in">jobs</span></span><br><span class="line"><span class="built_in">disown</span> -h %<span class="number">1</span></span><br><span class="line"><span class="built_in">jobs</span></span><br><span class="line"><span class="built_in">logout</span></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[admin@spark015010 ~]$ nc -d -l <span class="number">1234</span>|tar xvf - &amp;</span><br><span class="line">[<span class="number">2</span>] <span class="number">13672</span></span><br><span class="line">[admin@spark015010 ~]$ jobs</span><br><span class="line">[<span class="number">1</span>]+  Stopped                 nc -l <span class="number">1234</span> | tar xvf -</span><br><span class="line">[<span class="number">2</span>]-  Running                 nc -l <span class="number">1234</span> | tar xvf - &amp;</span><br><span class="line">[admin@spark015010 ~]$ disown -h %<span class="number">1</span></span><br><span class="line">[admin@spark015010 ~]$ ps -ef |grep <span class="number">1234</span></span><br><span class="line">admin    <span class="number">13671</span>  <span class="number">7993</span>  <span class="number">0</span> <span class="number">13</span>:<span class="number">38</span> pts/<span class="number">0</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> nc -l <span class="number">1234</span></span><br><span class="line">admin    <span class="number">13713</span>  <span class="number">7993</span>  <span class="number">0</span> <span class="number">13</span>:<span class="number">38</span> pts/<span class="number">0</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep <span class="number">1234</span></span><br><span class="line">[admin@spark015010 ~]$ logout</span><br><span class="line">There are stopped jobs.</span><br><span class="line">[admin@spark015010 ~]$ jobs</span><br><span class="line">[<span class="number">1</span>]+  Stopped                 nc -l <span class="number">1234</span> | tar xvf -</span><br><span class="line">[<span class="number">2</span>]-  Running                 nc -l <span class="number">1234</span> | tar xvf - &amp;</span><br><span class="line">[admin@spark015010 ~]$ jobs -p</span><br><span class="line"><span class="number">11590</span></span><br><span class="line"><span class="number">13671</span></span><br></pre></td></tr></table></figure>
<h2 id="8-_VI">8. VI</h2><p>显示行号: :set number<br>复制模式：:set paste<br>打开文件定位到最后一行: vi + file.txt ，或者<strong>G</strong><br>第一行：:0回车<br>从指定行删除到最后一行</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">方法<span class="number">1</span>： </span><br><span class="line">使用<span class="built_in">set</span> number计算当前行和最后一行的差比如<span class="number">100</span></span><br><span class="line">输入<span class="number">100</span>dd</span><br></pre></td></tr></table></figure>
<p>清空文件内容：先跳转到文件最后一行：<strong>G</strong>，<strong>:1,.d</strong>  </p>
<h2 id="9-_Awk/sed">9. Awk/sed</h2><h3 id="列编辑">列编辑</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">replace</span>(<span class="keyword">replace</span>(DISPATCHMEMO,chr(10),''),chr(9),'')</span><br><span class="line"></span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="string">"&lt;"</span> <span class="label">$0</span> <span class="string">"&gt; "</span>&#125;' O_FZ12345_CALLINFO&gt;O_FZ12345_CALLINFO2</span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="string">"select count(*) from "</span> <span class="label">$0</span> <span class="string">" union all \n"</span>&#125;' <span class="keyword">cl</span>&gt;cl2</span><br><span class="line"></span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="string">"replace(replace("</span> <span class="label">$0</span> <span class="string">",chr(10),''),chr(9),'')"</span>&#125;' O_FZ12345_CALLINFO&gt;O_FZ12345_CALLINFO2</span><br><span class="line"></span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="string">"replace(replace("</span> <span class="label">$0</span> <span class="string">",chr(10),''),chr(9),'')"</span>&#125;' c3&gt;c4</span><br><span class="line"></span><br><span class="line">%s/,)/,'')/</span><br><span class="line"></span><br><span class="line">alter <span class="keyword">table</span> owner to etl;</span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="string">"alter table "</span> <span class="label">$0</span> <span class="string">" owner to etl;"</span>&#125;' <span class="keyword">test</span>&gt;test2</span><br></pre></td></tr></table></figure>
<h3 id="列的最大长度">列的最大长度</h3><p>下面2个语句执行的结果不同??<br>打印结果时,用双引号<br>awk ‘{if (length($NF)&gt;maxlength) maxlength=length($NF)} END {print maxlength” “$1” “$2” “$NF}’ fz12345_original.txt   </p>
<p>awk ‘{s[$1] += $2}END{ for(i in s){  print i, s[i] } }’ file1 &gt; file2</p>
<p>awk ‘{s[$1” “$2] += $3}END{ for(i in s){print i, s[i] } }’  file1 &gt; file2</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">找出最后一列的最大长度  </span><br><span class="line">awk '&#123;<span class="keyword">if</span> (<span class="literal">length</span>(<span class="label">$NF</span>)&gt;maxlength) maxlength=<span class="literal">length</span>(<span class="label">$NF</span>)&#125; END &#123;<span class="keyword">print</span> maxlength &#125;' fz12345_original.txt   </span><br><span class="line"></span><br><span class="line">找出最大长度的那一条记录  </span><br><span class="line">awk '<span class="literal">length</span>(<span class="label">$NF</span>)==2016 &#123;<span class="keyword">print</span> <span class="label">$1</span><span class="string">" "</span><span class="label">$2</span><span class="string">" "</span><span class="label">$NF&#125;</span>' fz12345_original.txt  </span><br><span class="line"></span><br><span class="line">最后一列为空  </span><br><span class="line">awk '&#123;<span class="label">$NF</span>=<span class="string">""</span> ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt  | head  </span><br><span class="line"></span><br><span class="line">截取最后一列:  </span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900)&#125;' fz12345_original.txt  </span><br><span class="line"></span><br><span class="line">打印一整行:  </span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt  </span><br><span class="line"></span><br><span class="line">打印第一列和最后一列, 最后一列被截取, 以\t分割  </span><br><span class="line">awk '&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$1</span><span class="string">"\t"</span><span class="label">$NF&#125;</span>' fz12345_original.txt  | head  </span><br><span class="line"></span><br><span class="line">截取最后一列, 并打印整行. 但是分隔符变成空格. 如果原先内容有空格,则无法正确解析  </span><br><span class="line">awk '&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt  | head  </span><br><span class="line"></span><br><span class="line">截取最后一列, 打印整行, 分隔符为\t  </span><br><span class="line">awk 'BEGIN &#123;OFS=<span class="string">"\t"</span>&#125;&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt | head  </span><br><span class="line"></span><br><span class="line">行的字段数不一样. 是因为如果有些字段值为空: 如果是空值,则不会被计算为一列!  </span><br><span class="line">awk '&#123;<span class="keyword">print</span> NF&#125;' fz12345.txt | head  </span><br><span class="line">awk '&#123;<span class="keyword">print</span> NF&#125;' fz12345_original.txt | head  </span><br><span class="line"></span><br><span class="line">输出字段分隔符:  </span><br><span class="line">awk 'BEGIN &#123;OFS=<span class="string">"\t"</span>&#125;&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt &gt; fz12345.txt  </span><br><span class="line">输入字段分隔符,输出字段分隔符:  </span><br><span class="line">awk 'BEGIN &#123;FS=<span class="string">"\t"</span>;OFS=<span class="string">"\t"</span>&#125;&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt &gt; fz12345.txt  </span><br><span class="line">输入字段,输入行,输出字段,输出行分隔符:  </span><br><span class="line">awk 'BEGIN &#123;FS=<span class="string">"\t"</span>;RS=<span class="string">"\n"</span>;OFS=<span class="string">"\t"</span>;ORS=<span class="string">"\n"</span>;&#125;&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt &gt; fz12345.txt  </span><br><span class="line"></span><br><span class="line">awk FS=<span class="string">"\t"</span> '&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' OFS=<span class="string">"\t"</span> fz12345_original.txt &gt; fz12345.txt  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;http:<span class="comment">//coolshell.cn/articles/9070.html&gt;  </span></span><br><span class="line">awk  -F:'&#123;<span class="keyword">print</span> <span class="label">$1</span>,<span class="label">$3</span>,<span class="label">$6&#125;</span>' OFS=<span class="string">"\t"</span> /etc/passwd  </span><br><span class="line">/etc/passwd文件是以:为分隔符的. 取出第1,3,6列. 以\t分割!</span><br></pre></td></tr></table></figure>
<h3 id="一列转多行">一列转多行</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hadoop@hadoop:~$ echo 'A|B|C|aa,bb|DD' | awk -F\| 'BEGIN&#123;OFS=<span class="string">"|"</span>&#125;&#123;<span class="keyword">split</span>(<span class="label">$4</span>,a,<span class="string">","</span>);<span class="keyword">for</span>(i <span class="keyword">in</span> a)&#123;<span class="label">$4</span>=a[i];<span class="keyword">print</span> <span class="label">$0&#125;</span>&#125;'</span><br><span class="line">A|B|C|aa|DD</span><br><span class="line">A|B|C|bb|DD</span><br><span class="line">hadoop@hadoop:~$ echo 'A|B|C|aa|DD' | awk -F\| 'BEGIN&#123;OFS=<span class="string">"|"</span>&#125;&#123;<span class="keyword">split</span>(<span class="label">$4</span>,a,<span class="string">","</span>);<span class="keyword">for</span>(i <span class="keyword">in</span> a)&#123;<span class="label">$4</span>=a[i];<span class="keyword">print</span> <span class="label">$0&#125;</span>&#125;'</span><br><span class="line">A|B|C|aa|DD</span><br><span class="line"></span><br><span class="line">awk 'BEGIN&#123;FS=<span class="string">"\t"</span>;OFS=<span class="string">"\t"</span>&#125;&#123;<span class="keyword">split</span>(<span class="label">$5</span>,a,<span class="string">","</span>);<span class="keyword">for</span>(i <span class="keyword">in</span> a)&#123;<span class="label">$5</span>=a[i];<span class="keyword">print</span> <span class="label">$0&#125;</span>&#125;' \</span><br><span class="line">fz12345_dispatch_2014.txt &gt; fz12345_dispatch_2014_2.txt</span><br><span class="line"></span><br><span class="line">转换前文件:</span><br><span class="line">2251562 FZ15032600537   福州市信访局    2015-03-26 16:29:50     福州市城乡建设委员会,福州市交通运输委员会       0       1       10      2015-04-10 16:29:50</span><br><span class="line"></span><br><span class="line">转换后文件:</span><br><span class="line">2251562 FZ15032600537   福州市信访局    2015-03-26 16:29:50     福州市城乡建设委员会    0       1       10      2015-04-10 16:29:50</span><br><span class="line">2251562 FZ15032600537   福州市信访局    2015-03-26 16:29:50     福州市交通运输委员会    0       1       10      2015-04-10 16:29:50</span><br></pre></td></tr></table></figure>
<p>去掉文件中的所有双引号: sed -i ‘s/“//g’ dispatch2012.csv  </p>
<h3 id="将^M删除">将^M删除</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sed -<span class="tag">i</span> <span class="string">'s/"//g'</span> dispatch2012<span class="class">.csv</span>            ×</span><br><span class="line"><span class="tag">tr</span> -d <span class="string">'^M'</span> &lt; dispatch2012_2<span class="class">.csv</span>           ×</span><br><span class="line"></span><br><span class="line">alias dos2unix=<span class="string">"sed -i -e 's/'\"\$(printf '\015')\"'//g' "</span>    √</span><br><span class="line">dos2unix dispatch2012_2<span class="class">.csv</span>             </span><br><span class="line">sed -<span class="tag">i</span> -e <span class="string">'s/'</span>\<span class="string">"\$(printf '\015')\"'//g' dispatch2012_2.csv   报错: bash: 未预期的符号 `(' 附近有语法错误</span></span><br></pre></td></tr></table></figure>
<p>删除第一行: sed -i ‘1d;$d’ dispatch2012_2.csv</p>
<p>第五列因为乱码直接改为空字符串!<br>awk ‘BEGIN {FS=”,”;RS=”\n”;OFS=”\t”;ORS=”\n”;}{$5=””;print $0}’ dispatch2012.csv &gt; dispatch2012_2.csv</p>
<h3 id="指定行前/后插入内容">指定行前/后插入内容</h3><p>i表示在之前匹配Regex之前插入，a表示在之后插入</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -<span class="tag">i</span> <span class="string">'N;/Regex/i\插入的内容'</span> file</span><br><span class="line">sed -<span class="tag">i</span> <span class="string">'N;/Regex/a\插入的内容'</span> file</span><br></pre></td></tr></table></figure>
<p>比如</p>
<figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JVM_OPTS=<span class="string">"$JVM_OPTS -Djava.net.preferIPv4Stack=true"</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># JVM_OPTS=<span class="string">"$JVM_OPTS -Djava.rmi.server.hostname=&lt;public name&gt;"</span></span></span><br></pre></td></tr></table></figure>
<p>要在preferIPv4Stack这一行后面插入RMI</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i '<span class="keyword">N</span>;/preferIPv4Stack/a\JVM_OPTS=<span class="string">"$JVM_OPTS -Djava.rmi.server.hostname=192.168.6.52"</span>' apache-cassandra-2.2.6/<span class="keyword">conf</span>/cassandra-env.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JVM_OPTS=<span class="string">"<span class="variable">$JVM_OPTS</span> -Djava.net.preferIPv4Stack=true"</span></span><br><span class="line"></span><br><span class="line">JVM_OPTS=<span class="string">"<span class="variable">$JVM_OPTS</span> -Djava.rmi.server.hostname=192.168.6.52"</span></span><br><span class="line"><span class="comment"># JVM_OPTS="$JVM_OPTS -Djava.rmi.server.hostname=&lt;public name&gt;"</span></span><br></pre></td></tr></table></figure>
<h3 id="截取时间段日志文件">截取时间段日志文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">end=`date +<span class="string">"%Y-%m-%d %H"</span>`</span><br><span class="line">start=`date <span class="operator">-d</span> <span class="string">"-1 hour"</span> +<span class="string">"%Y-%m-%d %H"</span>`</span><br><span class="line">gcCount=`sed -n <span class="string">"/<span class="variable">$start</span>:[0-9][0-9]:[0-9][0-9]/,/<span class="variable">$end</span>:[0-9][0-9]:[0-9][0-9]/p"</span> /usr/install/cassandra/logs/system.log | grep <span class="string">'GC in [0-9][0-9][0-9][0-9][0-9]'</span> | wc <span class="operator">-l</span>`</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$start</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$end</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$gcCount</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$gcCount</span> &gt; <span class="number">0</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="comment">#告警</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -n '/2016-06-30 18:[<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]:[<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]/,/2016-06-30 19:[<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]:[<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]/p' /usr/install/cassandra/logs/system.log | grep 'GC in [<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>]' | wc -l</span><br></pre></td></tr></table></figure>
<h2 id="10-_脚本">10. 脚本</h2><h3 id="制作程序启动脚本:">制作程序启动脚本:</h3><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">arg=<span class="variable">$1</span></span><br><span class="line">cmd=<span class="string">''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$arg</span> == <span class="string">'idea'</span> ]; <span class="keyword">then</span></span><br><span class="line">  cmd=<span class="string">'/home/hadoop/tool/idea-IU-139.225.3/bin/idea.sh'</span></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo <span class="string">'应用程序启动命令：$cmd'</span></span><br><span class="line">nohup <span class="variable">$cmd</span> &amp;</span><br></pre></td></tr></table></figure>
<h3 id="免密码登录:expect">免密码登录:expect</h3><p><a href="http://wgkgood.blog.51cto.com/1192594/1271543" target="_blank" rel="external">http://wgkgood.blog.51cto.com/1192594/1271543</a><br><a href="http://blog.51yip.com/linux/1462.html" target="_blank" rel="external">http://blog.51yip.com/linux/1462.html</a><br><a href="http://bbs.chinaunix.net/thread-915007-1-1.html" target="_blank" rel="external">http://bbs.chinaunix.net/thread-915007-1-1.html</a><br><a href="http://os.51cto.com/art/200912/167898.htm" target="_blank" rel="external">http://os.51cto.com/art/200912/167898.htm</a> </p>
<p>访问服务器的通用命令是: ssh [-port] user@host. 通常情况下需要输入密码. 可以使用expect交互命令, 使用命令行直接登录.  </p>
<figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/expect</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> timeout <span class="number">30</span></span><br><span class="line">spawn ssh [<span class="keyword">lindex</span> <span class="variable">$argv</span> <span class="number">0</span>]@[<span class="keyword">lindex</span> <span class="variable">$argv</span> <span class="number">1</span>]</span><br><span class="line">expect &#123;</span><br><span class="line">        <span class="string">"(yes/no)?"</span></span><br><span class="line">        &#123;send <span class="string">"yes\n"</span>;exp_continue&#125;</span><br><span class="line">        <span class="string">"password:"</span></span><br><span class="line">        &#123;send <span class="string">"[lindex $argv 2]\n"</span>&#125;</span><br><span class="line">&#125;</span><br><span class="line">interact</span><br></pre></td></tr></table></figure>
<p>使用方式: 指定远程机器的用户名,IP地址,密码,[端口]: <code>./login.exp USERNAME HOST PASS [PORT]</code></p>
<h3 id="免密码登录:ssh">免密码登录:ssh</h3><p>在本机ssh-keygen,并将密钥拷贝到目标机器当前访问用户的<code>~/ssh/authorized_keys</code>下</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  ~  ssh qihuang.zheng<span class="macrocall">@JUMP_HOST</span></span><br><span class="line"></span><br><span class="line">每次都要输入用户名<span class="macrocall">@远程服务器地址</span>, 可以把它们写死在一个文件里. </span><br><span class="line">➜  ~  cat jump</span><br><span class="line">qihuang.zheng<span class="macrocall">@JUMP_HOST</span></span><br><span class="line">➜  ~  ssh <span class="string">`cat jump`</span></span><br><span class="line"></span><br><span class="line">或者把ssh qihuang.zheng<span class="macrocall">@JUMP_HOST整个写在一个脚本里并加入到PATH里</span>. 只需要执行脚本即可: sshjump</span><br></pre></td></tr></table></figure>
<h3 id="跳板机访问远程命令:端口映射">跳板机访问远程命令:端口映射</h3><p>👉 将远程ssh端口22映射到本地指定的端口:  <code>ssh -f qihuang.zheng@JUMP_HOST -L 127.0.0.1:2207:192.168.47.207:22 -N</code><br>解释下上面的命令了:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JUMP_HOST         👉跳板机</span><br><span class="line"><span class="number">192.168</span><span class="number">.47</span><span class="number">.207</span>:<span class="number">22</span> 👉远程机器的ssh默认端口</span><br><span class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">2207</span>      👉映射到本机端口</span><br></pre></td></tr></table></figure>
<p>👉 现在远程服务器的ssh端口已经映射到本地的2207端口了,所以可以直接:  <code>ssh -p 2207 qihuang.zheng@localhost</code><br>👉 或者使用调用expect脚本自动登陆的方式:  <code>login.exp qihuang.zheng localhost $pass $port</code></p>
<p>端口映射脚本:  </p>
<figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/expect</span></span><br><span class="line"><span class="keyword">set</span> timeout <span class="number">30</span></span><br><span class="line">spawn ssh -f qihuang.zheng@JUMP_HOST -L <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:[<span class="keyword">lindex</span> <span class="variable">$argv</span> <span class="number">1</span>]:<span class="number">192.168</span><span class="number">.47</span>.[<span class="keyword">lindex</span> <span class="variable">$argv</span> <span class="number">0</span>]:[<span class="keyword">lindex</span> <span class="variable">$argv</span> <span class="number">1</span>] -N</span><br><span class="line">expect &#123;</span><br><span class="line">  <span class="string">"password:"</span></span><br><span class="line">  &#123;send <span class="string">"YOURPASSWORD@\n"</span>&#125;</span><br><span class="line">&#125;</span><br><span class="line">interact</span><br></pre></td></tr></table></figure>
<p>使用方式: <code>map.exp 222 8888</code></p>
<h3 id="命令行时间格式转换(Mac版)">命令行时间格式转换(Mac版)</h3><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">当前时间撮</span><br><span class="line">➜  ~  date +<span class="variable">%s</span></span><br><span class="line"><span class="number">1448811002</span></span><br><span class="line"></span><br><span class="line">格式化当前时间</span><br><span class="line">➜ date <span class="string">"+<span class="variable">%Y</span><span class="variable">%m</span><span class="variable">%d</span><span class="variable">%H</span><span class="variable">%M</span><span class="variable">%S</span>"</span></span><br><span class="line"><span class="number">20150717085930</span></span><br><span class="line">➜ date <span class="string">"+<span class="variable">%Y</span>-<span class="variable">%m</span>-<span class="variable">%d</span> <span class="variable">%H</span>:<span class="variable">%M</span>:<span class="variable">%S</span>"</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">17</span> 09:<span class="number">01</span>:<span class="number">32</span></span><br><span class="line"></span><br><span class="line">指定一个时间的时间撮</span><br><span class="line">➜  ~  date -j -f <span class="string">"<span class="variable">%Y</span>-<span class="variable">%m</span>-<span class="variable">%d</span> <span class="variable">%H</span>:<span class="variable">%M</span>:<span class="variable">%S</span>"</span> <span class="string">"2015-10-21 18:03:00"</span> <span class="string">"+<span class="variable">%s</span>"</span></span><br><span class="line"><span class="number">1445421780</span></span><br><span class="line"></span><br><span class="line">将时间错转换为human格式</span><br><span class="line">➜  ~  date -r <span class="number">1445421780</span></span><br><span class="line"><span class="number">2015</span>年<span class="number">10</span>月<span class="number">21</span>日 星期三 <span class="number">18</span>时<span class="number">03</span>分<span class="number">00</span>秒 CST</span><br></pre></td></tr></table></figure>
<p>上面的是在shell终端的结果, 如果要在bash脚本中获取, 则使用``执行命令</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ echo `date <span class="string">"+%Y-%m-%d %H:%M:%S"</span>`</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">17</span> <span class="number">09</span>:<span class="number">04</span>:<span class="number">23</span></span><br><span class="line">➜ currentDate=`date <span class="string">"+%Y-%m-%d %H:%M:%S"</span>`</span><br><span class="line">➜ echo $currentDate</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">17</span> <span class="number">09</span>:<span class="number">05</span>:<span class="number">38</span></span><br></pre></td></tr></table></figure>
<h3 id="正则表达式">正则表达式</h3><p>日志开始部分</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(\d&#123;<span class="number">4</span>&#125;)-(<span class="number">0</span>\d&#123;<span class="number">1</span>&#125;|<span class="number">1</span>[<span class="number">0</span>-<span class="number">2</span>])-(\d&#123;<span class="number">2</span>&#125;) (\d&#123;<span class="number">2</span>&#125;):(\d&#123;<span class="number">2</span>&#125;):(\d&#123;<span class="number">2</span>&#125;):(\d&#123;<span class="number">3</span>&#125;) (\[main\])</span><br></pre></td></tr></table></figure>
<h2 id="11-_Shell">11. Shell</h2><h3 id="Condition">Condition</h3><p>if条件：</p>
<p>循环：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> `seq <span class="number">0</span> <span class="number">10</span>`; do echo <span class="variable">$i</span>; nodetool cfstats md5s.md5_id<span class="number">_</span><span class="variable">$i</span> | grep memory ;done</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nums=(<span class="string">'0'</span> <span class="string">'1'</span> <span class="string">'2'</span> <span class="string">'3'</span> <span class="string">'4'</span> <span class="string">'5'</span> <span class="string">'6'</span> <span class="string">'7'</span> <span class="string">'8'</span> <span class="string">'9'</span> <span class="string">'a'</span> <span class="string">'b'</span> <span class="string">'c'</span> <span class="string">'d'</span> <span class="string">'e'</span> <span class="string">'f'</span>)</span><br><span class="line"><span class="keyword">for</span> n1 <span class="keyword">in</span> <span class="variable">$&#123;nums[@]&#125;</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="variable">$n1</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"end."</span></span><br></pre></td></tr></table></figure>
<h3 id="Date">Date</h3><p>Mac:</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">date</span> -r <span class="number">1471190400</span></span><br></pre></td></tr></table></figure>
<p>Linux</p>
<figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">date</span> +<span class="variable">%s</span></span><br><span class="line"><span class="number">1471254010</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'@'</span></span><br><span class="line"><span class="number">2016</span>年 <span class="number">08</span>月 <span class="number">15</span>日 星期一 <span class="number">17</span>:<span class="number">40</span>:<span class="number">10</span> CST</span><br><span class="line"></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'2016-08-15'</span> +<span class="variable">%s</span></span><br><span class="line"><span class="number">1471190400</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'@1471190400'</span></span><br><span class="line"><span class="number">2016</span>年 <span class="number">08</span>月 <span class="number">15</span>日 星期一 <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> CST</span><br><span class="line"></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'2016-08-15 13:20:24'</span> +<span class="variable">%s</span></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'@1471238424'</span></span><br><span class="line"><span class="number">2016</span>年 <span class="number">08</span>月 <span class="number">15</span>日 星期一 <span class="number">13</span>:<span class="number">20</span>:<span class="number">24</span> CST</span><br><span class="line"></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'yesterday'</span></span><br><span class="line"><span class="number">2016</span>年 <span class="number">08</span>月 <span class="number">15</span>日 星期一 <span class="number">13</span>:<span class="number">20</span>:<span class="number">24</span> CST</span><br><span class="line"></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'yesterday'</span> +<span class="string">"%Y-%-m-%-d"</span></span><br></pre></td></tr></table></figure>
<p>bge=”2016-08-12 15:50:00”<br>end=”2016-08-12 16:10:00”</p>
<p>beg=<code>date -d &#39;2016-08-11 15:39:52&#39; +%s</code><br>end=<code>date -d &#39;2016-08-11 16:10:19&#39; +%s</code><br>head compaction.log | awk ‘{if($4/1000&gt;=$beg &amp;&amp; $4/1000&lt;=$end) print $0}’</p>
<p><a href="http://blog.csdn.net/jk110333/article/details/8590746" target="_blank" rel="external">http://blog.csdn.net/jk110333/article/details/8590746</a>  </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sh range.sh 20160401 20160405</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#datebeg="20160401"</span></span><br><span class="line"><span class="comment">#dateend="20160405"</span></span><br><span class="line">datebeg=<span class="variable">$1</span></span><br><span class="line">dateend=<span class="variable">$2</span></span><br><span class="line">beg_s=`date <span class="operator">-d</span> <span class="string">"<span class="variable">$datebeg</span>"</span> +%s`</span><br><span class="line">end_s=`date <span class="operator">-d</span> <span class="string">"<span class="variable">$dateend</span>"</span> +%s`</span><br><span class="line"></span><br><span class="line">excludes=<span class="string">"20150101 20150102"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> [ <span class="string">"<span class="variable">$beg_s</span>"</span> -le <span class="string">"<span class="variable">$end_s</span>"</span> ];<span class="keyword">do</span></span><br><span class="line">     day=`date <span class="operator">-d</span> @<span class="variable">$beg_s</span> +<span class="string">"%Y%m%d"</span>`;</span><br><span class="line">     beg_s=$((beg_s+<span class="number">86400</span>));</span><br><span class="line">     flag=<span class="literal">false</span></span><br><span class="line">     <span class="keyword">for</span> item <span class="keyword">in</span> <span class="variable">$excludes</span></span><br><span class="line">     <span class="keyword">do</span></span><br><span class="line">        <span class="keyword">if</span> [ <span class="string">"<span class="variable">$day</span>"</span> == <span class="string">"<span class="variable">$item</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">          <span class="built_in">echo</span> <span class="string">"<span class="variable">$day</span> In the list, skip"</span></span><br><span class="line">          flag=<span class="literal">true</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">     <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">     <span class="keyword">if</span> [ <span class="variable">$flag</span> == <span class="literal">false</span> ]; <span class="keyword">then</span></span><br><span class="line">       <span class="built_in">echo</span> <span class="variable">$day</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#代码中只要关心处理一天的记录,由脚本控制,执行多天</span></span><br><span class="line">        /usr/install/spark-<span class="number">1.6</span>.<span class="number">1</span>-bin-hadoop2.<span class="number">4</span>/bin/spark-submit \</span><br><span class="line">        --conf spark.mesos.role=production --conf spark.cores.max=<span class="number">30</span> --conf spark.executor.memory=<span class="number">10</span>g --conf spark.ui.port=<span class="number">4888</span> \</span><br><span class="line">        --conf spark.cassandra.connection.host=<span class="number">192.168</span>.<span class="number">48.163</span> \</span><br><span class="line">        --class cn.fraudmetrix.vulcan.velocity.VelocityRuleApp \</span><br><span class="line">        spark-cassandra-<span class="number">1.0</span>.<span class="number">1</span>-SNAPSHOT-jar-with-dependencies.jar cass <span class="variable">$day</span> &gt; VelocityRuleApp_<span class="variable">$day</span>.log</span><br><span class="line">     <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h1 id="Develop">Develop</h1><p>jar包解压查看是否有某个文件: jar -tvf abc.jar | grep FileName</p>
<p>jar包运行： java -cp xxx-dependency.jar MainClass args<br>如果是打成fat包，可以这么运行。</p>
<p>但是如果不是fat包，而且依赖了第三方包：java -cp third.jar -jar Run.jar MainClass</p>
<p>jmap -dump:live,format=b,file=<filename>.hprof <pid></pid></filename></p>
<p>jstats -gc <pid></pid></p>
<p>mat命令行：<a href="http://www.techpaste.com/2015/07/how-to-analyse-large-heap-dumps/" target="_blank" rel="external">http://www.techpaste.com/2015/07/how-to-analyse-large-heap-dumps/</a>  </p>
<p><img src="http://img.blog.csdn.net/20161221111359916" alt="mat"></p>
<p><img src="http://img.blog.csdn.net/20161222135615298" alt="mat"></p>
<p>mysql权限：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="string">`pontus`</span> <span class="keyword">DEFAULT</span> <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci;</span></span><br><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">user</span> <span class="string">'pontus'</span>@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'pontus'</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">grant</span> all <span class="keyword">on</span> pontus.* <span class="keyword">to</span> <span class="string">'pontus'</span>@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'pontus'</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">grant</span> all <span class="keyword">on</span> pontus.* <span class="keyword">to</span> <span class="string">'pontus'</span>@<span class="string">'dp0653'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'pontus'</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'pontus'</span> <span class="keyword">and</span> host=<span class="string">'dp0653'</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,<span class="keyword">password</span>,host <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'pontus'</span>;</span></span><br></pre></td></tr></table></figure>
<h2 id="快捷键">快捷键</h2><h3 id="Mac">Mac</h3><p><strong>👉按键</strong>  </p>
<figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Shift</span> - ⬆️      为什么是向上,因为<span class="keyword">Shift</span>在Ctrl+Alt+<span class="built_in">CMD</span>所有键的上方</span><br></pre></td></tr></table></figure>
<p><strong>👉触摸板</strong><br>右键: 双指单击<br>三指移动: 当前App移动位置<br>四指滑动: 全屏下App切换屏幕<br>两指滑动: Launchpad切换屏幕  </p>
<p><strong>👉箭头</strong><br>FN+⬅️ 一行的开始(Home)和结束位置(End)<br>FN+⬆️ 上一页(Up),下一页(Down)<br>ALT+⬅️ 一个一个单词地移动</p>
<p><strong>终端</strong><br>CMD+⬅️:  标签切换</p>
<p>Control+CMD+A: 使用QQ的截图</p>
<h3 id="IDEA">IDEA</h3><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">CMD</span>+<span class="number">4</span>  Run Window</span><br><span class="line">Go To Implementation实现方法:   Alt+<span class="built_in">CMD</span>+B</span><br><span class="line">Back and Forword前进后退:       Alt+<span class="built_in">CMD</span>+左右</span><br><span class="line"><span class="built_in">Type</span> Hierarchy类型树:           Ctl+H</span><br><span class="line"><span class="flow">Call</span> Hierarchy调用栈:           Ctl+Alt+H</span><br><span class="line">Into进入方法或类:                <span class="built_in">CMD</span>+单击 或者Fn+F4</span><br><span class="line">全局搜索字符串:                  <span class="built_in">CMD</span>+<span class="keyword">Shift</span>+F</span><br></pre></td></tr></table></figure>
<h3 id="Sublime">Sublime</h3><p><a href="http://www.jianshu.com/p/3cb5c6f2421c" target="_blank" rel="external">http://www.jianshu.com/p/3cb5c6f2421c</a></p>
<table>
<thead>
<tr>
<th>按键</th>
<th>常用指数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Shift+CMD+P</td>
<td></td>
<td>然后输入ip 安装PackageControl install    </td>
</tr>
<tr>
<td>Alt+CMD+O</td>
<td></td>
<td>预览MD(OmniMarkupPreview), 在本地浏览器打开  </td>
</tr>
<tr>
<td>CMD+R</td>
<td></td>
<td>Markdown的主题列表, 或者源文件的函数列表  </td>
</tr>
<tr>
<td>CMD+P</td>
<td>⭐️⭐️⭐️⭐️⭐️</td>
<td>查找整个Workspace的文件 </td>
</tr>
<tr>
<td>CMD+P，@</td>
<td>⭐️⭐️⭐️</td>
<td>Markdown的大纲</td>
</tr>
<tr>
<td>Alt+CMD+⬇</td>
<td></td>
<td>go to definition  </td>
</tr>
<tr>
<td>CMD+Shift+T</td>
<td></td>
<td>打开上一个关闭的文件  </td>
</tr>
</tbody>
</table>
<p>GoBuild: CMD+b</p>
<p><strong>Sublime自定义设置(以MD GFM为例)</strong><br>Preferences &gt; Package Settings &gt; Markdown Edit &gt; Markdown GFM Settings - User  </p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">word_wrap</span>": <span class="value"><span class="literal">false</span></span>,</span><br><span class="line">    "<span class="attribute">wrap_width</span>": <span class="value"><span class="number">150</span></span>,</span><br><span class="line">    "<span class="attribute">line_numbers</span>": <span class="value"><span class="literal">true</span></span>,</span><br><span class="line">    "<span class="attribute">highlight_line</span>": <span class="value"><span class="literal">true</span></span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>效果就是打开md文件, 预览时宽度变宽了, 而不再是窄窄的. 而且较长的行不会自动换行 </p>
<p><strong>在一个窗口内显示多个文件夹</strong><br>打开一个文件夹之后, Project &gt; Add Folder to Project &gt; Save Project As .. &gt; 保存在本地<br>下次直接选择keyspace的名称即可打开上次所有的文件夹</p>
<h1 id="Mac_Dailay">Mac Dailay</h1><p>date +%s<br>date -d “2010-07-20 10:25:30” +%s</p>
<h1 id="Git">Git</h1><h2 id="QuickStart">QuickStart</h2><table>
<thead>
<tr>
<th>说明</th>
<th>命令</th>
</tr>
</thead>
<tbody>
<tr>
<td>从master<strong>创建并切换</strong>到dev分支</td>
<td><code>git checkout -b dev</code></td>
</tr>
<tr>
<td>相当于创建分支和并切换到分支</td>
<td><code>git branch dev &amp;&amp; git checkout dev</code></td>
</tr>
<tr>
<td>在分支上正常<strong>提交</strong>所有文件</td>
<td><code>git add . &amp;&amp; git commit -m &#39;commit log&#39;</code></td>
</tr>
<tr>
<td><strong>切换</strong>回master分支</td>
<td><code>git checkout master</code></td>
</tr>
<tr>
<td>把dev的<strong>分支合并</strong>到当前分支上</td>
<td><code>git merge dev</code></td>
</tr>
<tr>
<td>删除dev分支</td>
<td><code>git branch -d dev</code></td>
</tr>
<tr>
<td>提交dev分支</td>
<td><code>git push origin dev</code></td>
</tr>
</tbody>
</table>
<h2 id="分支">分支</h2><p><strong>在主分支上创建一个新的分支,修改文件, 添加到版本库,提交,远程推送到分支</strong>  </p>
<p>1.在主分支上创建一个新的分支(-b),并切换(checkout)到这个新的分支上.<br>这里最后还可以跟上一个基础分支名称,表示要从哪个基础分支上创建新的分支,<br>这里因为在master上,所以是以master版本创建一个新的分支(默认在当前分支上创建新的分支).  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  test git:(master) git checkout -<span class="tag">b</span> branch001          创建并切换分支</span><br><span class="line">Switched to <span class="tag">a</span> new branch <span class="string">'branch001'</span></span><br></pre></td></tr></table></figure>
<p>2.在分支上的修改,提交和正常的一样</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  test git:(<span class="keyword">branch001) </span>vi README.md                    修改文件</span><br><span class="line">➜  test git:(<span class="keyword">branch001) </span>✗ git <span class="keyword">add </span>.                     加入版本控制</span><br><span class="line">➜  test git:(<span class="keyword">branch001) </span>✗ git commit -m <span class="string">'branch'</span>        提交</span><br><span class="line">[<span class="keyword">branch001 </span><span class="keyword">b078a04] </span><span class="keyword">branch</span><br><span class="line"></span> <span class="number">1</span> file changed, <span class="number">1</span> insertion(+)</span><br></pre></td></tr></table></figure>
<p>3.提交分支到远程仓库<br>在当前新的分支上执行git push origin 分支名字,表示将当前新的分支push到远程仓库对应的分支上.<br>origin是远程仓库的别名. git push origin master表示将master分支推送到远程的master分支上.<br>注意:不要在master分支上执行git push origin branch001.那样会把master分支推送到远程的branch001分支上.<br>也就是说,本地要push的分支和当前所在的分支有关,而origin后面的分支名称,代表的是远程的某个分支.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> ➜  test git:(branch001) git push origin branch001      远程PUSH</span><br><span class="line">Counting objects: <span class="number">3</span>, done.</span><br><span class="line">Delta compression <span class="keyword">using</span> up to <span class="number">4</span> threads.</span><br><span class="line">Compressing objects: <span class="number">100</span>% (<span class="number">2</span>/<span class="number">2</span>), done.</span><br><span class="line">Writing objects: <span class="number">100</span>% (<span class="number">3</span>/<span class="number">3</span>), <span class="number">287</span> bytes | <span class="number">0</span> bytes/s, done.</span><br><span class="line">Total <span class="number">3</span> (delta <span class="number">0</span>), reused <span class="number">0</span> (delta <span class="number">0</span>)</span><br><span class="line">To https:<span class="comment">//github.com/zqhxuyuan/test.git</span></span><br><span class="line"> * [<span class="keyword">new</span> branch]      branch001 -&gt; branch001</span><br></pre></td></tr></table></figure>
<p>4.在分支下文件被修改,回到主分支,文件没有被修改. 即分支的修改对于主分支不可见.     </p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜  test gi<span class="variable">t:</span>(branch001) <span class="keyword">cat</span> README.md                   分支的内容</span><br><span class="line"># test</span><br><span class="line">branch...</span><br><span class="line"></span><br><span class="line">➜  test gi<span class="variable">t:</span>(branch001) git checkout master             主分支</span><br><span class="line">Switched <span class="keyword">to</span> branch <span class="string">'master'</span></span><br><span class="line">Your branch <span class="keyword">is</span> <span class="keyword">up</span>-<span class="keyword">to</span>-date with <span class="string">'origin/master'</span>.</span><br><span class="line"></span><br><span class="line">➜  test gi<span class="variable">t:</span>(master) <span class="keyword">cat</span> README.md                      主分支不可见分支</span><br><span class="line"># test</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以把分支的文件合并到主分支上.操作方式是在主分支上merge其他分支: git merge branch001 </p>
</blockquote>
<h2 id="git_flow">git flow</h2><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">master<span class="prompt">&gt;&gt;  </span>git branch develop                            在master分支创建一个develop分支</span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git push -u origin develop                    提交develop分支,虽然这个分支和master分支是一样的,因为还没有做任何修改</span><br><span class="line"></span><br><span class="line">git clone <span class="symbol">ssh:</span>/<span class="regexp">/user@host/path</span><span class="regexp">/to/repo</span>.git              克隆仓库,因为上面提交了develop,所以开发分支也会被拉取到本地</span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git checkout -b develop origin/develop        基于远程仓库的develop分支,切换到develop分支(因为分支已经存在,-b不会在建了)</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git checkout -b some-feature develop          基于develop分支创建一个功能分支,并切换到这个功能分支</span><br><span class="line">feature<span class="prompt">&gt;&gt; </span>git status</span><br><span class="line">feature<span class="prompt">&gt;&gt; </span>git add .</span><br><span class="line">feature<span class="prompt">&gt;&gt; </span>git commit -m <span class="string">'feature'</span></span><br><span class="line">feature<span class="prompt">&gt;&gt; </span>git pull origin develop                       拉取最新的develop分支</span><br><span class="line">feature<span class="prompt">&gt;&gt; </span>git checkout develop                          切换到develop分支</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git merge some-feature                        合并功能分支到develop分支</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git push                                      提交本地的develop分支到远程的develop分支,因为上面设置了-u,所以可以不用手动添加origin develop</span><br><span class="line"></span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git checkout -b release-<span class="number">0</span>.<span class="number">1</span> develop           在最新的develop上创建一个用于新版本发布的分支</span><br><span class="line">release<span class="prompt">&gt;&gt; </span>....                                          在发布版本上的操作和普通的分支一样</span><br><span class="line">release<span class="prompt">&gt;&gt; </span>git checkout master                           切换到master分支</span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git merge release-<span class="number">0</span>.<span class="number">1</span>                         将发布版本的分支合并到本地的master分支</span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git push                                      提交本地的master分支提交到远程的master分支??</span><br><span class="line">master&gt;&gt;  git checkout develop                          切换到develop分支</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git merge release-<span class="number">0</span>.<span class="number">1</span>                         同样将发布版本的分支合并到本地的develop分支</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git push                                      提交到远程develop分支</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git branch -d release-<span class="number">0</span>.<span class="number">1</span>                     删除发布版本这个分支</span><br><span class="line"></span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git checkout -b issue-<span class="comment">#001 master             基于master分支新创建并切换到一个新的hotfix分支</span></span><br><span class="line">hotfix<span class="prompt">&gt;&gt;  </span><span class="comment"># Fix the bug                                 在hotfix分支上正常操作</span></span><br><span class="line">hotfix<span class="prompt">&gt;&gt;  </span>git checkout master                           切换回master分支</span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git merge issue-<span class="comment">#001                          合并hotfix分支的修改到本地master分支</span></span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git push                                      提交到远程master分支</span><br><span class="line"></span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git checkout develop                          hotfix的修改要同步到master和develop分支上</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git merge issue-<span class="comment">#001</span></span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git push</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git branch -d issue-<span class="comment">#001</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>-u选项设置本地分支去跟踪远程对应的分支。 设置好跟踪的分支后，可以使用git push命令省去指定推送分支的参数</p>
</blockquote>
<h2 id="解决冲突">解决冲突</h2><p>场景: 本地修改了pom.xml,和远程已有的pom.xml不一致.  </p>
<p>1.拉取数据时,指出了local changes发生在pom.xml文件. 如果直接提交,会被拒绝的.   </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">➜  td-offline <span class="string">git:</span>(master) ✗ git pull</span><br><span class="line"><span class="string">remote:</span> Counting <span class="string">objects:</span> <span class="number">14</span>, done.</span><br><span class="line"><span class="string">remote:</span> Compressing <span class="string">objects:</span> <span class="number">100</span>% (<span class="number">9</span>/<span class="number">9</span>), done.</span><br><span class="line"><span class="string">remote:</span> Total <span class="number">14</span> (delta <span class="number">5</span>), reused <span class="number">0</span> (delta <span class="number">0</span>)</span><br><span class="line">Unpacking <span class="string">objects:</span> <span class="number">100</span>% (<span class="number">14</span>/<span class="number">14</span>), done.</span><br><span class="line">From gitlab.fraudmetrix.<span class="string">cn:</span>dp/td-offline</span><br><span class="line">   <span class="number">9061</span>c47..ce0b695  master     -&gt; origin/master</span><br><span class="line">Updating <span class="number">9061</span>c47..ce0b695</span><br><span class="line"><span class="string">error:</span> Your local changes to the following files would be overwritten by <span class="string">merge:</span></span><br><span class="line">    pom.xml</span><br><span class="line">Please, commit your changes or stash them before you can merge.</span><br><span class="line">Aborting</span><br><span class="line"></span><br><span class="line">➜  td-offline <span class="string">git:</span>(master) ✗ git add .</span><br><span class="line">➜  td-offline <span class="string">git:</span>(master) ✗ git commit -m <span class="string">'split module out'</span></span><br><span class="line"></span><br><span class="line">➜  td-offline <span class="string">git:</span>(master) git push origin master</span><br><span class="line">To git<span class="annotation">@gitlab</span>.fraudmetrix.<span class="string">cn:</span>dp/td-offline.git</span><br><span class="line"> ! [rejected]        master -&gt; master (non-fast-forward)</span><br><span class="line"><span class="string">error:</span> failed to push some refs to <span class="string">'git@gitlab.fraudmetrix.cn:dp/td-offline.git'</span></span><br><span class="line"><span class="string">hint:</span> Updates were rejected because the tip of your current branch is behind</span><br><span class="line"><span class="string">hint:</span> its remote counterpart. Integrate the remote changes (e.g.</span><br><span class="line"><span class="string">hint:</span> <span class="string">'git pull ...'</span>) before pushing again.</span><br><span class="line"><span class="string">hint:</span> See the <span class="string">'Note about fast-forwards'</span> <span class="keyword">in</span> <span class="string">'git push --help'</span> <span class="keyword">for</span> details.</span><br></pre></td></tr></table></figure>
<p>2.再次pull,会把远程的pom.xml和本地修改后的pom.xml进行尝试合并.  </p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  td-offline git:(master) git pull</span><br><span class="line"><span class="type">Auto</span>-merging pom.xml</span><br><span class="line"><span class="type">CONFLICT</span> (content): <span class="type">Merge</span> conflict <span class="keyword">in</span> pom.xml</span><br><span class="line"><span class="type">Automatic</span> merge failed; fix conflicts <span class="keyword">and</span> then commit the <span class="literal">result</span>.</span><br></pre></td></tr></table></figure>
<p>上面显示合并失败,要自己去修复,然后提交.  </p>
<p>3.修改发生冲突的文件,一般是把自动添加的特殊字符删掉,然后提交.  </p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  td-offline git:(<span class="literal">master</span>) ✗ vi pom.<span class="keyword">xml</span></span><br><span class="line"><span class="title">➜  td-offline</span> git:(<span class="literal">master</span>) ✗ git add .</span><br><span class="line">➜  td-offline git:(<span class="literal">master</span>) ✗ git commit -m 'split module out'</span><br><span class="line">[<span class="keyword">master</span> <span class="title">6f86dcd</span>] split module out</span><br></pre></td></tr></table></figure>
<p>4.最后,成功push到远程分支上. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  td-offline git:(master) git push origin master</span><br><span class="line">Counting objects: <span class="number">67</span>, done.</span><br><span class="line">Delta compression <span class="keyword">using</span> up to <span class="number">4</span> threads.</span><br><span class="line">Compressing objects: <span class="number">100</span>% (<span class="number">35</span>/<span class="number">35</span>), done.</span><br><span class="line">Writing objects: <span class="number">100</span>% (<span class="number">67</span>/<span class="number">67</span>), <span class="number">47.34</span> KiB | <span class="number">0</span> bytes/s, done.</span><br><span class="line">Total <span class="number">67</span> (delta <span class="number">9</span>), reused <span class="number">0</span> (delta <span class="number">0</span>)</span><br><span class="line">To git@gitlab.fraudmetrix.cn:dp/td-offline.git</span><br><span class="line">   ce0b695.<span class="number">.6</span>f86dcd  master -&gt; master</span><br></pre></td></tr></table></figure>
<p>5.尝试拉取,已经是最新的代码了</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  td-offline gi<span class="variable">t:</span>(master) git pull</span><br><span class="line">Already <span class="keyword">up</span>-<span class="keyword">to</span>-date.</span><br></pre></td></tr></table></figure>
<h2 id="忽略文件">忽略文件</h2><p>已经提交的文件，可以从cache中删除</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">➜  riemann-cassandra git:(master) ✗ git <span class="operator"><span class="keyword">commit</span> -<span class="keyword">m</span> <span class="string">'update rieman and cassandra version'</span></span><br><span class="line">[<span class="keyword">master</span> <span class="number">460</span>d78c] <span class="keyword">update</span> rieman <span class="keyword">and</span> cassandra <span class="keyword">version</span></span><br><span class="line"> <span class="number">4</span> files <span class="keyword">changed</span>, <span class="number">118</span> insertions(+), <span class="number">22</span> deletions(-)</span><br><span class="line"> <span class="keyword">create</span> <span class="keyword">mode</span> <span class="number">100644</span> .gitignore</span><br><span class="line"> <span class="keyword">create</span> <span class="keyword">mode</span> <span class="number">100644</span> riemann-cassandra.iml</span><br><span class="line"></span><br><span class="line">➜  riemann-cassandra git:(<span class="keyword">master</span>) git rm  <span class="comment">--cached riemann-cassandra.iml</span></span><br><span class="line">rm <span class="string">'riemann-cassandra.iml'</span></span><br><span class="line"></span><br><span class="line">➜  riemann-cassandra git:(<span class="keyword">master</span>) ✗ git <span class="keyword">status</span></span><br><span class="line"><span class="keyword">On</span> branch <span class="keyword">master</span></span><br><span class="line">Your branch <span class="keyword">is</span> ahead <span class="keyword">of</span> <span class="string">'origin/master'</span> <span class="keyword">by</span> <span class="number">1</span> <span class="keyword">commit</span>.</span><br><span class="line">  (<span class="keyword">use</span> <span class="string">"git push"</span> <span class="keyword">to</span> publish your <span class="keyword">local</span> commits)</span><br><span class="line">Changes <span class="keyword">to</span> be committed:</span><br><span class="line">  (<span class="keyword">use</span> <span class="string">"git reset HEAD &lt;file&gt;..."</span> <span class="keyword">to</span> unstage)</span><br><span class="line"></span><br><span class="line">    deleted:    riemann-cassandra.iml</span><br><span class="line"></span><br><span class="line">➜  riemann-cassandra git:(<span class="keyword">master</span>) ✗ git <span class="keyword">commit</span> -<span class="keyword">m</span> <span class="string">'update rieman and cassandra version'</span></span><br><span class="line">[<span class="keyword">master</span> b56cd65] <span class="keyword">update</span> rieman <span class="keyword">and</span> cassandra <span class="keyword">version</span></span><br><span class="line"> <span class="number">1</span> <span class="keyword">file</span> <span class="keyword">changed</span>, <span class="number">58</span> deletions(-)</span><br><span class="line"> <span class="keyword">delete</span> <span class="keyword">mode</span> <span class="number">100644</span> riemann-cassandra.iml</span></span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;日常工作常用命令（Linux、Mac）&lt;br&gt;
    
    </summary>
    
      <category term="work" scheme="http://github.com/zqhxuyuan/categories/work/"/>
    
    
      <category term="ops" scheme="http://github.com/zqhxuyuan/tags/ops/"/>
    
  </entry>
  
  <entry>
    <title>监控工具集锦</title>
    <link href="http://github.com/zqhxuyuan/2016/12/31/BigData-Monitor-Tool/"/>
    <id>http://github.com/zqhxuyuan/2016/12/31/BigData-Monitor-Tool/</id>
    <published>2016-12-30T16:00:00.000Z</published>
    <updated>2016-06-13T23:59:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>监控工具：ZooKeeper，Kafka，Cassandra…<br><a id="more"></a></p>
<h2 id="ZooKeeper">ZooKeeper</h2><h3 id="node-zk-browser">node-zk-browser</h3><p>依赖条件：nodejs, npm</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/killme2008/node-zk-browser.git</span><br><span class="line">$ <span class="built_in">cd</span> node-zk-browser</span><br><span class="line">$ cat package.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"node-zk-browser"</span>,</span><br><span class="line">  <span class="string">"version"</span>: <span class="string">"0.0.2"</span>,</span><br><span class="line">  <span class="string">"dependencies"</span>: &#123;</span><br><span class="line">    <span class="string">"ejs"</span>: <span class="string">"0.7.2"</span>,</span><br><span class="line">    <span class="string">"express"</span>: <span class="string">"3.2.6"</span>,</span><br><span class="line">    <span class="string">"zookeeper"</span>:<span class="string">"*"</span>,</span><br><span class="line">    <span class="string">"express-namespace"</span>:<span class="string">"0.1.1"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">$ npm install <span class="operator">-d</span></span><br><span class="line">$ node app.js</span><br><span class="line">Express server listening on port <span class="number">3000</span></span><br><span class="line">zk session established, id=<span class="number">154</span>a418d5dd000b</span><br></pre></td></tr></table></figure>
<p><a href="http://localhost:3000" target="_blank" rel="external">http://localhost:3000</a> admin:admin</p>
<p><img src="http://img.blog.csdn.net/20160516095143332" alt="node-zk"></p>
<h3 id="zkui">zkui</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/DeemOpen/zkui.git</span><br><span class="line">$ mvn clean install</span><br><span class="line">$ cp target/zkui-<span class="number">2.0</span>-SNAPSHOT-jar-with-dependencies.jar .</span><br><span class="line">$ java -jar zkui-<span class="number">2.0</span>-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>
<p><a href="http://localhost:9090/" target="_blank" rel="external">http://localhost:9090/</a>  admin:manager</p>
<p><img src="http://img.blog.csdn.net/20160516094813768" alt="zkui"></p>
<h3 id="zk-web">zk-web</h3><p>依赖条件：lein</p>
<figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="char">$ </span>git clone <span class="method">https:</span>//github.com/qiuxiafei/zk-web</span><br><span class="line"><span class="char">$ </span>cd zk-web</span><br><span class="line"><span class="char">$ </span>lein deps</span><br><span class="line"><span class="char">$ </span>lein run</span><br></pre></td></tr></table></figure>
<p><a href="http://localhost:9090/" target="_blank" rel="external">http://localhost:9090/</a> admin:hello</p>
<p><img src="http://img.blog.csdn.net/20160516094831940" alt="zkweb"></p>
<h2 id="Kafka">Kafka</h2><h3 id="trifecta">trifecta</h3><p>下载<a href="https://github.com/ldaniels528/trifecta/releases">https://github.com/ldaniels528/trifecta/releases</a>中的zip包（web ui）， jar是命令行工具（trifecta_cli）。</p>
<p>依赖条件：scalascript</p>
<p><strong>编译并发布scalascript</strong>  </p>
<p>需要先将scalascript发布到本地仓库，默认sbt会发布到ivy下</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">➜  git clone https:<span class="comment">//github.com/ldaniels528/scalascript.git</span></span><br><span class="line">➜  cd scalascript </span><br><span class="line"></span><br><span class="line">修改sbt版本为当前安装的sbt版本，否则如果没有修改，会去下载，尽量使用已有的</span><br><span class="line">➜  cat project/build.properties  </span><br><span class="line">sbt.version=<span class="number">0.13</span><span class="number">.9</span></span><br><span class="line">➜  sbt publish-local  </span><br><span class="line">Getting org.scala-sbt sbt <span class="number">0.13</span><span class="number">.9</span> ...</span><br><span class="line">downloading http:<span class="comment">//repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt/0.13.9/jars/sbt.jar ...</span></span><br><span class="line">    [SUCCESSFUL ] org.scala-sbt<span class="preprocessor">#sbt;<span class="number">0.13</span><span class="number">.9</span>!sbt.jar (<span class="number">5084</span>ms)</span></span><br><span class="line"></span><br><span class="line">使用<span class="number">0.13</span><span class="number">.11</span>就不会下载了</span><br><span class="line">➜  sbt publish-local</span><br><span class="line">[info] Loading global plugins from /Users/zhengqh/.sbt/<span class="number">0.13</span>/plugins</span><br><span class="line">[info] Loading project definition from /Users/zhengqh/Soft/scalascript/project</span><br><span class="line">[info] Packaging /Users/zhengqh/Soft/scalascript/target/scala-<span class="number">2.11</span>/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>-<span class="number">0.2</span><span class="number">.20</span>.jar ...</span><br><span class="line">[info] Done packaging.</span><br><span class="line">[info]  published scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>/<span class="number">0.2</span><span class="number">.20</span>/poms/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>.pom</span><br><span class="line">[info]  published scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>/<span class="number">0.2</span><span class="number">.20</span>/jars/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>.jar</span><br><span class="line">[info]  published scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>/<span class="number">0.2</span><span class="number">.20</span>/srcs/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>-sources.jar</span><br><span class="line">[info]  published scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>/<span class="number">0.2</span><span class="number">.20</span>/docs/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>-javadoc.jar</span><br><span class="line">[info]  published ivy to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>/<span class="number">0.2</span><span class="number">.20</span>/ivys/ivy.xml</span><br><span class="line">[success] Total time: <span class="number">71</span> s, completed <span class="number">2016</span>-<span class="number">5</span>-<span class="number">16</span> <span class="number">10</span>:<span class="number">11</span>:<span class="number">20</span></span><br></pre></td></tr></table></figure>
<p><strong>编译并发布trifecta</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  git clone https:<span class="comment">//github.com/ldaniels528/trifecta.git</span></span><br><span class="line">➜  cd trifecta</span><br><span class="line">➜  sbt publish-local</span><br><span class="line">[info]  published trifecta_cli_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/trifecta_cli_2<span class="number">.11</span>/<span class="number">0.20</span><span class="number">.0</span>/poms/trifecta_cli_2<span class="number">.11</span>.pom</span><br><span class="line">[info]  published trifecta_cli_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/trifecta_cli_2<span class="number">.11</span>/<span class="number">0.20</span><span class="number">.0</span>/jars/trifecta_cli_2<span class="number">.11</span>.jar</span><br><span class="line">[info]  published trifecta_cli_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/trifecta_cli_2<span class="number">.11</span>/<span class="number">0.20</span><span class="number">.0</span>/srcs/trifecta_cli_2<span class="number">.11</span>-sources.jar</span><br><span class="line">[info]  published trifecta_cli_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/trifecta_cli_2<span class="number">.11</span>/<span class="number">0.20</span><span class="number">.0</span>/docs/trifecta_cli_2<span class="number">.11</span>-javadoc.jar</span><br><span class="line">[info]  published ivy to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/trifecta_cli_2<span class="number">.11</span>/<span class="number">0.20</span><span class="number">.0</span>/ivys/ivy.xml</span><br><span class="line">[success] Total time: <span class="number">75</span> s, completed <span class="number">2016</span>-<span class="number">5</span>-<span class="number">16</span> <span class="number">9</span>:<span class="number">51</span>:<span class="number">09</span></span><br></pre></td></tr></table></figure>
<p>现在已经编译好了trifecta_cli_2.11，这和release的cli.jar是一样的。 不过我们想要编译ui的话：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">➜  sbt "project trifecta_ui" dist</span><br><span class="line">[info] Loading global plugins from /Users/zhengqh/.sbt/0.13/plugins</span><br><span class="line">[info] Loading project definition from /Users/zhengqh/Soft/trifecta/project</span><br><span class="line">[info] <span class="operator"><span class="keyword">Set</span> <span class="keyword">current</span> <span class="keyword">project</span> <span class="keyword">to</span> trifecta_core (<span class="keyword">in</span> <span class="keyword">build</span> <span class="keyword">file</span>:/<span class="keyword">Users</span>/zhengqh/Soft/trifecta/)</span><br><span class="line">[info] <span class="keyword">Set</span> <span class="keyword">current</span> <span class="keyword">project</span> <span class="keyword">to</span> trifecta_cli (<span class="keyword">in</span> <span class="keyword">build</span> <span class="keyword">file</span>:/<span class="keyword">Users</span>/zhengqh/Soft/trifecta/)</span><br><span class="line">[info] <span class="keyword">Set</span> <span class="keyword">current</span> <span class="keyword">project</span> <span class="keyword">to</span> trifecta_ui (<span class="keyword">in</span> <span class="keyword">build</span> <span class="keyword">file</span>:/<span class="keyword">Users</span>/zhengqh/Soft/trifecta/)</span><br><span class="line">...</span><br><span class="line">[info] Done packaging.</span><br><span class="line">[info]</span><br><span class="line">[info] Your <span class="keyword">package</span> <span class="keyword">is</span> ready <span class="keyword">in</span> /<span class="keyword">Users</span>/zhengqh/Soft/trifecta/app-play/target/universal/trifecta_ui-<span class="number">0.20</span><span class="number">.0</span>.zip</span><br><span class="line">[info]</span><br><span class="line">[<span class="keyword">success</span>] Total <span class="keyword">time</span>: <span class="number">109</span> s, completed <span class="number">2016</span>-<span class="number">5</span>-<span class="number">16</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">01</span></span><br><span class="line"></span><br><span class="line">➜  unzip /<span class="keyword">Users</span>/zhengqh/Soft/trifecta/app-play/target/universal/trifecta_ui-<span class="number">0.20</span><span class="number">.0</span>.zip</span><br><span class="line"><span class="keyword">Archive</span>:  /<span class="keyword">Users</span>/zhengqh/Soft/trifecta/app-play/target/universal/trifecta_ui-<span class="number">0.20</span><span class="number">.0</span>.zip</span><br><span class="line">➜  cd trifecta_ui-<span class="number">0.20</span><span class="number">.0</span>/<span class="keyword">bin</span></span><br><span class="line">➜  ./trifecta_ui</span><br><span class="line">[info] <span class="keyword">p</span>.a.<span class="keyword">l</span>.<span class="keyword">c</span>.ActorSystemProvider - <span class="keyword">Starting</span> application <span class="keyword">default</span> Akka <span class="keyword">system</span>: application</span><br><span class="line">[info] application - Application has started</span><br><span class="line">[info] play.api.Play$ - Application started (Prod)</span><br><span class="line">[info] <span class="keyword">p</span>.<span class="keyword">c</span>.s.NettyServer$ - Listening <span class="keyword">for</span> <span class="keyword">HTTP</span> <span class="keyword">on</span> /<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">9000</span></span><br><span class="line">[info] application - Loading Trifecta configuration...</span><br><span class="line">[info] application - <span class="keyword">Starting</span> Zookeeper <span class="keyword">client</span>...</span><br><span class="line">[info] application - created actor com.github.ldaniels528.trifecta.actors.SSEClientHandlingActor</span><br><span class="line">[info] application - Registering <span class="keyword">new</span> SSE <span class="keyword">session</span> <span class="string">'8d82b54dc125406f875c98e9f05e4a4f'</span>...</span><br><span class="line">[info] application - <span class="keyword">GET</span> /api/sse/<span class="keyword">connect</span> ~&gt; <span class="number">200</span> [<span class="number">927.0</span> ms]</span></span><br></pre></td></tr></table></figure>
<p><a href="http://localhost:9000" target="_blank" rel="external">http://localhost:9000</a></p>
<p><img src="http://img.blog.csdn.net/20160516102612129" alt="trifecta"></p>
<h2 id="Cassandra">Cassandra</h2><h3 id="Riemman">Riemman</h3><h4 id="准备工作">准备工作</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rvm install <span class="number">1.9</span><span class="number">.3</span> --with-gcc=clang</span><br><span class="line">rvm <span class="number">1.9</span><span class="number">.3</span> --<span class="keyword">default</span></span><br><span class="line">rvm <span class="built_in">list</span></span><br><span class="line">ruby -v</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget https:<span class="comment">//aphyr.com/riemann/riemann-0.2.11.tar.bz2</span></span><br><span class="line">tar xvfj riemann-<span class="number">0.2</span><span class="number">.11</span>.tar.bz2</span><br><span class="line">cd riemann-<span class="number">0.2</span><span class="number">.11</span></span><br><span class="line"></span><br><span class="line">➜  riemann-<span class="number">0.2</span><span class="number">.11</span> bin/riemann etc/riemann.config</span><br><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">49</span>,<span class="number">069</span>] main - riemann.bin - PID <span class="number">70661</span></span><br><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">49</span>,<span class="number">078</span>] main - riemann.bin - Loading /Users/zhengqh/Soft/riemann-<span class="number">0.2</span><span class="number">.11</span>/etc/riemann.config</span><br><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">49</span>,<span class="number">334</span>] clojure-agent-send-off-pool-<span class="number">1</span> - riemann.transport.websockets - Websockets server <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> <span class="number">5556</span> online</span><br><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">49</span>,<span class="number">629</span>] clojure-agent-send-off-pool-<span class="number">2</span> - riemann.transport.udp - UDP server <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> <span class="number">5555</span> <span class="number">16384</span> -<span class="number">1</span> online</span><br><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">49</span>,<span class="number">656</span>] clojure-agent-send-off-pool-<span class="number">3</span> - riemann.transport.tcp - TCP server <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> <span class="number">5555</span> online</span><br><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">49</span>,<span class="number">659</span>] main - riemann.core - Hyperspace core online</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">gem install riemann-client riemann-tools riemann-dash</span><br><span class="line"></span><br><span class="line">➜  ~ riemann-dash</span><br><span class="line">DEPRECATION WARNING:</span><br><span class="line">Sass <span class="number">3.5</span> will no longer support Ruby <span class="number">1.9</span><span class="number">.3</span>.</span><br><span class="line">Please upgrade to Ruby <span class="number">2.0</span><span class="number">.0</span> or greater as soon as possible.</span><br><span class="line"></span><br><span class="line">No configuration loaded; <span class="keyword">using</span> defaults.</span><br><span class="line">[<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">12</span>:<span class="number">12</span>:<span class="number">35</span>] INFO  WEBrick <span class="number">1.3</span><span class="number">.1</span></span><br><span class="line">[<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">12</span>:<span class="number">12</span>:<span class="number">35</span>] INFO  ruby <span class="number">1.9</span><span class="number">.3</span> (<span class="number">2014</span>-<span class="number">11</span>-<span class="number">13</span>) [x86_64-darwin14<span class="number">.1</span><span class="number">.1</span>]</span><br><span class="line">== Sinatra (v1<span class="number">.4</span><span class="number">.7</span>) has taken the stage on <span class="number">4567</span> <span class="keyword">for</span> development with backup from WEBrick</span><br><span class="line">[<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">12</span>:<span class="number">12</span>:<span class="number">35</span>] INFO  WEBrick::HTTPServer<span class="preprocessor">#start: pid=<span class="number">20843</span> port=<span class="number">4567</span></span></span><br></pre></td></tr></table></figure>
<p>Mac下command并单击某个区域，这部分会变成深灰色</p>
<p><img src="http://img.blog.csdn.net/20160519131446630" alt="rieman1"></p>
<p>按快捷键e，出现编辑页面</p>
<p><img src="http://img.blog.csdn.net/20160519131501802" alt="rieman2"></p>
<p>点击Apply后，按Esc键，会退出编辑状态，深灰色变为正常颜色</p>
<p><img src="http://img.blog.csdn.net/20160519131513286" alt="rieman3"></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ riemann-health</span><br></pre></td></tr></table></figure>
<p>打包：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  riemann-cassandra <span class="string">git:</span>(master) ✗ mvn <span class="keyword">package</span></span><br><span class="line">[INFO] Replacing <span class="regexp">/Users/</span>zhengqh<span class="regexp">/Github/</span>riemann-cassandra<span class="regexp">/target/</span>riemann-cassandra-<span class="number">0.0</span><span class="number">.3</span>.jar with </span><br><span class="line"><span class="regexp">/Users/</span>zhengqh<span class="regexp">/Github/</span>riemann-cassandra<span class="regexp">/target/</span>riemann-cassandra-<span class="number">0.0</span><span class="number">.3</span>-shaded.jar</span><br></pre></td></tr></table></figure>
<p>启动客户端：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">java -jar target/riemann-cassandra-<span class="number">0.0</span><span class="number">.3</span>.jar \</span><br><span class="line">    -riemann_host localhost \</span><br><span class="line">    -riemann_port <span class="number">5555</span> \</span><br><span class="line">    -cassandra_host localhost \</span><br><span class="line">    -jmx_port <span class="number">7199</span> \</span><br><span class="line">    -jmx_username null \</span><br><span class="line">    -jmx_password null \</span><br><span class="line">    -interval_seconds <span class="number">5</span></span><br><span class="line"></span><br><span class="line">success start riemann-cassandra-client............@Thu May <span class="number">19</span> <span class="number">14</span>:<span class="number">50</span>:<span class="number">04</span> CST <span class="number">2016</span></span><br><span class="line">^Cclosed riemann-cassandra-client@Thu May <span class="number">19</span> <span class="number">16</span>:<span class="number">14</span>:<span class="number">28</span> CST <span class="number">2016</span></span><br></pre></td></tr></table></figure>
<p>Event的格式：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    <span class="attribute">host</span>: <span class="string">"www1"</span>, </span><br><span class="line">    <span class="attribute">service</span>: <span class="string">"http req"</span>, </span><br><span class="line">    <span class="attribute">metric</span>: <span class="number">2.53</span>, </span><br><span class="line">    <span class="attribute">state</span>: <span class="string">"critical"</span>, </span><br><span class="line">    <span class="attribute">description</span>: <span class="string">"Request took 2.53 seconds."</span>, </span><br><span class="line">    <span class="attribute">tags</span>: [<span class="string">"http"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应的riemann后台日志：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">14</span>:<span class="number">01</span>:<span class="number">46</span>,<span class="number">512</span>] Thread-<span class="number">7</span> - riemann.config - expired </span><br><span class="line">&#123;:host www1, :service http req, :state expired, :time <span class="number">365909426627</span>/<span class="number">250</span>, :ttl <span class="number">60</span>&#125;</span><br></pre></td></tr></table></figure>
<p>riemann-cassandra中的事件格式：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    <span class="attribute">host</span>: <span class="string">"localhost"</span>, </span><br><span class="line">    <span class="attribute">service</span>: <span class="string">"cassandra.db.forseti.velocity_global.max_row_size_ob"</span>, </span><br><span class="line">    <span class="attribute">metric</span>: <span class="number">148</span>,</span><br><span class="line">    <span class="attribute">tags</span>: [<span class="string">"cassandra"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>riemann服务端后台日志：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">47</span>:<span class="number">54</span>,<span class="number">195</span>] Thread-<span class="number">7</span> - riemann.config - expired </span><br><span class="line">&#123;:host <span class="number">10.57</span><span class="number">.2</span><span class="number">.36</span>, :service cassandra.db.forseti.velocity_partner.mean_row_size_kb, :state expired, :time <span class="number">292729534839</span>/<span class="number">200</span>, :ttl <span class="number">60</span>&#125;</span><br></pre></td></tr></table></figure>
<p>查询所有表的read_latency, query中支持不同的查询</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tagged <span class="string">"cassandra"</span>  <span class="keyword">and</span> service =~ <span class="string">"cassandra.db.forseti.%.read_%"</span></span><br></pre></td></tr></table></figure>
<h2 id="Tracing">Tracing</h2><ul>
<li><a href="http://htrace.incubator.apache.org/" target="_blank" rel="external">http://htrace.incubator.apache.org/</a></li>
<li><a href="https://github.com/naver/pinpoint">https://github.com/naver/pinpoint</a></li>
<li></li>
</ul>
<h3 id="zipkin">zipkin</h3><p>准备环境</p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">alias <span class="variable">npm=</span><span class="string">"npm --registry=https://registry.npm.taobao.org --disturl=https://npm.taobao.org/mirrors/node"</span></span><br><span class="line">yum install npm -y</span><br><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/creationix/nvm.git</span><br><span class="line">source nvm/install.sh</span><br><span class="line"><span class="keyword">node</span><span class="identifier"> </span><span class="title">-v</span> &amp;&amp; npm -v &amp;&amp; nvm -v</span><br><span class="line">nvm install v5.<span class="number">5.0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ tar zxf zipkin-<span class="number">1.40</span><span class="number">.1</span>.tar.gz</span><br><span class="line">$ cd zipkin-<span class="number">1.40</span><span class="number">.1</span></span><br><span class="line">$ bin/query</span><br></pre></td></tr></table></figure>
<p>vi zipkin-ui/webpack.config.js</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">:zipkin-ui:npmBuild</span><br><span class="line"></span><br><span class="line">&gt; zipkin-ui@<span class="number">0.0</span><span class="number">.0</span> build /home/qihuang.zheng/zipkin-<span class="number">1.40</span><span class="number">.1</span>/zipkin-ui</span><br><span class="line">&gt; node node_modules/webpack/bin/webpack.js --bail</span><br><span class="line"></span><br><span class="line">ModuleBuildError: Module build failed: Error: /home/qihuang.zheng/zipkin-<span class="number">1.40</span><span class="number">.1</span>/zipkin-ui/node_modules/node-sass/vendor/linux-x64-<span class="number">47</span>/binding.node: ELF load command past end of file</span><br><span class="line">npm ERR! Linux <span class="number">3.10</span><span class="number">.96</span>-<span class="number">1.</span>el6.elrepo.x86_64</span><br><span class="line">npm ERR! argv <span class="string">"/home/qihuang.zheng/zipkin-1.40.1/zipkin-ui/build/nodejs/node-v5.5.0-linux-x64/bin/node"</span> <span class="string">"/home/qihuang.zheng/zipkin-1.40.1/zipkin-ui/build/nodejs/node-v5.5.0-linux-x64/lib/node_modules/npm/bin/npm-cli.js"</span> <span class="string">"run-script"</span> <span class="string">"build"</span></span><br><span class="line">npm ERR! node v5<span class="number">.5</span><span class="number">.0</span></span><br><span class="line">npm ERR! npm  v3<span class="number">.3</span><span class="number">.12</span></span><br><span class="line">npm ERR! code ELIFECYCLE</span><br><span class="line">npm ERR! zipkin-ui@<span class="number">0.0</span><span class="number">.0</span> build: `node node_modules/webpack/bin/webpack.js --bail`</span><br><span class="line">npm ERR! Exit status <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="cat">cat</h3><h3 id="HTrace">HTrace</h3><h2 id="Operation">Operation</h2><h3 id="FrameGraph">FrameGraph</h3><p>git clone <a href="https://github.com/brendangregg/FlameGraph">https://github.com/brendangregg/FlameGraph</a><br>cd FlameGraph<br>sudo perf record –call-graph dwarf -p $(pgrep scylla)<br>sudo perf script | <code>pwd</code>/stackcollapse-perf.pl | <code>pwd</code>/flamegraph.pl &gt; flame.svg</p>
<p><a href="http://blog.csdn.net/justlinux2010/article/details/11520829" target="_blank" rel="external">http://blog.csdn.net/justlinux2010/article/details/11520829</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;监控工具：ZooKeeper，Kafka，Cassandra…&lt;br&gt;
    
    </summary>
    
      <category term="work" scheme="http://github.com/zqhxuyuan/categories/work/"/>
    
    
      <category term="ops" scheme="http://github.com/zqhxuyuan/tags/ops/"/>
    
  </entry>
  
  <entry>
    <title>译：Kafka事件驱动和流处理</title>
    <link href="http://github.com/zqhxuyuan/2016/11/18/Kafka-CQRS-Streams/"/>
    <id>http://github.com/zqhxuyuan/2016/11/18/Kafka-CQRS-Streams/</id>
    <published>2016-11-17T16:00:00.000Z</published>
    <updated>2016-10-29T03:50:13.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/" target="_blank" rel="external">http://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/</a><br><a id="more"></a></p>
<p>Event sourcing as an application architecture pattern is rising in popularity. Event sourcing involves modeling the state changes made by applications as an immutable sequence or “log” of events. Instead of modifying the state of the application in-place, event sourcing involves storing the event that triggers the state change in an immutable log and modeling the state changes as responses to the events in the log. We previously wrote about event sourcing, Apache Kafka and how they are related. In this post, I explore these ideas further and show how stream processing and, in particular, Kafka Streams helps to put Event sourcing and CQRS into practice.</p>
<blockquote>
<p>事件驱动（Event sourcing）作为应用程序架构的一种模式现在越来越流行了。事件驱动包括了将应用程序产生的状态改变数据建模成不可变的序列、或者说是事件的日志。它不是直接就地修改应用程序的状态，事件驱动会将触发状态改变的事件存储到一个不可变的日志，在日志中将状态改变建模成针对发生事件的响应。在<a href="http://www.confluent.io/blog/making-sense-of-stream-processing/" target="_blank" rel="external">之前的文章</a>中，我们阐述了事件驱动和Kafka的关系。本篇文章，我们会更进一步探索这些思想，并且会展示流处理，更准确的说是Kafka Streams，怎么帮助我们将事件驱动和CQRS运用在实际中。  </p>
</blockquote>
<p>Let’s take an example. Consider a Facebook-like social networking app (albeit a completely hypothetical one) that updates the profiles database when a user updates their Facebook profile. There are several applications that need to be notified when a user updates their profile — the search application so the user’s profile can be reindexed to be searchable on the changed attribute; the newsfeed application so the user’s connections can find out about the profile update; the data warehouse ETL application to load the latest profile data into the central data warehouse that powers various analytical queries and so on.</p>
<blockquote>
<p>举例类似于Facebook这样的社交网络应用，当用户在Facebook的设置页面更新自己的信息时会更新profiles数据库。有很多应用因为和个人信息相关，在用户更新他们的设置时，这些应用都需要被通知到。比如通知搜索应用，用户的设置可以被重新索引，被改变的属性就可以被搜索出来；通知新闻订阅应用，这样用户的连接就可以找出被更新过的设置；数据仓库应用，加载最新的设置数据到中央仓库，用于不同维护的数据查询和分析等等。</p>
</blockquote>
<p><img src="http://www.confluent.io/wp-content/uploads/2016/09/Event-sourced-based-architecture.jpeg" alt=""><br>基于事件驱动的架构</p>
<p>Event sourcing involves changing the profile web app to model the profile update as an event — something important that happened — and write it to a central log, like a Kafka topic. In this state of the world, all the applications that need to respond to the profile update event, merely subscribe to the Kafka topic and create the respective materialized views – be it a write to cache, index the event in Elasticsearch or simply compute an in-memory aggregate. The profile web app itself also subscribes to the same Kafka topic and writes the update to the profiles database.</p>
<blockquote>
<p>事件驱动会将profile应用程序的profile更新作为一个事件，写入到中央日志系统中，比如一个Kafka的主题。在这个充满状态的世界中，所有需要对profile更新事件作出响应的，所做的工作仅仅是订阅到Kafka的主题，并创建各自的物化视图（消费者读取数据，产生各自的数据），比如将其作为缓存的以写入、Elasticsearch的一个索引事件、或者只是内存中的简单聚合计算。当然profile应用程序本身也要订阅相同的Kafka主题（实际上不是订阅，而是生产，这里的意思是生产者和消费者使用相同的主题，否则生产者生产的消息没有被任何消费者订阅，就没有什么意义了。生产者这里是web应用程序产生的更新事件，消费者是下方各种数据源），并且会将更新写入到profiles数据库。</p>
</blockquote>
<h2 id="事件驱动：一些利弊">事件驱动：一些利弊</h2><p>There are several advantages to modeling applications to use event sourcing — It provides a complete log of every state change ever made to an object; so troubleshooting is easier. By expressing the user intent as an ordered log of immutable events, event sourcing gives the business an audit and compliance(合规性审计) log which also has the added benefit of providing data provenance(起源). It enables resilient(弹性) applications; rolling back applications amounts to rewinding(绕回) the event log and reprocessing data. It has better performance characteristics; writes and reads can be scaled independently. It enables a loosely coupled(松耦合) application architecture; one that makes it easier to move towards a microservices-based architecture. But most importantly:</p>
<p><strong>Event sourcing enables building a forward-compatible(向前兼容) application architecture — the ability to add more applications in the future that need to process the same event but create a different materialized view.</strong></p>
<blockquote>
<p>将应用程序建模成事件驱动有很多优点：它提供了针对一个对象的每个状态改变的完整日志，所以排查问题非常简单。</p>
</blockquote>
<p>For the upsides mentioned above, there are some downsides as well. Event sourcing has a higher learning curve(曲线); it is a new and unfamiliar programming model. The event log might involve more work to query it as it requires converting the events into the required materialized state suitable to query.</p>
<p>That was a quick introduction to event sourcing and some tradeoffs. This article is not meant to go into details of event sourcing or advocate for it’s usage. You can read more about event sourcing and various tradeoffs <a href="http://martinfowler.com/eaaDev/EventSourcing.html" target="_blank" rel="external">here</a>.</p>
<p>除了上面提到的一些优点，当然它也有缺点。事件驱动有更高的学习曲线，它是一种崭新的、不熟悉的编程模型。</p>
<h2 id="Kafka作为事件驱动的支柱">Kafka作为事件驱动的支柱</h2><h2 id="事件驱动和CQRS">事件驱动和CQRS</h2><h2 id="CQRS和Kafka_Streams">CQRS和Kafka Streams</h2><h3 id="方案1：应用程序状态存储到外部存储">方案1：应用程序状态存储到外部存储</h3><h3 id="方案2：应用程序状态存储到Kafka_Streams的本地状态">方案2：应用程序状态存储到Kafka Streams的本地状态</h3><h2 id="Kafka_Streams的交互式查询">Kafka Streams的交互式查询</h2><h2 id="交互式查询的用例">交互式查询的用例</h2><h2 id="使用Kafka作为事件驱动、CQRS">使用Kafka作为事件驱动、CQRS</h2><h3 id="零售店示例">零售店示例</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/&quot;&gt;http://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>ETL Tools</title>
    <link href="http://github.com/zqhxuyuan/2016/11/11/Data-Transform/"/>
    <id>http://github.com/zqhxuyuan/2016/11/11/Data-Transform/</id>
    <published>2016-11-10T16:00:00.000Z</published>
    <updated>2016-11-11T09:23:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>ETL Tools（Sqoop…）<br><a id="more"></a></p>
<h2 id="Sqoop">Sqoop</h2><h3 id="Sqoop1">Sqoop1</h3><ol>
<li>单张表、全部表、查询条件、Direct方式</li>
<li>RDBMS导入到HDFS、从HDFS导出到RDBMS</li>
<li>增量（增量方式、检查列、上一次的值）</li>
<li>Job用来支持增量和定时（重复执行）</li>
<li>Evaluation（DDL/DML），RDBMS的客户端工具而已</li>
</ol>
<p>增量查询示例（how to identify new data）：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span><br><span class="line">--connect jdbc:mysql://localhost/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--table emp \</span><br><span class="line">--m <span class="number">1</span> \</span><br><span class="line">--incremental append \</span><br><span class="line">--check-column id \</span><br><span class="line">-last value <span class="number">1205</span></span><br></pre></td></tr></table></figure>
<p>数据不会被更新：append<br>数据会被更新：lastmodified（check-column是一个时间列，表示记录的更新时间）  </p>
<p>定期执行增量任务，推荐采用作业（会自动存储上一次的值）。创建作业，执行作业：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ sqoop job --create myjob \</span><br><span class="line">--import \</span><br><span class="line">--connect jdbc:mysql://localhost/db \</span><br><span class="line">--username root \</span><br><span class="line">--table employee --m <span class="number">1</span></span><br><span class="line"></span><br><span class="line">$ sqoop job --show myjob</span><br><span class="line">Job: myjob </span><br><span class="line"> Tool: import Options:</span><br><span class="line"> ---------------------------- </span><br><span class="line"> direct.import = <span class="literal">true</span></span><br><span class="line"> codegen.input.delimiters.record = <span class="number">0</span></span><br><span class="line"> hdfs.append.dir = <span class="literal">false</span> </span><br><span class="line"> db.table = employee</span><br><span class="line"> ...</span><br><span class="line"> incremental.last.value = <span class="number">1206</span></span><br><span class="line"> ...</span><br><span class="line"></span><br><span class="line">$ sqoop job --exec myjob</span><br></pre></td></tr></table></figure>
<h3 id="Sqoop2">Sqoop2</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ETL Tools（Sqoop…）&lt;br&gt;
    
    </summary>
    
      <category term="etl" scheme="http://github.com/zqhxuyuan/categories/etl/"/>
    
    
      <category term="etl" scheme="http://github.com/zqhxuyuan/tags/etl/"/>
    
  </entry>
  
  <entry>
    <title>PlayFramework快速入门</title>
    <link href="http://github.com/zqhxuyuan/2016/11/11/Play-Quickstart/"/>
    <id>http://github.com/zqhxuyuan/2016/11/11/Play-Quickstart/</id>
    <published>2016-11-10T16:00:00.000Z</published>
    <updated>2017-01-18T04:07:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>Scala PlayFramework（2.4）入门<br>示例程序：<a href="https://github.com/zqhxuyuan/first-player">https://github.com/zqhxuyuan/first-player</a><br><a id="more"></a></p>
<h1 id="Run!_Run!!_Run!!!">Run! Run!! Run!!!</h1><p><strong>安装activator</strong>  </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">brew install typesafe-activator</span><br><span class="line">activator new first-player</span><br><span class="line"><span class="built_in">cd</span> first-player</span><br><span class="line">activator run</span><br></pre></td></tr></table></figure>
<p><strong>导入IntelliJ IDEA</strong>  </p>
<ol>
<li>install IDEA professional</li>
<li>install PlayFramework support</li>
<li>import SBT project</li>
<li>right click controllers.Products, Run Play2 App in idea</li>
</ol>
<h1 id="Hello_World!">Hello World!</h1><p><code>activator new first-player</code>生成的目录结构如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">➜  first-player tree -L <span class="number">2</span></span><br><span class="line">.</span><br><span class="line">├── activator                   使用activator new才有该文件</span><br><span class="line">├── activator-launch-<span class="number">1.3</span>.<span class="number">4</span><span class="class">.jar</span></span><br><span class="line">├── app </span><br><span class="line">│   ├── controllers             控制器</span><br><span class="line">│   ├── models                  模型</span><br><span class="line">│   └── views                   视图（Web页面）</span><br><span class="line">├── build<span class="class">.sbt</span></span><br><span class="line">├── conf</span><br><span class="line">│   ├── application<span class="class">.conf</span>        配置文件</span><br><span class="line">│   └── routes                  路由配置</span><br><span class="line">├── project</span><br><span class="line">│   ├── build<span class="class">.properties</span></span><br><span class="line">│   ├── plugins<span class="class">.sbt</span>             插件配置</span><br><span class="line">├── public                      样式和脚本</span><br><span class="line">│   ├── images</span><br><span class="line">│   ├── javascripts</span><br><span class="line">│   └── stylesheets</span><br></pre></td></tr></table></figure>
<h2 id="路由和Controller">路由和Controller</h2><p>路由配置：</p>
<figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># http://localhost:9000</span></span><br><span class="line"><span class="keyword">GET</span>     /                           controllers.HomeController.index</span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># http://localhost:9000/hello?n=Play!</span></span><br><span class="line"><span class="keyword">GET</span>     /hello                      controllers.HomeController.hello(n: <span class="built_in">String</span>)</span><br></pre></td></tr></table></figure>
<p>对应的HomeController方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> javax.inject._</span><br><span class="line"><span class="keyword">import</span> models.<span class="type">Product</span></span><br><span class="line"><span class="keyword">import</span> play.api._</span><br><span class="line"><span class="keyword">import</span> play.api.mvc._</span><br><span class="line"></span><br><span class="line"><span class="annotation">@Singleton</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HomeController</span> <span class="title">@Inject</span>(</span>) <span class="keyword">extends</span> <span class="type">Controller</span> &#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">index</span> =</span> <span class="type">Action</span> &#123;</span><br><span class="line">    <span class="type">Ok</span>(views.html.index(<span class="string">"Your new application is ready."</span>))  <span class="comment">//①</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hello</span>(</span>name: <span class="type">String</span>) = <span class="type">Action</span> &#123;</span><br><span class="line">    <span class="comment">//Ok("Hello " + name)</span></span><br><span class="line">    <span class="type">Ok</span>(views.html.hello(name))  <span class="comment">//②</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>①：index方法的Response响应会返回一个Html页面，指向<code>app/views/index.scala.html</code>页面<br>②：hello方法的响应返回<code>app/views/hello.scala.html</code>页面，只不过这个页面会渲染一个参数</p>
<blockquote>
<p>注：如果Controller的Action方法是Ok(“字符串”)则返回的是一个字符串。<br>2.2版本中Controller可以用object，在2.4需要用class，以及@Inject和@Singleton</p>
</blockquote>
<p>本节知识点：</p>
<ol>
<li>路由配置到控制器的映射</li>
<li>Action的Response返回值如何指向页面</li>
<li>Action的参数如何传递给页面</li>
</ol>
<h2 id="页面和渲染">页面和渲染</h2><p>views是Play的html页面，也可以在该页面下新建子目录。在IntelliJ中，在views下创建子目录，<br>也是创建一个Package，，因为views和controllers的等级相同。  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜  app tree views</span><br><span class="line">views</span><br><span class="line">├── hello<span class="class">.scala</span><span class="class">.html</span></span><br><span class="line">├── index<span class="class">.scala</span><span class="class">.html</span></span><br><span class="line">├── main<span class="class">.scala</span><span class="class">.html</span></span><br><span class="line">├── products</span><br><span class="line">│   ├── <span class="tag">details</span><span class="class">.scala</span><span class="class">.html</span></span><br><span class="line">│   ├── edit<span class="class">.scala</span><span class="class">.html</span></span><br><span class="line">│   ├── list<span class="class">.scala</span><span class="class">.html</span></span><br><span class="line">│   └── main<span class="class">.scala</span><span class="class">.html</span></span><br></pre></td></tr></table></figure>
<p>app/views/index.scala.html：index方法中的”Your new application is ready.”<br>会传递到index.scala.html作为第一行的message参数。  </p>
<p>@main调用的是同一个路径的main.scala.html模板页面，也可以用@views.html.main绝对路径。<br>Play的scala.html页面实际上是模板，可以用Scala代码的方式来调用。<br>所以@main方法的第一参数是字符串（标题），第二个参数是Html对象用来表示page body。<br>@play20.welcome是Play内置的一个方法（绝对路径是@views.html.play20）</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@(message: String)</span><br><span class="line"></span><br><span class="line">@main("Welcome to Play") &#123;</span><br><span class="line">    @play20.welcome(message, style = "Scala")</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的@main方法会调用同路径的main.scala.html模板页面（Layout嵌套）。<br>app/views/main.scala.html：第一行有两个参数，而且是参数列表的方式，而不是两个参数放在一起。  </p>
<p>index页面的”Welcome to Play”作为title参数，第二个参数则作为content参数，正好对应了Html类型。</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@(title: String)(content: Html)</span><br><span class="line"></span><br><span class="line"><span class="doctype">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">html</span> <span class="attribute">lang</span>=<span class="value">"en"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">title</span>&gt;</span>@title<span class="tag">&lt;/<span class="title">title</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">link</span> <span class="attribute">rel</span>=<span class="value">"stylesheet"</span> <span class="attribute">media</span>=<span class="value">"screen"</span> <span class="attribute">href</span>=<span class="value">"@routes.Assets.versioned("</span><span class="value">stylesheets</span>/<span class="attribute">main.css</span>")"&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">link</span> <span class="attribute">rel</span>=<span class="value">"shortcut icon"</span> <span class="attribute">type</span>=<span class="value">"image/png"</span> <span class="attribute">href</span>=<span class="value">"@routes.Assets.versioned("</span><span class="value">images</span>/<span class="attribute">favicon.png</span>")"&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">body</span>&gt;</span></span><br><span class="line">        @content</span><br><span class="line">    <span class="tag">&lt;/<span class="title">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>两个参数放在一起的话，类似：@(title: String, content: Html)。对应的调用方式也要更改为：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">@main</span>(<span class="string">"Welcome to Play"</span>, &#123;</span><br><span class="line">    <span class="variable">@play20</span>.<span class="function">welcome</span>(message, style = <span class="string">"Scala"</span>)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>本节知识点：</p>
<ol>
<li>views子目录的创建，以及在控制器中如何访问到指定子目录下的页面</li>
<li>页面的参数定义和使用（@标记后面的都是Scala代码），参数列表</li>
<li>嵌套模板的使用，对应的Scala方法</li>
<li>缺省的模板调用路径是和当前页面相同路径，绝对路径是<code>@views.html.</code></li>
</ol>
<h3 id="模板编译">模板编译</h3><p>views下的每个页面都会被编译成Scala的模板类，下面右图views.html实际上就和控制器的调用是类似的。  </p>
<blockquote>
<p>如果是其他格式，比如json、xml，则用views.json或者views.xml，可见不总是views.html。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161111154235071" alt="play vies"></p>
<p>HomeController的idnex方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span> =</span> <span class="type">Action</span> &#123;</span><br><span class="line">  <span class="type">Ok</span>(views.html.index(<span class="string">"Your new application is ready."</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编译后的文件路径：target/scala-2.11/twirl/main/views.html/index.template.scala<br>index是一个object，对应index类的apply方法，接收一个参数，参数名称为message。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">index_Scope0</span> &#123;</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">index</span>  &#123;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>message: <span class="type">String</span>):play.twirl.api.<span class="type">HtmlFormat</span>.<span class="type">Appendable</span> = &#123;...&#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">render</span>(</span>message:<span class="type">String</span>): play.twirl.api.<span class="type">HtmlFormat</span>.<span class="type">Appendable</span> = apply(message)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span>:</span>((<span class="type">String</span>) =&gt; play.twirl.api.<span class="type">HtmlFormat</span>.<span class="type">Appendable</span>) = (message) =&gt; apply(message)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ref</span>:</span> <span class="keyword">this</span>.<span class="keyword">type</span> = <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">index</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">index_Scope0</span>.<span class="title">index</span></span></span><br></pre></td></tr></table></figure>
<p>index.scala.html调用@main方法对应的views/main.scala.html</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="at_rule">@<span class="keyword">(message:</span> String)</span><br><span class="line">@<span class="function">main</span>(<span class="string">"Welcome to Play"</span>) </span>&#123;</span><br><span class="line">    <span class="at_rule">@<span class="keyword">play20.welcome(message,</span> style = <span class="string">"Scala"</span>)</span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure>
<p>main方法对应的编译文件：target/scala-2.11/twirl/main/views.html/main.template.scala<br>main类的apply方法有两个参数，分别是(title: String)(content: Html)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">main_Scope0</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">main</span> &#123;</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>title: <span class="type">String</span>)(content: <span class="type">Html</span>):play.twirl.api.<span class="type">HtmlFormat</span>.<span class="type">Appendable</span> = &#123; ... &#125;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">render</span>(</span>title:<span class="type">String</span>,content:<span class="type">Html</span>): play.twirl.api.<span class="type">HtmlFormat</span>.<span class="type">Appendable</span> = apply(title)(content)</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">f</span>:</span>((<span class="type">String</span>) =&gt; (<span class="type">Html</span>) =&gt; play.twirl.api.<span class="type">HtmlFormat</span>.<span class="type">Appendable</span>) = (title) =&gt; (content) =&gt; apply(title)(content)</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">ref</span>:</span> <span class="keyword">this</span>.<span class="keyword">type</span> = <span class="keyword">this</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">main</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">main_Scope0</span>.<span class="title">main</span></span></span><br></pre></td></tr></table></figure>
<h1 id="Products_Example">Products Example</h1><p>路由配置：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">POST    /products                      controllers<span class="class">.Products</span><span class="class">.save</span></span><br><span class="line">GET     /products/new                  controllers<span class="class">.Products</span><span class="class">.newProduct</span></span><br><span class="line">GET     /products                      controllers<span class="class">.Products</span><span class="class">.products</span></span><br><span class="line">GET     /products/:ean                 controllers<span class="class">.Products</span><span class="class">.show</span>(ean: Long)</span><br></pre></td></tr></table></figure>
<p>Products控制器，为了支持国际化，添加I18nSupport接口。注意每个Action都添加了<code>implicit request =&gt;</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="annotation">@Singleton</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Products</span> <span class="title">@Inject</span>(</span>) (<span class="keyword">val</span> messagesApi: <span class="type">MessagesApi</span>) <span class="keyword">extends</span> <span class="type">Controller</span> <span class="keyword">with</span> <span class="type">I18nSupport</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">products</span> =</span> <span class="type">Action</span> &#123; <span class="keyword">implicit</span> request =&gt;</span><br><span class="line">    <span class="keyword">val</span> products = <span class="type">Product</span>.findAll</span><br><span class="line">    <span class="type">Ok</span>(views.html.products.list(products))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">show</span>(</span>ean: <span class="type">Long</span>) = <span class="type">Action</span> &#123; <span class="keyword">implicit</span> request =&gt;</span><br><span class="line">    <span class="type">Product</span>.findByEan(ean).map &#123; product =&gt;</span><br><span class="line">      <span class="type">Ok</span>(views.html.products.details(product))</span><br><span class="line">    &#125;.getOrElse(<span class="type">NotFound</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>views/products/list.scala.html</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@(productList: List[Product])(implicit lang: Messages)</span><br><span class="line"></span><br><span class="line">@products.main(Messages("application.name")) &#123;</span><br><span class="line">    &lt;dl class="products"&gt;</span><br><span class="line">    @for(product &lt;- productList) &#123;</span><br><span class="line">        &lt;dt&gt;&lt;a href="@controllers.routes.Products.show(product.ean)"&gt;@product.ean&lt;/a&gt;&lt;/dt&gt;</span><br><span class="line">        &lt;dt&gt;@product.name&lt;/dt&gt;</span><br><span class="line">        &lt;dd&gt;@product.description&lt;/dd&gt;</span><br><span class="line">    &#125;</span><br><span class="line">    &lt;/dl&gt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意第一行的参数<code>productList</code>不能命名为：<code>products</code>，否则第二行的<code>@products.main</code>会报错，<br>它会认为<code>products</code>来自于第一行的参数，但这个参数并没有<code>main</code>方法（List没有main方法）。而实际上<code>@products.main</code><br>代表的是要调用<code>@views.html.products.main</code>对应的<code>views/products/main.scala.html</code>页面。<br>注意：<code>productList</code>参数名称不需要和<code>Products</code>控制器的<code>products</code>方法的<code>products</code>变量相同，只要类型相同即可，所以可以是任意的名称。</p>
</blockquote>
<p>这里还有一个隐式的Messages，用来做国际化的支持。</p>
<p>反向路由：在product的en上超链接：”@controllers.routes.Products.show(product.ean)”，也可以省略掉controllers：”@routes.Products.show(product.ean)”。<br>在从路由配置到控制器时，Play会帮我们创建一个控制器到路由的反向路由配置。比如这里页面要访问”Products.show”方法。  </p>
<p><img src="http://img.blog.csdn.net/20161111154733495" alt="reverse route"></p>
<p>本节知识点：</p>
<ol>
<li>国际化的支持，以及implicit隐式参数的使用</li>
<li>参数名称不能和嵌套模板的子目录名称一样（比如参数productList，子目录是products，两者不能相同）</li>
<li>在页面中访问控制器方法，使用反向路由</li>
</ol>
<h2 id="implicit">implicit</h2><p>正常的Action调用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//normal invoke, must pass Cart Object</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">catalog10</span>(</span>) = <span class="type">Action</span> &#123; request =&gt;</span><br><span class="line">  <span class="keyword">val</span> products = <span class="type">ProductDAO</span>.list</span><br><span class="line">  <span class="type">Ok</span>(views.html.shop.catalog10(products, cart(request)))</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cart</span>(</span>request: <span class="type">RequestHeader</span>) = &#123;</span><br><span class="line">  <span class="type">Cart</span>.demoCart()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应的页面：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@(products: Seq[Product3], cart: Cart)</span><br><span class="line"></span><br><span class="line">@shop.cart10("")(cart) &#123;</span><br><span class="line">    <span class="tag">&lt;<span class="title">h2</span>&gt;</span>Catalog<span class="tag">&lt;/<span class="title">h2</span>&gt;</span></span><br><span class="line">    @views.html.shop.productList(products)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用implicit，不需要传递Cat对象，不过request需要使用implicit，同时还要提供一个能够生成Cart对象的方法或者trait接口</p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//<span class="type">implicit</span> invoke, no need to <span class="keyword">pass</span> Cart</span><br><span class="line">def catalog11() = <span class="keyword">Action</span> &#123; <span class="type">implicit</span> request =&gt;</span><br><span class="line">  val products = ProductDAO.list</span><br><span class="line">  //too many arguments for method apply: (content: C)(<span class="type">implicit</span> writeable: play.api.http.Writeable[C])play.api.mvc.Result <span class="type">in</span> <span class="keyword">class</span> <span class="keyword">Status</span></span><br><span class="line">  //Ok(views.html.shop.catalog11(products), cart(request))</span><br><span class="line"></span><br><span class="line">  //如果没有定义<span class="type">implicit</span>的方法,是不会直接使用上面的cart方法的</span><br><span class="line">  //could not find <span class="type">implicit</span> <span class="keyword">value</span> for <span class="type">parameter</span> cart: models.Cart</span><br><span class="line">  Ok(views.html.shop.catalog11(products))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//方法名称任意,只要定义成<span class="type">implicit</span>即可</span><br><span class="line">//注意:如果去掉返回类型: models.Cart,也会报错说找不到models.Cart,所以要显示声明返回类型</span><br><span class="line"><span class="type">implicit</span> def cartImplicit(<span class="type">implicit</span> request: RequestHeader): models.Cart = &#123;</span><br><span class="line">  Cart.demoCart()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应的页面：可以看到参数中cart变成implicit，调用时，不需要传递cart了。</p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@(products: Seq[Product3])(<span class="type">implicit</span> cart: Cart)</span><br><span class="line"></span><br><span class="line">@shop.cart11(<span class="string">""</span>) &#123;</span><br><span class="line">    &lt;h2&gt;Catalog&lt;/h2&gt;</span><br><span class="line">    @views.html.shop.productList(products)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果没有implicit方法没有显示指定类型，也会报错：</p>
<p><img src="http://img.blog.csdn.net/20161111170901722" alt="implicit 1"></p>
<p>We’ve moved the Cart parameter to a second parameter list and made it implicit,<br>so we can apply this template and omit the second parameter list if an implicit Cart is available on the calling side. </p>
<p>Now we’ve declared the cart method as implicit. In addition, we’ve declared the RequestHeader parameter of<br>both our action and the cart method as implicit. If we now call the views.html.shop.catalog template and<br>omit the Cart parameter, the Scala compiler will look for an implicit Cart in scope. It’ll find the cart method,<br>which requires a RequestHeader parameter that’s also declared as implicit, but that’s also available.<br>We can make our newly created cart method reusable, by moving it into a trait. Then We can<br>now mix this trait into every controller where we need access to our implicit Cart.</p>
<p>我们定义了Action方法的RequestHeader（implicit request）以及cart方法（implicit def cartImplicit）都是implicit。<br>那么调用catalog模板（views.html.shop.catalog11(products)）时就可以省略Cart参数，Scala的编译器会在范围内寻找隐式的Cart对象。<br>它会找到cart方法（implicit def cartImplicit）。为了让隐式的Cart方法可重用，可以定义在一个接口中，这样其他控制器都可以使用这个方法</p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">trait WithCart &#123;</span><br><span class="line">  <span class="type">implicit</span> def cart(<span class="type">implicit</span> request: RequestHeader) = &#123;</span><br><span class="line">    // Get cart from session</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20161111170829019" alt="implicit cart"></p>
<p>虽然使用implicit有诸多好处，但是在需要添加其他implicit参数时，还要在页面中依次添加对应的(implicit)声明。解决办法是用一个对象来表示所有的隐式参数<br>然后在页面中使用这个隐式的对象即可，当需要添加隐式参数时，往对象中添加即可，页面中因为引用的是对象，所以自动继承了这个对象的所有隐式参数。</p>
<p>It’s often necessary to pass multiple values from your controller into your main template. Even with implicit parameters,<br>it would be a hassle to have to add another one each time, because you’d still have to add the implicit parameter to all<br>of the template definitions. One straightforward solution is to create a single class that contains all the objects you<br>need in your template, and pass an instance of that. If you want to add a value to it, you only need to adapt the template<br>where you use it, and the method that constructs it.  </p>
<p>It’s common to pass the RequestHeader or Request to templates, as we’ll see in section 6.7.2.<br>Play provides a WrappedRequest class, which wraps a Request and implements the interface itself as well,<br>so it’s usable as if it were a regular Request. But by extending WrappedRequest, you can add other fields:</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">case class UserDataRequest[<span class="link_label">A</span>](<span class="link_url">val user: User, val cart: Cart, request: Request[A]</span>) extends WrappedRequest(request)</span><br></pre></td></tr></table></figure>
<p>If you pass an instance of this UserDataRequest to your template, you have a refer- ence to the Request, User, and Cart.</p>
<h1 id="Spark集成">Spark集成</h1><p>Play和Spark使用的jackson依赖包有冲突，如果没有显示指定jackson，报错：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">play.api.http.HttpErrorHandlerExceptions$<span class="variable">$anon</span><span class="variable">$1</span>: Execution exception[[RuntimeException: </span><br><span class="line">java.lang.VerifyError: class com.fasterxml.jackson.module.scala.ser.ScalaIteratorSerializer overrides final method withResolved.</span><br><span class="line">(Lcom/fasterxml/jackson/databind/BeanProperty;Lcom/fasterxml/jackson/databind/jsontype/TypeSerializer;</span><br><span class="line">  Lcom/fasterxml/jackson/databind/JsonSerializer;)Lcom/fasterxml/jackson/databind/ser/std/AsArraySerializerBase;]]</span><br><span class="line">  at play.api.http.HttpErrorHandlerExceptions$.throwableToUsefulException(HttpErrorHandler.scala:<span class="number">280</span>)</span><br><span class="line">  at play.api.http.DefaultHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">206</span>)</span><br><span class="line">  at play.api.GlobalSettings<span class="variable">$class</span>.onError(GlobalSettings.scala:<span class="number">160</span>)</span><br><span class="line">  at Global$.onError(Global.scala:<span class="number">11</span>)</span><br><span class="line">  at play.api.http.GlobalSettingsHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">98</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">99</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">344</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">343</span>)</span><br><span class="line">  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<p><a href="https://groups.google.com/forum/#!topic/play-framework/LIf_Ughidcc" target="_blank" rel="external">https://groups.google.com/forum/#!topic/play-framework/LIf_Ughidcc</a><br><a href="https://github.com/FasterXML/jackson-module-scala/issues/214">https://github.com/FasterXML/jackson-module-scala/issues/214</a><br><a href="http://stackoverflow.com/questions/33815396/spark-com-fasterxml-jackson-module-error" target="_blank" rel="external">http://stackoverflow.com/questions/33815396/spark-com-fasterxml-jackson-module-error</a>  </p>
<p>没有指定jackson时，jackson-module-scala和jackson-databind的不一致：</p>
<p><img src="http://img.blog.csdn.net/20161118100439795" alt="play-spark-jackson1"></p>
<p>在build.sbt手动添加<code>jackson-module-scala</code>的依赖，版本和Play的一致。  </p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">libraryDependencies</span> += <span class="string">"com.fasterxml.jackson.module"</span> % <span class="string">"jackson-module-scala_2.11"</span> % <span class="string">"2.7.6"</span></span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20161118100453598" alt="play-spark-jackson2"></p>
<p>如果Actor版本不对，执行Spark时可能报错：比如引入了libraryDependencies += “com.github.dnvriend” %% “akka-persistence-jdbc” % “2.6.8”<br>是的Actor版本从2.4.10升级到2.4.12</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">play.api.http.HttpErrorHandlerExceptions$<span class="variable">$anon</span><span class="variable">$1</span>: Execution exception[[RuntimeException: java.lang.NoSuchMethodError: akka.actor.LocalActorRefProvider.log()Lakka/event/LoggingAdapter;]]</span><br><span class="line">  at play.api.http.HttpErrorHandlerExceptions$.throwableToUsefulException(HttpErrorHandler.scala:<span class="number">293</span>)</span><br><span class="line">  at play.api.http.DefaultHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">220</span>)</span><br><span class="line">  at play.api.GlobalSettings<span class="variable">$class</span>.onError(GlobalSettings.scala:<span class="number">160</span>)</span><br><span class="line">  at Global$.onError(Global.scala:<span class="number">11</span>)</span><br><span class="line">  at play.api.http.GlobalSettingsHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">99</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">344</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">343</span>)</span><br><span class="line">  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<h2 id="play-spark-module">play-spark-module</h2><p><a href="https://github.com/JoaoVasques/play-spark-module">https://github.com/JoaoVasques/play-spark-module</a>  </p>
<p>修改SparkController的timeout时间为30s，访问：<a href="http://localhost:9000/spark/count" target="_blank" rel="external">http://localhost:9000/spark/count</a>  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[warn] o<span class="class">.a</span><span class="class">.h</span><span class="class">.u</span><span class="class">.NativeCodeLoader</span> - Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line">[info] <span class="tag">p</span><span class="class">.m</span><span class="class">.i</span><span class="class">.j</span><span class="class">.p</span><span class="class">.s</span><span class="class">.w</span><span class="class">.SparkJobWorker</span> - Starting job <span class="number">1561847669</span> future</span><br><span class="line"><span class="number">3.216</span></span><br><span class="line">[info] <span class="tag">p</span><span class="class">.m</span><span class="class">.i</span><span class="class">.j</span><span class="class">.p</span><span class="class">.s</span><span class="class">.w</span><span class="class">.SparkJobWorker</span> - Job is done. Shutting down worker</span><br></pre></td></tr></table></figure>
<h2 id="Spark_Launcher">Spark Launcher</h2><p>Package org.apache.spark.launcher：Library for launching Spark applications.<br>This library allows applications to launch Spark programmatically. There’s only one entry point to the library - the SparkLauncher class.</p>
<p>The SparkLauncher.startApplication( org.apache.spark.launcher.SparkAppHandle.Listener…) can be used to start Spark and provide a handle to monitor and control the running application:</p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"><span class="keyword">import</span> org.apache.spark.launcher.SparkAppHandle;</span></span><br><span class="line"><span class="preprocessor"><span class="keyword">import</span> org.apache.spark.launcher.SparkLauncher;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyLauncher</span> </span>&#123;</span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(String[] args) throws Exception &#123;</span><br><span class="line">   SparkAppHandle handle = <span class="keyword">new</span> SparkLauncher()</span><br><span class="line">     .setAppResource(<span class="string">"/my/app.jar"</span>)</span><br><span class="line">     .setMainClass(<span class="string">"my.spark.app.Main"</span>)</span><br><span class="line">     .setMaster(<span class="string">"local"</span>)</span><br><span class="line">     .setConf(SparkLauncher.DRIVER_MEMORY, <span class="string">"2g"</span>)</span><br><span class="line">     .startApplication();</span><br><span class="line">   <span class="comment">// Use handle API to monitor / control application.</span></span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>It’s also possible to launch a raw child process, using the SparkLauncher.launch() method:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.launcher.SparkLauncher;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyLauncher</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">   Process spark = <span class="keyword">new</span> SparkLauncher()</span><br><span class="line">     .setAppResource(<span class="string">"/my/app.jar"</span>)</span><br><span class="line">     .setMainClass(<span class="string">"my.spark.app.Main"</span>)</span><br><span class="line">     .setMaster(<span class="string">"local"</span>)</span><br><span class="line">     .setConf(SparkLauncher.DRIVER_MEMORY, <span class="string">"2g"</span>)</span><br><span class="line">     .launch();</span><br><span class="line">   spark.waitFor();</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>This method requires the calling code to manually manage the child process, including its output streams (to avoid possible deadlocks). It’s recommended that SparkLauncher.startApplication( org.apache.spark.launcher.SparkAppHandle.Listener…) be used instead.</p>
<h3 id="mesos">mesos</h3><p>代码中指定本地文件（不推荐用法）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /Users/zhengqh/Github/scala/simple-app</span><br><span class="line">mvn clean package</span><br><span class="line">~/Soft/spark-<span class="number">1.6</span><span class="number">.2</span>-bin-hadoop2<span class="number">.6</span>/bin/spark-submit --<span class="keyword">class</span> <span class="string">"SimpleApp"</span> --master local\[<span class="number">2</span>\] \</span><br><span class="line">target/scala-<span class="number">2.10</span>/simple-project_2<span class="number">.10</span>-<span class="number">1.0</span>.jar</span><br></pre></td></tr></table></figure>
<p>命令行指定file路径（也可以指定文件类型，比如本地或者hdfs，推荐用法）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~/Soft/spark-<span class="number">1.6</span><span class="number">.2</span>-bin-hadoop2<span class="number">.6</span>/bin/spark-submit --<span class="keyword">class</span> <span class="string">"SimpleApp"</span> --master local\[<span class="number">2</span>\] \</span><br><span class="line">--conf spark.app.logfile=<span class="string">"file:///Users/zhengqh/Github/scala/simple-app/README.md"</span> \</span><br><span class="line">target/scala-<span class="number">2.10</span>/simple-project_2<span class="number">.10</span>-<span class="number">1.0</span>.jar</span><br></pre></td></tr></table></figure>
<p>如果本地环境变量有HADOOP_CONF_DIR，需要注释掉，否则会使用hdfs，报错：  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread "main" java.net.ConnectException: <span class="operator"><span class="keyword">Call</span> <span class="keyword">From</span> zqhmac/<span class="number">10.57</span><span class="number">.2</span><span class="number">.31</span> <span class="keyword">to</span> localhost:<span class="number">9000</span> </span><br><span class="line"><span class="keyword">failed</span> <span class="keyword">on</span> <span class="keyword">connection</span> <span class="keyword">exception</span>: <span class="keyword">java</span>.net.ConnectException: <span class="keyword">Connection</span> refused;</span> For more details see: </span><br><span class="line">http://wiki.apache.org/hadoop/ConnectionRefused</span><br></pre></td></tr></table></figure>
<p>命令行也可以指定其他运行方式，比如mesos，所以不推荐在代码中写死setMaster，除非开发时设置为setMaster(“local[*]”)</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~/Soft/spark-<span class="number">1.6</span><span class="number">.2</span>-bin-hadoop2<span class="number">.6</span>/bin/spark-submit --<span class="keyword">class</span> <span class="string">"SimpleApp"</span> --master <span class="string">"mesos://192.168.6.52:5050"</span> \</span><br><span class="line">--conf spark.app.logfile=<span class="string">"file:///Users/zhengqh/Github/scala/simple-app/README.md"</span> \</span><br><span class="line">target/scala-<span class="number">2.10</span>/simple-project_2<span class="number">.10</span>-<span class="number">1.0</span>.jar</span><br></pre></td></tr></table></figure>
<p>在本地运行时报错：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Failed to <span class="operator"><span class="keyword">load</span> <span class="keyword">native</span> Mesos <span class="keyword">library</span> <span class="keyword">from</span> /<span class="keyword">Users</span>/zhengqh/<span class="keyword">Library</span>/<span class="keyword">Java</span>/Extensions:/<span class="keyword">Library</span>/<span class="keyword">Java</span>/Extensions:</span><br><span class="line">/Network/<span class="keyword">Library</span>/<span class="keyword">Java</span>/Extensions:/<span class="keyword">System</span>/<span class="keyword">Library</span>/<span class="keyword">Java</span>/Extensions:/usr/lib/<span class="keyword">java</span>:.</span><br><span class="line"><span class="keyword">Exception</span> <span class="keyword">in</span> <span class="keyword">thread</span> <span class="string">"main"</span> <span class="keyword">java</span>.lang.UnsatisfiedLinkError: <span class="keyword">no</span> mesos <span class="keyword">in</span> <span class="keyword">java</span>.<span class="keyword">library</span>.<span class="keyword">path</span></span></span><br></pre></td></tr></table></figure>
<p>所以需要在本地安装有mesos环境，Mac安装mesos可以参考<a href="https://mesosphere.com/blog/2014/07/07/installing-mesos-on-your-mac-with-homebrew/" target="_blank" rel="external">https://mesosphere.com/blog/2014/07/07/installing-mesos-on-your-mac-with-homebrew/</a>  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brew <span class="operator"><span class="keyword">install</span> mesos</span><br><span class="line">/usr/<span class="keyword">local</span>/sbin/mesos-<span class="keyword">master</span> <span class="comment">--registry=in_memory --ip=127.0.0.1</span></span></span><br></pre></td></tr></table></figure>
<p>环境变量添加mesos:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For Linux</span></span><br><span class="line">$ <span class="built_in">export</span> MESOS_NATIVE_JAVA_LIBRARY=<span class="string">'/usr/local/lib/libmesos.so'</span></span><br><span class="line"><span class="comment"># For OSX</span></span><br><span class="line">$ <span class="built_in">export</span> MESOS_NATIVE_JAVA_LIBRARY=<span class="string">'/usr/local/lib/libmesos.dylib'</span></span><br></pre></td></tr></table></figure>
<p>本地安装好mesos,设置好mesos库，指定远程mesos：mesos://192.168.6.52:5050</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.</span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">I1121 15:07:33.008949 778174464 sched.cpp:157] Version: 0.22.1</span><br><span class="line">I1121 15:07:33.016233 651489280 sched.cpp:254] New master detected at master<span class="comment">@192.168.6.52:5050</span></span><br></pre></td></tr></table></figure>
<p>但是，执行到上面后就不动了，可能是本地的mesos和远程mesos版本不对：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">~/Soft/spark-<span class="number">1.6</span>.<span class="number">2</span>-bin-hadoop2.<span class="number">6</span>/bin/spark-submit --class <span class="string">"SimpleApp"</span> --master mesos:<span class="comment">//zk://192.168.6.55:2181,192.168.6.56:2181,192.168.6.57:2181/mesos \</span></span><br><span class="line">--conf spark<span class="class">.app</span><span class="class">.logfile</span>=<span class="string">"file:///Users/zhengqh/Github/scala/simple-app/README.md"</span> \</span><br><span class="line">target/scala-<span class="number">2.10</span>/simple-project_2.<span class="number">10</span>-<span class="number">1.0</span><span class="class">.jar</span></span><br><span class="line"></span><br><span class="line">I1121 <span class="number">15</span>:<span class="number">34</span>:<span class="number">42.195302</span> <span class="number">655667200</span> group<span class="class">.cpp</span>:<span class="number">313</span>] Group process (<span class="function"><span class="title">group</span><span class="params">(<span class="number">1</span>)</span></span>@<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">61225</span>) connected to ZooKeeper</span><br><span class="line">I1121 <span class="number">15</span>:<span class="number">34</span>:<span class="number">42.195796</span> <span class="number">655667200</span> group<span class="class">.cpp</span>:<span class="number">790</span>] Syncing group operations: queue size (joins, cancels, datas) = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">I1121 <span class="number">15</span>:<span class="number">34</span>:<span class="number">42.196069</span> <span class="number">655667200</span> group<span class="class">.cpp</span>:<span class="number">385</span>] Trying to create path <span class="string">'/mesos'</span> <span class="keyword">in</span> ZooKeeper</span><br><span class="line">I1121 <span class="number">15</span>:<span class="number">34</span>:<span class="number">42.208238</span> <span class="number">662253568</span> detector<span class="class">.cpp</span>:<span class="number">138</span>] Detected <span class="tag">a</span> new leader: (id=<span class="string">'62'</span>)</span><br><span class="line">I1121 <span class="number">15</span>:<span class="number">34</span>:<span class="number">42.208575</span> <span class="number">655667200</span> group<span class="class">.cpp</span>:<span class="number">659</span>] Trying to get <span class="string">'/mesos/json.info_0000000062'</span> <span class="keyword">in</span> ZooKeeper</span><br><span class="line">Failed to detect <span class="tag">a</span> master: Failed to parse data of unknown <span class="tag">label</span> <span class="string">'json.info</span></span><br></pre></td></tr></table></figure>
<p>如果指定本地mesos（要先在本地启动mesos-local）：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">~/Soft/spark-<span class="number">1.6</span><span class="number">.2</span>-bin-hadoop2<span class="number">.6</span>/bin/spark-submit --<span class="keyword">class</span> <span class="string">"SimpleApp"</span> --master <span class="string">"mesos://localhost:5050"</span> \</span><br><span class="line">--conf spark.app.logfile=<span class="string">"file:///Users/zhengqh/Github/scala/simple-app/README.md"</span> \</span><br><span class="line">target/scala-<span class="number">2.10</span>/simple-project_2<span class="number">.10</span>-<span class="number">1.0</span>.jar</span><br><span class="line"></span><br><span class="line">I1121 <span class="number">15</span>:<span class="number">11</span>:<span class="number">07.611460</span> <span class="number">515149824</span> sched.cpp:<span class="number">448</span>] Framework registered with <span class="number">20161121</span>-<span class="number">150645</span>-<span class="number">16777343</span>-<span class="number">5050</span>-<span class="number">14247</span>-<span class="number">0000</span> 出现Framework registered，实际上就说明连接本地mesos成功了</span><br><span class="line"><span class="number">16</span>/<span class="number">11</span>/<span class="number">21</span> <span class="number">15</span>:<span class="number">11</span>:<span class="number">07</span> INFO mesos.CoarseMesosSchedulerBackend: Registered as framework ID <span class="number">20161121</span>-<span class="number">150645</span>-<span class="number">16777343</span>-<span class="number">5050</span>-<span class="number">14247</span>-<span class="number">0000</span></span><br><span class="line"><span class="number">16</span>/<span class="number">11</span>/<span class="number">21</span> <span class="number">15</span>:<span class="number">11</span>:<span class="number">08</span> INFO mesos.CoarseMesosSchedulerBackend: SchedulerBackend is ready <span class="keyword">for</span> scheduling beginning after reached minRegisteredResourcesRatio: <span class="number">0.0</span></span><br><span class="line"><span class="number">16</span>/<span class="number">11</span>/<span class="number">21</span> <span class="number">15</span>:<span class="number">11</span>:<span class="number">08</span> INFO mesos.CoarseMesosSchedulerBackend: Mesos task <span class="number">0</span> is now TASK_RUNNING</span><br><span class="line"><span class="number">16</span>/<span class="number">11</span>/<span class="number">21</span> <span class="number">15</span>:<span class="number">11</span>:<span class="number">12</span> INFO spark.SparkContext: Starting job: count at SimpleApp.scala:<span class="number">13</span></span><br><span class="line">Lines with a: <span class="number">1</span>, Lines with b: <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>或者将jar包上传到具有mesos环境的客户端节点，因为远程spark-mesos已经集成好了，所以可以不写–master</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/usr/install/spark/bin/spark-submit --<span class="keyword">class</span> <span class="string">"SimpleApp"</span> --master <span class="string">"mesos://192.168.6.52:5050"</span> \</span><br><span class="line">--conf spark.app.logfile=<span class="string">"/user/qihuang.zheng/hello.txt"</span> \</span><br><span class="line">simple-project_2<span class="number">.10</span>-<span class="number">1.0</span>.jar</span><br></pre></td></tr></table></figure>
<h3 id="问题">问题</h3><p>远程运行任务，并指定proxy-user</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">sudo -u admin /usr/install/spark/bin/spark-submit \</span><br><span class="line">--master mesos:<span class="comment">//zk://192.168.6.55:2181,192.168.6.56:2181,192.168.6.57:2181/mesos \</span></span><br><span class="line">--class cn<span class="class">.fraudmetrix</span><span class="class">.pontus</span><span class="class">.demo</span><span class="class">.SimpleApp</span> \</span><br><span class="line">--proxy-user qihuang<span class="class">.zheng</span> \</span><br><span class="line">--conf spark<span class="class">.app</span><span class="class">.logfile</span>=<span class="string">"/user/qihuang.zheng/hello.txt"</span> \</span><br><span class="line">/usr/install/pontus/pontus-spark-<span class="number">1.0</span>.<span class="number">0</span>-SNAPSHOT-fat<span class="class">.jar</span></span><br><span class="line"></span><br><span class="line">sudo -u admin /usr/install/spark/bin/spark-submit \</span><br><span class="line">--master <span class="string">"mesos://zk://192.168.6.55:2181,192.168.6.56:2181,192.168.6.57:2181/mesos"</span> \</span><br><span class="line">--proxy-user qihuang<span class="class">.zheng</span> \</span><br><span class="line">--class cn<span class="class">.fraudmetrix</span><span class="class">.pontus</span><span class="class">.cassandra</span><span class="class">.PontusCassandra2HDFSJobHandler</span> \</span><br><span class="line">--conf spark<span class="class">.app</span><span class="class">.logfile</span>=<span class="string">"/usr/install/spark/README.md"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.destTable</span>=<span class="string">""</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.tableId</span>=<span class="string">""</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.destType</span>=<span class="string">"HDFS"</span> \</span><br><span class="line">--conf spark<span class="class">.cores</span><span class="class">.max</span>=<span class="string">"10"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.execution</span><span class="class">.id</span>=<span class="string">"3"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.destUsername</span>=<span class="string">""</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.creator</span>=<span class="string">"qihuang.zheng"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.sourceUri</span>=<span class="string">"192.168.6.53/keyspace1"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.sourcePassword</span>=<span class="string">""</span> \</span><br><span class="line">--conf spark<span class="class">.mesos</span><span class="class">.role</span>=<span class="string">"test"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.sourceTable</span>=<span class="string">"standard1"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.sourceType</span>=<span class="string">"Cassandra"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.readMode</span>=<span class="string">"1"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.tableTs</span>=<span class="string">""</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.destUri</span>=<span class="string">"/user/tongdun/pontus/standard1"</span> \</span><br><span class="line">--conf spark<span class="class">.executor</span><span class="class">.memory</span>=<span class="string">"4g"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.name</span>=<span class="string">"cassandra-hdfs"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.destPassword</span>=<span class="string">""</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.id</span>=<span class="string">"2"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.writeMode</span>=<span class="string">"1"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.sourceUsername</span>=<span class="string">""</span> \</span><br><span class="line">/usr/install/pontus/pontus-spark-<span class="number">1.0</span>.<span class="number">0</span>-SNAPSHOT-fat.jar</span><br></pre></td></tr></table></figure>
<p>mesos kill framework, but won’t kill spark driver</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">curl</span> -XPOST <span class="url">http://192.168.6.52:5050/master/teardown</span> -d <span class="string">'frameworkId=475189a7-dcde-4859-9af8-6fc2e63be94e-0556'</span></span><br></pre></td></tr></table></figure>
<p>手动设置spark.app.id无效：</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/usr/install/spark/bin/spark-submit \</span></span><br><span class="line">-<span class="ruby">-master <span class="symbol">mesos:</span>/<span class="regexp">/zk:/</span><span class="regexp">/192.168.6.55:2181,192.168.6.56:2181,192.168.6.57:2181/mesos</span> \</span><br><span class="line"></span>-<span class="ruby">-<span class="class"><span class="keyword">class</span> <span class="title">cn</span>.<span class="title">fraudmetrix</span>.<span class="title">pontus</span>.<span class="title">demo</span>.<span class="title">SimpleApp</span> \</span></span><br><span class="line"></span>-<span class="ruby">-conf <span class="string">"spark.app.id=SimpleApp"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.app.logfile=<span class="string">"/user/qihuang.zheng/hello.txt"</span> \</span><br><span class="line"></span><span class="comment">/usr/install/pontus/pontus-spark-1.0.0-SNAPSHOT-fat.jar</span></span><br></pre></td></tr></table></figure>
<p>序列化不适配？？ spark_2.10和2.11的版本问题？？</p>
<p>play_2.5.9不支持scal_2.10: <a href="https://www.playframework.com/documentation/2.5.x/Migration25" target="_blank" rel="external">https://www.playframework.com/documentation/2.5.x/Migration25</a><br>Play 2.3 and 2.4 supported both Scala 2.10 and 2.11. Play 2.5 has dropped support for Scala 2.10 and now only supports Scala 2.11.  </p>
<p>Jira: <a href="https://issues.apache.org/jira/browse/SPARK-13956" target="_blank" rel="external">https://issues.apache.org/jira/browse/SPARK-13956</a><br>说的是driver版本和executor版本不一致导致的。</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">I1209 20:58:27.190430  3039 sched.cpp:703] Framework registered with 475189a7-dcde-4859-9af8-6fc2e63be94e-0423</span><br><span class="line">[info] p.<span class="keyword">m</span>.i.j.p.s.w.SparkJobWorker - Starting job -1651747420 future</span><br><span class="line">[Stage 0:&gt;                                                          (0 + 0) / 2][<span class="keyword">error</span>] o.a.s.<span class="keyword">n</span>.s.TransportRequestHandler - <span class="keyword">Error</span> <span class="keyword">while</span> invoking RpcHandler#receive() <span class="keyword">on</span> RPC id 8897300371223004861</span><br><span class="line">java.io.InvalidClassException: org.apache.spark.rpc.netty.RequestMessage; <span class="keyword">local</span> <span class="keyword">class</span> incompatible: stream classdesc serialVersionUID = -2221986757032131007, <span class="keyword">local</span> <span class="keyword">class</span> serialVersionUID = -5447855329526097695</span><br><span class="line">  at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)</span><br><span class="line">  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1623)</span><br><span class="line">  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)</span><br><span class="line">  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)</span><br><span class="line">  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)</span><br><span class="line">  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)</span><br><span class="line">  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.<span class="keyword">scala</span>:76)</span><br><span class="line">  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.<span class="keyword">scala</span>:109)</span><br><span class="line">  at org.apache.spark.rpc.netty.NettyRpcEnv$<span class="label">$anonfun</span><span class="label">$deserialize</span><span class="label">$1</span>$<span class="label">$anonfun</span><span class="label">$apply</span><span class="label">$1</span>.apply(NettyRpcEnv.<span class="keyword">scala</span>:258)</span><br><span class="line">  at <span class="keyword">scala</span>.util.DynamicVariable.withValue(DynamicVariable.<span class="keyword">scala</span>:58)</span><br><span class="line">[<span class="keyword">error</span>] o.a.s.<span class="keyword">n</span>.s.TransportRequestHandler - <span class="keyword">Error</span> <span class="keyword">while</span> invoking RpcHandler#receive() <span class="keyword">on</span> RPC id 9207561725902043069</span><br><span class="line">java.io.InvalidClassException: org.apache.spark.rpc.netty.RequestMessage; <span class="keyword">local</span> <span class="keyword">class</span> incompatible: stream classdesc serialVersionUID = -2221986757032131007, <span class="keyword">local</span> <span class="keyword">class</span> serialVersionUID = -5447855329526097695</span><br><span class="line">  at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)</span><br><span class="line">  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1623)</span><br><span class="line">  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)</span><br><span class="line">  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)</span><br><span class="line">  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)</span><br><span class="line">  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)</span><br><span class="line">  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.<span class="keyword">scala</span>:76)</span><br><span class="line">  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.<span class="keyword">scala</span>:109)</span><br><span class="line">  at org.apache.spark.rpc.netty.NettyRpcEnv$<span class="label">$anonfun</span><span class="label">$deserialize</span><span class="label">$1</span>$<span class="label">$anonfun</span><span class="label">$apply</span><span class="label">$1</span>.apply(NettyRpcEnv.<span class="keyword">scala</span>:258)</span><br><span class="line">  at <span class="keyword">scala</span>.util.DynamicVariable.withValue(DynamicVariable.<span class="keyword">scala</span>:58)</span><br><span class="line">[warn] o.a.s.s.TaskSchedulerImpl - Initial job has not accepted any resources; check your <span class="keyword">cluster</span> UI to ensure that workers are registered and have sufficient resources</span><br><span class="line">[<span class="keyword">error</span>] application -</span><br><span class="line"></span><br><span class="line">! @72b33a0kb - Internal server <span class="keyword">error</span>, <span class="keyword">for</span> (GET) [/spark] -&gt;</span><br><span class="line"></span><br><span class="line">play.api.UnexpectedException: Unexpected exception[TimeoutException: Futures timed <span class="keyword">out</span> after [30 seconds]]</span><br><span class="line">  at play.api.http.HttpErrorHandlerExceptions$.throwableToUsefulException(HttpErrorHandler.<span class="keyword">scala</span>:276)</span><br><span class="line">  at play.api.http.DefaultHttpErrorHandler.onServerError(HttpErrorHandler.<span class="keyword">scala</span>:206)</span><br><span class="line">  at play.api.GlobalSettings<span class="label">$class</span>.onError(GlobalSettings.<span class="keyword">scala</span>:160)</span><br><span class="line">  at util.<span class="keyword">Global</span>$.onError(<span class="keyword">Global</span>.<span class="keyword">scala</span>:22)</span><br><span class="line">  at play.api.http.GlobalSettingsHttpErrorHandler.onServerError(HttpErrorHandler.<span class="keyword">scala</span>:98)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="label">$anonfun</span><span class="label">$2</span>$<span class="label">$anonfun</span><span class="label">$apply</span><span class="label">$1</span>.applyOrElse(PlayRequestHandler.<span class="keyword">scala</span>:100)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="label">$anonfun</span><span class="label">$2</span>$<span class="label">$anonfun</span><span class="label">$apply</span><span class="label">$1</span>.applyOrElse(PlayRequestHandler.<span class="keyword">scala</span>:99)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.Future$<span class="label">$anonfun</span><span class="label">$recoverWith</span><span class="label">$1</span>.apply(Future.<span class="keyword">scala</span>:344)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.Future$<span class="label">$anonfun</span><span class="label">$recoverWith</span><span class="label">$1</span>.apply(Future.<span class="keyword">scala</span>:343)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.impl.CallbackRunnable.<span class="keyword">run</span>(Promise.<span class="keyword">scala</span>:32)</span><br><span class="line">Caused <span class="keyword">by</span>: java.util.concurrent.TimeoutException: Futures timed <span class="keyword">out</span> after [30 seconds]</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.impl.Promise<span class="label">$DefaultPromise</span>.ready(Promise.<span class="keyword">scala</span>:219)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.impl.Promise<span class="label">$DefaultPromise</span>.result(Promise.<span class="keyword">scala</span>:223)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.Await$<span class="label">$anonfun</span><span class="label">$result</span><span class="label">$1</span>.apply(package.<span class="keyword">scala</span>:190)</span><br><span class="line">  at akka.dispatch.MonitorableThreadFactory<span class="label">$AkkaForkJoinWorkerThread</span>$<span class="label">$anon</span><span class="label">$3</span>.block(ThreadPoolBuilder.<span class="keyword">scala</span>:167)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinPool.managedBlock(ForkJoinPool.java:3640)</span><br><span class="line">  at akka.dispatch.MonitorableThreadFactory<span class="label">$AkkaForkJoinWorkerThread</span>.blockOn(ThreadPoolBuilder.<span class="keyword">scala</span>:165)</span><br><span class="line">  at akka.dispatch.BatchingExecutor<span class="label">$BlockableBatch</span>.blockOn(BatchingExecutor.<span class="keyword">scala</span>:106)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.Await$.result(package.<span class="keyword">scala</span>:190)</span><br><span class="line">  at controllers.SparkController$<span class="label">$anonfun</span><span class="label">$sparkJob</span><span class="label">$1</span>.apply(SparkController.<span class="keyword">scala</span>:56)</span><br><span class="line">  at controllers.SparkController$<span class="label">$anonfun</span><span class="label">$sparkJob</span><span class="label">$1</span>.apply(SparkController.<span class="keyword">scala</span>:42)</span><br><span class="line">[warn] o.a.s.s.TaskSchedulerImpl - Initial job has not accepted any resources; check your <span class="keyword">cluster</span> UI to ensure that workers are registered and have sufficient resources</span><br></pre></td></tr></table></figure>
<p>把spark-core打包方式改为provided是不行的。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java<span class="class">.lang</span><span class="class">.NoClassDefFoundError</span>: org/apache/spark/Logging</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.defineClass1</span>(Native Method)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.defineClass</span>(ClassLoader<span class="class">.java</span>:<span class="number">760</span>)</span><br><span class="line">  at java<span class="class">.security</span><span class="class">.SecureClassLoader</span><span class="class">.defineClass</span>(SecureClassLoader<span class="class">.java</span>:<span class="number">142</span>)</span><br><span class="line">  at java<span class="class">.net</span><span class="class">.URLClassLoader</span><span class="class">.defineClass</span>(URLClassLoader<span class="class">.java</span>:<span class="number">467</span>)</span><br><span class="line">  at java<span class="class">.net</span><span class="class">.URLClassLoader</span><span class="class">.access</span>$<span class="number">100</span>(URLClassLoader<span class="class">.java</span>:<span class="number">73</span>)</span><br><span class="line">  at java<span class="class">.net</span><span class="class">.URLClassLoader</span>$<span class="number">1</span>.<span class="function"><span class="title">run</span><span class="params">(URLClassLoader.java:<span class="number">368</span>)</span></span></span><br><span class="line">  at java<span class="class">.net</span><span class="class">.URLClassLoader</span>$<span class="number">1</span>.<span class="function"><span class="title">run</span><span class="params">(URLClassLoader.java:<span class="number">362</span>)</span></span></span><br><span class="line">  at java<span class="class">.security</span><span class="class">.AccessController</span><span class="class">.doPrivileged</span>(Native Method)</span><br><span class="line">  at java<span class="class">.net</span><span class="class">.URLClassLoader</span><span class="class">.findClass</span>(URLClassLoader<span class="class">.java</span>:<span class="number">361</span>)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.loadClass</span>(ClassLoader<span class="class">.java</span>:<span class="number">424</span>)</span><br><span class="line">  at sun<span class="class">.misc</span><span class="class">.Launcher</span><span class="variable">$AppClassLoader</span>.<span class="function"><span class="title">loadClass</span><span class="params">(Launcher.java:<span class="number">331</span>)</span></span></span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.loadClass</span>(ClassLoader<span class="class">.java</span>:<span class="number">357</span>)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.Class</span><span class="class">.getDeclaredConstructors0</span>(Native Method)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.Class</span><span class="class">.privateGetDeclaredConstructors</span>(Class<span class="class">.java</span>:<span class="number">2671</span>)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.Class</span><span class="class">.getDeclaredConstructors</span>(Class<span class="class">.java</span>:<span class="number">2020</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.spi</span><span class="class">.InjectionPoint</span><span class="class">.forConstructorOf</span>(InjectionPoint<span class="class">.java</span>:<span class="number">245</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorBindingImpl</span><span class="class">.create</span>(ConstructorBindingImpl<span class="class">.java</span>:<span class="number">99</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.createUninitializedBinding</span>(InjectorImpl<span class="class">.java</span>:<span class="number">658</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.createJustInTimeBinding</span>(InjectorImpl<span class="class">.java</span>:<span class="number">882</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.createJustInTimeBindingRecursive</span>(InjectorImpl<span class="class">.java</span>:<span class="number">805</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.getJustInTimeBinding</span>(InjectorImpl<span class="class">.java</span>:<span class="number">282</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.getBindingOrThrow</span>(InjectorImpl<span class="class">.java</span>:<span class="number">214</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.getProviderOrThrow</span>(InjectorImpl<span class="class">.java</span>:<span class="number">1006</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.getProvider</span>(InjectorImpl<span class="class">.java</span>:<span class="number">1038</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.getProvider</span>(InjectorImpl<span class="class">.java</span>:<span class="number">1001</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.getInstance</span>(InjectorImpl<span class="class">.java</span>:<span class="number">1051</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.guice</span><span class="class">.GuiceInjector</span><span class="class">.instanceOf</span>(GuiceInjectorBuilder<span class="class">.scala</span>:<span class="number">405</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.RoutesProvider</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>.<span class="function"><span class="title">apply</span><span class="params">(BuiltinModule.scala:<span class="number">82</span>)</span></span></span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.RoutesProvider</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>.<span class="function"><span class="title">apply</span><span class="params">(BuiltinModule.scala:<span class="number">82</span>)</span></span></span><br><span class="line">  at scala<span class="class">.Option</span><span class="class">.fold</span>(Option<span class="class">.scala</span>:<span class="number">158</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.RoutesProvider</span><span class="class">.get</span><span class="variable">$lzycompute</span>(BuiltinModule<span class="class">.scala</span>:<span class="number">82</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.RoutesProvider</span><span class="class">.get</span>(BuiltinModule<span class="class">.scala</span>:<span class="number">78</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.RoutesProvider</span><span class="class">.get</span>(BuiltinModule<span class="class">.scala</span>:<span class="number">77</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ProviderInternalFactory</span><span class="class">.provision</span>(ProviderInternalFactory<span class="class">.java</span>:<span class="number">81</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.BoundProviderFactory</span><span class="class">.provision</span>(BoundProviderFactory<span class="class">.java</span>:<span class="number">72</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ProviderInternalFactory</span><span class="class">.circularGet</span>(ProviderInternalFactory<span class="class">.java</span>:<span class="number">61</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.BoundProviderFactory</span><span class="class">.get</span>(BoundProviderFactory<span class="class">.java</span>:<span class="number">62</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.SingleParameterInjector</span><span class="class">.inject</span>(SingleParameterInjector<span class="class">.java</span>:<span class="number">38</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.SingleParameterInjector</span><span class="class">.getAll</span>(SingleParameterInjector<span class="class">.java</span>:<span class="number">62</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorInjector</span><span class="class">.provision</span>(ConstructorInjector<span class="class">.java</span>:<span class="number">104</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorInjector</span><span class="class">.construct</span>(ConstructorInjector<span class="class">.java</span>:<span class="number">85</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorBindingImpl</span><span class="variable">$Factory</span>.<span class="function"><span class="title">get</span><span class="params">(ConstructorBindingImpl.java:<span class="number">267</span>)</span></span></span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.FactoryProxy</span><span class="class">.get</span>(FactoryProxy<span class="class">.java</span>:<span class="number">56</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.SingleParameterInjector</span><span class="class">.inject</span>(SingleParameterInjector<span class="class">.java</span>:<span class="number">38</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.SingleParameterInjector</span><span class="class">.getAll</span>(SingleParameterInjector<span class="class">.java</span>:<span class="number">62</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorInjector</span><span class="class">.provision</span>(ConstructorInjector<span class="class">.java</span>:<span class="number">104</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorInjector</span><span class="class">.construct</span>(ConstructorInjector<span class="class">.java</span>:<span class="number">85</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorBindingImpl</span><span class="variable">$Factory</span>.<span class="function"><span class="title">get</span><span class="params">(ConstructorBindingImpl.java:<span class="number">267</span>)</span></span></span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ProviderToInternalFactoryAdapter</span>$<span class="number">1</span>.<span class="function"><span class="title">call</span><span class="params">(ProviderToInternalFactoryAdapter.java:<span class="number">46</span>)</span></span></span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.callInContext</span>(InjectorImpl<span class="class">.java</span>:<span class="number">1103</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ProviderToInternalFactoryAdapter</span><span class="class">.get</span>(ProviderToInternalFactoryAdapter<span class="class">.java</span>:<span class="number">40</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.SingletonScope</span>$<span class="number">1</span>.<span class="function"><span class="title">get</span><span class="params">(SingletonScope.java:<span class="number">145</span>)</span></span></span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InternalFactoryToProviderAdapter</span><span class="class">.get</span>(InternalFactoryToProviderAdapter<span class="class">.java</span>:<span class="number">41</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.FactoryProxy</span><span class="class">.get</span>(FactoryProxy<span class="class">.java</span>:<span class="number">56</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InternalInjectorCreator</span>$<span class="number">1</span>.<span class="function"><span class="title">call</span><span class="params">(InternalInjectorCreator.java:<span class="number">205</span>)</span></span></span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InternalInjectorCreator</span>$<span class="number">1</span>.<span class="function"><span class="title">call</span><span class="params">(InternalInjectorCreator.java:<span class="number">199</span>)</span></span></span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.callInContext</span>(InjectorImpl<span class="class">.java</span>:<span class="number">1092</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InternalInjectorCreator</span><span class="class">.loadEagerSingletons</span>(InternalInjectorCreator<span class="class">.java</span>:<span class="number">199</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InternalInjectorCreator</span><span class="class">.injectDynamically</span>(InternalInjectorCreator<span class="class">.java</span>:<span class="number">180</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InternalInjectorCreator</span><span class="class">.build</span>(InternalInjectorCreator<span class="class">.java</span>:<span class="number">110</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.Guice</span><span class="class">.createInjector</span>(Guice<span class="class">.java</span>:<span class="number">96</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.Guice</span><span class="class">.createInjector</span>(Guice<span class="class">.java</span>:<span class="number">84</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.guice</span><span class="class">.GuiceBuilder</span><span class="class">.injector</span>(GuiceInjectorBuilder<span class="class">.scala</span>:<span class="number">181</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.guice</span><span class="class">.GuiceApplicationBuilder</span><span class="class">.build</span>(GuiceApplicationBuilder<span class="class">.scala</span>:<span class="number">123</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.guice</span><span class="class">.GuiceApplicationLoader</span><span class="class">.load</span>(GuiceApplicationLoader<span class="class">.scala</span>:<span class="number">21</span>)</span><br><span class="line">  at play<span class="class">.core</span><span class="class">.server</span><span class="class">.ProdServerStart</span>$.<span class="function"><span class="title">start</span><span class="params">(ProdServerStart.scala:<span class="number">47</span>)</span></span></span><br><span class="line">  at play<span class="class">.core</span><span class="class">.server</span><span class="class">.ProdServerStart</span>$.<span class="function"><span class="title">main</span><span class="params">(ProdServerStart.scala:<span class="number">22</span>)</span></span></span><br><span class="line">  at play<span class="class">.core</span><span class="class">.server</span><span class="class">.ProdServerStart</span><span class="class">.main</span>(ProdServerStart.scala)</span><br><span class="line">Caused by: java<span class="class">.lang</span><span class="class">.ClassNotFoundException</span>: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.Logging</span></span><br><span class="line">  at java<span class="class">.net</span><span class="class">.URLClassLoader</span><span class="class">.findClass</span>(URLClassLoader<span class="class">.java</span>:<span class="number">381</span>)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.loadClass</span>(ClassLoader<span class="class">.java</span>:<span class="number">424</span>)</span><br><span class="line">  at sun<span class="class">.misc</span><span class="class">.Launcher</span><span class="variable">$AppClassLoader</span>.<span class="function"><span class="title">loadClass</span><span class="params">(Launcher.java:<span class="number">331</span>)</span></span></span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.loadClass</span>(ClassLoader<span class="class">.java</span>:<span class="number">357</span>)</span><br><span class="line">  ... <span class="number">68</span> more</span><br></pre></td></tr></table></figure>
<p>在pontus-web中直接运行Spark作业</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">! @<span class="number">72</span>ck129fb - Internal server error, <span class="keyword">for</span> (GET) [/preview/HDFS/activity] -&gt;</span><br><span class="line"></span><br><span class="line">play.api.UnexpectedException: Unexpected exception[RuntimeException: java.lang.NoSuchMethodError: akka.actor.LocalActorRefProvider.log()Lakka/event/LoggingAdapter;]</span><br><span class="line">  at play.api.http.HttpErrorHandlerExceptions$.throwableToUsefulException(HttpErrorHandler.scala:<span class="number">289</span>)</span><br><span class="line">  at play.api.http.DefaultHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">220</span>)</span><br><span class="line">  at play.api.GlobalSettings<span class="variable">$class</span>.onError(GlobalSettings.scala:<span class="number">160</span>)</span><br><span class="line">  at util.Global$.onError(Global.scala:<span class="number">20</span>)</span><br><span class="line">  at play.api.http.GlobalSettingsHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">99</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">344</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">343</span>)</span><br><span class="line">  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:<span class="number">32</span>)</span><br><span class="line">Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodError: akka.actor.LocalActorRefProvider.log()Lakka/event/LoggingAdapter;</span><br><span class="line">  at play.api.mvc.ActionBuilder$<span class="variable">$anon</span><span class="variable">$2</span>.apply(Action.scala:<span class="number">463</span>)</span><br><span class="line">  at play.api.mvc.Action$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$5</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$6</span>.apply(Action.scala:<span class="number">112</span>)</span><br><span class="line">  at play.api.mvc.Action$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$5</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$6</span>.apply(Action.scala:<span class="number">112</span>)</span><br><span class="line">  at play.utils.Threads$.withContextClassLoader(Threads.scala:<span class="number">21</span>)</span><br><span class="line">  at play.api.mvc.Action$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$5</span>.apply(Action.scala:<span class="number">111</span>)</span><br><span class="line">  at play.api.mvc.Action$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$5</span>.apply(Action.scala:<span class="number">110</span>)</span><br><span class="line">  at scala.Option.<span class="keyword">map</span>(Option.scala:<span class="number">146</span>)</span><br><span class="line">  at play.api.mvc.Action$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>.apply(Action.scala:<span class="number">110</span>)</span><br><span class="line">  at play.api.mvc.Action$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>.apply(Action.scala:<span class="number">103</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$flatMap</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">251</span>)</span><br><span class="line">Caused by: java.lang.NoSuchMethodError: akka.actor.LocalActorRefProvider.log()Lakka/event/LoggingAdapter;</span><br><span class="line">  at akka.remote.RemoteActorRefProvider.&lt;init&gt;(RemoteActorRefProvider.scala:<span class="number">128</span>)</span><br><span class="line">  at sun.reflect.NativeConstructorAccessorImpl.newInstance<span class="number">0</span>(Native Method)</span><br><span class="line">  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:<span class="number">62</span>)</span><br><span class="line">  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:<span class="number">45</span>)</span><br><span class="line">  at java.lang.reflect.Constructor.newInstance(Constructor.java:<span class="number">422</span>)</span><br><span class="line">  at akka.actor.ReflectiveDynamicAccess$<span class="variable">$anonfun</span><span class="variable">$createInstanceFor</span><span class="variable">$2</span>.apply(ReflectiveDynamicAccess.scala:<span class="number">32</span>)</span><br><span class="line">  at scala.util.Try$.apply(Try.scala:<span class="number">192</span>)</span><br><span class="line">  at akka.actor.ReflectiveDynamicAccess.createInstanceFor(ReflectiveDynamicAccess.scala:<span class="number">27</span>)</span><br><span class="line">  at akka.actor.ReflectiveDynamicAccess$<span class="variable">$anonfun</span><span class="variable">$createInstanceFor</span><span class="variable">$3</span>.apply(ReflectiveDynamicAccess.scala:<span class="number">38</span>)</span><br><span class="line">  at akka.actor.ReflectiveDynamicAccess$<span class="variable">$anonfun</span><span class="variable">$createInstanceFor</span><span class="variable">$3</span>.apply(ReflectiveDynamicAccess.scala:<span class="number">38</span>)</span><br></pre></td></tr></table></figure>
<p><a href="http://stackoverflow.com/questions/40883978/migration-to-play-2-5-leads-to-this-error-nosuchmethoderror-akka-actor-locala" target="_blank" rel="external">http://stackoverflow.com/questions/40883978/migration-to-play-2-5-leads-to-this-error-nosuchmethoderror-akka-actor-locala</a>  </p>
<p>监听器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ssc.sparkContext.addSparkListener(new SparkListener()&#123;</span><br><span class="line">  override <span class="function"><span class="keyword">def</span> <span class="title">onJobEnd</span><span class="params">(jobEnd: SparkListenerJobEnd)</span>:</span> Unit = &#123;&#125;</span><br><span class="line">  override <span class="function"><span class="keyword">def</span> <span class="title">onApplicationEnd</span><span class="params">(applicationEnd: SparkListenerApplicationEnd)</span>:</span> Unit = &#123;</span><br><span class="line">    println(<span class="string">"PontusKafka2HDFSJobHandler onApplicationEnd...."</span>)</span><br><span class="line">    PontusExecutionDao.updateStatus(conf)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>使用命令行测试，当关闭程序后，会打印onApplicationEnd事件。在mesos上使用kill driver直接杀死应用程序，也会调用监听器的方法</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@716: Client environment:host.name=dp0653</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@723: Client environment:os.name=Linux</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@724: Client environment:os.arch=<span class="number">3.10.97-1</span>.el6.elrepo.x86_64</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@725: Client environment:os.version=#1 SMP Sat Feb <span class="number">20 11:55:29</span> EST 2016</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@733: Client environment:user.name=qihuang.zheng</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@741: Client environment:user.home=/home/qihuang.zheng</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@753: Client environment:user.dir=/home/qihuang.zheng</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=<span class="number">192.168.6.55</span>:<span class="number">2181,192.168</span>.<span class="number">6.56:2181</span>,<span class="number">192.168.6.57</span>:2181 sessionTimeout=10000 watcher=<span class="number">0x2b1d00</span>0e1645 sessionId=0 sessionPasswd=&lt;null&gt; context=<span class="number">0x2b1d44</span>000960 flags=0</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">5:41791(0</span>x<span class="number">2b1d2950</span>a700):ZOO_INFO@check_events@1703: initiated connection to server [<span class="number">192.168.6.57</span>:2181]</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.536334</span> 42031 sched.cpp:222] Version: 0.28.2</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,55</span><span class="number">5:41791(0</span>x<span class="number">2b1d2950</span>a700):ZOO_INFO@check_events@1750: session establishment complete on server [<span class="number">192.168.6.57</span>:2181], sessionId=<span class="number">0x3589e76</span>ef2f82e0, negotiated timeout=10000</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.556421</span> 42015 group.cpp:349] Group process (group(1)@<span class="number">192.168.6.53</span>:45534) connected to ZooKeeper</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.556823</span> 42015 group.cpp:831] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.556903</span> 42015 group.cpp:427] Trying to create path '/mesos' in ZooKeeper</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.561131 42</span>008 detector.cpp:152] Detected a new leader: (id='70')</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.561971</span> 42011 group.cpp:700] Trying to get '/mesos/json.info_<span class="number">0000000070</span>' in ZooKeeper</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.565317</span> 42023 detector.cpp:479] <span class="keyword">A</span> new leading master (UPID=master@<span class="number">192.168.6.52</span>:5050) is detected</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.565773</span> 42026 sched.cpp:326] New master detected at master@<span class="number">192.168.6.52</span>:5050</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.567448</span> 42026 sched.cpp:336] No credentials provided. Attempting to register without authentication</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.574049</span> 42026 sched.cpp:703] Framework registered with <span class="number">475189a7</span>-dcde-4859-9af8-6fc2e63be94e-0558</span><br><span class="line"></span><br><span class="line">^CPontusKafka2HDFSJobHandler onApplicationEnd....</span><br><span class="line">I<span class="number">1213 11:42</span>:<span class="number">21.526136</span>  4384 sched.cpp:1911] Asked to stop the driver</span><br><span class="line">I<span class="number">1213 11:42</span>:<span class="number">21.526562</span> 42020 sched.cpp:1143] Stopping framework '<span class="number">475189a7</span>-dcde-4859-9af8-6fc2e63be94e-0558'</span><br></pre></td></tr></table></figure>
<h2 id="Spark_Cassandra">Spark Cassandra</h2><p>IDE中直接运行，spark依赖包不能为provided，否则找不到Spark的相关包，运行正常</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-core_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-sql_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-hive_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>使用maven-assembly-plugin插件打包运行，spark环境使用本地安装，scope=provide</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-core_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-sql_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-hive_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>打包，并用完整的依赖包执行spark-submit</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mvn clean <span class="keyword">package</span></span><br><span class="line"></span><br><span class="line"><span class="regexp">/Users/</span>zhengqh<span class="regexp">/Soft/</span>spark-<span class="number">1.6</span>.<span class="number">2</span>-bin-hadoop2.<span class="number">6</span><span class="regexp">/bin/</span>spark-submit \</span><br><span class="line">--master <span class="string">"local[2]"</span> \</span><br><span class="line">--<span class="keyword">class</span> cn.fraudmetrix.pontus.demo.CassandraReadWrite \</span><br><span class="line"><span class="regexp">/Users/</span>zhengqh<span class="regexp">/Github/</span>vulcan<span class="regexp">/pontus-spark/</span>target<span class="regexp">/pontus-spark-1.0.0-SNAPSHOT-jar-with-dependencies.jar</span></span><br></pre></td></tr></table></figure>
<p>报错guava版本不对，即使手动加上guava版本也报错：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com<span class="class">.google</span><span class="class">.guava</span>&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;guava&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;<span class="number">19.0</span>&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java<span class="class">.lang</span><span class="class">.ExceptionInInitializerError</span></span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.DefaultConnectionFactory</span>$.<span class="function"><span class="title">clusterBuilder</span><span class="params">(CassandraConnectionFactory.scala:<span class="number">35</span>)</span></span></span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.DefaultConnectionFactory</span>$.<span class="function"><span class="title">createCluster</span><span class="params">(CassandraConnectionFactory.scala:<span class="number">87</span>)</span></span></span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.CassandraConnector</span>$.com<span class="variable">$datastax</span><span class="variable">$spark</span><span class="variable">$connector</span><span class="variable">$cql</span><span class="variable">$CassandraConnector</span>$<span class="variable">$createSession</span>(CassandraConnector<span class="class">.scala</span>:<span class="number">153</span>)</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.CassandraConnector</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>.<span class="function"><span class="title">apply</span><span class="params">(CassandraConnector.scala:<span class="number">148</span>)</span></span></span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.CassandraConnector</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>.<span class="function"><span class="title">apply</span><span class="params">(CassandraConnector.scala:<span class="number">148</span>)</span></span></span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.RefCountedCache</span><span class="class">.createNewValueAndKeys</span>(RefCountedCache<span class="class">.scala</span>:<span class="number">31</span>)</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.RefCountedCache</span><span class="class">.acquire</span>(RefCountedCache<span class="class">.scala</span>:<span class="number">56</span>)</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.CassandraConnector</span><span class="class">.openSession</span>(CassandraConnector<span class="class">.scala</span>:<span class="number">81</span>)</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.CassandraConnector</span><span class="class">.withSessionDo</span>(CassandraConnector<span class="class">.scala</span>:<span class="number">109</span>)</span><br><span class="line">  at cn<span class="class">.fraudmetrix</span><span class="class">.pontus</span><span class="class">.demo</span><span class="class">.CassandraReadWrite</span>$.cn<span class="variable">$fraudmetrix</span><span class="variable">$pontus</span><span class="variable">$demo</span><span class="variable">$CassandraReadWrite</span>$<span class="variable">$executeCommand</span>(CassandraReadWrite<span class="class">.scala</span>:<span class="number">45</span>)</span><br><span class="line">  at cn<span class="class">.fraudmetrix</span><span class="class">.pontus</span><span class="class">.demo</span><span class="class">.CassandraReadWrite</span><span class="variable">$delayedInit</span><span class="variable">$body</span>.<span class="function"><span class="title">apply</span><span class="params">(CassandraReadWrite.scala:<span class="number">53</span>)</span></span></span><br><span class="line">  at scala.Function0<span class="variable">$class</span>.apply<span class="variable">$mcV</span><span class="variable">$sp</span>(Function0<span class="class">.scala</span>:<span class="number">40</span>)</span><br><span class="line">  at scala<span class="class">.runtime</span><span class="class">.AbstractFunction0</span><span class="class">.apply</span><span class="variable">$mcV</span><span class="variable">$sp</span>(AbstractFunction0<span class="class">.scala</span>:<span class="number">12</span>)</span><br><span class="line">  at scala.App$<span class="variable">$anonfun</span><span class="variable">$main</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(App.scala:<span class="number">71</span>)</span></span></span><br><span class="line">  at scala.App$<span class="variable">$anonfun</span><span class="variable">$main</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(App.scala:<span class="number">71</span>)</span></span></span><br><span class="line">  at scala<span class="class">.collection</span><span class="class">.immutable</span><span class="class">.List</span><span class="class">.foreach</span>(List<span class="class">.scala</span>:<span class="number">318</span>)</span><br><span class="line">  at scala<span class="class">.collection</span><span class="class">.generic</span><span class="class">.TraversableForwarder</span><span class="variable">$class</span>.<span class="function"><span class="title">foreach</span><span class="params">(TraversableForwarder.scala:<span class="number">32</span>)</span></span></span><br><span class="line">  at scala.App<span class="variable">$class</span>.<span class="function"><span class="title">main</span><span class="params">(App.scala:<span class="number">71</span>)</span></span></span><br><span class="line">  at cn<span class="class">.fraudmetrix</span><span class="class">.pontus</span><span class="class">.demo</span><span class="class">.CassandraReadWrite</span>$.<span class="function"><span class="title">main</span><span class="params">(CassandraReadWrite.scala:<span class="number">36</span>)</span></span></span><br><span class="line">  at cn<span class="class">.fraudmetrix</span><span class="class">.pontus</span><span class="class">.demo</span><span class="class">.CassandraReadWrite</span><span class="class">.main</span>(CassandraReadWrite.scala)</span><br><span class="line">  at sun<span class="class">.reflect</span><span class="class">.NativeMethodAccessorImpl</span><span class="class">.invoke0</span>(Native Method)</span><br><span class="line">  at sun<span class="class">.reflect</span><span class="class">.NativeMethodAccessorImpl</span><span class="class">.invoke</span>(NativeMethodAccessorImpl<span class="class">.java</span>:<span class="number">62</span>)</span><br><span class="line">  at sun<span class="class">.reflect</span><span class="class">.DelegatingMethodAccessorImpl</span><span class="class">.invoke</span>(DelegatingMethodAccessorImpl<span class="class">.java</span>:<span class="number">43</span>)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.reflect</span><span class="class">.Method</span><span class="class">.invoke</span>(Method<span class="class">.java</span>:<span class="number">497</span>)</span><br><span class="line">  at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.org<span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$deploy</span><span class="variable">$SparkSubmit</span>$<span class="variable">$runMain</span>(SparkSubmit<span class="class">.scala</span>:<span class="number">731</span>)</span><br><span class="line">  at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.doRunMain$<span class="number">1</span>(SparkSubmit<span class="class">.scala</span>:<span class="number">181</span>)</span><br><span class="line">  at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.<span class="function"><span class="title">submit</span><span class="params">(SparkSubmit.scala:<span class="number">206</span>)</span></span></span><br><span class="line">  at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.<span class="function"><span class="title">main</span><span class="params">(SparkSubmit.scala:<span class="number">121</span>)</span></span></span><br><span class="line">  at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span><span class="class">.main</span>(SparkSubmit.scala)</span><br><span class="line">Caused by: java<span class="class">.lang</span><span class="class">.IllegalStateException</span>: Detected Guava issue <span class="hexcolor">#163</span>5 which indicates that <span class="tag">a</span> version of Guava less than <span class="number">16.01</span> is <span class="keyword">in</span> use.  This introduces codec resolution issues and potentially other incompatibility issues <span class="keyword">in</span> the driver.  Please upgrade to Guava <span class="number">16.01</span> or later.</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.driver</span><span class="class">.core</span><span class="class">.SanityChecks</span><span class="class">.checkGuava</span>(SanityChecks<span class="class">.java</span>:<span class="number">62</span>)</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.driver</span><span class="class">.core</span><span class="class">.SanityChecks</span><span class="class">.check</span>(SanityChecks<span class="class">.java</span>:<span class="number">36</span>)</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.driver</span><span class="class">.core</span><span class="class">.Cluster</span>.&lt;clinit&gt;(Cluster<span class="class">.java</span>:<span class="number">67</span>)</span><br><span class="line">  ... <span class="number">29</span> more</span><br></pre></td></tr></table></figure>
<p>参考：<a href="http://stackoverflow.com/questions/36877897/detected-guava-issue-1635-which-indicates-that-a-version-of-guava-less-than-16" target="_blank" rel="external">http://stackoverflow.com/questions/36877897/detected-guava-issue-1635-which-indicates-that-a-version-of-guava-less-than-16</a><br>添加maven-shade-plugin插件，重新打包，除了其他包，还会生成fat包，并用fat包运行，最后正常</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ll target</span><br><span class="line">-rw-r--r--  <span class="number">1</span> zhengqh  staff    <span class="number">14</span>M <span class="number">11</span> <span class="number">24</span> <span class="number">19</span>:<span class="number">17</span> pontus-spark-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT-fat.jar</span><br><span class="line">-rw-r--r--  <span class="number">1</span> zhengqh  staff    <span class="number">19</span>M <span class="number">11</span> <span class="number">24</span> <span class="number">19</span>:<span class="number">15</span> pontus-spark-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line">-rw-r--r--  <span class="number">1</span> zhengqh  staff    <span class="number">14</span>K <span class="number">11</span> <span class="number">24</span> <span class="number">19</span>:<span class="number">15</span> pontus-spark-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT-sources.jar</span><br><span class="line">-rw-r--r--  <span class="number">1</span> zhengqh  staff    <span class="number">94</span>K <span class="number">11</span> <span class="number">24</span> <span class="number">19</span>:<span class="number">15</span> pontus-spark-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT.jar</span><br><span class="line"></span><br><span class="line">/Users/zhengqh/Soft/spark-<span class="number">1.6</span><span class="number">.2</span>-bin-hadoop2<span class="number">.6</span>/bin/spark-submit \</span><br><span class="line">--master <span class="string">"local[2]"</span> \</span><br><span class="line">--<span class="keyword">class</span> cn.fraudmetrix.pontus.demo.CassandraReadWrite \</span><br><span class="line">/Users/zhengqh/Github/vulcan/pontus-spark/target/pontus-spark-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT-fat.jar</span><br></pre></td></tr></table></figure>
<p>测试读取Cassandra写入到HDFS是否正确（需要启动本地dfs和Cassandra）</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/Users/zhengqh/Soft/spark-1.6.2-bin-hadoop2.6/bin/spark-submit \</span></span><br><span class="line">-<span class="ruby">-master <span class="string">"local[2]"</span> \</span><br><span class="line"></span>-<span class="ruby">-<span class="class"><span class="keyword">class</span> <span class="title">cn</span>.<span class="title">fraudmetrix</span>.<span class="title">pontus</span>.<span class="title">cassandra</span>.<span class="title">PontusCassandra2HDFSJobHandler</span> \</span></span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.destTable=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.tableId=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.destType=<span class="string">"HDFS"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.destUsername=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.creator=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.sourceUri=<span class="string">"localhost/demo"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.sourcePassword=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.sourceTable=<span class="string">"sales"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.sourceType=<span class="string">"Cassandra"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.readMode=<span class="string">"1"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.tableTs=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.destUri=<span class="string">"hdfs://localhost:9000/test"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.name=<span class="string">"cassandra-hdfs"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.destPassword=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.id=<span class="string">"1"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.writeMode=<span class="string">"1"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.sourceUsername=<span class="string">""</span> \</span><br><span class="line"></span><span class="comment">/Users/zhengqh/Github/vulcan/pontus-spark/target/pontus-spark-1.0.0-SNAPSHOT-fat.jar</span></span><br><span class="line"></span><br><span class="line">16/11/24 19:19:16 INFO DAGScheduler: Job 0 finished: show at PontusCassandra2HDFSJobHandler.scala:42, took 4.044479 s</span><br><span class="line">+---------+--------+-----+</span><br><span class="line">|   center|products|total|</span><br><span class="line">+---------+--------+-----+</span><br><span class="line">|  Sevilla|      28| 3200|</span><br><span class="line">| Valencia|      23| 3300|</span><br><span class="line">|   Bilbao|      25| 2500|</span><br><span class="line">|   Madrid|      51| 7700|</span><br><span class="line">|Barcelona|      47| 6400|</span><br><span class="line">+---------+--------+-----+</span><br></pre></td></tr></table></figure>
<p>上面的Spark作业会将Parquet文件写入HDFS的根路径，可以用spark-shell测试读取parquet文件。</p>
<p>Spark作业执行页面：</p>
<p><img src="http://img.blog.csdn.net/20161125100527204" alt="pontus-spark"></p>
<p>Pontus作业执行成功页面：</p>
<p><img src="http://img.blog.csdn.net/20161125100553908" alt="pontus-job"></p>
<p>HDFS追加作业：</p>
<p><img src="http://img.blog.csdn.net/20161129174641219" alt="hdfs"></p>
<p>空值问题：解决办法：Option</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">play.api.http.HttpErrorHandlerExceptions$<span class="variable">$anon</span><span class="variable">$1</span>: Execution exception[[RuntimeException: ColumnName(PONTUS_EXECUTION.LOGOUT,Some(LOGOUT))]]</span><br><span class="line">  at play.api.http.HttpErrorHandlerExceptions$.throwableToUsefulException(HttpErrorHandler.scala:<span class="number">293</span>)</span><br><span class="line">  at play.api.http.DefaultHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">220</span>)</span><br><span class="line">  at play.api.GlobalSettings<span class="variable">$class</span>.onError(GlobalSettings.scala:<span class="number">160</span>)</span><br><span class="line">  at util.Global$.onError(Global.scala:<span class="number">8</span>)</span><br><span class="line">  at play.api.http.GlobalSettingsHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">99</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">344</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">343</span>)</span><br><span class="line">  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:<span class="number">32</span>)</span><br><span class="line">Caused by: java.lang.RuntimeException: ColumnName(PONTUS_EXECUTION.LOGOUT,Some(LOGOUT))</span><br><span class="line">  at scala.sys.package$.error(package.scala:<span class="number">27</span>)</span><br><span class="line">  at anorm.SqlRequestError<span class="variable">$class</span>.toFailure(Anorm.scala:<span class="number">20</span>)</span><br><span class="line">  at anorm.UnexpectedNullableFound.toFailure(Anorm.scala:<span class="number">37</span>)</span><br><span class="line">  at anorm.Sql$<span class="variable">$anonfun</span><span class="variable">$asTry</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$7</span>.apply(Anorm.scala:<span class="number">303</span>)</span><br><span class="line">  at anorm.Sql$<span class="variable">$anonfun</span><span class="variable">$asTry</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$7</span>.apply(Anorm.scala:<span class="number">303</span>)</span><br><span class="line">  at anorm.SqlResult<span class="variable">$class</span>.fold(SqlResult.scala:<span class="number">23</span>)</span><br><span class="line">  at anorm.Error.fold(SqlResult.scala:<span class="number">31</span>)</span><br><span class="line">  at anorm.Sql$<span class="variable">$anonfun</span><span class="variable">$asTry</span><span class="variable">$2</span>.apply(Anorm.scala:<span class="number">303</span>)</span><br><span class="line">  at anorm.Sql$<span class="variable">$anonfun</span><span class="variable">$asTry</span><span class="variable">$2</span>.apply(Anorm.scala:<span class="number">303</span>)</span><br><span class="line">  at scala.util.Success.flatMap(Try.scala:<span class="number">231</span>)</span><br></pre></td></tr></table></figure>
<h2 id="预览">预览</h2><p><a href="http://192.168.6.53:9000/preview/HDFS/%2Fuser%2Ftongdun%2Fpontus%2Fantifraud%2Ffeedback%2Fyear=2016%2Fmonth=12%2Fday=15" target="_blank" rel="external">http://192.168.6.53:9000/preview/HDFS/%2Fuser%2Ftongdun%2Fpontus%2Fantifraud%2Ffeedback%2Fyear=2016%2Fmonth=12%2Fday=15</a></p>
<p><a href="http://192.168.6.53:9000/preview/HDFS/hdfs:%2F%2F192.168.6.52:9000%2Fuser%2Ftongdun%2Fpontus%2Fantifraud%2Ffeedback%2Fyear=2016%2Fmonth=12%2Fday=15" target="_blank" rel="external">http://192.168.6.53:9000/preview/HDFS/hdfs:%2F%2F192.168.6.52:9000%2Fuser%2Ftongdun%2Fpontus%2Fantifraud%2Ffeedback%2Fyear=2016%2Fmonth=12%2Fday=15</a></p>
<p><a href="http://192.168.6.53:9000/preview/HDFS/hdfs:%2F%2Ftdfs%2Fuser%2Ftongdun%2Fpontus%2Fantifraud%2Ffeedback%2Fyear=2016%2Fmonth=12%2Fday=15" target="_blank" rel="external">http://192.168.6.53:9000/preview/HDFS/hdfs:%2F%2Ftdfs%2Fuser%2Ftongdun%2Fpontus%2Fantifraud%2Ffeedback%2Fyear=2016%2Fmonth=12%2Fday=15</a></p>
<h2 id="Actor">Actor</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SparkJobActor <span class="function"><span class="title">preStart</span><span class="params">()</span></span>.............</span><br><span class="line"><span class="function"><span class="title">PontusSchedule</span><span class="params">(<span class="number">1</span>,<span class="number">1</span>,qihuang.zheng,<span class="number">2016</span>-<span class="number">11</span>-<span class="number">29</span> <span class="number">21</span>:<span class="number">10</span>:<span class="number">00.0</span>,<span class="number">5</span>d)</span></span></span><br><span class="line">[warn] o<span class="class">.a</span><span class="class">.h</span><span class="class">.u</span><span class="class">.NativeCodeLoader</span> - Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line">Global <span class="function"><span class="title">onStart</span><span class="params">()</span></span>.............</span><br><span class="line">mode:demo</span><br><span class="line"><span class="function"><span class="title">PontusSchedule</span><span class="params">(<span class="number">1</span>,<span class="number">1</span>,qihuang.zheng,<span class="number">2016</span>-<span class="number">11</span>-<span class="number">29</span> <span class="number">21</span>:<span class="number">10</span>:<span class="number">00.0</span>,<span class="number">5</span>d)</span></span></span><br><span class="line">[info] play<span class="class">.api</span><span class="class">.Play</span> - Application started (Dev)</span><br></pre></td></tr></table></figure>
<h2 id="Quartz">Quartz</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.JobBuilder</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.TriggerBuilder</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.SimpleScheduleBuilder</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.CronScheduleBuilder</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.CalendarIntervalScheduleBuilder</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.JobKey</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.TriggerKey</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.DateBuilder</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.impl</span><span class="class">.matchers</span><span class="class">.KeyMatcher</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.impl</span><span class="class">.matchers</span><span class="class">.GroupMatcher</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.impl</span><span class="class">.matchers</span><span class="class">.AndMatcher</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.impl</span><span class="class">.matchers</span><span class="class">.OrMatcher</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.impl</span><span class="class">.matchers</span><span class="class">.EverythingMatcher</span><span class="class">._</span></span><br></pre></td></tr></table></figure>
<h3 id="Job依赖注入">Job依赖注入</h3><p>Job注入其他实例报错无法实例化，那么如何注入DAO对象？</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">PontusSparkJob</span> @<span class="type">Inject</span><span class="container">()</span><span class="container">(<span class="title">configuration</span>: <span class="type">Configuration</span>,</span><br><span class="line">                               <span class="title">executionRepository</span>: <span class="type">ExecutionRepository</span>,</span><br><span class="line">                               <span class="title">scheduleRepository</span>: <span class="type">ScheduleRepository</span>,</span><br><span class="line">                               <span class="title">jobRepository</span>: <span class="type">JobRepository</span></span><br><span class="line">                              )</span> extends <span class="type">Job</span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[error] o.q.c.<span class="type">ErrorLogger</span> - <span class="type">An</span> error occured instantiating job to be executed. job= '<span class="type">Cassandra</span>.pontusjob_1'</span><br><span class="line">org.quartz.<span class="type">SchedulerException</span>: <span class="type">Problem</span> instantiating <span class="keyword">class</span> 'schedule.<span class="type">PontusSparkJob'</span></span><br><span class="line">  at org.quartz.simpl.<span class="type">SimpleJobFactory</span>.newJob<span class="container">(<span class="type">SimpleJobFactory</span>.<span class="title">java</span>:58)</span></span><br><span class="line">  at org.quartz.simpl.<span class="type">PropertySettingJobFactory</span>.newJob<span class="container">(<span class="type">PropertySettingJobFactory</span>.<span class="title">java</span>:69)</span></span><br><span class="line">  at org.quartz.core.<span class="type">JobRunShell</span>.initialize<span class="container">(<span class="type">JobRunShell</span>.<span class="title">java</span>:127)</span></span><br><span class="line">  at org.quartz.core.<span class="type">QuartzSchedulerThread</span>.run<span class="container">(<span class="type">QuartzSchedulerThread</span>.<span class="title">java</span>:375)</span></span><br><span class="line"><span class="type">Caused</span> by: java.lang.<span class="type">InstantiationException</span>: schedule.<span class="type">PontusSparkJob</span></span><br><span class="line">  at java.lang.<span class="type">Class</span>.newInstance<span class="container">(<span class="type">Class</span>.<span class="title">java</span>:427)</span></span><br><span class="line">  at org.quartz.simpl.<span class="type">SimpleJobFactory</span>.newJob<span class="container">(<span class="type">SimpleJobFactory</span>.<span class="title">java</span>:56)</span></span><br><span class="line">  ... 3 common frames omitted</span><br><span class="line"><span class="type">Caused</span> by: java.lang.<span class="type">NoSuchMethodException</span>: schedule.<span class="type">PontusSparkJob</span>.&lt;init&gt;<span class="container">()</span></span><br><span class="line">  at java.lang.<span class="type">Class</span>.getConstructor0<span class="container">(<span class="type">Class</span>.<span class="title">java</span>:3082)</span></span><br><span class="line">  at java.lang.<span class="type">Class</span>.newInstance<span class="container">(<span class="type">Class</span>.<span class="title">java</span>:412)</span></span><br><span class="line">  ... 4 common frames omitted</span></span><br></pre></td></tr></table></figure>
<p>解决办法：启动时通过Global注册Application实例，在Job中通过Scheduler获取</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Global</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">GlobalSettings</span> &#123;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span>(</span>application: play.api.<span class="type">Application</span>) &#123;</span><br><span class="line">    schedule.<span class="type">PontusQuartzScheduler</span>.scheduler.start()</span><br><span class="line">    schedule.<span class="type">PontusQuartzScheduler</span>.scheduler.getContext.put(<span class="string">"configuration"</span>, application.configuration)</span><br><span class="line">    schedule.<span class="type">PontusQuartzScheduler</span>.scheduler.getContext.put(<span class="string">"application"</span>, application)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PontusSparkJob</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">Job</span> &#123;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span>(</span>context: <span class="type">JobExecutionContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> application = context.getScheduler.getContext.get(<span class="string">"application"</span>).asInstanceOf[<span class="type">Application</span>]</span><br><span class="line">    <span class="keyword">val</span> configuration = context.getScheduler.getContext.get(<span class="string">"configuration"</span>).asInstanceOf[<span class="type">Configuration</span>]</span><br><span class="line">    <span class="keyword">val</span> jobRepository = application.injector.instanceOf(classOf[<span class="type">JobRepository</span>])</span><br><span class="line">    <span class="keyword">val</span> executionRepository = application.injector.instanceOf(classOf[<span class="type">ExecutionRepository</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jobId = context.getMergedJobDataMap.getString(<span class="string">"jobId"</span>).toLong</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>TODO：自定义JobFactory</p>
<p>mesos任务报错，页面会显示申请不到资源：</p>
<figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">I1205 <span class="number">15</span>:<span class="number">15</span>:<span class="number">20.217545</span> <span class="number">19517</span> exec.cpp:<span class="number">143</span>] Version: <span class="number">0.28</span>.<span class="number">2</span></span><br><span class="line">I1205 <span class="number">15</span>:<span class="number">15</span>:<span class="number">20.243422</span> <span class="number">19521</span> exec.cpp:<span class="number">217</span>] Executor registered <span class="keyword">on</span> slave <span class="number">475189</span>a7-dcde-<span class="number">4859</span>-<span class="number">9</span>af8-<span class="number">6</span>fc2e63be94e-S0</span><br><span class="line">Unrecognized VM option <span class="string">'UseCompressedStrings'</span></span><br><span class="line">Error: Could <span class="keyword">not</span> <span class="keyword">create</span> the Java <span class="keyword">Virtual</span> Machine.</span><br><span class="line">Error: A fatal exception <span class="keyword">has</span> occurred. Program will <span class="keyword">exit</span>.</span><br></pre></td></tr></table></figure>
<p>去掉spark中executor的jvm配置</p>
<h3 id="Quartz作业的自动重启">Quartz作业的自动重启</h3><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">创建时：</span><br><span class="line"></span><br><span class="line">mysql&gt; select <span class="keyword">*</span> from QRTZ_TRIGGERS;</span><br><span class="line">+-----------------+--------------+---------------+-------------+-----------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">|<span class="string"> SCHED_NAME      </span>|<span class="string"> TRIGGER_NAME </span>|<span class="string"> TRIGGER_GROUP </span>|<span class="string"> JOB_NAME    </span>|<span class="string"> JOB_GROUP </span>|<span class="string"> DESCRIPTION </span>|<span class="string"> NEXT_FIRE_TIME </span>|<span class="string"> PREV_FIRE_TIME </span>|<span class="string"> PRIORITY </span>|<span class="string"> TRIGGER_STATE </span>|<span class="string"> TRIGGER_TYPE </span>|<span class="string"> START_TIME    </span>|<span class="string"> END_TIME </span>|<span class="string"> CALENDAR_NAME </span>|<span class="string"> MISFIRE_INSTR </span>|<span class="string"> JOB_DATA </span>|</span><br><span class="line">+-----------------+--------------+---------------+-------------+-----------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">|<span class="string"> QuartzScheduler </span>|<span class="string"> trigger2_1   </span>|<span class="string"> Cassandra     </span>|<span class="string"> pontusjob_1 </span>|<span class="string"> Cassandra </span>|<span class="string"> NULL        </span>|<span class="string">  1480523100000 </span>|<span class="string">  1480519500000 </span>|<span class="string">        5 </span>|<span class="string"> WAITING       </span>|<span class="string"> SIMPLE       </span>|<span class="string"> 1480515900000 </span>|<span class="string">        0 </span>|<span class="string"> NULL          </span>|<span class="string">             1 </span>|<span class="string">          </span>|</span><br><span class="line">+-----------------+--------------+---------------+-------------+-----------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">1 row in set (10.33 sec)</span><br><span class="line"></span><br><span class="line">创建时间（START_TIME）：         1480515900000 - 2016年11月30日 星期三 22时25分00秒 CST</span><br><span class="line">第一次执行时间（PREV_FIRE_TIME）：1480519500000 - 2016年11月30日 星期三 23时25分00秒 CST</span><br><span class="line">下一次执行时间（NEXT_FIRE_TIME）：1480523100000 - 2016年12月 1日 星期四 00时25分00秒 CST</span><br><span class="line"></span><br><span class="line">停止服务器</span><br><span class="line"></span><br><span class="line">第二天早上重启，重启后如果没有访问页面，仍然不会调度，但是只要登录，就会开始调度</span><br><span class="line"></span><br><span class="line">mysql&gt; select <span class="keyword">*</span> from QRTZ_TRIGGERS;</span><br><span class="line">+-----------------+--------------+---------------+-------------+-----------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">|<span class="string"> SCHED_NAME      </span>|<span class="string"> TRIGGER_NAME </span>|<span class="string"> TRIGGER_GROUP </span>|<span class="string"> JOB_NAME    </span>|<span class="string"> JOB_GROUP </span>|<span class="string"> DESCRIPTION </span>|<span class="string"> NEXT_FIRE_TIME </span>|<span class="string"> PREV_FIRE_TIME </span>|<span class="string"> PRIORITY </span>|<span class="string"> TRIGGER_STATE </span>|<span class="string"> TRIGGER_TYPE </span>|<span class="string"> START_TIME    </span>|<span class="string"> END_TIME </span>|<span class="string"> CALENDAR_NAME </span>|<span class="string"> MISFIRE_INSTR </span>|<span class="string"> JOB_DATA </span>|</span><br><span class="line">+-----------------+--------------+---------------+-------------+-----------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">|<span class="string"> QuartzScheduler </span>|<span class="string"> trigger2_1   </span>|<span class="string"> Cassandra     </span>|<span class="string"> pontusjob_1 </span>|<span class="string"> Cassandra </span>|<span class="string"> NULL        </span>|<span class="string">  1480561432130 </span>|<span class="string">  1480557832130 </span>|<span class="string">        5 </span>|<span class="string"> WAITING       </span>|<span class="string"> SIMPLE       </span>|<span class="string"> 1480557832130 </span>|<span class="string">        0 </span>|<span class="string"> NULL          </span>|<span class="string">             1 </span>|<span class="string">          </span>|</span><br><span class="line">+-----------------+--------------+---------------+-------------+-----------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">1 row in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">START_TIME：     1480557832130 - 2016年12月 1日 星期四 10时03分52秒 CST</span><br><span class="line">PREV_FIRE_TIME： 1480557832130 - 2016年12月 1日 星期四 10时03分52秒 CST</span><br><span class="line">NEXT_FIRE_TIME： 1480561432130 - 2016年12月 1日 星期四 11时03分52秒 CST</span><br></pre></td></tr></table></figure>
<p>相同Job不允许同时有两个Trigger在运行。解决办法：可以设置不同的JobId。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">! @<span class="number">729</span>o8hm27 - Internal server error, <span class="keyword">for</span> (GET) [/job/execute/Cassandra/HDFS/<span class="number">2</span>] -&gt;</span><br><span class="line"></span><br><span class="line">play<span class="class">.api</span><span class="class">.UnexpectedException</span>: Unexpected exception[ObjectAlreadyExistsException: Unable to store Job : <span class="string">'Cassandra-HDFS.pontusjob_2'</span>, because one already exists with this identification.]</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.http</span><span class="class">.HttpErrorHandlerExceptions</span>$.<span class="function"><span class="title">throwableToUsefulException</span><span class="params">(HttpErrorHandler.scala:<span class="number">289</span>)</span></span></span><br><span class="line">  at play<span class="class">.api</span><span class="class">.http</span><span class="class">.DefaultHttpErrorHandler</span><span class="class">.onServerError</span>(HttpErrorHandler<span class="class">.scala</span>:<span class="number">220</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.GlobalSettings</span><span class="variable">$class</span>.<span class="function"><span class="title">onError</span><span class="params">(GlobalSettings.scala:<span class="number">160</span>)</span></span></span><br><span class="line">  at util.Global$.<span class="function"><span class="title">onError</span><span class="params">(Global.scala:<span class="number">23</span>)</span></span></span><br><span class="line">  at play<span class="class">.api</span><span class="class">.http</span><span class="class">.GlobalSettingsHttpErrorHandler</span><span class="class">.onServerError</span>(HttpErrorHandler<span class="class">.scala</span>:<span class="number">100</span>)</span><br><span class="line">  at play<span class="class">.core</span><span class="class">.server</span><span class="class">.netty</span><span class="class">.PlayRequestHandler</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span>$<span class="number">1</span>.<span class="function"><span class="title">applyOrElse</span><span class="params">(PlayRequestHandler.scala:<span class="number">100</span>)</span></span></span><br><span class="line">  at play<span class="class">.core</span><span class="class">.server</span><span class="class">.netty</span><span class="class">.PlayRequestHandler</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span>$<span class="number">1</span>.<span class="function"><span class="title">applyOrElse</span><span class="params">(PlayRequestHandler.scala:<span class="number">99</span>)</span></span></span><br><span class="line">  at scala<span class="class">.concurrent</span><span class="class">.Future</span>$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(Future.scala:<span class="number">344</span>)</span></span></span><br><span class="line">  at scala<span class="class">.concurrent</span><span class="class">.Future</span>$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(Future.scala:<span class="number">343</span>)</span></span></span><br><span class="line">  at scala<span class="class">.concurrent</span><span class="class">.impl</span><span class="class">.CallbackRunnable</span><span class="class">.run</span>(Promise<span class="class">.scala</span>:<span class="number">32</span>)</span><br><span class="line">Caused by: org<span class="class">.quartz</span><span class="class">.ObjectAlreadyExistsException</span>: Unable to store Job : <span class="string">'Cassandra-HDFS.pontusjob_2'</span>, because one already exists with this identification.</span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreSupport</span><span class="class">.storeJob</span>(JobStoreSupport<span class="class">.java</span>:<span class="number">1108</span>)</span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreSupport</span>$<span class="number">2</span>.<span class="function"><span class="title">executeVoid</span><span class="params">(JobStoreSupport.java:<span class="number">1062</span>)</span></span></span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreSupport</span><span class="variable">$VoidTransactionCallback</span>.<span class="function"><span class="title">execute</span><span class="params">(JobStoreSupport.java:<span class="number">3715</span>)</span></span></span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreSupport</span><span class="variable">$VoidTransactionCallback</span>.<span class="function"><span class="title">execute</span><span class="params">(JobStoreSupport.java:<span class="number">3713</span>)</span></span></span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreSupport</span><span class="class">.executeInNonManagedTXLock</span>(JobStoreSupport<span class="class">.java</span>:<span class="number">3799</span>)</span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreTX</span><span class="class">.executeInLock</span>(JobStoreTX<span class="class">.java</span>:<span class="number">93</span>)</span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreSupport</span><span class="class">.storeJobAndTrigger</span>(JobStoreSupport<span class="class">.java</span>:<span class="number">1058</span>)</span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.core</span><span class="class">.QuartzScheduler</span><span class="class">.scheduleJob</span>(QuartzScheduler<span class="class">.java</span>:<span class="number">886</span>)</span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.StdScheduler</span><span class="class">.scheduleJob</span>(StdScheduler<span class="class">.java</span>:<span class="number">249</span>)</span><br><span class="line">  at controllers.PontusJobController$<span class="variable">$anonfun</span><span class="variable">$execute</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(PontusJobController.scala:<span class="number">72</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>spark streaming一次性作业：</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select <span class="keyword">*</span> from QRTZ_TRIGGERS;</span><br><span class="line">+-----------------+--------------+----------------+--------------+----------------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">|<span class="string"> SCHED_NAME      </span>|<span class="string"> TRIGGER_NAME </span>|<span class="string"> TRIGGER_GROUP  </span>|<span class="string"> JOB_NAME     </span>|<span class="string"> JOB_GROUP      </span>|<span class="string"> DESCRIPTION </span>|<span class="string"> NEXT_FIRE_TIME </span>|<span class="string"> PREV_FIRE_TIME </span>|<span class="string"> PRIORITY </span>|<span class="string"> TRIGGER_STATE </span>|<span class="string"> TRIGGER_TYPE </span>|<span class="string"> START_TIME    </span>|<span class="string"> END_TIME </span>|<span class="string"> CALENDAR_NAME </span>|<span class="string"> MISFIRE_INSTR </span>|<span class="string"> JOB_DATA </span>|</span><br><span class="line">+-----------------+--------------+----------------+--------------+----------------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">|<span class="string"> QuartzScheduler </span>|<span class="string"> trigger1_2   </span>|<span class="string"> Kafka-HDFS     </span>|<span class="string"> pontusjob1_2 </span>|<span class="string"> Kafka-HDFS     </span>|<span class="string"> NULL        </span>|<span class="string">             -1 </span>|<span class="string">  1481165690342 </span>|<span class="string">        5 </span>|<span class="string"> COMPLETE      </span>|<span class="string"> SIMPLE       </span>|<span class="string"> 1481165690342 </span>|<span class="string">        0 </span>|<span class="string"> NULL          </span>|<span class="string">             0 </span>|<span class="string">          </span>|</span><br><span class="line">|<span class="string"> QuartzScheduler </span>|<span class="string"> trigger2_1   </span>|<span class="string"> Cassandra-HDFS </span>|<span class="string"> pontusjob2_1 </span>|<span class="string"> Cassandra-HDFS </span>|<span class="string"> NULL        </span>|<span class="string">  1481165913700 </span>|<span class="string">  1481165433700 </span>|<span class="string">        5 </span>|<span class="string"> WAITING       </span>|<span class="string"> SIMPLE       </span>|<span class="string"> 1481103993700 </span>|<span class="string">        0 </span>|<span class="string"> NULL          </span>|<span class="string">             1 </span>|<span class="string">          </span>|</span><br><span class="line">+-----------------+--------------+----------------+--------------+----------------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">2 rows in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select <span class="keyword">*</span> from QRTZ_SIMPLE_TRIGGERS;</span><br><span class="line">+-----------------+--------------+----------------+--------------+-----------------+-----------------+</span><br><span class="line">|<span class="string"> SCHED_NAME      </span>|<span class="string"> TRIGGER_NAME </span>|<span class="string"> TRIGGER_GROUP  </span>|<span class="string"> REPEAT_COUNT </span>|<span class="string"> REPEAT_INTERVAL </span>|<span class="string"> TIMES_TRIGGERED </span>|</span><br><span class="line">+-----------------+--------------+----------------+--------------+-----------------+-----------------+</span><br><span class="line">|<span class="string"> QuartzScheduler </span>|<span class="string"> trigger1_2   </span>|<span class="string"> Kafka-HDFS     </span>|<span class="string">            0 </span>|<span class="string">               0 </span>|<span class="string">               1 </span>|</span><br><span class="line">|<span class="string"> QuartzScheduler </span>|<span class="string"> trigger2_1   </span>|<span class="string"> Cassandra-HDFS </span>|<span class="string">           -1 </span>|<span class="string">          480000 </span>|<span class="string">             129 </span>|</span><br><span class="line">+-----------------+--------------+----------------+--------------+-----------------+-----------------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>
<p>过了会儿，一次性的任务信息会从Quartz删除，但是实际上spark streaming还在运行！<br>这时，如果再次点击执行，虽然triggerKey不存在，但是也不应该允许再次执行！否则就会存在两个spark streaming程序！<br>还有一个问题：由于spark streaming不会结束，所以无法看到日志，状态也永远是正在执行。</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="header">mysql&gt; select * from QRTZ_SIMPLE_TRIGGERS;</span><br><span class="line">+-----------------+--------------+----------------+--------------+-----------------+-----------------+</span></span><br><span class="line"><span class="header">| SCHED_NAME      | TRIGGER_NAME | TRIGGER_GROUP  | REPEAT_COUNT | REPEAT_INTERVAL | TIMES_TRIGGERED |</span><br><span class="line">+-----------------+--------------+----------------+--------------+-----------------+-----------------+</span></span><br><span class="line"><span class="header">| QuartzScheduler | trigger2_1   | Cassandra-HDFS |           -1 |          480000 |             129 |</span><br><span class="line">+-----------------+--------------+----------------+--------------+-----------------+-----------------+</span></span><br><span class="line">1 row in set (0.01 sec)</span><br><span class="line"></span><br><span class="line"><span class="header">mysql&gt; select * from QRTZ_TRIGGERS;</span><br><span class="line">+-----------------+--------------+----------------+--------------+----------------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span></span><br><span class="line"><span class="header">| SCHED_NAME      | TRIGGER_NAME | TRIGGER_GROUP  | JOB_NAME     | JOB_GROUP      | DESCRIPTION | NEXT_FIRE_TIME | PREV_FIRE_TIME | PRIORITY | TRIGGER_STATE | TRIGGER_TYPE | START_TIME    | END_TIME | CALENDAR_NAME | MISFIRE_INSTR | JOB_DATA |</span><br><span class="line">+-----------------+--------------+----------------+--------------+----------------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span></span><br><span class="line"><span class="header">| QuartzScheduler | trigger2_1   | Cassandra-HDFS | pontusjob2_1 | Cassandra-HDFS | NULL        |  1481165913700 |  1481165433700 |        5 | WAITING       | SIMPLE       | 1481103993700 |        0 | NULL          |             1 |          |</span><br><span class="line">+-----------------+--------------+----------------+--------------+----------------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span></span><br><span class="line">1 row in set (0.01 sec)</span><br></pre></td></tr></table></figure>
<h2 id="Kafka">Kafka</h2><p>创建测试主题</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">2181</span> --replication-factor <span class="number">1</span> --partitions <span class="number">1</span> --topic graylog_nginx</span><br><span class="line">bin/kafka-console-producer.sh --broker-<span class="built_in">list</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">9092</span> --topic graylog_nginx</span><br><span class="line">bin/kafka-console-consumer.sh --zookeeper <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">2181</span> --topic graylog_nginx --from-beginning</span><br></pre></td></tr></table></figure>
<p>准备json数据</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">1</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Betty"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"bsmithrl@simplemachines.org"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Eláteia"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Greece"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"9.19.204.44"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">2</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Anna"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"alewisrm@canalblog.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Shangjing"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"China"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"14.207.119.126"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">3</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"David"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"dgarrettrn@japanpost.jp"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Tsarychanka"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Ukraine"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"111.252.63.159"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">4</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Heather"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"hgilbertro@skype.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Koilás"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Greece"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"29.57.181.250"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">5</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Diane"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"ddanielsrp@statcounter.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Mapiripán"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Colombia"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"19.205.181.99"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">6</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Philip"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"pfullerrq@reuters.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"El Cairo"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Colombia"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"210.248.121.194"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">7</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Maria"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"mfordrr@shop-pro.jp"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Karabash"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Russia"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"224.21.41.52"</span></span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">8</span></span>,"<span class="attribute">username</span>":<span class="value"><span class="string">"Bety"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"bsmithrl@simplemachines.org"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Eláteia"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Greece"</span></span>,"<span class="attribute">ipAddress</span>":<span class="value"><span class="string">"9.19.204.44"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">9</span></span>,"<span class="attribute">username</span>":<span class="value"><span class="string">"Ana"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"alewisrm@canalblog.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Shangjing"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"China"</span></span>,"<span class="attribute">ipAddress</span>":<span class="value"><span class="string">"14.207.119.126"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">10</span></span>,"<span class="attribute">username</span>":<span class="value"><span class="string">"David"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"dgarrettrn@japanpost.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Tsarychanka"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Ukraine"</span></span>,"<span class="attribute">ipAddress</span>":<span class="value"><span class="string">"111.252.63.159"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">11</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Calis"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"carlis@google.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Koran"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Greece"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"29.57.181.250"</span></span>&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Nginx">Nginx</h2><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">worker_processes</span>  <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="title">events</span> &#123;</span><br><span class="line">    <span class="title">worker_connections</span>  <span class="number">1024</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="title">http</span> &#123;</span><br><span class="line">  <span class="title">include</span>       mime.types;</span><br><span class="line">  <span class="title">default_type</span>  application/octet-stream;</span><br><span class="line"></span><br><span class="line">  <span class="title">sendfile</span>        <span class="built_in">on</span>;</span><br><span class="line">  <span class="title">keepalive_timeout</span>  <span class="number">65</span>;</span><br><span class="line"></span><br><span class="line">  <span class="title">proxy_buffering</span>    <span class="built_in">off</span>;</span><br><span class="line">  <span class="title">proxy_set_header</span>   X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">  <span class="title">proxy_set_header</span>   X-Forwarded-Proto <span class="variable">$scheme</span>;</span><br><span class="line">  <span class="title">proxy_set_header</span>   X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">  <span class="title">proxy_set_header</span>   Host <span class="variable">$http_host</span>;</span><br><span class="line">  <span class="title">proxy_http_version</span> <span class="number">1</span>.<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="title">upstream</span> my-backend &#123;</span><br><span class="line">     <span class="title">server</span> <span class="number">192.168.6.53:9000</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="title">server</span> &#123;</span><br><span class="line">    <span class="title">listen</span>       <span class="number">80</span>;</span><br><span class="line">    <span class="title">server_name</span> www.mysite.com;</span><br><span class="line">    <span class="title">location</span> / &#123;</span><br><span class="line">       <span class="title">proxy_pass</span> <span class="url">http://my-backend</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="附录">附录</h1><h2 id="sbt私服">sbt私服</h2><p>安装repox：<a href="https://github.com/Centaur/repox/wiki">https://github.com/Centaur/repox/wiki</a><br>可以在本机开发环境编译jar包，再上次到服务器（因为服务器上可能没有编译所需的node环境）。  </p>
<p>启动repox，然后打开：<a href="http://192.168.6.53:8078/admin/admin.html#/upstreams" target="_blank" rel="external">http://192.168.6.53:8078/admin/admin.html#/upstreams</a>，密码：zhimakaimen</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">nohup</span> <span class="tag">java</span> <span class="tag">-Xmx512m</span> <span class="tag">-jar</span> <span class="tag">repox-assembly-0</span><span class="class">.1-SNAPSHOT</span><span class="class">.jar</span> &amp;</span><br></pre></td></tr></table></figure>
<p>在<code>~/.sbt/repositories</code>添加两行配置：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">repox-maven: http://192.168.6.53:8078/</span><br><span class="line">repox-ivy: http://192.168.6.53:8078/, [<span class="link_label">organization</span>]/[<span class="link_label">module</span>]/(scala<span class="emphasis">_[scalaVersion]/)(sbt_</span>[<span class="link_label">sbtVersion</span>]/)[<span class="link_label">revision</span>]/[<span class="link_label">type</span>]s/[<span class="link_label">artifact</span>](<span class="link_url">-[classifier]</span>).[ext]</span><br></pre></td></tr></table></figure>
<p>完整的repositories文件：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[repositories]</span><br><span class="line">local</span><br><span class="line">local-maven: file:///Users/zhengqh/.m2/repository/</span><br><span class="line">repox-maven: http://192.168.6.53:8078/</span><br><span class="line">repox-ivy: http://192.168.6.53:8078/, [<span class="link_label">organization</span>]/[<span class="link_label">module</span>]/(scala<span class="emphasis">_[scalaVersion]/)(sbt_</span>[<span class="link_label">sbtVersion</span>]/)[<span class="link_label">revision</span>]/[<span class="link_label">type</span>]s/[<span class="link_label">artifact</span>](<span class="link_url">-[classifier]</span>).[ext]</span><br><span class="line"><span class="header">#osc: http://maven.oschina.net/content/groups/public/</span></span><br><span class="line"><span class="header">#oschina-ivy: http://maven.oschina.net/content/groups/public/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]</span></span><br><span class="line">typesafe: http://repo.typesafe.com/typesafe/ivy-releases/, [<span class="link_label">organization</span>]/[<span class="link_label">module</span>]/(scala<span class="emphasis">_[scalaVersion]/)(sbt_</span>[<span class="link_label">sbtVersion</span>]/)[<span class="link_label">revision</span>]/[<span class="link_label">type</span>]s/[<span class="link_label">artifact</span>](<span class="link_url">-[classifier]</span>).[ext], bootOnly</span><br><span class="line">sonatype-oss-releases</span><br><span class="line">maven-central</span><br><span class="line">sonatype-oss-snapshots</span><br></pre></td></tr></table></figure>
<p>使用activator运行play时，创建activatorconfig.txt文件，添加以下配置：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat ~/.activator/activatorconfig<span class="class">.txt</span></span><br><span class="line">-Dsbt<span class="class">.override</span><span class="class">.build</span><span class="class">.repos</span>=true</span><br></pre></td></tr></table></figure>
<p>命令行运行activator，如果出现<code>downloading http://192.168.6.53:8078</code>，表示私服搭建成功。<br>如果本地不存在jar包，则私服会自动下载，然后再下载到本地。  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">➜  first-player git:(master) ✗ ./activator run</span><br><span class="line">[info] Loading global plugins from /Users/zhengqh/.sbt/<span class="number">0.13</span>/plugins</span><br><span class="line">[info] Loading project definition from /Users/zhengqh/Github/first-player/project</span><br><span class="line">[info] Updating &#123;file:/Users/zhengqh/Github/first-player/project/&#125;first-player-build...</span><br><span class="line">[info] Resolving org.fusesource.jansi<span class="preprocessor">#jansi;<span class="number">1.4</span> ...</span></span><br><span class="line">[info] downloading http:<span class="comment">//192.168.6.53:8078/com.typesafe.play/sbt-plugin/scala_2.10/sbt_0.13/2.5.10/jars/sbt-plugin.jar ...</span></span><br><span class="line">[info]  [SUCCESSFUL ] com.typesafe.play<span class="preprocessor">#sbt-plugin;<span class="number">2.5</span><span class="number">.10</span>!sbt-plugin.jar (<span class="number">7242</span>ms)</span></span><br><span class="line">[info] downloading http:<span class="comment">//192.168.6.53:8078/com/typesafe/play/sbt-routes-compiler_2.10/2.5.10/sbt-routes-compiler_2.10-2.5.10.jar ...</span></span><br><span class="line">[info]  [SUCCESSFUL ] com.typesafe.play<span class="preprocessor">#sbt-routes-compiler_2<span class="number">.10</span>;<span class="number">2.5</span><span class="number">.10</span>!sbt-routes-compiler_2<span class="number">.10</span>.jar (<span class="number">9753</span>ms)</span></span><br><span class="line">[info] downloading http:<span class="comment">//192.168.6.53:8078/com/typesafe/play/sbt-run-support_2.10/2.5.10/sbt-run-support_2.10-2.5.10.jar ...</span></span><br><span class="line">[info]  [SUCCESSFUL ] com.typesafe.play<span class="preprocessor">#sbt-run-support_2<span class="number">.10</span>;<span class="number">2.5</span><span class="number">.10</span>!sbt-run-support_2<span class="number">.10</span>.jar (<span class="number">6686</span>ms)</span></span><br><span class="line">[info] downloading http:<span class="comment">//192.168.6.53:8078/com/typesafe/play/build-link/2.5.10/build-link-2.5.10.jar ...</span></span><br><span class="line">[info]  [SUCCESSFUL ] com.typesafe.play<span class="preprocessor">#build-link;<span class="number">2.5</span><span class="number">.10</span>!build-link.jar (<span class="number">6589</span>ms)</span></span><br><span class="line">[info] downloading http:<span class="comment">//192.168.6.53:8078/com/typesafe/play/play-exceptions/2.5.10/play-exceptions-2.5.10.jar ...</span></span><br><span class="line">[info]  [SUCCESSFUL ] com.typesafe.play<span class="preprocessor">#play-exceptions;<span class="number">2.5</span><span class="number">.10</span>!play-exceptions.jar (<span class="number">2358</span>ms)</span></span><br><span class="line">[info] Done updating.</span><br></pre></td></tr></table></figure>
<p>使用sbt运行时（sbt run），需要更改sbt的选项，比如下面是mac环境使用brew安装sbt配置</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cat /usr/<span class="built_in">local</span>/bin/sbt</span><br><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line"><span class="built_in">export</span> SBT_OPTS=<span class="string">"-Dsbt.override.build.repos=true"</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="operator">-f</span> <span class="string">"<span class="variable">$HOME</span>/.sbtconfig"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Use of ~/.sbtconfig is deprecated, please migrate global settings to /usr/local/etc/sbtopts"</span> &gt;&amp;<span class="number">2</span></span><br><span class="line">  . <span class="string">"<span class="variable">$HOME</span>/.sbtconfig"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">exec</span> <span class="string">"/usr/local/Cellar/sbt/0.13.8/libexec/sbt"</span> <span class="string">"<span class="variable">$@</span>"</span></span><br></pre></td></tr></table></figure>
<p>IDEA开发环境，还需要配置：</p>
<p><img src="http://img.blog.csdn.net/20161118153135046" alt="idea sbt"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scala PlayFramework（2.4）入门&lt;br&gt;示例程序：&lt;a href=&quot;https://github.com/zqhxuyuan/first-player&quot;&gt;https://github.com/zqhxuyuan/first-player&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="scala" scheme="http://github.com/zqhxuyuan/categories/scala/"/>
    
    
      <category term="scala" scheme="http://github.com/zqhxuyuan/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Streams 博客</title>
    <link href="http://github.com/zqhxuyuan/2016/11/06/Kafka-Streams-blog/"/>
    <id>http://github.com/zqhxuyuan/2016/11/06/Kafka-Streams-blog/</id>
    <published>2016-11-05T16:00:00.000Z</published>
    <updated>2016-11-06T14:44:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka Streams流处理 英文博客翻译<br><a id="more"></a></p>
<h1 id="Kafka_Process_API">Kafka Process API</h1><p><a href="http://codingjunkie.net/kafka-processor-part1/" target="_blank" rel="external">http://codingjunkie.net/kafka-processor-part1/</a></p>
<p>The aim of the Processor API is to introduce a client to enable processing data consumed from Kafka and writing the results back into Kafka. There are two components of the processor client:</p>
<ol>
<li>A “lower-level” processor that providea API’s for data-processing, composable processing and local state storage.</li>
<li>A “higher-level” stream DSL that would cover most processor implementation needs.</li>
</ol>
<p>Potential Use Cases For the Processor API</p>
<ol>
<li>There is a need for notification/alerts on singular values as they are processed. In other words the business requirements are such that you don’t need to establish patterns or examine the value(s) in context with other data being processed. For example you want immediate notification that a fraudulent credit card has been used.</li>
<li>You filter your data when running analytics. Filtering out a medium to large percentage of data ideally should be re-partitioned to avoid data-skew issues. Partitioning is an expensive operation, so by filtering out what data is delivered to your analytics cluster, you can save the filter-repartition step.</li>
<li>You want to run analytics on only a portion of your source data, while delivering the entirety of you data to another store.</li>
</ol>
<h1 id="KStream_API">KStream API</h1><h1 id="使用Kafka_Streams在用户活动事件流上做分布式实时join和聚合">使用Kafka Streams在用户活动事件流上做分布式实时join和聚合</h1><p><a href="https://www.confluent.io/blog/distributed-real-time-joins-and-aggregations-on-user-activity-events-using-kafka-streams/" target="_blank" rel="external">https://www.confluent.io/blog/distributed-real-time-joins-and-aggregations-on-user-activity-events-using-kafka-streams/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kafka Streams流处理 英文博客翻译&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Streams中文翻译</title>
    <link href="http://github.com/zqhxuyuan/2016/11/02/Kafka-Streams-cn/"/>
    <id>http://github.com/zqhxuyuan/2016/11/02/Kafka-Streams-cn/</id>
    <published>2016-11-01T16:00:00.000Z</published>
    <updated>2017-01-02T04:03:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>Confluent Kafka Streams Documentation 中文翻译：  <a href="http://docs.confluent.io/3.0.1/streams/introduction.html" target="_blank" rel="external">http://docs.confluent.io/3.0.1/streams/introduction.html</a><br><a id="more"></a></p>
<h1 id="介绍">介绍</h1><h2 id="Kafka_Streams">Kafka Streams</h2><p>Kafka Streams, a component of open source Apache Kafka, is a powerful, easy-to-use library for building highly scalable, fault-tolerant, distributed stream processing applications on top of Apache Kafka. It builds upon important concepts for stream processing such as properly distinguishing between event-time and processing-time, handling of late-arriving data, and efficient management of application state.</p>
<blockquote>
<p>Kafka Streams是构建在Apache Kafka的一个组件，它是一个功能强大的、对于构建高可用、故障容错、分布式流处理应用程序都很容易使用的库。它构建在流处理的重要概念之上，比如正确地区分事件时间（event-time）和处理时间（process-time），处理延时数据，高效的应用程序状态管理。  </p>
</blockquote>
<p>One of the mantras(祷文) of Kafka Streams is to “Build apps, not clusters!”, which means to bring stream processing out of the Big Data niche into the world of mainstream application development. Using the Kafka Streams library you can implement standard Java applications to solve your stream processing needs – whether at small or at large scale – and then run these applications on client machines at the perimeter(边界) of your Kafka cluster. Deployment-wise you are free to chose from any technology that can deploy Java applications, including but not limited to Puppet, Chef, Ansible, Docker, Mesos, YARN, Kubernetes, and so on. This lightweight and integrative(综合) approach of Kafka Streams is in stark(完全、突出) contrast(对比) to other stream processing tools that require you to install and operate separate stream processing clusters and similar heavy-weight infrastructure that come with their own special set of rules on how to use and interact with them.</p>
<blockquote>
<p>Kafka Streams的一个思想是“构建应用程序，不要集群”，这意味着将流处理从大数据生态圈中解放出来，而专注于主流的应用程序开发。使用Kafka Streams客户端库，你可以用标准的Java应用程序（main方法）来解决你的流处理需求（不管是小规模还是大规模的数据），然后可以在你的Kafka集群之外的客户端机器执行这些应用程序。你可以选择任何可以部署Java应用的技术来部署Kafka Streams，包括但不限于Puppet、Chef、Ansible、Docker、Mesos、YARN、Kubernetes等等。Kafka Streams的轻量级以及综合能力使得它和其他流处理工具形成了鲜明的对比，后者需要你单独安装并维护一个流处理集群，需要依赖重量级的基础架构设施。</p>
</blockquote>
<p>The following list highlights several key capabilities and aspects of Kafka Streams that make it a compelling(引人注目) choice for use cases such as stream processing applications, event-driven systems, continuous queries and transformations, reactive applications, and microservices.</p>
<blockquote>
<p>下面列出了Kafka Streams的几个重要的功能，对于这些用例都是个吸引人的选择：流处理应用程序、事件驱动系统、持续查询和转换、响应式应用程序、微服务。</p>
</blockquote>
<p><strong>Powerful</strong>功能强大</p>
<ul>
<li>Highly scalable, elastic, fault-tolerant 高可用、可扩展性、故障容错</li>
<li>Stateful and stateless processing 有状态和无状态的处理</li>
<li>Event-time processing with windowing, joins, aggregations 针对事件的窗口函数、联合操作、聚合操作</li>
</ul>
<p><strong>Lightweight</strong>轻量级</p>
<ul>
<li>No dedicated cluster required 不需要专用的集群</li>
<li>No external dependencies 不需要外部的依赖</li>
<li>“It’s a library, not a framework.” 它是一个客户端库，不是一个框架</li>
</ul>
<p><strong>Fully integrated</strong>完全完整的</p>
<ul>
<li>100% compatible with Kafka 0.10.0.x 和Kafka完全兼容</li>
<li>Easy to integrate into existing applications 和已有应用程序容易集成</li>
<li>No artificial rules for deploying applications 对部署方式没有严格的规则限制</li>
</ul>
<p><strong>Real-time</strong>实时的</p>
<ul>
<li>Millisecond processing latency 微秒级别的处理延迟</li>
<li>Does not micro-batch messages 不是micro-batch处理</li>
<li>Windowing with out-of-order data 对无序数据的窗口操作</li>
<li>Allows for arrival of late data 允许延迟的数据</li>
</ul>
<h2 id="A_closer_look">A closer look</h2><p>Before we dive into the details such as the concepts and architecture of Kafka Streams or getting our feet wet by following the Kafka Streams quickstart guide, let us provide more context to the previous list of capabilities.</p>
<p>在深入研究Kafka Streams的概念和架构细节之前，你应该先看下<a href="http://docs.confluent.io/3.0.1/streams/quickstart.html#streams-quickstart" target="_blank" rel="external">快速指南</a>，现在我们为上面的那些特点提供更多的上下文信息（背景知识）。</p>
<p>1.Stream Processing Made Simple: Designed as a lightweight library in Apache Kafka, much like the Kafka producer and consumer client libraries. You can easily embed and integrate Kafka Streams into your own applications, which is a significant departure from framework-based stream processing tools that dictate many requirements upon you such as how you must package and “submit” processing jobs to their cluster.</p>
<blockquote>
<p>流处理更加简单：被设计为一个轻量级的库，就像Kafka的生产者和消费者客户端库一样。你可以很方便地将Kafka Streams集成到你自己的应用程序中，这是和其他基于框架的流处理工具的主要区别，它们对你会有很多要求，比如你必须打包然后把流处理作业提交到集群上执行。</p>
</blockquote>
<p>Has no external dependencies on systems other than Apache Kafka and can be used in any Java application. Read: You do not need to deploy and operate a separate cluster for your stream processing needs. Your Operations and Info Sec teams, among others, will surely be happy to hear this.</p>
<blockquote>
<p>在应用程序中除了Apache Kafka之外没有别的依赖：你不需要为你的流处理需求部署或维护一个单独的集群。你们的运维和安全团队肯定听到这个信息肯定很happy吧。</p>
</blockquote>
<p>2.Leverages Kafka as its internal messaging layer instead of (re)implementing a custom messaging layer like many other stream processing tools. Notably, Kafka Streams uses Kafka’s partitioning model to horizontally scale processing while maintaining strong ordering guarantees. This ensures high performance, scalability, and operational simplicity for production environments. A key benefit of this design decision is that you do not have to understand and tune two different messaging layers – one for moving data streams at scale (Kafka) plus a separate one for your stream processing tool. Similarly, any performance and reliability improvements of Kafka will automatically be available to Kafka Streams, too, thus tapping into the momentum of Kafka’s strong developer community.</p>
<blockquote>
<p>利用Kafka作为它的内部消息层而不像其他流处理工具一样重新造轮子。特别是，Kafka Streams使用Kafka的分区模型在维护强一致性的同时也具备了线性的处理能力，这种设计的优点是：你不需要理解或者调整两种消息模型（一种是线性地移动数据流，另外一种是流处理的消息）。同样，任何针对Kafka的性能和可靠性的提升，Kafka Streams都会自动具备，这也促使了Kafka开发者社区的动力。</p>
</blockquote>
<p>3.Is agnostic(不可知论) to resource management and configuration tools, so it integrates much more seamlessly(无缝) into the existing development, packaging, deployment, and operational practices of your organization. You are free to use your favorite tools such as Java application servers, Puppet, Ansible, Mesos, YARN, Docker – or even to run your application manually on a single machine for proof-of-concept scenarios.</p>
<blockquote>
<p>Kafka Streams不需要依赖资源管理和配置工具，所以它可以和已有的开发环境、打包、部署等工具无缝集成。可以运行在Java应用服务器，甚至在单机环境下做原型验证（POC）。</p>
</blockquote>
<p>4.Supports fault-tolerant local state, which enables very fast and efficient stateful operations like joins and windowed aggregations. Local state is replicated to Kafka so that, in case of a machine failure, another machine can automatically restore the local state and resume the processing from the point of failure.</p>
<blockquote>
<p>支持本地状态的故障容错，这使得有状态的操作（比如联合、窗口聚合）更快速和高效。由于本地状态本身通过Kafka进行复制，所以当一个机器宕机时，其他机器可以自动恢复本地状态，并且从故障出错的那个点继续处理。</p>
</blockquote>
<p>5.Employs one-record-at-a-time processing to achieve low processing latency, which is crucial(重要，决定性) for a variety of use cases such as fraud detection. This makes Kafka Streams different from micro-batch based stream processing tools.</p>
<blockquote>
<p>一次处理一条记录的流处理模型，所以处理延迟很低，对于像欺诈检测等场景来说非常重要。这也是Kafka Streams有别于基于micro-batch的流处理工具的区别。</p>
</blockquote>
<p>Furthermore, Kafka Streams has a strong focus on usability(可用性) and a great developer experience. It offers all the necessary stream processing primitives to allow applications to read data from Kafka as streams, process the data, and then either write the resulting data back to Kafka or send the final output to an external system. Developers can choose between a high-level DSL with commonly used operations like filter, map, join, as well as a low-level API for developers who need maximum control and flexibility.</p>
<blockquote>
<p>另外，Kafka Streams对开发者是易用和友好的。它提供了所有必要的流处理算子，允许应用程序将从Kafka读取出来的数据作为一个流，然后处理数据，最后可以将处理结果写回到Kafka或者发送给外部系统。开发者可以使用高级DSL（提供很多常用的操作比如filter、map、join）或者低级API两种方式（当需要更好地控制和灵活性时）。</p>
</blockquote>
<p>Finally, Kafka Streams helps with scaling developers, too – yes, the human side – because it has a low barrier(接线) to entry and a smooth path to scale from development to production: You can quickly write and run a small-scale proof-of-concept on a single machine because you don’t need to install or understand a distributed stream processing cluster; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently(透明地) handles the load balancing of multiple instances of the same application by leveraging Kafka’s parallelism model.</p>
<blockquote>
<p>最后，Kafka Streams对于开发者也是扩展的。是的，从程序员的视角来看的话，它从开发环境到生产环境几乎没有界线：你可以在一台机器上运行一个很小批量的POC，因为你不需要安装或者理解一个分布式的流处理集群是怎么样（不需要知道程序在分布式环境下会有什么不同）；在多台机器上时，你只需要多运行几个应用程序实例就可以扩展到大规模的生产负载（生产环境下负载很高，只需多启动几个新的实例）。Kafka Streams会利用Kafka的并行模型透明底在相同应用程序多个实例之间处理负载均衡。</p>
</blockquote>
<p>In summary, Kafka Streams is a compelling choice for building stream processing applications. Give it a try and run your first Hello World Streams application! The next sections in this documentation will get you started.</p>
<blockquote>
<p>总之Kafka Streams对于构建流处理应用程序是一个非常不错的选择。快来运行一个<a href="http://docs.confluent.io/3.0.1/streams/quickstart.html#streams-quickstart" target="_blank" rel="external">Hello World的流处理应用吧</a>。</p>
</blockquote>
<h1 id="快速开始">快速开始</h1><h2 id="本节目标">本节目标</h2><p>The goal of this quickstart guide is to provide you with a first hands-on look at Kafka Streams. We will demonstrate how to run your first Java application that uses the Kafka Streams library by showcasing a simple end-to-end data pipeline powered by Kafka.</p>
<p>It is worth noting that this quickstart will only scratch the surface of Kafka Streams. More details are provided in the remainder of the Kafka Streams documentation, and we will include pointers throughout the quickstart to give you directions.</p>
<blockquote>
<p>本节的目标是让你亲自看看Kafka Streams是如何实现的。我们会向你展示使用Kafka完成的一个端到端的数据流管道，以及运行你的第一个使用Kafka库的Java应用程序。注意这里仅仅会涉及到Kafka Streams的表层，后续的部分会深入一些细节。</p>
</blockquote>
<h2 id="我们要做什么">我们要做什么</h2><p>下面是使用Java8实现的WordCount示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Serializers/deserializers (serde) for String and Long types</span></span><br><span class="line"><span class="keyword">final</span> Serde&lt;String&gt; stringSerde = Serdes.String();</span><br><span class="line"><span class="keyword">final</span> Serde&lt;Long&gt; longSerde = Serdes.Long();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Construct a `KStream` from the input topic ""streams-file-input", where message values</span></span><br><span class="line"><span class="comment">// represent lines of text (for the sake of this example, we ignore whatever may be stored in the message keys).</span></span><br><span class="line">KStream&lt;String, String&gt; textLines = builder.stream(stringSerde, stringSerde, <span class="string">"streams-file-input"</span>);</span><br><span class="line"></span><br><span class="line">KStream&lt;String, Long&gt; wordCounts = textLines</span><br><span class="line">    <span class="comment">// Split each text line, by whitespace, into words.  The text lines are the message</span></span><br><span class="line">    <span class="comment">// values, i.e. we can ignore whatever data is in the message keys and thus invoke</span></span><br><span class="line">    <span class="comment">// `flatMapValues` instead of the more generic `flatMap`.</span></span><br><span class="line">    .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split(<span class="string">"\\W+"</span>)))</span><br><span class="line">    <span class="comment">// We will subsequently invoke `countByKey` to count the occurrences of words, so we use</span></span><br><span class="line">    <span class="comment">// `map` to ensure the words are available as message keys, too.</span></span><br><span class="line">    .map((key, value) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(value, value))</span><br><span class="line">    <span class="comment">// Count the occurrences of each word (message key).</span></span><br><span class="line">    <span class="comment">// This will change the stream type from `KStream&lt;String, String&gt;` to</span></span><br><span class="line">    <span class="comment">// `KTable&lt;String, Long&gt;` (word -&gt; count), hence we must provide serdes for `String` and `Long`.</span></span><br><span class="line">    .countByKey(stringSerde, <span class="string">"Counts"</span>)</span><br><span class="line">    <span class="comment">// Convert the `KTable&lt;String, Long&gt;` into a `KStream&lt;String, Long&gt;`.</span></span><br><span class="line">    .toStream();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Write the `KStream&lt;String, Long&gt;` to the output topic.</span></span><br><span class="line">wordCounts.to(stringSerde, longSerde, <span class="string">"streams-wordcount-output"</span>);</span><br></pre></td></tr></table></figure>
<p>然后，我们会执行如下步骤来完成第一个流应用程序：</p>
<ol>
<li>在一台机器上启动一个Kafka集群</li>
<li>使用Kafka内置的控制台生产者模拟往一个Kafka主题中写入一些示例数据</li>
<li>使用Kafka Streams库处理输入的数据，处理程序就是上面的wordcount示例</li>
<li>使用Kafka内置的控制台消费者检查应用程序的输出</li>
<li>停止Kafka集群</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">wget http://packages.confluent.io/archive/3.0.1/confluent-3.0.1-2.11.zip</span><br><span class="line">unzip confluent-3.0.1-2.11.zip</span><br><span class="line">cd confluent-3.0.1/</span><br><span class="line">bin/zookeeper-server-<span class="operator"><span class="keyword">start</span> ./etc/kafka/zookeeper.properties</span><br><span class="line"><span class="keyword">bin</span>/kafka-<span class="keyword">server</span>-<span class="keyword">start</span> ./etc/kafka/<span class="keyword">server</span>.properties</span><br><span class="line"></span><br><span class="line"><span class="keyword">bin</span>/kafka-topics <span class="comment">--create \</span></span><br><span class="line">          <span class="comment">--zookeeper localhost:2181 \</span></span><br><span class="line">          <span class="comment">--replication-factor 1 \</span></span><br><span class="line">          <span class="comment">--partitions 1 \</span></span><br><span class="line">          <span class="comment">--topic streams-file-input</span></span><br><span class="line"></span><br><span class="line">echo -<span class="keyword">e</span> <span class="string">"all streams lead to kafka\nhello kafka streams\njoin kafka summit"</span> &gt; /tmp/<span class="keyword">file</span>-<span class="keyword">input</span>.txt</span><br><span class="line"></span><br><span class="line">cat /tmp/<span class="keyword">file</span>-<span class="keyword">input</span>.txt | ./<span class="keyword">bin</span>/kafka-console-producer <span class="comment">--broker-list localhost:9092 --topic streams-file-input</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">bin</span>/kafka-run-<span class="keyword">class</span> org.apache.kafka.streams.examples.wordcount.WordCountDemo</span></span><br></pre></td></tr></table></figure>
<p>Similar to the bounded variant, it is a stateful algorithm that tracks and updates the counts of words. However, since it must assume potentially unbounded input data, it will periodically output its current state and results while continuing to process more data because it cannot know when it has processed “all” the input data. This is a typical difference between the class of algorithms that operate on unbounded streams of data and, say, batch processing algorithms such as Hadoop MapReduce. It will be easier to understand this difference once we inspect the actual output data later on.</p>
<blockquote>
<p>WordCount程序会计算输入单词出现次数的直方图，和之前看到的其他应用程序在有界数据集上不同的是，本例是在一个无限的、无界的数据流上操作。和有界操作相同的是，它也是一个有状态的算法（跟踪和更新单词的次数）。不过，由于它必须假设无限的输入数据，它会定时地输出当前状态和结果，并且持续地处理更多的数据，因为它不知道什么时候它已经处理完了所有的输入数据。这和在有界流数据上的算法是不同的比如Hadoop的MapReduce。在我们检查了实际的输出结果后，你就会更加容易地理解这里的不同点。</p>
</blockquote>
<p>The WordCount demo application will read from the input topic streams-file-input, perform the computations of the WordCount algorithm on the input data, and continuously write its current results to the output topic streams-wordcount-output (the names of its input and output topics are hardcoded). The demo will run for a few seconds and then, unlike typical stream processing applications, terminate automatically.</p>
<blockquote>
<p>这个WordCount示例会从Kafka的输入主题“streams-file-input”中读取数据，在输入数据上执行WorldCount算法，并且持续地将当前结果写入到输出主题“streams-wordcount-output”。不过和其他流处理程序不同的是，这里为了实验，仅仅运行几秒钟后就会退出，通常实际运行的流应用程序是永远不会停止的。</p>
</blockquote>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer --zookeeper localhost:<span class="number">2181</span> \</span><br><span class="line">          --topic streams-wordcount-output \</span><br><span class="line">          --from-beginning \</span><br><span class="line">          --formatter kafka<span class="class">.tools</span><span class="class">.DefaultMessageFormatter</span> \</span><br><span class="line">          --property print.key=true \</span><br><span class="line">          --property key.deserializer=org<span class="class">.apache</span><span class="class">.kafka</span><span class="class">.common</span><span class="class">.serialization</span><span class="class">.StringDeserializer</span> \</span><br><span class="line">          --property value.deserializer=org<span class="class">.apache</span><span class="class">.kafka</span><span class="class">.common</span><span class="class">.serialization</span><span class="class">.LongDeserializer</span></span><br></pre></td></tr></table></figure>
<p>打印信息如下，这里第一列是Kafka消息的键（字符串格式），第二列是消息的值（Long类型）。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">all     <span class="number">1</span></span><br><span class="line">streams <span class="number">1</span></span><br><span class="line">lead    <span class="number">1</span></span><br><span class="line">to      <span class="number">1</span></span><br><span class="line">kafka   <span class="number">1</span></span><br><span class="line">hello   <span class="number">1</span></span><br><span class="line">kafka   <span class="number">2</span></span><br><span class="line">streams <span class="number">2</span></span><br><span class="line">join    <span class="number">1</span></span><br><span class="line">kafka   <span class="number">3</span></span><br><span class="line">summit  <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>等等，输出结果看起来有点奇怪，为什么有重复的条目比如”streams”出现了两次，”kafka”出现了三次，难道不应该是下面这样的吗：  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#为什么不是这样，你可能会有疑问</span></span><br><span class="line">all     <span class="number">1</span></span><br><span class="line">lead    <span class="number">1</span></span><br><span class="line">to      <span class="number">1</span></span><br><span class="line">hello   <span class="number">1</span></span><br><span class="line">streams <span class="number">2</span></span><br><span class="line">join    <span class="number">1</span></span><br><span class="line">kafka   <span class="number">3</span></span><br><span class="line">summit  <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>The explanation is that the output of the WordCount application is actually a continuous stream of updates, where each data record (i.e. each line in the original output above) is an updated count of a single word aka record key such as “kafka”. For multiple records with the same key, each later record is an update of the previous one.</p>
<blockquote>
<p>合理的解释是：WordCount应用程序的输出实际上是一个持续不断的”更新流”，每条数据记录（上面示例中每一行的输出结果）都是一个单词（记录的key，比如”kafka”）的更新次数。对于相同key的多条记录，后面的记录都是对前面记录的更新。</p>
</blockquote>
<p>The two diagrams below illustrate what is essentially(本质) happening behind the scenes. The first column shows the evolution of the current state of the KTable<string, long=""> that is counting word occurrences for countByKey. The second column shows the change records that result from state updates to the KTable and that eventually, once converted to a KStream</string,></p>
<blockquote>
<p>下面的两幅图展示了发生在背后的本质，第一列表示<code>KTable&lt;String, Long&gt;</code>的当前状态的进化，通过<code>countByKey</code>计算单词的出现次数。第二列的结果显示了从状态改变到KTable的变更记录，最终被转换为一个KStream。</p>
</blockquote>
<p>First the text line “all streams lead to kafka” is being processed. The KTable is being built up as each new word results in a new table entry (highlighted with a green background), and a corresponding change record is sent to the downstream KStream.</p>
<p>When the second text line “hello kafka streams” is processed, we observe, for the first time, that existing entries in the KTable are being updated (here: for the words “kafka” and for “streams”). And again, change records are being sent to the KStream.</p>
<blockquote>
<p>当第一次处理文本行“all streams lead to kafka”时，KTable会在每个表的条目中构建一个新的单词结果（绿色高亮），并且<strong>把对应的变更记录发送给下游的KStream</strong>。<br>当处理第二个文本行“hello kafka streams”时，我们注意到，和第一次不同的是，存在于KTable的条目会被更新（比如这里的”kafka”和”streams”），并且同样的，变更记录也会被发送到KStream。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161103092411029" alt="10-1 ktable kstream"></p>
<p>And so on (we skip the illustration of how the third line is being processed). This explains why the output topic has the contents we showed above, because it contains the full record of changes, i.e. the information shown in the second column for KStream above:</p>
<blockquote>
<p>第三行的处理也是类似的，这里就不再累述。这就解释了为什么上面输出的主题内容是我们看到的那样，因为它包含了所有完整的变更记录，即上面第二列KStream的内容：</p>
</blockquote>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">all     <span class="number">1</span></span><br><span class="line">streams <span class="number">1</span></span><br><span class="line">lead    <span class="number">1</span></span><br><span class="line"><span class="built_in">to</span>      <span class="number">1</span></span><br><span class="line">kafka   <span class="number">1</span>  &lt;- <span class="keyword">first</span> <span class="built_in">line</span> <span class="function"><span class="keyword">end</span></span><br><span class="line"><span class="title">hello</span>   <span class="title">1</span></span></span><br><span class="line">kafka   <span class="number">2</span></span><br><span class="line">streams <span class="number">2</span>  &lt;- <span class="keyword">second</span> <span class="built_in">line</span> <span class="function"><span class="keyword">end</span></span><br><span class="line"><span class="title">join</span>    <span class="title">1</span></span></span><br><span class="line">kafka   <span class="number">3</span></span><br><span class="line">summit  <span class="number">1</span>  &lt;- <span class="keyword">third</span> <span class="built_in">line</span> <span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>为什么不输出KTable，这是因为KTable每次处理一条记录，都会发送变更记录给下游的KStream，即KTable每次处理一条记录，产生一条变更记录。而KTable本身是有状态的，可以看到在处理第一个单词时，KTable有一条记录，在处理第二个不同的单词时，KTable有两条记录，这个状态是一直保存的，如果说把KTable作为输出，那么就会有重复的问题，比如下面这样的输出肯定不是我们希望看到的：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">all     <span class="number">1</span>  &lt;-处理第一个单词后的KTable</span><br><span class="line">all     <span class="number">1</span></span><br><span class="line">streams <span class="number">1</span>  &lt;-处理第二个单词后的KTable</span><br><span class="line">all     <span class="number">1</span></span><br><span class="line">streams <span class="number">1</span></span><br><span class="line">lead    <span class="number">1</span>  &lt;-处理第三个单词后的KTable</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>Looking beyond the scope of this concrete example, what Kafka Streams is doing here is to leverage the duality(二元性，对偶) between a table and a changelog stream (here: table = the KTable, changelog stream = the downstream KStream): you can publish every change of the table to a stream, and if you consume the entire changelog stream from beginning to end, you can reconstruct the contents of the table.</p>
<blockquote>
<p>Kafka Streams这里做的工作利用了一张表和一个变更流的二元性（表指的是KTable，变更流指的是下游的KStream）：你可以将表的每个变更记录发布给一个流，如果你从整个变更流的最开始消费到最后，你就可以重新构造出表的内容。</p>
</blockquote>
<h1 id="概念">概念</h1><h2 id="Kafka_101">Kafka 101</h2><p>Kafka Streams is, by deliberate(深思熟虑) design, tightly integrated with Apache Kafka: it uses Kafka as its internal messaging layer. As such it is important to familiarize yourself with the key concepts of Kafka, too, notably the sections 1. Getting Started and 4. Design in the Kafka documentation. In particular you should understand:</p>
<p>Kafka Streams是经过深思熟虑的设计，它和Apache Kafka仅仅地集成：它使用Kafka作为内部的消息层。所以理解Kafka的关键概念非常重要，如果不熟悉，可以看Kafka的文档。</p>
<ul>
<li>The who’s who: Kafka distinguishes producers, consumers, and brokers. In short, producers publish data to Kafka brokers, and consumers read published data from Kafka brokers. Producers and consumers are totally decoupled. A Kafka cluster consists of one or more brokers.</li>
<li>The data: Data is stored in topics. The topic is the most important abstraction provided by Kafka: it is a category or feed name to which data is published by producers. Every topic in Kafka is split into one or more partitions, which are replicated across Kafka brokers for fault tolerance.</li>
<li>Parallelism: Partitions of Kafka topics, and especially their number for a given topic, are also the main factor that determines the parallelism of Kafka with regards to reading and writing data. Because of their tight integration the parallelism of Kafka Streams is heavily influenced by and depending on Kafka’s parallelism.</li>
</ul>
<ol>
<li>Kafka分成生产者、消费者、Brokers。生产者发布数据给Kafka的Brokers，消费者从Kafka的Brokers读取发布过的数据。生产者和消费者完全解耦。一个Kafka集群包括一个或多个Broekrs节点。</li>
<li>数据以主题的形式存储。主题是Kafka提供的最重要的一个抽象：它是生产者发布数据的一种分类（相同类型的消息应该发布到相同的主题）。每个主题会分成一个或多个分区，并且为了故障容错，每个分区都会在Kafka的Brokers中进行复制。</li>
<li>Kafka主题的分区数量决定了读取或写入数据的并行度。因为Kafka Streams和Kafka结合的很紧，所以Kafka Streams也依赖于Kafka的并行度。</li>
</ol>
<h2 id="流、流处理、拓扑、算子">流、流处理、拓扑、算子</h2><p>A stream is the most important abstraction provided by Kafka Streams: it represents an unbounded, continuously updating data set, where unbounded means “of unknown or of unlimited size”. A stream is an ordered, replayable, and fault-tolerant sequence of immutable data records, where a data record is defined as a key-value pair.</p>
<blockquote>
<p>流是Kafka Streams提供的最重要的抽象：它代表了一个无界的、持续更新的数据集。流是一个有序的、可重放的、容错的不可变数据记录序列，其中每个数据记录被定义成一个key-value键值对</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161103101744946" alt="stream-record">  </p>
<p>A stream processing application is any program that makes use of the Kafka Streams library. In practice, this means it is probably “your” Java application. It may define its computational logic through one or more processor topologies (see next section).</p>
<blockquote>
<p>流处理应用程序是任何使用了Kafka Streams库进行开发的应用程序，它会通过一个或多个处理拓扑定义计算逻辑。</p>
</blockquote>
<p>A processor topology or simply topology defines the computational logic of the data processing that needs to be performed by a stream processing application. A topology is a graph of stream processors (nodes) that are connected by streams (edges). Developers can define topologies either via the low-level Processor API or via the Kafka Streams DSL, which builds on top of the former.</p>
<p>处理拓扑或者叫拓扑定义了流处理应用程序对数据处理的计算逻辑。拓扑是一张由流处理算子和相连接的流组成的DAG图，其中算子是图的节点，流是图的边。开发者可以通过低级的Processor API或者高级的Kafka Streams DSL定义拓扑，其中后者实际上是构建在前者之上的。</p>
<p>A stream processor is a node in the processor topology(as shown in the diagram of section Processor Topology). It represents a processing step in a topology, i.e. it is used to transform data in streams. Standard operations such as map, filter, join, and aggregations are examples of stream processors that are available in Kafka Streams out of the box. A stream processor receives one input record at a time from its upstream processors in the topology, applies its operation to it, and may subsequently produce one or more output records to its downstream processors.</p>
<blockquote>
<p>流算子是处理拓扑中的节点，它代表了在拓扑中的处理步骤，比如转换算子会在流中转换数据。标准的算子包括map/filter/join/aggregation，这些都是流算子的示例，并且内置在Kafka Streams中开箱即用。一个流算子从它在拓扑中的上游算子一次接收一条输入记录，将操作运用到记录，并且可能会产生一条或多条输出记录给下游的算子。Kafka Streams提供了两种方式来定义算子：</p>
</blockquote>
<ul>
<li>The Kafka Streams DSL provides the most common data transformation operations such as map and filter so you don’t have to implement these stream processors from scratch.</li>
<li>The low-level Processor API allows developers to define and connect custom processors as well as to interact with state stores.</li>
</ul>
<ol>
<li>Kafka Streams DSL提供了最通用的数据转换操作，比如map、filter，这样你不需要自己实现这些算子</li>
<li>低级的Processor API，允许开发者定义和连接定制的算子，并且还可以和状态存储交互</li>
</ol>
<h2 id="时间">时间</h2><p>A critical aspect in stream processing is the the notion of time, and how it is modeled and integrated. For example, some operations such as Windowing are defined based on time boundaries.</p>
<blockquote>
<p>流处理的一个重要概念是时间，如何对时间进行建模和整合非常重要，因为有些操作比如窗口函数会基于时间的边界来定义。有几种类型的时间表示方式：</p>
</blockquote>
<ul>
<li>Event-time: The point in time when an event or data record occurred, i.e. was originally created “by the source”. Achieving event-time semantics typically requires embedding timestamps in the data records at the time a data record is being produced. Example: If the event is a geo-location change reported by a GPS sensor in a car, then the associated event-time would be the time when the GPS sensor captured the location change.</li>
<li>Processing-time: The point in time when the event or data record happens to be processed by the stream processing application, i.e. when the record is being consumed. The processing-time may be milliseconds, hours, or days etc. later than the original event-time. Example: Imagine an analytics application that reads and processes the geo-location data reported from car sensors to present it to a fleet management dashboard. Here, processing-time in the analytics application might be milliseconds or seconds (e.g. for real-time pipelines based on Apache Kafka and Kafka Streams) or hours (e.g. for batch pipelines based on Apache Hadoop or Apache Spark) after event-time.</li>
<li>Ingestion-time: The point in time when an event or data record is stored in a topic partition by a Kafka broker. Ingestion-time is similar to event-time, as a timestamp gets embedded in the data record itself. The difference is, that the timestamp is generated when the record is appended to the target topic by the Kafka broker, not when the record is created “at the source”. Ingestion-time may approximate event-time reasonably well if we assume that the time difference between creation of the record and its ingestion into Kafka is sufficiently small, where “sufficiently” depends on the specific use case. Thus, ingestion-time may be a reasonable alternative for use cases where event-time semantics are not possible, e.g. because the data producers don’t embed timestamps (e.g. older versions of Kafka’s Java producer client) or the producer cannot assign timestamps directly (e.g., it does not have access to a local clock).</li>
</ul>
<ol>
<li>事件时间：事件或数据记录发生的时间点，它是由事件源创建的。实现事件时间语义通常需要在记录中有内置的时间撮字段，表示这条记录在什么时候产生。比如一条事件是车辆传感器报告的地理位置变更，那么对应的事件时间表示GPS传感器捕获位置变更的时间点。</li>
<li>处理时间：事件被流处理应用程序处理的时间点，比如就被消费的时候，处理时间会比原始的事件时间要晚。举例一个分析应用程序读取并处理车辆上传的地理位置，并且呈现到一个dashboard上。这里分析程序的处理时间可能比事件的时间晚几毫米、几秒、甚至几个小时。</li>
<li>摄取时间：事件存储到Kafka Brokers的主题分区中的时间点。摄取时间和事件时间类似，它也是作为数据记录本身的一个内置字段，不同的是<strong>摄取时间是在追加到Kafka中时自动生成的，而不是数据源创建的时间</strong>。如果我们假设记录的创建时间和摄取到Kafka的时间间隔足够短的话，可以认为摄取时间近似于事件时间，当然足够短这个时间跟具体的用例有关。什么场景下采用摄取时间比较合理呢？比如数据源没有内置的事件时间（比如旧版本的Java生产者客户端在消息中不会带有时间撮，新版本则有），或者说生产者不能直接分配时间撮（无法获取到本地时钟）。</li>
</ol>
<p>The choice between event-time and ingestion-time is actually done through the configuration of Kafka (not Kafka Streams): From Kafka 0.10.x onwards, timestamps are automatically embedded into Kafka messages. Depending on Kafka’s configuration these timestamps represent event-time or ingestion-time. The respective Kafka configuration setting can be specified on the broker level or per topic. The default timestamp extractor in Kafka Streams will retrieve these embedded timestamps as-is. Hence, the effective time semantics of your application depend on the effective Kafka configuration for these embedded timestamps. See :ref:Developer Guide <timestamp extractor=""> for detailed information.</timestamp></p>
<blockquote>
<p>选择事件时间还是摄取时间，是通过Kafka的配置文件（不是Kafka Streams的配置），在0.10版本之后，时间撮会自动内嵌到Kafka的消息中。根据Kafka的配置，时间撮可以指定为事件时间还是摄取时间，这个配置可以设置到Broker级别，也可以是每个Topic。默认的Kafka Streams时间撮抽取方式会取出内置的时间撮字段。所以应用程序的有效时间语义依赖于Kafka的内置时间撮。</p>
</blockquote>
<p>Kafka Streams assigns a timestamp to every data record via so-called timestamp extractors. These per-record timestamps describe the progress of a stream with regards to time (although records may be out-of-order within the stream) and are leveraged by time-dependent operations such as joins. They are also used to synchronize multiple input streams within the same application.</p>
<blockquote>
<p>Kafka Streams会通过时间撮抽取器把一个时间撮分配给每条记录。每条记录的时间撮描述了一条流关于时间的进度（尽管流中的记录可能没有顺序），这个时间撮会被时间相关的操作比如join所使用。同时它们也会被用来在同一个应用程序中多个输入流的同步。</p>
</blockquote>
<p>Concrete implementations of timestamp extractors may retrieve or compute timestamps based on the actual contents of data records such as an embedded timestamp field to provide event-time or ingestion-time semantics, or use any other approach such as returning the current wall-clock time at the time of processing, thereby yielding processing-time semantics to stream processing applications. Developers can thus enforce(执行，强制) different notions/semantics of time depending on their business needs.</p>
<blockquote>
<p>具体的时间撮抽取器实现，基于数据记录实际内容的时间撮，可能会读取或者计算，比如（数据记录中）提供的事件时间或者摄取时间语义的时间撮字段，或者使用其他的方式，比如返回当前的处理时间，即流处理应用程序的处理时间语义。开发者可以根据他们的业务需求使用不同的时间语义。</p>
</blockquote>
<p>Be aware that ingestion-time in Kafka Streams is used slightly different as in other stream processing systems. Ingestion-time could mean the time when a record is fetched by a stream processing application’s source operator. In Kafka Streams, ingestion-time refers to the time when a record was appended to a Kafka topic partition.</p>
<blockquote>
<p>注意Kafka Streams的摄取时间可能和其他流处理系统的使用方式有点不同。摄取时间可以表示为被流处理的源算子获取的时间点。而在Kafka Streams中，摄取时间指的是当一条记录被追加到Kafka主题分区的那个时间点（即Producer写入分区的时间）。</p>
</blockquote>
<h2 id="有状态的流处理">有状态的流处理</h2><p>Some stream processing applications don’t require state, which means the processing of a message is independent from the processing of all other messages. If you only need to transform one message at a time, or filter out messages based on some condition, the topology defined in your stream processing application can be simple.</p>
<blockquote>
<p>有些流处理应用程序并不需要状态，这意味着一条消息的处理和其他所有消息的处理都是独立的。如果你只需要在一个时间点转换一条消息，或者基于某些条件对消息进行过滤，你的流计算应用层序的拓扑可以非常简单。</p>
</blockquote>
<p>However, being able to maintain state opens up many possibilities for sophisticated(复杂) stream processing applications: you can join input streams, or group and aggregate data records. Many such stateful operators are provided by the Kafka Streams DSL.</p>
<blockquote>
<p>不过，对于复杂的流处理应用程序，为了能够维护状态，会有很多可能性：联合不同的输入流，对数据记录进行分组和聚合。Kafka Streams的DSL提供了很多有状态的操作算子。</p>
</blockquote>
<h2 id="Streams和Tables的二元性">Streams和Tables的二元性</h2><p>Before we discuss concepts such as aggregations in Kafka Streams we must first introduce tables, and most importantly the relationship between tables and streams: the so-called stream-table duality. Essentially, this duality means that a stream can be viewed as a table, and vice versa. Kafka’s log compaction feature, for example, exploits(功绩，利用，开发) this duality.</p>
<p>在介绍Kafka Streams的概念之前（比如聚合），我们必须先介绍tables，以及tables和streams的关系（所谓的stream-table二元性）。从本质上来说，二元性意味着一个流可以被看做是一张表，反过来也是成立的。Kafka的日志压缩特性，可以实现这样的二元转换。一张表，简单来说就是一系列的键值对，或者被叫做字典、关联数组。</p>
<p><img src="http://img.blog.csdn.net/20161103130945109" alt="k-table"></p>
<p>stream-table二元性描述了两者的紧密关系：</p>
<ul>
<li>Stream as Table: A stream can be considered a changelog of a table, where each data record in the stream captures a state change of the table. A stream is thus a table in disguise(假装), and it can be easily turned into a “real” table by replaying the changelog from beginning to end to reconstruct the table. Similarly, in a more general analogy(类比), aggregating data records in a stream – such as computing the total number of pageviews by user from a stream of pageview events – will return a table (here with the key and the value being the user and its corresponding pageview count, respectively).</li>
<li>Table as Stream: A table can be considered a snapshot, at a point in time, of the latest value for each key in a stream (a stream’s data records are key-value pairs). A table is thus a stream in disguise, and it can be easily turned into a “real” stream by iterating over each key-value entry in the table.</li>
</ul>
<ol>
<li>将流作为表：一个流可以被认为是一张表的变更记录(changelog)，流中的每条数据记录捕获了表的每个变更状态。一个流可以<em>假装</em>是一张表，也可以通过从头到尾重放变更日志来重构表，很容易地变成<em>真正</em>的表。同样地，在流中聚合记录（比如从PV事件流中计算用户的PV数）会返回一张表（这里键值分别是用户，以及对应的PV数）。</li>
<li>将表作为流：一张表可以认为是在某个时间点的一份快照，是流的每个key对应的最近的值。一张表因此也可以假装是一个流，也可以通过迭代表中所有的键值条目转换成一个真实的流。</li>
</ol>
<p>Let’s illustrate this with an example. Imagine a table that tracks the total number of pageviews by user (first column of diagram below). Over time, whenever a new pageview event is processed, the state of the table is updated accordingly. Here, the state changes between different points in time – and different revisions(修正) of the table – can be represented as a changelog stream (second column).</p>
<p>Interestingly, because of the stream-table duality, the same stream can be used to reconstruct the original table (third column):</p>
<p>举例，一张表会跟踪用户的PV总数（左图第一列），当一条新的pageview事件被处理的时候，表的状态会相应地被更新。这里不同时间点的状态变更（针对表的不同修改），可以作为一个变更日志流（左图第二列）。有趣的是，由于stream-table的二元性，相同的流可以被用来构造出原始的表（右图第三列）。</p>
<p><img src="http://img.blog.csdn.net/20161103131309314" alt="k stream table durable"></p>
<p>The same mechanism(机制) is used, for example, to replicate databases via change data capture (CDC) and, within Kafka Streams, to replicate its so-called state stores across machines for fault-tolerance. The stream-table duality is such an important concept that Kafka Streams models it explicitly(明确地) via the KStream and KTable interfaces, which we describe in the next sections.</p>
<blockquote>
<p>这种类似的机制也被用在其他系统中，比如通过CDC复制数据库。在Kafka Streams中，为了容错处理，会将它的状态存储复制到多台机器上。stream-table的二元性是很重要的概念，Kafka Streams通过KStream和KTable接口对它们进行建模。</p>
</blockquote>
<h2 id="KStream（记录流_record_stream）">KStream（记录流 record stream）</h2><p>A KStream is an abstraction of a record stream, where each data record represents a self-contained datum(基准，资料) in the unbounded data set. Using the table analogy(类比), data records in a record stream are always interpreted as “inserts” – think: append-only ledger(分类) – because no record replaces an existing row with the same key. Examples are a credit card transaction, a page view event, or a server log entry.</p>
<blockquote>
<p>一个KStream是对记录流的抽象，每条数据记录能够表示在无限数据集中自包含的数据。用传统数据库中的表这个概念来类比，记录流中的数据可以理解为“插入”（只有追加），因为不会有记录会替换已有的相同key的行。比如信用卡交易、访问时间、服务端日志条目。举例有两条记录发送到流中：  </p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">"alice"</span>, <span class="number">1</span>) --&gt; (<span class="string">"alice"</span>, <span class="number">3</span>)  <span class="comment">//这两条记录依次发送到流中</span></span><br></pre></td></tr></table></figure>
<p>If your stream processing application were to sum the values per user, it would return 4 for alice. Why? Because the second data record would not be considered an update of the previous record. Compare this behavior of KStream to KTable below, which would return 3 for alice.</p>
<blockquote>
<p>如果你的流处理应用程序是为每个用户求和（记录的value含义不是很明确，但是我们只是要对value值求和），那么alice用户的返回结果是4。因为第二条记录不会被认为是对前一条记录的更新（第一条记录和第二条记录是同时存在的）。如果将其和下面的KTable对比，KTable中alice用户的返回结果是3。</p>
</blockquote>
<h2 id="KTable（变更流_changelog_stream）">KTable（变更流 changelog stream）</h2><p>A KTable is an abstraction of a changelog stream, where each data record represents an update. More precisely, the value in a data record is considered to be an update of the last value for the same record key, if any (if a corresponding key doesn’t exist yet, the update will be considered a create). Using the table analogy, a data record in a changelog stream is interpreted as an update because any existing row with the same key is overwritten.</p>
<blockquote>
<p>一个KTable是对变更日志流的抽象，每条数据记录代表的是一个更新。更准确的说，数据记录中的值被认为是对已有相同记录的key的值更新（如果存在key则更新，如果key不存在，更新操作会被认为是创建）。用传统数据库中的表这个概念来类比，变更流中的数据可以理解为“更新”，因为任何已经存在相同key的行都会被覆盖。</p>
</blockquote>
<p>If your stream processing application were to sum the values per user, it would return 3 for alice. Why? Because the second data record would be considered an update of the previous record. Compare this behavior of KTable with the illustration for KStream above, which would return 4 for alice.</p>
<blockquote>
<p>还是以上面的两条记录发送到流中为例，如果也是为每个用户求和，那么alice用户的返回结果是3。因为第二条记录会被认为是对前一条记录的更新（那么第一条记录实际上就不存在了）。如果将其和上面的KStream对比，KStream中alice用户的返回结果是4。</p>
</blockquote>
<p>Effects of Kafka’s log compaction: Another way of thinking about KStream and KTable is as follows: If you were to store a KTable into a Kafka topic, you’d probably want to enable Kafka’s log compaction feature, e.g. to save storage space.</p>
<blockquote>
<p>理解KStream和KTable的另外一种思路是：如果将KTable存储到Kafka主题中，你应该开启Kafka的日志压缩功能。</p>
</blockquote>
<p>However, it would not be safe to enable log compaction in the case of a KStream because, as soon as log compaction would begin purging older data records of the same key, it would break the semantics of the data. To pick up the illustration example again, you’d suddenly get a 3 for alice instead of a 4 because log compaction would have removed the (“alice”, 1) data record. Hence log compaction is perfectly safe for a KTable (changelog stream) but it is a mistake for a KStream (record stream).</p>
<blockquote>
<p>如果是KStream，开启日志压缩不是一个安全的做法，因为日志压缩会清除相同key的不同数据，这会破坏数据的语义。举例，你可能会突然看到用户alice的结果为3而不是4，因为日志压缩会删除(“alice”, 1)这条记录。所以日志压缩对于KTable是安全的，而对KSteram则是错误的用法。</p>
</blockquote>
<p>We have already seen an example of a changelog stream in the section Duality of Streams and Tables. Another example are change data capture (CDC) records in the changelog of a relational database, representing which row in a database table was inserted, updated, or deleted.</p>
<blockquote>
<p>在stream-table二元性中，我们已经看到了一个变更日志流的示例。另一个示例是关系型数据库中的CDC变更日志，表示数据库中哪一行执行了插入，更新、删除动作。</p>
</blockquote>
<p>KTable also provides an ability to look up current values of data records by keys. This table-lookup functionality is available through join operations (see also Joining Streams in the Developer Guide).</p>
<blockquote>
<p>KTable也支持根据记录的key查询当前的value，这种特性会在join操作时使用。</p>
</blockquote>
<h2 id="窗口操作">窗口操作</h2><p>A stream processor may need to divide data records into time buckets, i.e. to window the stream by time. This is usually needed for for join and aggregation operations, etc. Windowed stream buckets can be maintained in the processor’s local state.</p>
<blockquote>
<p>一个流算子可能需要将数据记录分成多个时间段，比如对流按照时间做成一个个窗口。通常在联合和聚合操作时需要这么做。在算子的本地状态中会维护窗口流。</p>
</blockquote>
<p>Windowing operations are available in the Kafka Streams DSL, where users can specify a retention period for the window. This allows Kafka Streams to retain old window buckets for a period of time in order to wait for the late arrival of records whose timestamps fall within the window interval. If a record arrives after the retention period has passed, the record cannot be processed and is dropped.</p>
<blockquote>
<p>窗口算子在Kafka Stream DSL中可以使用，用户可以指定窗口的保留时间。这样允许Kafka Streams会在一段时间内保留旧的窗口段，目的是等待迟来的记录，这些记录的时间撮落在窗口间隔内（虽然不一定是当前窗口，但可能是旧的窗口，如果没有保留旧窗口的话，迟来的记录就会被直接丢弃了，因为当前窗口不能存放旧记录）。如果一条记录在保留时间过去之后才到达，这条记录就不会被处理，只能被丢弃了。</p>
</blockquote>
<p>Late-arriving records are always possible in real-time data streams. However, it depends on the effective time semantics how late records are handled. Using processing-time, the semantics are “when the data is being processed”, which means that the notion of late records is not applicable as, by definition, no record can be late. Hence, late-arriving records only really can be considered as such (i.e. as arriving “late”) for event-time or ingestion-time semantics. In both cases, Kafka Streams is able to properly handle late-arriving records.</p>
<blockquote>
<p>迟到的记录在实时数据流中总是可能发生的。不过，它取决于记录到底有多晚才被处理的有效时间语义。使用处理时间，语义是“当数据正在被处理”，这就意味着迟来的记录是不适合的，也就是说不会有记录迟到的。所以迟到的记录只能针对事件时间或者摄取时间这两种语义。这两种情况下，Kafka Streams都可以很好地处理迟到的记录。</p>
</blockquote>
<h2 id="联合操作">联合操作</h2><p>A join operation merges two streams based on the keys of their data records, and yields a new stream. A join over record streams usually needs to be performed on a windowing basis because otherwise the number of records that must be maintained for performing the join may grow indefinitely.</p>
<blockquote>
<p>联合操作会合并两个流，基于他们的数据记录的keys，并产生一个新的流。在记录流上的join操作通常需要在窗口的基础上执行，否则为了执行join而需要维护的记录数量会无限膨胀（在无限的记录集上无法做join操作，因为你不知道什么时候结束，就无法join，联合操作必须是在有限的记录集上，而窗口正好是有限的记录集）。</p>
</blockquote>
<p>The join operations available in the Kafka Streams DSL differ based on which kinds of streams are being joined (e.g. KStream-KStream join versus KStream-KTable join).</p>
<blockquote>
<p>Kafka Streams DSL中的join操作跟流的类型有关，比如KStream-KStream进行join，或者KStream-KTable进行join。</p>
</blockquote>
<h2 id="聚合操作">聚合操作</h2><p>An aggregation operation takes one input stream, and yields a new stream by combining multiple input records into a single output record. Examples of aggregations are computing counts or sum. An aggregation over record streams usually needs to be performed on a windowing basis because otherwise the number of records that must be maintained for performing the aggregation may grow indefinitely.</p>
<blockquote>
<p>一个聚合操作会接受一个输入流，然后通过合并多条输入记录产生一个新的流，最终生成一个单一的输出记录，聚合算子的示例比如计算次数或求和。和join操作一样，聚合操作也需要在窗口的基础上执行。</p>
</blockquote>
<p>In the Kafka Streams DSL, an input stream of an aggregation can be a KStream or a KTable, but the output stream will always be a KTable. This allows Kafka Streams to update an aggregate value upon the late arrival of further records after the value was produced and emitted. When such late arrival happens, the aggregating KStream or KTable simply emits a new aggregate value. Because the output is a KTable, the new value is considered to overwrite the old value with the same key in subsequent processing steps.</p>
<blockquote>
<p>在Kafka Streams DSL中，<strong>聚合操作的输入流可以是一个KStream或者是一个KTable，但是输出流只能是一个KTable</strong>。这就允许Kafka Streams在value被产生并发送出去之后，即使迟到的记录到来时，也可以更新聚合结果（第一次产生的结果是在当前窗口，然后把结果发送出去，第二次产生的结果已经不在当前窗口，它属于旧的窗口，也会更新对应的聚合结果，然后再把最新的结果发送出去）。当这样的迟到记录到来时，聚合的KStream或者KTable仅仅简单地发送新的聚合结果。由于输出是一个KTable，相同key下，在后续的处理步骤中，新的值会覆盖旧的值。</p>
</blockquote>
<h1 id="架构">架构</h1><p>Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers. Below is Logical view of a Kafka Streams application that contains multiple stream threads, each of which in turn containing multiple stream tasks.</p>
<p>Kafka Streams是构建在Kafka生产者和消费者的库，并且利用了Kafka本身的特性提供了数据的并行、分布式协调、容错，简化了应用程序的开发。下图是Kafka Streams应用程序的逻辑视图，包括了多个流线程，每个线程包括多个流任务。</p>
<p><img src="http://img.blog.csdn.net/20161103180445363" alt="kstream arch overview"></p>
<h2 id="拓扑">拓扑</h2><p>A processor topology or simply topology defines the stream processing computational logic for your application, i.e., how input data is transformed into output data. A topology is a graph of stream processors (nodes) that are connected by streams (edges). There are two special processors in the topology:</p>
<blockquote>
<p>拓扑定义了流处理应用程序的计算逻辑，比如输入数据怎么转换成输出数据。拓扑是由流处理算子和相连的流组成的一张图。在拓扑中有两种特殊类型的流算子：</p>
</blockquote>
<ul>
<li>Source Processor: A source processor is a special type of stream processor that does not have any upstream processors. It produces an input stream to its topology from one or multiple Kafka topics by consuming records from these topics and forward them to its down-stream processors.</li>
<li>Sink Processor: A sink processor is a special type of stream processor that does not have down-stream processors. It sends any received records from its up-stream processors to a specified Kafka topic.</li>
</ul>
<ol>
<li>源算子：没有任何上游算子，它从一个或多个Kafka主题中消费记录，然后产生一个到拓扑的输入流，并且转发到下游的算子</li>
<li>目标算子：没有任何的下游算子，它会把从上游算子接收到的任何记录，发送给指定的Kafka主题</li>
</ol>
<p><img src="http://img.blog.csdn.net/20161103180512627" alt="kstream topo"></p>
<p>A stream processing application – i.e., your application – may define one or more such topologies, though typically it defines only one. Developers can define topologies either via the low-level Processor API or via the Kafka Streams DSL, which builds on top of the former.</p>
<p>A processor topology is merely a logical abstraction for your stream processing code. At runtime, the logical topology is instantiated and replicated inside the application for parallel processing (see Parallelism Model).</p>
<p>一个流处理应用程序可以定义一个或多个拓扑，尽管通常你只会定义一个。开发者可以通过低级的Processor API或者高级的DSL方式定义拓扑。一个拓扑仅仅是流处理代码的逻辑抽象，在运行时，逻辑拓扑会被实例化，并且在应用程序中进行复制以获得并行处理的能力。</p>
<h2 id="并行模型">并行模型</h2><h3 id="Stream_Partitions_and_Tasks">Stream Partitions and Tasks</h3><p>Kafka Streams uses the concepts of partitions and tasks as logical units of its parallelism model. There are close links between Kafka Streams and Kafka in the context of parallelism:</p>
<p>Kafka Streams使用分区和任务的概念作为它的并行模型的逻辑单元。Kafka Streams和Kafka在并行这个上下文上有紧密的联系：  </p>
<ul>
<li>Each stream partition is a totally ordered sequence of data records and maps to a Kafka topic partition.</li>
<li>A data record in the stream maps to a Kafka message from that topic.</li>
<li>The keys of data records determine the partitioning of data in both Kafka and Kafka Streams, i.e., how data is routed to specific partitions within topics.</li>
</ul>
<ol>
<li>每个分区流完全是一个有序的数据记录序列，映射到Kafka的主题分区</li>
<li>流中的一条数据记录，对应了Kafka主题中的一条消息</li>
<li>数据记录的Key决定了它在Kafka和Kafka Streams中的分区方式，比如数据怎么路由到主题的指定分区</li>
</ol>
<p>An application’s processor topology is scaled by breaking it into multiple tasks. More specifically, Kafka Streams creates a fixed number of tasks based on the input stream partitions for the application, with each task assigned a list of partitions from the input streams (i.e., Kafka topics). The assignment of partitions to tasks never changes so that each task is a fixed unit of parallelism of the application. Tasks can then instantiate their own processor topology based on the assigned partitions; they also maintain a buffer for each of its assigned partitions and process messages one-at-a-time from these record buffers. As a result stream tasks can be processed independently and in parallel without manual intervention.</p>
<blockquote>
<p>应用程序的处理拓扑会被分成多个任务来进行扩展。更具体来说，Kafka Streams会基于输入流的分区创建固定数量的任务，每个任务会从输入流（Kafka的主题）分配到多个分区。每个任务分配的分区永远不会改变，这样每个任务作为应用程序固定的并行单元。任务可以基于分配给它们的分区实例化它们自己的处理拓扑；它们也会为每个分配的分区维护一个缓冲区，并且从这些记录的缓冲区中一次只处理一条消息。这样的好处是所有的流任务都各自独立地并行处理，并不需要人工干预。</p>
</blockquote>
<p>Sub-topologies aka topology sub-graphs: If there are multiple processor topologies specified in a Kafka Streams application, each task will only instantiate one of the topologies for processing. In addition, a single processor topology may be decomposed(分离分解) into independent sub-topologies (sub-graphs) as long as sub-topologies are not connected by any streams in the topology; here, each task may instantiate only one such sub-topology for processing. This further scales out the computational workload to multiple tasks.</p>
<blockquote>
<p>子拓扑或者叫拓扑子图：如果在Kafka Streams应用程序中指定了多个处理拓扑，每个任务只会实例化其中的一个拓扑并处理。另外，一个拓扑也可能分成多个独立的子拓扑，只要子拓扑不和拓扑中的任何流存在连接。这里每个任务可能只会实例化一个子拓扑并处理。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161103181154902" alt="kstream task"></p>
<p>It is important to understand that Kafka Streams is not a resource manager, but a library that “runs” anywhere its stream processing application runs. Multiple instances of the application are executed either on the same machine, or spread across multiple machines and tasks can be distributed automatically by the library to those running application instances. The assignment of partitions to tasks never changes; if an application instance fails, all its assigned tasks will be restarted on other instances and continue to consume from the same stream partitions.</p>
<blockquote>
<p>注意Kafka Streams不是一个资源管理器，而是一个可以在和流处理应用程序一起运行在任何地方的客户端库。你可以在一台机器上运行应用程序的多个实例；或者分散在多台机器上，任务就会自动分布式地运行这些应用程序实例。注意分配给任务的分区永远不会改变（和Kafka消费者有点不同，消费者分配的分区是可以改变的）；如果一个应用程序的实例失败了，它的所有任务会在其他实例上重新启动，并且从相同的流分区继续消费。总结下：一个流处理应用程序实例（进程）有多个Task，每个Task分配多个固定的分区，如果进程挂了，其上的所有Task都会在其他进程上执行。而不会说把分区重新分配给剩下的Task。由于Task的分区固定，实际上Task的数量也是固定的，Task会分布式地在多个进程上执行。</p>
</blockquote>
<h3 id="Threading_Model">Threading Model</h3><p>Kafka Streams allows the user to configure the number of threads that the library can use to parallelize processing within an application instance. Each thread can execute one or more tasks with their processor topologies independently.</p>
<blockquote>
<p>Kafka Streams允许用户配置线程的数量，这样Kafka Streams库可以用来决定在一个应用程序实例中的处理并行粒度。每个线程可以执行一个或多个任务，它们的拓扑也都是独立的。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161103181203678" alt="kstream thread"></p>
<p>Starting more stream threads or more instances of the application merely amounts to replicating the topology and having it process a different subset of Kafka partitions, effectively parallelizing processing. It is worth noting that there is no shared state amongst the threads, so no inter-thread coordination is necessary. This makes it very simple to run topologies in parallel across the application instances and threads. The assignment of Kafka topic partitions amongst the various stream threads is transparently handled by Kafka Streams leveraging Kafka’s server-side coordination functionality.</p>
<blockquote>
<p>开启更多的流线程或者更多的应用程序实例仅仅相当于复制拓扑，并且处理不同的Kafka分区子集，有效地并行化处理。注意线程之间不会共享状态，所以不需要内部的线程进行协调。这使得在多个应用程序或者线程之间并行地运行拓扑变得非常简单。不同线程分配到的Kafka主题分区会被Kafka Streams透明地处理，利用的是Kafka服务端的协调者特性。</p>
</blockquote>
<p>As we described above, scaling your stream processing application with Kafka Streams is easy: you merely need to start additional instances of your application, and Kafka Streams takes care of distributing partitions amongst tasks that run in the application instances. You can start as many threads of the application as there are input Kafka topic partitions so that, across all running instances of an application, every thread (or rather, the tasks it runs) has at least one input partition to process.</p>
<blockquote>
<p>正如上面所描述的，使用Kafka Streams扩展你的流处理应用程序非常简单：你只需要为你的应用程序启动额外的实例，然后Kafka Streams就会自动帮你将分区分布在任务之间，任务会运行在应用程序实例中。你可以启动和Kafka的输入主题分区相同数量的应用程序线程，这样在一个应用程序的所有运行实例中，每个线程（更精确地说，是运行的任务）至少都会处理一个输入分区。</p>
</blockquote>
<h3 id="Example">Example</h3><p>To understand the parallelism model that Kafka Streams offers, let’s walk through an example.</p>
<p>Imagine a Kafka Streams application that consumes from two topics, A and B, with each having 3 partitions. If we now start the application on a single machine with the number of threads configured to 2, we end up with two stream threads instance1-thread1 and instance1-thread2. Kafka Streams will break this topology by default into three tasks because the maximum number of partitions across the input topics A and B is max(3, 3) == 3, and then distribute the six input topic partitions evenly across these three tasks; in this case, each task will consume from one partition of each input topic, for a total of two input partitions per task. Finally, these three tasks will be spread evenly – to the extent this is possible – across the two available threads, which in this example means that the first thread will run 2 tasks (consuming from 4 partitions) and the second thread will run 1 task (consuming from 2 partitions).</p>
<p>为了理解Kafka Streams提供的并行度模型，我们来看一个示例。假设有一个Kafka Streams应用程序会消费两个主题：A和B，每个主题都有3个分区。如果我们在一台机器上启动了一个应用程序，配置的线程数量为2，最终我们会有两个流线程：instance1-thread1和instance1-thread2。Kafka Streams会默认将拓扑分成三个任务，因为所有输入主题A和B的最大分区数是max(3,3)=3，然后会将6个输入分区平均分配到这三个任务上。这种情况下，每个任务都会消费每个输入主题的一个分区，即每个任务分配到了总共两个分区。最后，这三个任务会被均匀地分散到两个可用的线程中，这里因为有两个线程，这就意味着第一个线程会运行两个任务（消费了4个分区），第二个线程会运行一个任务（消费了2个分区）。</p>
<p>Now imagine we want to scale out this application later on, perhaps because the data volume has increased significantly. We decide to start running the same application but with only a single thread on another, different machine. A new thread instance2-thread1 will be created, and input partitions will be re-assigned similar to:</p>
<p>现在假设我们要扩展应用程序，可能是因为数据量增长的很明显。我们决定在其他机器上运行相同的应用程序，不过只配置了一个线程。那么一个新的线程instance2-thread1就会被创建，输入分区会被重新分配成下面右图那样。</p>
<p><img src="http://img.blog.csdn.net/20161103181630440" alt="kstream example"></p>
<p>When the re-assignment occurs, some partitions – and hence their corresponding tasks including any local state stores – will be “migrated” from the existing threads to the newly added threads (here: from instance1-thread1 on the first machine to instance2-thread1 on the second machine). As a result, Kafka Streams has effectively rebalanced the workload among instances of the application at the granularity of Kafka topic partitions.</p>
<blockquote>
<p>当重新分配发生时，一些分区，以及它们对应的任务，包括本地存储的状态，都会从已有的线程迁移到新添加的线程。比如这里第一台机器的instance1-thread1线程会迁移到第二台机器的instance2-thread1线程。最终，Kafka Streamsh会在所有应用程序实例中有效地平衡负载，而且是以Kafka主题分区的粒度进行负载均衡。</p>
</blockquote>
<p>What if we wanted to add even more instances of the same application? We can do so until a certain point, which is when the number of running instances is equal to the number of available input partitions to read from. At this point, before it would make sense to start further application instances, we would first need to increase the number of partitions for topics A and B; otherwise, we would over-provision the application, ending up with idle instances that are waiting for partitions to be assigned to them, which may never happen.</p>
<p>如果想要添加相同应用程序的更多实例呢？我们可以像上面那样做，直到运行实例的数量等于读取的可用分区数量（所有主题）。在这之后，如果想要启动更多的应用程序实例变得有意义，我们需要先为主题A和B增加分区；否则会存在空闲的应用程序实例，它们会等待有可用的分区分配给它们，但这<strong>可能</strong>永远都不会发生（虽然应用程序的实例比分区多，导致有些应用程序实例是空闲的，但是如果有应用程序挂掉了，那些空闲的应用程序就有可能分配到分区，而不再空闲。就像Kafka的消费者一样，如果消费者数量比分区数要多，空闲的消费者也会得不到分区，但如果有消费者挂掉了，空闲的消费者也是有机会得到分区的。不过我们无法保证空闲的应用程序实例或者消费者就一定有机会得到分区）。</p>
<h2 id="状态">状态</h2><p>Kafka Streams provides so-called state stores, which can be used by stream processing applications to store and query data, which is an important capability when implementing stateful operations. Every task in Kafka Streams embeds one or more state stores that can be accessed via APIs to store and query data required for processing. These state stores can either be a RocksDB database, an in-memory hash map, or another convenient data structure. Kafka Streams offers fault-tolerance and automatic recovery for local state stores.</p>
<p>Kafka Streams提供了所谓的状态存储，可以被流处理应用程序用来存储和查询数据，这在实现有状态的操作时是一个非常重要的功能。Kafka Streams中的每个任务内置了一个或多个状态存储，并且可以在流处理时通过API的方式存储或者查询状态存储中的数据。这些状态存储可以是RocksDB数据库、内存的hash map、或者其他的数据结构。Kafka Streams为本地状态存储提供了容错和自动恢复机制。</p>
<p><img src="http://img.blog.csdn.net/20161104091711617" alt="kstream state"></p>
<h2 id="容错">容错</h2><p>Kafka Streams builds on fault-tolerance capabilities integrated natively within Kafka. Kafka partitions are highly available and replicated; so when stream data is persisted to Kafka it is available even if the application fails and needs to re-process it. Tasks in Kafka Streams leverage the fault-tolerance capability offered by the Kafka consumer client to handle failures. If a task runs on a machine that fails, Kafka Streams automatically restarts the task in one of the remaining running instances of the application.</p>
<blockquote>
<p>Kafka Streams的容错能力基于原生的Kafka，Kafka的分区是高可用和复制的；所以当流数据持久化到Kafka中，即使应用程序失败了或者需要重新处理，数据也还是可用的。Kafka Streams的任务利用了Kafka消费者客户端提供的容错机制来处理故障。如果运行在一台机器上的一个任务失败了，Kafka Streams会在剩余的应用程序实例选择一个自动重启任务。</p>
</blockquote>
<p>In addition, Kafka Streams makes sure that the local state stores are robust to failures, too. It follows a similar approach as Apache Samza and, for each state store, maintains a replicated changelog Kafka topic in which it tracks any state updates. These changelog topics are partitioned as well so that each local state store instance, and hence the task accessing the store, has its own dedicated changelog topic partition. Log compaction is enabled on the changelog topics so that old data can be purged safely to prevent the topics from growing indefinitely. If tasks run on a machine that fails and are restarted on another machine, Kafka Streams guarantees to restore their associated state stores to the content before the failure by replaying the corresponding changelog topics prior to resuming the processing on the newly started tasks. As a result, failure handling is completely transparent to the end user.</p>
<blockquote>
<p>另外，Kafka Streams确保了本地状态的存储对于故障是鲁棒性的。它采用了和Apache Samza类似的方法，每个状态存储，都维护了具有复制的变更日志（Kafka主题），变更日志（changelog）会跟踪每次状态的更新。这些变更日志主题（change topic）会进行分区，每个本地状态存储的实例（local state store instance），都可以被任务获取，任务都有自己专属的变更日志分区（changelog topic partition）。在变更日志主题上会开启日志压缩，来安全地删除旧数据，防止旧数据无限膨胀。如果在一台机器上的任务运行失败，会在其他机器上重新启动，Kafka Streams可以保证恢复故障发生之前相关的状态存储。这是通过在新启动的任务上恢复处理之前，重放对应的变更日志主题来做到的。最终，故障处理对终端用户而言是透明的。</p>
</blockquote>
<p>Optimization: In order to minimize the time for the state restoration and hence the cost of task (re)initialization, users can configure their applications to have shadow copies of local states. When a task migration happens, Kafka Streams then attempts to assign a task to where a standby replica exists in order to minimize the task initialization cost. See setting num.standby.replicas at Optional configuration parameters in the Developer Guide.</p>
<blockquote>
<p>优化点：为了最小化恢复状态的时间以及任务重新初始化的代价，用户可以为应用程序配置一个本地状态的shadow副本。当一个任务迁移发生时，Kafka Streams会尝试将任务分配到备用副本所在的节点，以尽可能最小化任务初始化的代价。</p>
</blockquote>
<h2 id="流处理的保证">流处理的保证</h2><p>Kafka Streams currently supports at-least-once processing guarantees in the presence of failure. This means that if your stream processing application fails, no data records are lost and fail to be processed, but some data records may be re-read and therefore re-processed.</p>
<p>It depends on the specific use case whether at-least-once processing guarantees are acceptable or whether you may need exactly-once processing.</p>
<blockquote>
<p>Kafka Streams目前支持在错误场景下至少一次的处理语义。这意味着如果你的流处理应用程序失败了，数据不会丢失，也不会被漏掉处理，但是有些数据可能会被重复读取，并被重复处理。根据不同的用例，用户自己决定是否可以接受至少处理一次的保证，还是需要正好一次的处理。</p>
</blockquote>
<p>For many processing use cases, at-least-once processing turns out to be perfectly acceptable: Generally, as long as the effect of processing a data record is idempotent, it is safe for the same data record to be processed more than once. Also, some use cases can tolerate processing data records more than once even if the processing is not idempotent. For example, imagine you are counting hits by IP address to auto-generate blacklists that help with mitigating DDoS attacks against your infrastructure; here, some overcounting is tolerable because hits from malicious IP addresses involved(涉及) in an attack(攻击) will vastly(极大地) outnumber hits from benign(良性的) IP addresses anyway.</p>
<blockquote>
<p>对于很多处理场景，至少一次的处理被证明是可接受的：通常而言，只要处理一条记录的影响是幂等的，那么多次处理同一条记录就是安全的。同时，有些用例也允许容忍多次处理，即使处理的影响不是幂等的。比如，想象下你要根据IP地址计算命中次数，来生成帮你你与DDOS攻击的黑名单；这里，（重复处理导致）过高的计数也是允许的，因为来自恶意IP地址的计数参与的攻击相比良性的IP地址数量上会更多。</p>
</blockquote>
<p>In general however, for non-idempotent operations such as counting, at-least-once processing guarantees may yield incorrect results. If a Kafka Streams application fails and restarts, it may double-count some data records that were processed shortly before the failure. We are planning to address this limitation and will support stronger guarantees and exactly-once processing semantics in a future release of Kafka Streams.</p>
<blockquote>
<p>不过非幂等操作比如计数，在至少一次的处理语义下有可能得到错误的结果。如果流应用程序失败或重启，那么在错误发生前一小段时间内，相同的记录可能会被重复计数。我们正在考虑解决这种限制，并且尝试支持更强的消息处理保证。</p>
</blockquote>
<h2 id="流控">流控</h2><p>Kafka Streams regulates(控制) the progress of streams by the timestamps of data records by attempting to synchronize all source streams in terms of time. By default, Kafka Streams will provide your application with event-time processing semantics. This is important especially when an application is processing multiple streams (i.e., Kafka topics) with a large amount of historical data. For example, a user may want to re-process past data in case the business logic of an application was changed significantly, e.g. to fix a bug in an analytics algorithm. Now it is easy to retrieve a large amount of past data from Kafka; however, without proper flow control, the processing of the data across topic partitions may become out-of-sync and produce incorrect results.</p>
<blockquote>
<p>Kafka Streams通过数据记录的时间撮控制流的进度，它会尝试根据时间来同步所有数据源产生的流。默认Kafka Streams为应用程序提供事件时间的处理语义。对于应用程序处理多个具有大量历史数据的流这种场景是特别重要的。举例应用程序的业务逻辑变化很显著时，用户可能想要重新处理过去的数据，比如在一个分析型的算法中修复一个错误。现在，我们可以很容易地从Kafka中接收大量的历史数据，不过如果没有做恰当的流控，在Kafka主题分区之间的数据处理可能变得不同步，并且产生错误的结果。</p>
</blockquote>
<p>As mentioned in the Concepts section, each data record in Kafka Streams is associated with a timestamp. Based on the timestamps of the records in its stream record buffer, stream tasks determine the next assigned partition to process among all its input streams. However, Kafka Streams does not reorder records within a single stream for processing since reordering would break the delivery semantics of Kafka and make it difficult to recover in the face of failure. This flow control is of course best-effort(尽最大努力) because it is not always possible to strictly enforce execution order across streams by record timestamp; in fact, in order to enforce strict execution ordering, one must either wait until the system has received all the records from all streams (which may be quite infeasible in practice) or inject additional information about timestamp boundaries or heuristic estimates(启发式的预估) such as MillWheel’s watermarks.</p>
<blockquote>
<p>Kafka Streams中的每条数据记录都关联了一个时间撮。基于<strong>流记录缓冲区</strong>（stream record buffer）中每条记录的时间撮，流任务会在所有输入流中决定下一个需要处理的分配分区。不过Kafka Streams不会在处理一个单一的流时对记录重新排序，因为重新排序会破坏Kafka的消息传递语义，并且在故障发生时不容易恢复（消息的顺序）。这种流控当然会尽最大努力，因为在流中并不可能总是按照记录的时间撮严格限制执行的顺序。实际上，如果要实现严格的执行顺序，一个流要么需要等待，直到系统（流处理应用程序）从所有的流中接收到所有的记录（这在实际中是不可实行的），或者注入关于时间边界的额外信息，或者使用类似MillWheel的水位概念做一些启发式的预估。</p>
</blockquote>
<h2 id="背压">背压</h2><p>Kafka Streams does not use a backpressure mechanism because it does not need one. Using a depth-first processing strategy, each record consumed from Kafka will go through the whole processor (sub-)topology for processing and for (possibly) being written back to Kafka before the next record will be processed. As a result, no records are being buffered in-memory between two connected stream processors. Also, Kafka Streams leverages Kafka’s consumer client behind the scenes, which works with a pull-based messaging model that allows downstream processors to control the pace(步伐) at which incoming data records are being read.</p>
<blockquote>
<p>Kafka Streams不适用背压机制，因为它并不需要。使用深入优先的处理策略，从Kafka中消费的每条记录在处理时会流经整个处理拓扑，并且有可能会在处理下一条记录之前回写到Kafka。结果就是：在两个链接的流处理算子中不会有记录被缓存在内存中。同时Kafka Streams利用了Kafka中基于消息拉取模型的消费者客户端，允许下游处理算子控制读取的输入数据的消费速度。</p>
</blockquote>
<p>The same applies to the case of a processor topology that contains multiple independent sub-topologies, which will be processed independently from each other (cf. Parallelism Model). For example, the following code defines a topology with two independent sub-topologies:</p>
<blockquote>
<p>同样的方式也运用在包含多个独立子拓扑的处理拓扑，每个子拓扑都会各自独立地处理，比如下面的代码定义了一个拓扑，具有两个独立的子拓扑：</p>
</blockquote>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">strea<span class="title">m1.</span>to<span class="comment">("my-topic")</span>;</span><br><span class="line">strea<span class="title">m2</span> = builder.stream<span class="comment">("my-topic")</span>;</span><br></pre></td></tr></table></figure>
<p>Any data exchange between sub-topologies will happen through Kafka, i.e. there is no direct data exchange (in the example above, data would be exchanged through the topic “my-topic”). For this reason there is no need for a backpressure mechanism in this scenario, too.</p>
<p>在子拓扑中的任何数据交换都会经过Kafka，在上面的示例中，并没有直接的数据交换，而是通过”my-topic”进行数据交换。基于这些原因，这种场景下也不需要一个背压机制。</p>
<h1 id="开发者指南">开发者指南</h1><h2 id="Kafka_Streams配置">Kafka Streams配置</h2><p>Kafka Streams的配置通过一个StreamsConfig实例完成。其中下面三个是必须要有的配置项：</p>
<ol>
<li>application.id：流处理应用程序的编号，在Kafka集群中必须是唯一的</li>
<li>bootstrap.servers：建立和Kafka集群的初始连接</li>
<li>zookeeper.connect：管理Kafka主题的ZooKeeper</li>
</ol>
<p>每个流处理应用程序的编号必须是唯一的，相同的应用程序编号会给应用程序所有的实例，编号作为资源隔离的标识，用在下面几个地方</p>
<ol>
<li>作为默认的Kafka生产者、消费者的client.id前缀</li>
<li>作为Kafka消费者的group.id，会用来协调工作</li>
<li>作为状态目录（state.dir）的子目录名称</li>
<li>作为内部Kafka主题名称的前缀</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"></span><br><span class="line">Properties settings = <span class="keyword">new</span> Properties();</span><br><span class="line"><span class="comment">// Set a few key parameters</span></span><br><span class="line">settings.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"my-first-streams-application"</span>);</span><br><span class="line">settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"kafka-broker1:9092"</span>);</span><br><span class="line">settings.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, <span class="string">"zookeeper1:2181"</span>);</span><br><span class="line"><span class="comment">// Any further settings</span></span><br><span class="line">settings.put(... , ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an instance of StreamsConfig from the Properties instance</span></span><br><span class="line">StreamsConfig config = <span class="keyword">new</span> StreamsConfig(settings);</span><br></pre></td></tr></table></figure>
<p><strong>Ser-/Deserialization (key.serde, value.serde)</strong>: Serialization and deserialization in Kafka Streams happens whenever data needs to be materialized, i.e.,:</p>
<ul>
<li>Whenever data is read from or written to a Kafka topic (e.g., via the KStreamBuilder#stream() and KStream#to() methods).</li>
<li>Whenever data is read from or written to a state store.</li>
</ul>
<p>当数据需要物化时，在Kafka Streams中会发生序列化和反序列化：  </p>
<ol>
<li>当从Kafka主题中读取数据，或者写入数据到Kafka主题（比如通过KStreamBuilder.stream()或者KStream.to()方法）</li>
<li>当从状态存储中读取数据，或者写入数据到状态存储</li>
</ol>
<p><strong>Number of Standby Replicas (num.standby.replicas)</strong>: This specifies the number of standby replicas. Standby replicas are shadow copies of local state stores. Kafka Streams attempts to create the specified number of replicas and keep them up to date as long as there are enough instances running. Standby replicas are used to minimize the latency of task failover. A task that was previously running on a failed instance is preferred(优先) to restart on an instance that has standby replicas so that the local state store restoration process from its changelog can be minimized. Details about how Kafka Streams makes use of the standby replicas to minimize the cost of resuming tasks on failover can be found in the State section.</p>
<p>指定备用副本的数量，备用副本是本地状态存储的shadow复制。Kafka Streams会尝试创建指定数量的副本，并且使这些副本一直保持最新的状态，只要有足够的实例在运行的话。备用副本会在任务发生故障切换时最小化延迟。运行在失败实例上的任务会优先在含有备用副本的实例上重启任务，这样可以最小化从变更日志中恢复本地的状态存储。</p>
<p><strong>Number of Stream Threads (num.stream.threads)</strong>: This specifies the number of stream threads in an instance of the Kafka Streams application. The stream processing code runs in these threads. Details about Kafka Streams threading model can be found in section Threading Model.</p>
<p>指定一个Kafka Streams应用程序实例的流线程数量。流处理代码运行在这些线程上。</p>
<p><strong>Replication Factor of Internal Topics (replication.factor)</strong>: This specifies the replication factor of internal topics that Kafka Streams creates when local states are used or a stream is repartitioned for aggregation. Replication is important for fault tolerance. Without replication even a single broker failure may prevent progress of the stream processing application. It is recommended to use a similar replication factor as source topics.</p>
<p>指定内部主题的副本因子，在使用本地状态或者流在聚合需要重新分区时，Kafka Streams会创建内部主题。副本对于故障容错非常重要。如果没有副本机制，即使一个Broker挂掉后，也会阻止流处理应用程序的正常进行。推荐设置为和源主题相同的副本因子。</p>
<p><strong>State Directory (state.dir)</strong>: Kafka Streams persists local states under the state directory. Each application has a subdirectory on its hosting machine, whose name is the application id, directly under the state directory. The state stores associated with the application are created under this subdirectory.</p>
<p>Kafka Streams会在状态目录下持久化本地状态。每个应用程序在它的物理机的状态目录下都有一个子目录，名称是应用程序的编号。和应用程序关联的状态存储都会在这个子目录下创建。</p>
<p><strong>Timestamp Extractor (timestamp.extractor)</strong>: A timestamp extractor extracts a timestamp from an instance of ConsumerRecord. Timestamps are used to control the progress of streams.</p>
<p>The default extractor is ConsumerRecordTimestampExtractor. This extractor retrieves built-in timestamps that are automatically embedded into Kafka messages by the Kafka producer client (introduced in Kafka 0.10.0.0, see KIP-32: Add timestamps to Kafka message). Depending on the setting of Kafka’s log.message.timestamp.type parameter, this extractor will provide you with:</p>
<ul>
<li>event-time processing semantics if log.message.timestamp.type is set to CreateTime aka “producer time” (which is the default). This represents the time when the Kafka producer sent the original message.</li>
<li>ingestion-time processing semantics if log.message.timestamp.type is set to LogAppendTime aka “broker time”. This represents the time when the Kafka broker received the original message.</li>
</ul>
<p>从一个ConsumerRecord实例解析出时间撮的解析器，时间撮会用来控制流的进度。默认的时间撮解析器是<code>ConsumerRecordTimestampExtractor</code>，这个解析器会获取被自动嵌入到Kafka消息中的内置时间撮（KIP-32：生产者产生消息时，会嵌入一个时间段到消息中）。根据<code>log.message.timestamp.type</code>的设置，有两种类型的解析器：  </p>
<ol>
<li>设置类型为<code>CreateTime</code>，即事件时间的处理语义。也是Producer的时间，作为默认值。表示Kafka生产者发送原始消息的时间点</li>
<li>设置类型为<code>LogAppendTime</code>，即摄取时间的处理语义。也是Broker的时间。表示Kafka Broker接收原始消息的时间点</li>
</ol>
<p>Another built-in extractor is WallclockTimestampExtractor. This extractor does not actually “extract” a timestamp from the consumed record but rather returns the current time in milliseconds from the system clock, which effectively means Streams will operate on the basis(基础) of the so-called processing-time of events.</p>
<p>You can also provide your own timestamp extractors, for instance to retrieve timestamps embedded in the payload of messages. Here is an example of a custom TimestampExtractor implementation:</p>
<p>另一个内置的解析器是<code>WallclockTimestampExtractor</code>，这个解析器并不会从消费记录中解析出一个时间撮，而是返回当前的系统时钟。你也可以提供自定义的时间撮解析器，比如从消息的内容（payload）中获取时间撮（通常是数据源自带的时间，而不是摄取时间），下面是一个自定义的TimestampExtractor实现类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.TimestampExtractor;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Extracts the embedded timestamp of a record (giving you "event time" semantics).</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyEventTimeExtractor</span> <span class="keyword">implements</span> <span class="title">TimestampExtractor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extract</span><span class="params">(ConsumerRecord&lt;Object, Object&gt; record)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// `Foo` is your own custom class, which we assume has a method that returns</span></span><br><span class="line">    <span class="comment">// the embedded timestamp (in milliseconds).</span></span><br><span class="line">    Foo myPojo = (Foo) record.value();</span><br><span class="line">    <span class="keyword">if</span> (myPojo != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> myPojo.getTimestampInMillis();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Kafka allows `null` as message value.  How to handle such message values</span></span><br><span class="line">      <span class="comment">// depends on your use case.  In this example, we decide to fallback to</span></span><br><span class="line">      <span class="comment">// wall-clock time (= processing-time).</span></span><br><span class="line">      <span class="keyword">return</span> System.currentTimeMillis();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You would then define the custom timestamp extractor in your Streams configuration as follows:<br>然后你需要在Streams配置中指定自定义的时间撮解析器（就像自定义序列化和反序列化器一样，都需要在配置文件中明确指定）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"></span><br><span class="line">Properties settings = <span class="keyword">new</span> Properties();</span><br><span class="line">settings.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MyEventTimeExtractor.class.getName());</span><br></pre></td></tr></table></figure>
<p><strong>Partition Grouper (partition.grouper)</strong>: A partition grouper is used to create a list of stream tasks given the partitions of source topics, where each created task is assigned with a group of source topic partitions. The default implementation provided by Kafka Streams is DefaultPartitionGrouper, which assigns each task with at most one partition for each of the source topic partitions; therefore, the generated number of tasks is equal to the largest number of partitions among the input topics. Usually an application does not need to customize the partition grouper.</p>
<p>分区分组用来在给定源主题的分区下创建流任务列表，每个创建的任务都会分配一组源主题的分区。默认的实现是<code>DefaultPartitionGrouper</code>，每个任务<strong>至多</strong>分配到每个源主题分区的一个分区。因此生成的任务数量等于输入主题中的最大分区数量（假设主题A有3个分区，主题B有4个分区，任务数量就等于max(3,4)=4）。通常应用程序不需要自定义分区分组方式。</p>
<p>Apart from Kafka Streams’ own configuration parameters (see previous sections) you can also specify parameters for the Kafka consumers and producers that are used internally, depending on the needs of your application. Similar to the Streams settings you define any such consumer and/or producer settings via StreamsConfig:</p>
<p>除了Kafka Streams自己的配置，你也可以根据你自己应用程序的需求设置内部的Kafka消费者和生产者的配置。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Properties streamsSettings = <span class="keyword">new</span> Properties();</span><br><span class="line"><span class="comment">// Example of a "normal" setting for Kafka Streams</span></span><br><span class="line">streamsSettings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"kafka-broker-01:9092"</span>);</span><br><span class="line"><span class="comment">// Customize the Kafka consumer settings of your Streams application</span></span><br><span class="line">streamsSettings.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, <span class="number">60000</span>);</span><br><span class="line">StreamsConfig config = <span class="keyword">new</span> StreamsConfig(streamsSettings);</span><br></pre></td></tr></table></figure>
<h2 id="编写一个流处理应用程序">编写一个流处理应用程序</h2><p>Any Java application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).</p>
<p>Currently Kafka Streams provides two sets of APIs to define the processor topology:</p>
<ul>
<li>A low-level Processor API that lets you add and connect processors as well as interact directly with state stores.</li>
<li>A high-level Kafka Streams DSL that provides common data transformation operations in a functional programming style such as map and filter operations. The DSL is the recommended starting point for developers new to Kafka Streams, and should cover many use cases and stream processing needs.</li>
</ul>
<p>任何使用Kafka Streams客户端库的Java应用程序被认为是一个Kafka Streams应用程序。Kafka Streams应用程序的计算逻辑被定义为一个处理拓扑，它是流处理算子和流的一张图。目前支持两种API定义处理拓扑：  </p>
<ol>
<li>低级Processor API：允许你添加和连接Processor，以及和状态存储直接交互</li>
<li>高级DSL：以函数式变成的风格提供通用的数据转换算子</li>
</ol>
<p>首先需要在pom.xml中定义Kafka Streams的依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-streams<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.10.0.1<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.10.0.1<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Dependencies below are required/recommended only when using Apache Avro. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>io.confluent<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-avro-serializer<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>3.0.1<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.avro<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>avro<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.avro<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>avro-maven-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>你可以在应用程序代码的任何地方调用Kafka Streams库，通常是在main方法中。首先你需要创建一个KafkaStreams实例。KafkaStream构造器的第一个参数接收一个用来定义拓扑的builder（Kafka STreams DSL使用KStreamBuilder，Processor API使用TopologyBuilder）；第二个参数是StreamsConfig实例，定义了这个指定拓扑的配置。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.KafkaStreams;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KStreamBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.TopologyBuilder;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the builders to define the actual processing topology, e.g. to specify</span></span><br><span class="line"><span class="comment">// from which input topics to read, which stream operations (filter, map, etc.)</span></span><br><span class="line"><span class="comment">// should be called, and so on.</span></span><br><span class="line"></span><br><span class="line">KStreamBuilder builder = ...;  <span class="comment">// when using the Kafka Streams DSL</span></span><br><span class="line"><span class="comment">// OR</span></span><br><span class="line">TopologyBuilder builder = ...; <span class="comment">// when using the Processor API</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the configuration to tell your application where the Kafka cluster is,</span></span><br><span class="line"><span class="comment">// which serializers/deserializers to use by default, to specify security settings,</span></span><br><span class="line"><span class="comment">// and so on.</span></span><br><span class="line">StreamsConfig config = ...;</span><br><span class="line"></span><br><span class="line">KafkaStreams streams = <span class="keyword">new</span> KafkaStreams(builder, config);</span><br></pre></td></tr></table></figure>
<p>现在内部结果已经初始化完毕，不过处理工作还没有开始。你需要手动调用start()方法显示地启动Kafka Streams的线程：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Start the Kafka Streams threads</span></span><br><span class="line">streams.start();</span><br></pre></td></tr></table></figure>
<p>如果这个流处理应用程序还有其他实例运行在其他地方，Kafka Streams针对用户会透明地将任务从已经存在的实例<strong>重新分配</strong>到你刚刚启动的新实例上。为了捕获一些非预期的异常，你需要在启动应用程序之前设置一个<code>UncaughtExceptionHandler</code>异常处理器，这个处理器会在无论什么时候流处理线程因为非预期的异常而终结的时候被调用。</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">streams<span class="built_in">.</span>setUncaughtExceptionHandler(<span class="literal">new</span> <span class="keyword">Thread</span><span class="built_in">.</span>UncaughtExceptionHandler() &#123;</span><br><span class="line">    <span class="keyword">public</span> uncaughtException(<span class="keyword">Thread</span> t, throwable e) &#123;</span><br><span class="line">        <span class="comment">// here you should examine the exception and perform an appropriate action!</span></span><br><span class="line">    &#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>停止应用程序实例的方式是调用KafkaStreams的close()方法</p>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Stop the Kafka Streams threads</span></span><br><span class="line">streams.close<span class="comment">()</span>;</span><br></pre></td></tr></table></figure>
<p>为了保证你的应用程序在响应SIGTERM时优雅地退出，推荐在关闭钩子中调用KafkaStreams.close()方法，Java8中的关闭钩子使用方式如下：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add shutdown hook to stop the Kafka Streams threads</span></span><br><span class="line">Runtime<span class="built_in">.</span>getRuntime()<span class="built_in">.</span>addShutdownHook(<span class="literal">new</span> <span class="keyword">Thread</span>(streams<span class="tag">::close</span>));</span><br></pre></td></tr></table></figure>
<p>Java7中的而关闭钩子使用方式如下：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add shutdown hook</span></span><br><span class="line">Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="comment">// Stop the Kafka Streams threads</span></span><br><span class="line">      streams.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;));</span><br></pre></td></tr></table></figure>
<p>当一个特定的应用程序实例停止时，Kafka Streams会将运行在这个实例上的任何任务迁移到其他运行的实例上（假设还存在这样的应用程序实例）。下面我们会详细描述两种API的使用方式。</p>
<h3 id="Processor_API">Processor API</h3><p>As mentioned in the Concepts section, a stream processor is a node in the processor topology that represents a single processing step. With the Processor API users can define arbitrary stream processors that processes one received record at a time, and connect these processors with their associated state stores to compose the processor topology.</p>
<p>流处理算子是处理拓扑的一个节点，代表了一个单独的处理步骤。使用Processor API，用户可以定义任意的流处理算子，一次处理一条接收到的记录，并且将这些算子和它们对应的状态存储连接在一起，组成算子拓扑。用户可以实现Processor接口来定义定制的流算子，Processor接口有两个主要的方法：</p>
<p><strong>DEFINING A STREAM PROCESSOR</strong>  </p>
<p>Users can define their customized stream processor by implementing the Processor interface, which provides two main API methods: process() and punctuate().</p>
<ul>
<li>process() is called on each of the received record.</li>
<li>punctuate() is called periodically based on advanced record timestamps. For example, if processing-time is used as the record timestamp, then punctuate() will be triggered every specified period of time.</li>
</ul>
<ol>
<li>process()会在每个接收到的记录上调用</li>
<li>punctuate()会基于记录的时间撮被定时调用</li>
</ol>
<p>The Processor interface also has an init() method, which is called by the Kafka Streams library during task construction phase. Processor instances should perform any required initialization in this method. The init() method passes in a ProcessorContext instance, which provides access to the metadata of the currently processed record, including its source Kafka topic and partition, its corresponding message offset, and further such information . This context instance can also be used to schedule the punctuation period (via ProcessorContext#schedule()) for punctuate(), to forward a new record as a key-value pair to the downstream processors (via ProcessorContext#forward()), and to commit the current processing progress (via ProcessorContext#commit()).</p>
<p>Processor接口也有一个init()方法，它会在Kafka Streams库初始化任务的阶段调用。Processor实例需要在该方法上执行一些必须的初始化工作。init()方法传递了一个ProcessorContext实例，为当前处理的记录提供元数据的访问接口，包括输入源的Kafka主题和分区，对应的消息偏移量，以及更多的一些信息。这个上下文对象也可以被用来调度punctuation()方法的间隔（通过schedule方法），将新的记录作为键值对转发到下游的处理算子（通过forward方法），或者提交当前的处理进度（通过commit方法）。</p>
<p>下面的Processor实现类定义了一个简单的WordCount算法：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountProcessor</span> <span class="keyword">implements</span> <span class="title">Processor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> ProcessorContext context;</span><br><span class="line">  <span class="keyword">private</span> KeyValueStore&lt;String, Long&gt; kvStore;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessorContext context)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// keep the processor context locally because we need it in punctuate() and commit()</span></span><br><span class="line">      <span class="keyword">this</span>.context = context;</span><br><span class="line">      <span class="comment">// call this processor's punctuate() method every 1000 time units.</span></span><br><span class="line">      <span class="keyword">this</span>.context.schedule(<span class="number">1000</span>);</span><br><span class="line">      <span class="comment">// retrieve the key-value store named "Counts"</span></span><br><span class="line">      kvStore = (KeyValueStore) context.getStateStore(<span class="string">"Counts"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String dummy, String line)</span> </span>&#123;</span><br><span class="line">      String[] words = line.toLowerCase().split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">          Long oldValue = kvStore.get(word);</span><br><span class="line">          <span class="keyword">if</span> (oldValue == <span class="keyword">null</span>) &#123;</span><br><span class="line">              kvStore.put(word, <span class="number">1L</span>);</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              kvStore.put(word, oldValue + <span class="number">1L</span>);</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">punctuate</span><span class="params">(<span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">      KeyValueIterator&lt;String, Long&gt; iter = <span class="keyword">this</span>.kvStore.all();</span><br><span class="line">      <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">          KeyValue&lt;String, Long&gt; entry = iter.next();</span><br><span class="line">          context.forward(entry.key, entry.value.toString());</span><br><span class="line">      &#125;</span><br><span class="line">      iter.close();</span><br><span class="line">      <span class="comment">// commit the current processing progress</span></span><br><span class="line">      context.commit();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="comment">// close the key-value store</span></span><br><span class="line">      kvStore.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的实现中，执行了下列操作：</p>
<ul>
<li>In the init() method, schedule the punctuation every 1000 time units (the time unit is normally milliseconds, which in this example would translate to punctuation every 1 second) and retrieve the local state store by its name “Counts”.</li>
<li>In the process() method, upon each received record, split the value string into words, and update their counts into the state store (we will talk about this later in this section).</li>
<li>In the punctuate() method, iterate the local state store and send the aggregated counts to the downstream processor (we will talk about downstream processors later in this section), and commit the current stream state.</li>
</ul>
<ol>
<li>init()方法，调度punctuation每隔1000m个时间单元（这个时间单元通常是ms，这里表示每隔一秒调度一次punctuation），并且通过“Counts”名称获取本地的状态存储</li>
<li>process()方法，当接收到每条记录时，将字符串分成多个单词，并且将它们的次数更新到状态存储中</li>
<li>punctuate()方法，迭代本地状态存储，发送聚合次数给下游的处理算子，最后提交当前的流状态</li>
</ol>
<p><strong>DEFINING A STATE STORE 定义一个状态存储</strong>  </p>
<p>Note that the WordCountProcessor defined above can not only access the currently received record in the process() method, but also maintain processing states to keep recently arrived records for stateful processing needs such as aggregations and joins. To take advantage of these states, users can define a state store by implementing the StateStore interface (the Kafka Streams library also has a few extended interfaces such as KeyValueStore); in practice, though, users usually do not need to customize such a state store from scratch but can simply use the Stores factory to define a state store by specifying whether it should be persistent, log-backed, etc.</p>
<p>上面定义的WordCountProcessor不仅可以在process()方法中访问当前接收到的记录，也会维护处理状态，并保持最近到达的记录，可以被用于有状态的处理比如聚合和联合操作。为了利用这些状态的优势，用户可以实现自定义的StateStore接口。不过实际中用户通常不需要从头开始实现一个这样的状态存储，而只需要使用Stores的工厂类来定义一个状态存储。下面的示例中，定义了一个持久化的键值存储，名字叫做“Counts”，并且Key的类型是String，Value的类型是Long。</p>
<figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StateStoreSupplier</span> countStore = <span class="type">Stores</span>.create(<span class="string">"Counts"</span>)</span><br><span class="line">              .withKeys(<span class="type">Serdes</span>.<span class="type">String</span><span class="literal">()</span>)</span><br><span class="line">              .withValues(<span class="type">Serdes</span>.<span class="type">Long</span><span class="literal">()</span>)</span><br><span class="line">              .persistent<span class="literal">()</span></span><br><span class="line">              .build<span class="literal">()</span>;</span><br></pre></td></tr></table></figure>
<p><strong>CONNECTING PROCESSORS AND STORES 连接处理算子和状态</strong>  </p>
<p>Now that we have defined the processor and the state stores, we can now construct the processor topology by connecting these processors and state stores together by using the TopologyBuilder instance. In addition, users can add source processors with the specified Kafka topics to generate input data streams into the topology, and sink processors with the specified Kafka topics to generate output data streams out of the topology.</p>
<p>现在我们已经定义了处理算子和状态存储，我们可以开始构造处理拓扑：通过使用TopologyBuilder的实例来连接这些处理算子以及状态存储。另外，用户可以添加输入源处理算子（source processors），并且指定特定的Kafka主题，这样就可以（读取Kafka主题）生成输入数据流进入到拓扑中；也可以指定目标处理算子（sink processors），也会指定特定的Kafka主题，这样就可以（写入Kafka主题）生成输出数据流到拓扑之外。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line"></span><br><span class="line"><span class="comment">// add the source processor node that takes Kafka topic "source-topic" as input</span></span><br><span class="line">builder.addSource(<span class="string">"Source"</span>, <span class="string">"source-topic"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add the WordCountProcessor node which takes the source processor as its upstream processor</span></span><br><span class="line">    .addProcessor(<span class="string">"Process"</span>, () -&gt; <span class="keyword">new</span> WordCountProcessor(), <span class="string">"Source"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create the countStore associated with the WordCountProcessor processor</span></span><br><span class="line">    .addStateStore(countStore, <span class="string">"Process"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add the sink processor node that takes Kafka topic "sink-topic" as output</span></span><br><span class="line">    <span class="comment">// and the WordCountProcessor node as its upstream processor</span></span><br><span class="line">    .addSink(<span class="string">"Sink"</span>, <span class="string">"sink-topic"</span>, <span class="string">"Process"</span>);</span><br></pre></td></tr></table></figure>
<p>There are several steps in the above implementation to build the topology, and here is a quick walk through:<br>上面的实现中构造一个拓扑有几个步骤：</p>
<ul>
<li>A source processor node named “Source” is added to the topology using the addSource method, with one Kafka topic “source-topic” fed to it.</li>
<li>A processor node named “Process” with the pre-defined WordCountProcessor logic is then added as the downstream processor of the “Source” node using the addProcessor method.</li>
<li>A predefined persistent key-value state store countStore is created and associated to the “Process” node.</li>
<li>A sink processor node is then added to complete the topology using the addSink method, taking the “Process” node as its upstream processor and writing to a separate “sink-topic” Kafka topic.</li>
</ul>
<ol>
<li>一个叫做“Source”的源算子被添加到拓扑中，使用了addSource方法，并且有一个Kafka的主题“source-topic”会（将数据）喂到这个源算子里</li>
<li>一个叫做“Process”的处理节点，使用了预定义的WordCountProcessor逻辑，然后通过addProcessor方法将其添加到（作为）“Source”节点的下游处理算子</li>
<li>创建了一个预定义的持久化键值存储，保存了countStore，并且关联了“Process”节点</li>
<li>使用addSink方法添加了一个目标处理节点，构成了一个完整的拓扑。将“Process”节点作为它的上游处理算子，并且写到一个Kafka的输出主题</li>
</ol>
<p>In this defined topology, the “Process” stream processor node is considered a downstream processor of the “Source” node, and an upstream processor of the “Sink” node. As a result, whenever the “Source” node forward a newly fetched record from Kafka to its downstream “Process” node, WordCountProcessor#process() method is triggered to process the record and update the associated state store; and whenever context#forward() is called in the WordCountProcessor#punctuate() method, the aggregate key-value pair will be sent via the “Sink” processor node to the Kafka topic “sink-topic”. Note that in the WordCountProcessor implementation, users need to refer with the same store name “Counts” when accessing the key-value store; otherwise an exception will be thrown at runtime indicating that the state store cannot be found; also if the state store itself is not associated with the processor in the TopologyBuilder code, accessing it in the processor’s init() method will also throw an exception at runtime indicating the state store is not accessible from this processor.</p>
<p>在上面定义的拓扑中，“Process”流处理节点被认为是“Source”节点的下游处理算子，也是“Sink”节点的上游处理算子。结果就是：无论什么时候，当”Source“节点从Kafka拉取一条新的记录，并转发给下游的”Process“节点，WordCountProcessor#process()方法就会被触发，并且会处理这条记录，以及更新相应的状态存储；并且无论什么时候当在WordCountProcessor#punctuate()方法中调用context#forward()时，聚合的键值对会通过”Sink“处理节点发送到Kafka的输出主题“sink-topic”中。注意在WordCountProcessor的实现中，用户在获取键值存储时需要参考这里（构造拓扑）相同的状态存储名称“Counts”（即拓扑定义的Counts键值存储名称，在WordCountProcessor中为了获取这个键值存储，两者的名称必须是一致的），否则在运行时会抛出一个异常说状态存储未找到。如果状态存储本身没有和TopologyBuilder代码中的处理节点（Processor节点，只有WordCountProcessor这一个节点）相关联的话，在Processor的init方法中访问状态存储也会抛出运行时的异常（状态存储不能在当前Processor节点访问）。</p>
<p>With the defined processor topology, users can now start running a Kafka Streams application instance. Please read how to run a Kafka Streams application for details.</p>
<p>有了定义好的处理拓扑，用户现在就可以启动一个Kafka Streams应用程序实例了。</p>
<h3 id="Kafka_Streams_DSL">Kafka Streams DSL</h3><p>As mentioned in the Concepts section, a stream is an unbounded, continuously updating data set. With the Kafka Streams DSL users can define the processor topology by concatenating multiple transformation operations where each operation transforming one stream into other stream(s); the resulted topology then takes the input streams from source Kafka topics and generates the final output streams throughout its concatenated transformations. However, different streams may have different semantics in their data records:</p>
<p>一个流是一个无界的，持续更新的数据集。使用Kakfa Streams的DSL，用户可以通过连接多个转换操作的方式来定义处理拓扑，其中每个操作会从一个流转换到新的其他流。最终拓扑会将从Kafka源读取的主题作为输入流，并且在贯穿连接的转换操作中，生成最终的输出流。不过不同的流在它们的数据记录中，可能有不同的语义：</p>
<ul>
<li>In some streams, each record represents a new immutable datum in their unbounded data set; we call these record streams.</li>
<li>In other streams, each record represents a revision (or update) of their unbounded data set in chronological(按时间顺序) order; we call these changelog streams.</li>
</ul>
<ol>
<li>在一些流中，每条记录代表了无界数据集中新的不可变数据，我们叫做记录流（record streams）</li>
<li>在其他流中，每条记录代表了无界数据集中按照时间顺序排序的更新，我们叫做变更日志流（changelog streams）</li>
</ol>
<p>这两种类型的流都可以存储成Kafka的主题。不过，它们的计算语义则截然不同。举例有一个聚合操作，为指定的key计算记录的次数。对于记录流（record streams）而言，每条记录都是来自于Kafka主题中带有key的消息（比如一个页面浏览的数据流会以用户的编号作为key）：</p>
<p>Both of these two types of streams can be stored as Kafka topics. However, their computational semantics can be quite different. Take the example of an aggregation operation that counts the number of records for the given key. For record streams, each record is a keyed message from a Kafka topic (e.g., a page view stream keyed by user ids):</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># Example: a record stream for page view events</span></span><br><span class="line"><span class="preprocessor"># Notation is &lt;record key&gt; =&gt; &lt;record value&gt;</span></span><br><span class="line"><span class="number">1</span> =&gt; &#123;<span class="string">"time"</span>:<span class="number">1440557383335</span>, <span class="string">"user_id"</span>:<span class="number">1</span>, <span class="string">"url"</span>:<span class="string">"/home?user=1"</span>&#125;</span><br><span class="line"><span class="number">5</span> =&gt; &#123;<span class="string">"time"</span>:<span class="number">1440557383345</span>, <span class="string">"user_id"</span>:<span class="number">5</span>, <span class="string">"url"</span>:<span class="string">"/home?user=5"</span>&#125;</span><br><span class="line"><span class="number">2</span> =&gt; &#123;<span class="string">"time"</span>:<span class="number">1440557383456</span>, <span class="string">"user_id"</span>:<span class="number">2</span>, <span class="string">"url"</span>:<span class="string">"/profile?user=2"</span>&#125;</span><br><span class="line"><span class="number">1</span> =&gt; &#123;<span class="string">"time"</span>:<span class="number">1440557385365</span>, <span class="string">"user_id"</span>:<span class="number">1</span>, <span class="string">"url"</span>:<span class="string">"/profile?user=1"</span>&#125;</span><br></pre></td></tr></table></figure>
<p>The counting operation for record streams is trivial to implement: you can maintain a local state store that tracks the latest count for each key, and, upon receiving a new record, update the corresponding key by incrementing its count by one.</p>
<p>对记录流进行计数操作非常容易实现：你可以维护一个本地的状态存储，用来跟踪每个key的最近的次数。并且，当接收到一条新的记录时，更新对应key的值，把它的次数加1。</p>
<p>For changelog streams, on the other hand, each record is an update of the unbounded data set (e.g., a changelog stream for a user profile table, where the user id serves as both the primary key for the table and as the record key for the stream; here, a new record represents a changed row of the table). In practice you would usually store such streams in Kafka topics where log compaction is enabled.</p>
<p>对于变更日志流而言，每条记录都是对于无界数据集的一次更新（比如一个变更日志流是针对用户的个人信息表，用户的编号既作为表的主键，也会作为日志流记录的key，这里一条新记录表示的是表的一行更新）。实际应用中在Kafka的主题存储这样的路，应该开启日志压缩。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># Example: a changelog stream for a user profile table</span></span><br><span class="line"><span class="number">1</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557383335</span>, <span class="string">"user_id"</span>:<span class="number">1</span>, <span class="string">"email"</span>:<span class="string">"user1@aol.com"</span>&#125;</span><br><span class="line"><span class="number">5</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557383345</span>, <span class="string">"user_id"</span>:<span class="number">5</span>, <span class="string">"email"</span>:<span class="string">"user5@gmail.com"</span>&#125;</span><br><span class="line"><span class="number">2</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557383456</span>, <span class="string">"user_id"</span>:<span class="number">2</span>, <span class="string">"email"</span>:<span class="string">"user2@yahoo.com"</span>&#125;</span><br><span class="line"><span class="number">1</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557385365</span>, <span class="string">"user_id"</span>:<span class="number">1</span>, <span class="string">"email"</span>:<span class="string">"user1-new-email-addr@comcast.com"</span>&#125;</span><br><span class="line"><span class="number">2</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557385395</span>, <span class="string">"user_id"</span>:<span class="number">2</span>, <span class="string">"email"</span>:null&#125;  &lt;-- user has been deleted</span><br></pre></td></tr></table></figure>
<p>As a result the counting operation for changelog streams is no longer monotonically incrementing: you need to also decrement the counts when a delete update record is received on some given key as well. In addition, even for counting aggregations on an record stream, the resulting aggregate is no longer an record stream but a relation / table, which can then be represented as a changelog stream of updates on the table.</p>
<p>对变更日志流的计数操作不再是单调递增的了：当接收到指定key的一个删除更新记录时，你需要减少计数。另外，即使是在记录流上做聚合计数，聚合的结果也不是一个记录流，而是一张表，它代表的是在表上的变更日志流的更新。</p>
<p>The counting operation of a user profile changelog stream is peculiar(罕见的) because it will generate, for a given key, a count of either 0 (meaning the key does not exist or does not exist anymore) or 1 (the key exists) only. Multiple records for the same key are considered duplicate, old information of the most recent record, and thus will not contribute to the count.</p>
<p>For changelog streams developers usually prefer counting a non-primary-key field. We use the example above just for the sake of illustration.</p>
<p>对用户的个人信息变更日志流做计数操作是比较少见的，因为它为指定的key生成的计数要么是0（表示key不存在），要么是1（key存在）。相同key的多个记录被认为是重复的、相对当前最近记录而言是旧的信息，因此不会对计数结果产生影响。对于变更日志流，开发者通常会在非主键字段上计数，上面的示例仅仅是为了演示（有主键）。</p>
<p>One of the key design principles of the Kafka Streams DSL is to understand and distinguish between record streams and changelog streams and to provide operators with the correct semantics for these two different types of streams. More concretely, in the Kafka Streams DSL we use the KStream interface to represent record streams, and we use a separate KTable interface to represent changelog streams. The Kafka Streams DSL is therefore the recommended API to implement a Kafka Streams application. Compared to the lower-level Processor API, its benefits are:</p>
<p>Kafka Streams DSL的一个主要设计原则是理解和区分记录流合变更日志流，并且为这两种类型流的操作提供正确的语义。更具体地来说，在Kafka Streams DSL中，我们使用KStream接口来表示记录流，使用KTable接口代表变更日志流。所以Kafka Streams DSL是用来在实现Kafka Streams应用程序时推荐的API，和低级Processor API比较，它有几个优点：</p>
<ul>
<li>More concise and expressive code, particularly when using Java 8+ with lambda expressions.</li>
<li>Easier to implement stateful transformations such as joins and aggregations.</li>
<li>Understands the semantic differences of record streams and changelog streams, so that transformations such as aggregations work as expected depending on which type of stream they operate against.</li>
</ul>
<ol>
<li>更简洁、更具有表达力的代码，尤其是使用Java8的lambda表达式时</li>
<li>可以很容易地实现一个有状态的操作，比如联合和聚合操作</li>
<li>理解记录流和变更日志流的语义区别，这样转换操作（比如聚合）根据流的类型按照预期的方式工作</li>
</ol>
<h4 id="CREATING_SOURCE_STREAMS_FROM_KAFKA">CREATING SOURCE STREAMS FROM KAFKA</h4><p>Both KStream or KTable objects can be created as a source stream from one or more Kafka topics via KStreamBuilder, an extended class of TopologyBuilder used in the lower-level Processor API (for KTable you can only create the source stream from a single topic).</p>
<p>KStream和KTable对象都可以通过KStreamBuilder创建，作为从一个或多个Kafka主题的输入源流。KStreamBuilder是TopologyBuilder（低级的Processor API）的继承类。对于KTable，你只能从一个Kafka主题中创建一个输入源流。</p>
<table>
<thead>
<tr>
<th>Interface</th>
<th>How to instantiate</th>
</tr>
</thead>
<tbody>
<tr>
<td>KStream<k, v=""></k,></td>
<td>KStreamBuilder#stream(…)</td>
</tr>
<tr>
<td>KTable<k, v=""></k,></td>
<td>KStreamBuilder#table(…)</td>
</tr>
</tbody>
</table>
<p>When creating an instance, you may override the default serdes for record keys (K) and record values (V) used for reading the data from Kafka topics (see Data types and serdes for more details); otherwise the default serdes specified through StreamsConfig will be used.</p>
<p>当创建一个（KStream或KTable）实例时，需要指定输入源的Kafka主题，你可能需要指定读取记录的序列化器，如果不指定时，会使用StreamsConfig中指定的序列化器。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KStreamBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KTable;</span><br><span class="line"></span><br><span class="line">KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line"></span><br><span class="line"><span class="comment">// In this example we assume that the default serdes for keys and values are</span></span><br><span class="line"><span class="comment">// the String serde and the generic Avro serde, respectively.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a stream of page view events from the PageViews topic, where the key of</span></span><br><span class="line"><span class="comment">// a record is assumed to be the user id (String) and the value an Avro GenericRecord</span></span><br><span class="line"><span class="comment">// that represents the full details of the page view event.</span></span><br><span class="line">KStream&lt;String, GenericRecord&gt; pageViews = builder.stream(<span class="string">"PageViews"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a changelog stream for user profiles from the UserProfiles topic,</span></span><br><span class="line"><span class="comment">// where the key of a record is assumed to be the user id (String) and its value</span></span><br><span class="line"><span class="comment">// an Avro GenericRecord.</span></span><br><span class="line">KTable&lt;String, GenericRecord&gt; userProfiles = builder.table(<span class="string">"UserProfiles"</span>);</span><br></pre></td></tr></table></figure>
<h4 id="TRANSFORM_A_STREAM">TRANSFORM A STREAM</h4><p>KStream and KTable support a variety of transformation operations. Each of these operations can be translated into one or more connected processors into the underlying processor topology. Since KStream and KTable are strongly typed, all these transformation operations are defined as generics functions where users could specify the input and output data types.</p>
<p>KStream和KTable支持很多类型的转换操作。每种操作都可以翻译成底层处理拓扑的一个或多个相连起来的处理算子。由于KStream和KTable是强类型的，所有这些转换操作被定义为通用的函数，这样用户可以指定输入和输出的数据类型。</p>
<p>Some KStream transformations may generate one or more KStream objects (e.g., filter and map on KStream generate another KStream, while branch on KStream can generate multiple KStream) while some others may generate a KTable object (e.g., aggregation) interpreted(理解) as the changelog stream to the resulted relation. This allows Kafka Streams to continuously update the computed value upon arrivals of late records after it has already been produced to the downstream transformation operators. As for KTable, all its transformation operations can only generate another KTable (though the Kafka Streams DSL does provide a special function to convert a KTable representation into a KStream, which we will describe later). Nevertheless, all these transformation methods can be chained together to compose a complex processor topology.</p>
<p>有些KStream转换可能产生不止一个KStream对象（在KStream上进行过滤和映射会生成新的KStream，而在KStream上进行分支则会产生多个KStream），而有些可能产生一个KTable对象（比如聚合转换）。这就允许Kafka Streams即使在记录已经发送给下游的转换算子的情况下，在迟到的记录到来时，也可以持续地更新已经计算过的值。对于KTable而言，它的所有转换操作只会生成新的KTable（尽管Kafka Streams DSL提供额外的函数可以将一个KTable转换成KStream）。不仅如此，所有这些转换操作都可以被链接在一起，从而构造出一个复杂的处理拓扑。</p>
<p>We describe these transformation operations in the following subsections, categorizing them as two categories: stateless and stateful transformations.</p>
<p>下面我们会将这些转换操作分成两类：无状态的和有状态的转换。</p>
<h4 id="STATELESS_TRANSFORMATIONS">STATELESS TRANSFORMATIONS</h4><p>Stateless transformations include filter, filterNot, foreach, map, mapValues, selectKey, flatMap, flatMapValues, branch. Most of them can be applied to both KStream and KTable, where users usually pass a customized function to these functions as a parameter; e.g. a Predicate for filter, a KeyValueMapper for map, and so on. Stateless transformations, by definition, do not depend on any state for processing, and hence implementation-wise they do not require a state store associated with the stream processor.</p>
<p>无状态的转换包括：<code>filter, filterNot, foreach, map, mapValues, selectKey, flatMap, flatMapValues, branch</code>。它们中的大部分都可以被同时用在KStream和KTable上，用户通常需要为这些函数传递一个自定义的函数作为参数。比如对于filter算子，需要传递一个Predicate，对于map算子需要传递一个KeyValueMapper等等。无状态的操作，从定义上来说，不依赖于处理的任何状态，因此它们并不需要和流处理算子相关联的状态存储。</p>
<p>下面是mapValues分别使用Java8的lambda，以及Java7的示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Java8</span></span><br><span class="line">KStream&lt;Long, String&gt; uppercased =</span><br><span class="line">    nicknameByUserId.mapValues(nickname -&gt; nickname.toUpperCase());</span><br><span class="line"></span><br><span class="line"><span class="comment">//Java7</span></span><br><span class="line">KStream&lt;Long, String&gt; uppercased =</span><br><span class="line">    nicknameByUserId.mapValues(</span><br><span class="line">        <span class="keyword">new</span> ValueMapper&lt;String&gt;() &#123;</span><br><span class="line">            <span class="annotation">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">apply</span><span class="params">(String nickname)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> nickname.toUpperCase();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    );</span><br></pre></td></tr></table></figure>
<p>The function is applied to each record, and its result will trigger the creation a new record.  </p>
<p>这个函数会作用于每条记录，它的结果会创建一个新的记录。</p>
<h4 id="STATEFUL_TRANSFORMATIONS">STATEFUL TRANSFORMATIONS</h4><p>有状态的操作包括如下几个： </p>
<ul>
<li>joins (KStream/KTable): join, leftJoin, outerJoin</li>
<li>aggregations (KStream): countByKey, reduceByKey, aggregateByKey</li>
<li>aggregations (KTable): groupBy plus count, reduce, aggregate (via KGroupedTable)</li>
<li>general transformations (KStream): process, transform, transformValues</li>
</ul>
<p>Stateful transformations are transformations where the processing logic requires accessing an associated state for processing and producing outputs. For example, in join and aggregation operations, a windowing state is usually used to store all the records received so far within the defined window boundary. The operators can then access accumulated records in the store and compute based on them (see Windowing a Stream for details).</p>
<p>有状态的操作指的是转换的处理逻辑需要访问相关的状态，来做处理操作和产生结果。举例联合和聚合操作，会使用一个窗口状态，存储定义的窗口边界内接收到的所有记录。操作算子就可以从存储中访问收集到的记录，基于这些记录做计算。使用Java8的WordCoun示例：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// We assume message values represent lines of text.  For the sake of this example, we ignore</span></span><br><span class="line"><span class="comment">// whatever may be stored in the message keys.</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; textLines = <span class="attribute">...</span>;</span><br><span class="line"></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, Long&gt; wordCounts = textLines</span><br><span class="line">    <span class="comment">// Split each text line, by whitespace, into words.  The text lines are the message</span></span><br><span class="line">    <span class="comment">// values, i.e. we can ignore whatever data is in the message keys and thus invoke</span></span><br><span class="line">    <span class="comment">// `flatMapValues` instead of the more generic `flatMap`.</span></span><br><span class="line">    <span class="built_in">.</span>flatMapValues(value <span class="subst">-&gt; </span>Arrays<span class="built_in">.</span>asList(value<span class="built_in">.</span>toLowerCase()<span class="built_in">.</span>split(<span class="string">"\\W+"</span>)))</span><br><span class="line">    <span class="comment">// We will subsequently invoke `countByKey` to count the occurrences of words, so we use</span></span><br><span class="line">    <span class="comment">// `map` to ensure the key of each record contains the respective word.</span></span><br><span class="line">    <span class="built_in">.</span><span class="built_in">map</span>((key, word) <span class="subst">-&gt; </span><span class="literal">new</span> KeyValue&lt;&gt;(word, word))</span><br><span class="line">    <span class="comment">// Count the occurrences of each word (record key).</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// This will change the stream type from `KStream&lt;String, String&gt;` to</span></span><br><span class="line">    <span class="comment">// `KTable&lt;String, Long&gt;` (word -&gt; count).  We must provide a name for</span></span><br><span class="line">    <span class="comment">// the resulting KTable, which will be used to name e.g. its associated</span></span><br><span class="line">    <span class="comment">// state store and changelog topic.</span></span><br><span class="line">    <span class="built_in">.</span>countByKey(<span class="string">"Counts"</span>)</span><br><span class="line">    <span class="comment">// Convert the `KTable&lt;String, Long&gt;` into a `KStream&lt;String, Long&gt;`.</span></span><br><span class="line">    <span class="built_in">.</span>toStream();</span><br></pre></td></tr></table></figure>
<p>WordCount使用Java 7:</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Code below is equivalent to the previous Java 8+ example above.</span></span><br><span class="line">KStream&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; textLines = ...;</span><br><span class="line"></span><br><span class="line">KStream&lt;<span class="keyword">String</span>, Long&gt; wordCounts = textLines</span><br><span class="line">    .flatMapValues(<span class="keyword">new</span> ValueMapper&lt;<span class="keyword">String</span>, Iterable&lt;<span class="keyword">String</span>&gt;&gt;() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        <span class="keyword">public</span> Iterable&lt;<span class="keyword">String</span>&gt; apply(<span class="keyword">String</span> value) &#123;</span><br><span class="line">            <span class="keyword">return</span> Arrays.asList(value.toLowerCase().<span class="built_in">split</span>(<span class="string">"\\W+"</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .<span class="built_in">map</span>(<span class="keyword">new</span> KeyValueMapper&lt;<span class="keyword">String</span>, <span class="keyword">String</span>, KeyValue&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;&gt;() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        <span class="keyword">public</span> KeyValue&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; apply(<span class="keyword">String</span> <span class="variable">key</span>, <span class="keyword">String</span> word) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> KeyValue&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;(word, word);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .countByKey(<span class="string">"Counts"</span>)</span><br><span class="line">    .toStream();</span><br></pre></td></tr></table></figure>
<h4 id="WINDOWING_A_STREAM">WINDOWING A STREAM</h4><p>Windowing is a common prerequisite for stateful transformations which group records in a stream, for example, by their timestamps. A local state store is usually needed for a windowing operation to store recently received records based on the window interval, while old records in the store are purged after the specified window retention period. The retention time can be set via Windows#until().</p>
<p>窗口是有状态转换操作的基本条件，它会在流中对记录进行分组，比如根据时间撮的方式进行分组。一个窗口相关的操作通常需要一个本地状态存储，来保存最近接收到的记录，这些记录是基于窗口间隔，状态存储中旧的记录会在指定的窗口保留时间过去后会被删除。保留时间是通过Windows#until()设置的。Kafka Streams目前支持以下类型的窗口：</p>
<table>
<thead>
<tr>
<th>Window name</th>
<th>Behavior</th>
<th>Short description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tumbling time window（翻转窗口）</td>
<td>Time-based</td>
<td>Fixed-size, non-overlapping, gap-less windows</td>
</tr>
<tr>
<td>Hopping time window（跳跃时间窗口）</td>
<td>Time-based</td>
<td>Fixed-size, overlapping windows</td>
</tr>
<tr>
<td>Sliding time window（滑动时间窗口）</td>
<td>Time-based</td>
<td>Fixed-size, overlapping windows that work on differences between record timestamps</td>
</tr>
</tbody>
</table>
<p><strong>Tumbling time windows</strong> are a special case of hopping time windows and, like the latter, are windows based on time intervals. They model fixed-size, non-overlapping, gap-less windows. A tumbling window is defined by a single property: the window’s size. A tumbling window is a hopping window whose window size is equal to its advance interval. Since tumbling windows never overlap, a data record will belong to one and only one window.</p>
<p>翻转窗口是跳跃窗口的一个特例，和后者一样，所有的窗口都是基于时间间隔。窗口的大小是固定的，窗口之间不会重复，也没有间隙。一个翻转窗口之定义了窗口大小这个简单的属性，默认它的窗口大小和前进间隔相等。由于翻转窗口不会有重叠，所以一条记录指挥属于一个窗口。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A tumbling time window with a size 60 seconds (and, by definition, an implicit</span></span><br><span class="line"><span class="comment">// advance interval of 60 seconds).</span></span><br><span class="line"><span class="comment">// The window's name -- the string parameter -- is used to e.g. name the backing state store.</span></span><br><span class="line"><span class="keyword">long</span> windowSizeMs = <span class="number">60</span> * <span class="number">1000L</span>;</span><br><span class="line">TimeWindows.of(<span class="string">"tumbling-window-example"</span>, windowSizeMs);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The above is equivalent to the following code:</span></span><br><span class="line">TimeWindows.of(<span class="string">"tumbling-window-example"</span>, windowSizeMs).advanceBy(windowSizeMs);</span><br></pre></td></tr></table></figure>
<p><img src="https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/02/52/ASAQL-1.png" alt=""></p>
<p><strong>Hopping time windows</strong> are windows based on time intervals. They model fixed-sized, (possibly) overlapping windows. A hopping window is defined by two properties: the window’s size and its advance interval (aka “hop”). The advance interval specifies by how much a window moves forward relative to the previous one. For example, you can configure a hopping window with a size 5 minutes and an advance interval of 1 minute. Since hopping windows can overlap – and in general they do – a data record may belong to more than one such windows.</p>
<p>跳跃时间窗口是基于时间间隔的窗口，固定大小，但是窗口之间可能有重叠。跳跃窗口定义了两个属性：窗口大小和跳跃间隔（hop的中文意思是跳跃）。跳跃间隔指定了一个窗口相对于前一个窗口的移动大小。比如你可以配置一个跳跃窗口的大小为5分钟，跳跃间隔为1分钟。由于跳跃窗口允许重叠，所以一条记录可能属于不止一个窗口。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A hopping time window with a size of 5 minutes and an advance interval of 1 minute.</span></span><br><span class="line"><span class="comment">// The window's name -- the string parameter -- is used to e.g. name the backing state store.</span></span><br><span class="line"><span class="keyword">long</span> windowSizeMs = <span class="number">5</span> * <span class="number">60</span> * <span class="number">1000L</span>;</span><br><span class="line"><span class="keyword">long</span> advanceMs =    <span class="number">1</span> * <span class="number">60</span> * <span class="number">1000L</span>;</span><br><span class="line">TimeWindows.of(<span class="string">"hopping-window-example"</span>, windowSizeMs).advanceBy(advanceMs);</span><br></pre></td></tr></table></figure>
<p><img src="https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/02/52/ASAQL-2.png" alt=""></p>
<p>Pay attention, that tumbling and hopping time windows are aligned to the epoch and that the lower window time interval bound is inclusive, while the upper bound is exclusive.</p>
<p>Aligned to the epoch means, that the first window starts at timestamp zero. For example, hopping windows with size of 5000ms and advance of 3000ms, have window boundaries [0;5000),[3000;8000),…— and not [1000;6000),[4000;9000),… or even something “random” like [1452;6452),[4452;9452),…, which might be the case if windows get initialized depending on system/application start-up time, introducing non-determinism.</p>
<p>注意，翻转窗口和跳跃窗口是和时间点对齐的，所以窗口时间的下界（lower bound）被包含，而下界（upper bound）不被包含。和时间点对齐表示，第一个窗口从时间点为0开始。比如跳跃窗口的大小=5000ms，跳跃间隔=3000ms，对应的窗口边界分别是<code>[0;5000),[3000;8000),...</code>，而不是<code>[1000;6000),[4000;9000),...</code>，也不是<code>[1452;6452),[4452;9452),...</code>。窗口计数的示例如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">KStream&lt;String, GenericRecord&gt; viewsByUser = ...;</span><br><span class="line"></span><br><span class="line">KTable&lt;Windowed&lt;String&gt;, Long&gt; userCounts =</span><br><span class="line">    <span class="comment">// count users, using hopping windows of size 5 minutes that advance every 1 minute</span></span><br><span class="line">    viewsByUser.countByKey(TimeWindows.of(<span class="string">"GeoPageViewsWindow"</span>, <span class="number">5</span> * <span class="number">60</span> * <span class="number">1000L</span>).advanceBy(<span class="number">60</span> * <span class="number">1000L</span>));</span><br></pre></td></tr></table></figure>
<p>Unlike non-windowed aggregates that we have seen previously, windowed aggregates return a windowed KTable whose key type is Windowed<k>. This is to differentiate aggregate values with the same key from different windows. The corresponding window instance and the embedded key can be retrieved as Windowed#window() and Windowed#key(), respectively.</k></p>
<p>和前面看到的没有窗口的聚合不同的是，窗口聚合操作返回一个带有窗口的KTable，它的key是Windowed<k>，目的是区分不同窗口中存在相同的key（如果没有带窗口，那么相同的key在不同的窗口中就无法区分）。对应的窗口实例以及内置的key分别可以通过<code>Windowed#window()</code>和<code>Windowed#key()</code>获取到。</k></p>
<p><strong>Sliding windows</strong> are actually quite different from hopping and tumbling windows. A sliding window models a fixed-size window that slides continuously over the time axis; here, two data records are said to be included in the same window if the difference of their timestamps is within the window size. Thus, sliding windows are not aligned to the epoch, but on the data record timestamps. Pay attention, that in contrast to hopping and tumbling windows, lower and upper window time interval bounds are both inclusive. In Kafka Streams, sliding windows are used only for join operations, and can be specified through the JoinWindows class.</p>
<p>滑动窗口和跳跃窗口、翻转窗口都不同，它也有固定的窗口大小，不过是在时间轴上持续地滑动。比如有两条记录的时间撮差别是在窗口大小内的，这两条记录就会被包含在同一个窗口中。所以滑动窗口并不是和时间点对齐，而是和记录的时间撮对齐。注意和跳跃窗口、翻转窗口相反的是，滑动窗口会同时包含窗口边界的上界和下界。在Kafka Streams中，滑动窗口仅用于join操作。</p>
<p><img src="https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/02/52/ASAQL-3.png" alt=""></p>
<h4 id="JOINING_STREAMS">JOINING STREAMS</h4><p>Many stream processing applications can be coded as stream join operations. For example, applications backing an online shop might need to access multiple, updating database tables (e.g. sales prices, inventory, customer information) when processing a new record. These applications can be implemented such that they work on the tables’ changelog streams directly, i.e. without requiring to make a database query over the network for each record. In this example, the KTable concept in Kafka Streams would enable you to track the latest state (think: snapshot) of each table in a local key-value store, thus greatly reducing the processing latency as well as reducing the load of the upstream databases.</p>
<p>许多流处理应用程序都需要流式的join操作。比如在线商店应用程序在处理一条新记录时可能需要访问或更新多张数据库表（比如销售价格表，库存表，用户信息表）。这些应用程序可以在表的变更日志流（KTable是变更日志流，KStream是记录流）上直接实现，比如对每条记录不需要跨网络的数据库查询。在这个示例中，Kafka Streams的KTable概念使你可以在一个本地键值存储中跟踪每张表的最新状态（快照），因此可以很明显地减少处理延迟，以及减少上游数据库的压力。在Kafka Streams中，有下面两种的join操作：</p>
<ul>
<li>Join a KStream with another KStream or KTable.</li>
<li>Join a KTable with another KTable only.</li>
</ul>
<p>有三种join组合方式：</p>
<ul>
<li>KStream-to-KStream Joins are always windowed joins, since otherwise the join result size might grow infinitely in size. Here, a newly received record from one of the streams is joined with the other stream’s records within the specified window interval to produce one result for each matching pair based on user-provided ValueJoiner. A new KStream instance representing the result stream of the join is returned from this operator.</li>
<li>KTable-to-KTable Joins are join operations designed to be consistent(一致) with the ones in relational databases. Here, both changelog streams are materialized into local state stores to represent the latest snapshot of the their data table duals(双重). When a new record is received from one of the streams, it is joined with the other stream’s materialized state stores to produce one result for each matching pair based on user-provided ValueJoiner. A new KTable instance representing the result stream of the join, which is also a changelog stream of the represented table, is returned from this operator.</li>
<li>KStream-to-KTable Joins allow you to perform table lookups against a changelog stream (KTable) upon receiving a new record from another record stream (KStream). An example use case would be to enrich(使丰富) a stream of user activities (KStream) with the latest user profile information (KTable). Only records received from the record stream will trigger the join and produce results via ValueJoiner, not vice versa (i.e., records received from the changelog stream will be used only to update the materialized state store). A new KStream instance representing the result stream of the join is returned from this operator.</li>
</ul>
<ol>
<li><strong>KStream-to-KStream Join</strong>操作总是针对窗口的Join，否则Join结果的大小会无限膨胀。从其中一个流接收到的新记录会和另外一个流的记录进行Join，后者的流会指定窗口间隔，最后会基于用户提供的<code>ValueJoiner</code>为每个匹配对产生一个结果。这里返回的结果是一个新的KStream实例，代表了Join操作的结果流。</li>
<li><strong>KTable-to-KTable Join</strong>操作被设计为和关系型数据库类似的操作。两个变更日志流（KTable）都会被物化成本地状态存储，表示数据表的最近快照。当从其中的一个流接收到一条新记录，它会和另外一个流的物化状态存储进行join，并根据用户提供的ValueJoiner产生一个匹配的结果。返回的结果是新的KTable实例，代表Join操作的结果流，它也是一个变更日志流。</li>
<li><strong>KStream-to-KTable Join</strong>操作允许你在记录流（KStream）上接收到一条新记录时，从一个变更日志流（KTable）上执行表（级别的记录）查询。比如对一个用户活动流（KStream）使用最近的用户个人信息（KTable）进行信息增强。只有从记录流接收的记录才会触发join操作，并通过ValueJoiner产生结果，反过来则不行（比如从变更日志流中接收的新记录只会被用来更新物化的状态存储，而不会和KStream记录流进行join）。返回的结果是一个新的KStream，代表了Join操作的结果流。</li>
</ol>
<p>根据操作对象，不同操作类型支持不同的join语义：</p>
<table>
<thead>
<tr>
<th>Join operands</th>
<th>(INNER) JOIN</th>
<th>OUTER JOIN</th>
<th>LEFT JOIN</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>KStream-to-KStream</td>
<td>Supported</td>
<td>Supported</td>
<td>Supported</td>
<td>KStream</td>
</tr>
<tr>
<td>KTable-to-KTable</td>
<td>Supported</td>
<td>Supported</td>
<td>Supported</td>
<td>KTable</td>
</tr>
<tr>
<td>KStream-to-KTable</td>
<td>N/A</td>
<td>N/A</td>
<td>Supported</td>
<td>KStream</td>
</tr>
</tbody>
</table>
<p>Kafka Streams的Join语义类似于关系型数据库的操作符：</p>
<ul>
<li>Inner join produces new joined records when the join operator finds some records with the same key in the other stream / materialized store.</li>
<li>Outer join works like inner join if some records are found in the windowed stream / materialized store. The difference is that outer join still produces a record even when no records are found. It uses null as the value of missing record.</li>
<li>Left join is like outer join except that for KStream-KStream join it is always driven by record arriving from the primary stream; while for KTable-KTable join it is driven by both streams to make the result consistent with the left join of databases while only permits missing records in the secondary stream. In a KStream-KTable left join, a KStream record will only join a KTable record if the KTable record arrived before the KStream record (and is in the KTable). Otherwise, the join result will be null</li>
</ul>
<ol>
<li><strong>Inner join</strong>：当join操作符在两个流或物化存储中都找到相同key的记录，产生新的join记录。</li>
<li><strong>Outer join</strong>：如果在窗口流或物化视图中找到记录，则和inner join类似。不同的是，outer join即使在没有找到记录也会输出一条记录，对于缺失的记录使用null作为value。</li>
<li><strong>Left join</strong>：和outer join类似，不过对于KStream-KSteram的join（KStream left join KStream），它总是在主要流（A left join B，则A是主要的流）的记录到达时驱动的；对于KTable-KTable的join（KTable left join KTable），它是由两个流一起驱动，并且结果和left join左边的流是一致的，只允许右边流的记录缺失；对于KStream-KTable的left join（KStream left join KTable），一条KStream的记录只会和一条KTable的记录join，并且这条KTable的记录必须要在KStream的记录之前到达（当然必须在KTable中），否则join结果为null。</li>
</ol>
<p>Since stream joins are performed over the keys of records, it is required that joining streams are co-partitioned by key, i.e., their corresponding Kafka topics must have the same number of partitions and partitioned on the same key so that records with the same keys are delivered to the same processing thread. This is validated by Kafka Streams library at runtime (we talked about the threading model and data parallelism with more details in the Architecture section).</p>
<p>由于流的join是在记录的key上执行的，这就要求参与join的流能够按照key协同分区。比如它们（流）对应的Kafka主题必须有相等数量的分区，而且在相同的key上进行分区。这样相同key的记录会被发送到相同的处理线程。上面的限制会在Kafka Streams库中在运行时进行验证。</p>
<p>Join示例，使用Java8：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Key is user, value is number of clicks by that user</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, Long&gt; userClicksStream =  <span class="attribute">...</span>;</span><br><span class="line"><span class="comment">// Key is user, value is the geo-region of that user</span></span><br><span class="line">KTable&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; userRegionsTable = <span class="attribute">...</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// KStream-KTable join</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, RegionWithClicks&gt; userClicksWithRegion = userClicksStream</span><br><span class="line">    <span class="comment">// Null values possible: In general, null values are possible for region (i.e. the value of</span></span><br><span class="line">    <span class="comment">// the KTable we are joining against) so we must guard against that (here: by setting the</span></span><br><span class="line">    <span class="comment">// fallback region "UNKNOWN").</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Also, we need to return a tuple of (region, clicks) for each user.  But because Java does</span></span><br><span class="line">    <span class="comment">// not support tuples out-of-the-box, we must use a custom class `RegionWithClicks` to</span></span><br><span class="line">    <span class="comment">// achieve the same effect.  This class two fields -- the region (String) and the number of</span></span><br><span class="line">    <span class="comment">// clicks (Long) for that region -- as well as a matching constructor, which we use here.</span></span><br><span class="line">    <span class="built_in">.</span>leftJoin(userRegionsTable,</span><br><span class="line">      (clicks, region) <span class="subst">-&gt; </span><span class="literal">new</span> RegionWithClicks(region == <span class="built_in">null</span> ? <span class="string">"UNKNOWN"</span> : region, clicks));</span><br></pre></td></tr></table></figure>
<p>Join示例，使用Java7：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Key is user, value is number of clicks by that user</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, Long&gt; userClicksStream =  <span class="attribute">...</span>;</span><br><span class="line"><span class="comment">// Key is user, value is the geo-region of that user</span></span><br><span class="line">KTable&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; userRegionsTable = <span class="attribute">...</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// KStream-KTable join</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, RegionWithClicks&gt; userClicksWithRegion = userClicksStream</span><br><span class="line">    <span class="comment">// Null values possible: In general, null values are possible for region (i.e. the value of</span></span><br><span class="line">    <span class="comment">// the KTable we are joining against) so we must guard against that (here: by setting the</span></span><br><span class="line">    <span class="comment">// fallback region "UNKNOWN").</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Also, we need to return a tuple of (region, clicks) for each user.  But because Java does</span></span><br><span class="line">    <span class="comment">// not support tuples out-of-the-box, we must use a custom class `RegionWithClicks` to</span></span><br><span class="line">    <span class="comment">// achieve the same effect.  This class two fields -- the region (String) and the number of</span></span><br><span class="line">    <span class="comment">// clicks (Long) for that region -- as well as a matching constructor, which we use here.</span></span><br><span class="line">    <span class="built_in">.</span>leftJoin(userRegionsTable, <span class="literal">new</span> ValueJoiner&lt;Long, <span class="built_in">String</span>, RegionWithClicks&gt;() &#123;</span><br><span class="line">      @Override</span><br><span class="line">      <span class="keyword">public</span> RegionWithClicks apply(Long clicks, <span class="built_in">String</span> region) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">new</span> RegionWithClicks(region == <span class="built_in">null</span> ? <span class="string">"UNKNOWN"</span> : region, clicks);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>
<h4 id="APPLYING_A_CUSTOM_PROCESSOR">APPLYING A CUSTOM PROCESSOR</h4><p>Beyond the provided transformation operators, users can also specify any customized processing logic on their stream data via the KStream#process() method, which takes an implementation of the ProcessorSupplier interface as its parameter. This is essentially equivalent to the addProcessor() method in the Processor API.</p>
<p>The following example shows how to leverage, via the process() method, a custom processor that sends an email notification whenever a page view count reaches a predefined threshold.</p>
<p>除了Kafka Streams提供的转换操作（DSL），用户可以在流数据中通过KStream#process()方法指定定制的处理逻辑：将ProcessorSupplier接口的实现作为参数，这和Processor API的addProcessor()方法是等价的。下面的示例展示了通过process()方法如何使用自定义处理器，这个处理器会在当页面浏览计数到达指定的阈值时发送一个邮件通知。</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Send an email notification when the view count of a page reaches one thousand. </span></span><br><span class="line"><span class="comment">// JAVA8</span></span><br><span class="line">pageViews.countByKey(<span class="string">"PageViewCounts"</span>)</span><br><span class="line">         .filter((PageId pageId, <span class="keyword">Long</span> viewCount) -&gt; viewCount == <span class="number">1000</span>)</span><br><span class="line">         <span class="comment">// PopularPageEmailAlert is your custom processor that implements the</span></span><br><span class="line">         <span class="comment">// `Processor` interface, see further down below.</span></span><br><span class="line">         .process(() -&gt; <span class="keyword">new</span> PopularPageEmailAlert(<span class="string">"alerts@yourcompany.com"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// JAVA7</span></span><br><span class="line">pageViews.countByKey(<span class="string">"PageViewCounts"</span>)</span><br><span class="line">         .filter(</span><br><span class="line">            <span class="keyword">new</span> Predicate&lt;PageId, <span class="keyword">Long</span>&gt;() &#123;</span><br><span class="line">              <span class="keyword">public</span> <span class="keyword">boolean</span> test(PageId pageId, <span class="keyword">Long</span> viewCount) &#123;</span><br><span class="line">                <span class="keyword">return</span> viewCount == <span class="number">1000</span>;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">         .process(</span><br><span class="line">           <span class="keyword">new</span> ProcessorSupplier&lt;PageId, <span class="keyword">Long</span>&gt;() &#123;</span><br><span class="line">             <span class="keyword">public</span> Processor&lt;PageId, <span class="keyword">Long</span>&gt; get() &#123;</span><br><span class="line">               <span class="comment">// PopularPageEmailAlert is your custom processor that implements</span></span><br><span class="line">               <span class="comment">// the `Processor` interface, see further down below.</span></span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">new</span> PopularPageEmailAlert(<span class="string">"alerts@yourcompany.com"</span>);</span><br><span class="line">             &#125;</span><br><span class="line">           &#125;);</span><br></pre></td></tr></table></figure>
<p>上面的示例中PopularPageEmailAlert是一个自定义实现了Processor接口的流处理算子：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A processor that sends an alert message about a popular page to a configurable email address</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PopularPageEmailAlert</span> <span class="keyword">implements</span> <span class="title">Processor</span>&lt;<span class="title">PageId</span>, <span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> String emailAddress;</span><br><span class="line">  <span class="keyword">private</span> ProcessorContext;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">PopularPageEmailAlert</span><span class="params">(String emailAddress)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.emailAddress = emailAddress;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessorContext context)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.context = context;</span><br><span class="line">    <span class="comment">// Here you would perform any additional initializations</span></span><br><span class="line">    <span class="comment">// such as setting up an email client.</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">(PageId pageId, Long count)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Here would format and send the alert email.</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// In this specific example, you would be able to include information</span></span><br><span class="line">    <span class="comment">// about the page's ID and its view count (because the class implements</span></span><br><span class="line">    <span class="comment">// `Processor&lt;PageId, Long&gt;`).</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">punctuate</span><span class="params">(<span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Stays empty.  In this use case there would be no need for a periodical</span></span><br><span class="line">    <span class="comment">// action of this processor.</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Any code for clean up would go here.</span></span><br><span class="line">    <span class="comment">// This processor instance will not be used again after this call.</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>就像前面提到的，一个流处理算子可以通过调用ProcessorContext#getStateStore()方法访问任何可用的状态存储。可用指的是：这些状态存储的名称在调用KStream#process()方法时指定（注意和Processor#process()方法不同，KStream#process()是在构造拓扑时定义）。</p>
<h4 id="WRITING_STREAMS_BACK_TO_KAFKA">WRITING STREAMS BACK TO KAFKA</h4><p>Any streams may be (continuously) written back to a Kafka topic via KStream#to() and KTable#to().</p>
<p>任何的流都可能会通过KStream#to()和KTable#to()方法持续地（将记录）写到Kafka主题中。</p>
<figure class="highlight fsharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Write the stream userCountByRegion to the output topic 'RegionCountsTopic'</span></span><br><span class="line">userCountByRegion.<span class="keyword">to</span>(<span class="string">"RegionCountsTopic"</span>);</span><br></pre></td></tr></table></figure>
<p>Best practice: It is strongly recommended to manually create output topics ahead of time rather than relying on auto-creation of topics. First, auto-creation of topics may be disabled in your Kafka cluster. Second, auto-creation will always apply the default topic settings such as the replicaton factor, and these default settings might not be what you want for certain output topics (cf. auto.create.topics.enable=true in the Kafka broker configuration).</p>
<blockquote>
<p>最佳实践：推荐手动创建输出主题而不是依赖于自动创建。首先自动创建主题可能会被你的Kafka集群禁用掉；其二，自动创建会运用一些默认的设置，比如副本因子，而这些默认的设置可能在某些输出主题上不是你想要的。</p>
</blockquote>
<p>If your application needs to continue reading and processing the records after they have been written to a topic via to() above, one option is to construct a new stream that reads from the output topic:</p>
<p>如果你的应用程序需要持续读取并处理的记录是通过to()方法写到的输出主题，一种方式是从输出主题中构造新的流：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Write to a Kafka topic.</span></span><br><span class="line">userCountByRegion<span class="built_in">.</span><span class="keyword">to</span>(<span class="string">"RegionCountsTopic"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read from the same Kafka topic by constructing a new stream from the</span></span><br><span class="line"><span class="comment">// topic RegionCountsTopic, and then begin processing it (here: via `map`)</span></span><br><span class="line">builder<span class="built_in">.</span>stream(<span class="string">"RegionCountsTopic"</span>)<span class="built_in">.</span><span class="built_in">map</span>(<span class="attribute">...</span>)<span class="attribute">...</span>;</span><br></pre></td></tr></table></figure>
<p>Kafka Streams provides a convenience method called through() that is equivalent to the code above:</p>
<p>Kafka Streams还提供了一种方便的方式：调用through()方法和上面的代码是类似的：</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// <span class="string">`through`</span> combines <span class="keyword">write</span>-to-Kafka-topic <span class="keyword">and</span> <span class="keyword">read</span>-from-same-Kafka-topic operations</span><br><span class="line">userCountByRegion.through(<span class="string">"RegionCountsTopic"</span>).<span class="keyword">map</span>(...)...;</span><br></pre></td></tr></table></figure>
<p>Whenever data is read from or written to a Kafka topic, Streams must know the serdes to be used for the respective data records. By default the to() and through() methods use the default serdes defined in the Streams configuration. You can override these default serdes by passing explicit serdes to the to() and through() methods.</p>
<p>当数据从一个Kafka主题读取或者写入时，流必须知道记录使用的序列化方式，默认to()和through()方法使用了Streams配置中默认的序列化方式。你可以通过传递明确的序列化器给to()和through()方法来覆写这些默认的配置。</p>
<p>Besides writing the data back to Kafka, users can also apply a custom processor as mentioned above to write to any other external stores, for example, to materialize a data store, as stream sinks at the end of the processing.</p>
<p>除了将数据写回到Kafka，用户也可以像上面提到的那样运用一个自定义的处理器，并写到其他的外部存储介质中，比如物化数据存储，作为流处理的目标。</p>
<h2 id="运行流处理程序">运行流处理程序</h2><p>A Java application that uses the Kafka Streams library can be run just like any other Java application – there is no special magic or requirement on the side of Kafka Streams.</p>
<p>一个使用Kafka Streams客户端库的Java应用程序，它的运行方式和其他普通的Java应用程序一样，没有特殊的魔法，也没有额外的限制。比如你可以将你的Java应用程序打成一个fat jar包，然后启动：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># <span class="operator"><span class="keyword">Start</span> the application <span class="keyword">in</span> <span class="keyword">class</span> <span class="string">`com.example.MyStreamsApp`</span></span><br><span class="line"># <span class="keyword">from</span> the fat jar named <span class="string">`path-to-app-fatjar.jar`</span>.</span><br><span class="line">$ <span class="keyword">java</span> -cp <span class="keyword">path</span>-<span class="keyword">to</span>-app-fatjar.jar com.example.MyStreamsApp</span></span><br></pre></td></tr></table></figure>
<p>It is important to understand that, when starting your application as described above, you are actually launching what Kafka Streams considers to be one instance of your application. More than one instance of your application may be running at a time, and in fact the common scenario is that there are indeed multiple instances of your application running in parallel. See Parallelism Model for further information.</p>
<p>当启动应用程序时，在Kafka Streams看来，是启动了应用程序的一个实例。可能在同一个时间点，你的应用程序会同时运行多个实例，而实际上通常的场景的确是你的多个应用程序实例是并行执行的。</p>
<h3 id="水平扩展你的应用程序">水平扩展你的应用程序</h3><p>Kafka Streams makes your stream processing applications elastic and scalable: you can add and remove processing capacity dynamically during the runtime of your application, and you can do so without any downtime or data loss. This means that, unlike other stream processing technologies, with Kafka Streams you do not have to completely stop your application, recompile/reconfigure, and then restart. This is great not just for intentionally(故意地) adding or removing processing capacity, but also for being resilient(有弹性的) in the face of failures (e.g. machine crashes, network outages) and for allowing maintenance work (e.g. rolling upgrades).</p>
<p>Kafka Streams会让你的流处理应用程序可伸缩和可扩展。你可以在应用程序的运行时动态地添加或删除流处理能力，并且不需要停机维护，也不会丢失数据。这意味着，和其他流处理技术不同，使用Kafka Streams你不需要完全地停止你的应用程序，重新编译，重新配置，然后重启。对于故意地添加或删除处理能力，或者失败时的弹性机制，以及维护工作都是有好处的。</p>
<p>If you are wondering how this elasticity is actually achieved behind the scenes, you may want to read the Architecture chapter, notably the Parallelism Model section. In a nutshell(概括), Kafka Streams leverages existing functionality in Kafka, notably its group management functionality. This group management, which is built right into the Kafka wire protocol, is the foundation that enables the elasticity of Kafka Streams applications: members of a group will coordinate and collaborate(合作) jointly(共同地) on the consumption and processing of data in Kafka. On top of this foundation Kafka Streams provides some additional functionality, e.g. to enable stateful processing and to allow for fault-tolerante state in environment where application instances may come and go at any time.</p>
<p>如果你对如何实现扩展能力的背后机制感兴趣，可以阅读架构章节，尤其是并行模型那一部分。简单来说，Kafka Streams利用了Kafka已有的特性，尤其是组管理协议的功能。组管理协议构建在Kafka的协议之上，是Kafka Streams应用程序具有可伸缩性的基础：组的成员会协调合作，共同消费和处理Kafka中的数据。基于这些基础，Kafka Streams还提供了额外的功能，比如有状态的处理，以及在应用程序实例随时添加和删除的环境中，允许故障容错的状态存储。</p>
<h3 id="增加处理能力（Expand）">增加处理能力（Expand）</h3><p>If you need more processing capacity for your stream processing application, you can simply start another instance of your stream processing application, e.g. on another machine, in order to scale out. The instances of your application will become aware of each other and automatically begin to share the processing work. More specifically, what will be handed over from the existing instances to the new instances is (some of) the stream tasks that have been run by the existing instances. Moving stream tasks from one instance to another results in moving the processing work plus any internal state of these stream tasks (the state of a stream task will be re-created in the target instance by restoring the state from its corresponding changelog topic).</p>
<p>如果你需要为流处理应用程序添加更多的处理能力，你只需要在其他机器上简单地启动新的流处理应用程序实例，来达到扩展的目的。应用程序的所有示例都会彼此感知，并且自动地开始共享处理工作。更具体地说，从已有的实例移交给新实例的工作是在已有实例上运行的流任务。将流任务从一个实例移动到另一个实例的结果是，将这些流任务的处理工作以及内部状态都一起移动（一个流任务的状态会在目标实例中重新创建，从对应的变更日志主题中恢复数据来构造状态）。</p>
<p>The various instances of your application each run in their own JVM process, which means that each instance can leverage all the processing capacity that is available to their respective JVM process (minus the capacity that any non-Kafka-Streams part of your application may be using). This explains why running additional instances will grant your application additional processing capacity. The exact capacity you will be adding by running a new instance depends of course on the environment in which the new instance runs: available CPU cores, available main memory and Java heap space, local storage, network bandwidth, and so on. Similarly, if you stop any of the running instances of your application, then you are removing and freeing up the respective processing capacity.</p>
<p>应用程序的不同实例运行在它们各自的JVM进程中，这意味着每个实例可以利用它们对应的JVM进程的所有处理能力/资源（减去应用程序中不是Kafka Streams部分使用的资源）。这就解释了为什么运行额外的实例可以提升应用程序的处理能力。不过运行一个新实例期望新增加的处理能力当然会和新实例所在的环境有关：比如可用的CPU核，可用的主内存和Java堆空间大小，本地存储，网络带宽等等。同样，如果你停止了运行中的任意一个实例，你就删除并且释放掉相应的处理能力。</p>
<p><img src="http://img.blog.csdn.net/20161105125241586" alt="kstream expand"></p>
<p>Before adding capacity: only a single instance of your Kafka Streams application is running. At this point the corresponding Kafka consumer group of your application contains only a single member (this instance). All data is being read and processed by this single instance.</p>
<p>After adding capacity: now two additional instances of your Kafka Streams application are running, and they have automatically joined the application’s Kafka consumer group for a total of three current members. These three instances are automatically splitting the processing work between each other. The splitting is based on the Kafka topic partitions from which data is being read.</p>
<p>上图左边是添加处理能力之前，只有一个Kafka Streams应用程序实例在运行，这时你的应用程序对应的Kafka消费组只有一个成员（就是这个实例），所有的数据都通过这个唯一的实例读取并处理。右图是添加处理能力之后，现在增加了两个额外的应用程序运行实例，而且它们自动加入到应用程序对应的Kafka消费组中，这个消费组目前总共有3个成员。这三个实例两两之间都会自动地均摊处理工作。分摊是基于Kafka的主题分区，即（每个实例）从不同分区读取不同的数据。</p>
<h3 id="减少处理能力（Shrink）">减少处理能力（Shrink）</h3><p>If you need less processing capacity for your stream processing application, you can simply stop one or more running instances of your stream processing application, e.g. shut down 2 of 4 running instances. The remaining instances of your application will become aware that other instances were stopped and automatically take over the processing work of the stopped instances. More specifically, what will be handed over from the stopped instances to the remaining instances is the stream tasks that were run by the stopped instances. Moving stream tasks from one instance to another results in moving the processing work plus any internal state of these stream tasks (the state of a stream task will be re-created in the target instance by restoring the state from its corresponding changelog topic).</p>
<p>如果你的流处理应用程序需要较少的处理能力，你只需要停止一个或多个运行的流处理应用程序即可，比如将4个运行的实例关闭掉2个。剩余的应用程序实例会感知到其他实例已经被停止了，并且会自动接管这些停掉示例的处理工作。更具体的来说，从停止实例移交给剩余实例的工作是在停止实例上运行的流任务。将流任务从一个实例移动到另一个实例的结果是，将这些流任务的处理工作以及内部状态都一起移动。</p>
<p><img src="http://img.blog.csdn.net/20161105125255977" alt="kstream shrink"></p>
<p>If one of the application instances is stopped (e.g. intentional reduction of capacity, maintenance, machine failure), it will automatically leave the application’s consumer group, which causes the remaining instances to automatically take over the stopped instance’s processing work.</p>
<p>图中如果停止（故意减少容量，维护，或者机器故障都可能停止）了一个应用程序实例，它就会自动离开应用程序的消费组，并导致剩余的示例自动接管这个停止实例的处理工作。</p>
<h3 id="运行多少个应用程序实例">运行多少个应用程序实例</h3><p>How many instances can or should you run for your application? Is there an upper limit for the number of instances and, similarly, for the parallelism of your application? In a nutshell, the parallelism of a Kafka Streams application – similar to the parallelism of Kafka – is primarily determined by the number of partitions of the input topic(s) from which your application is reading. For example, if your application reads from a single topic that has 10 partitions, then you can run up to 10 instances of your applications (note that you can run further instances but these will be idle).</p>
<p>The number of topic partitions is the upper limit for the parallelism of your Kafka Streams application and thus for the number of running instances of your application.</p>
<p>那么到底可以或者应该运行多少个应用程序实例？是否有一个数量的上限，来并行化你的应用程序？简单来说，一个Kafka Streams应用程序的并行度，类似于Kafka的并行度，主要的决定因素是：应用程序所读取的输入主题的分区数量。比如你的应用程序读取的一个主题有10个分区，那么你就可以运行最多10个应用程序实例（虽然你可以运行更多的实例，但是会有一些实例是空闲的）。所以，主题分区的数量是你的流处理应用程序并行度的上限，因此也是你的应用程序运行实例的上限。</p>
<p>How to achieve(获得) a balanced processing workload across application instances to prevent processing hotpots: The balance of the processing work between application instances depends on factors such as how well data messages are balanced between partitions (think: if you have 2 topic partitions, having 1 million messages in each partition is better than having 2 million messages in the first partition and no messages in the second) and how much processing capacity is required to process the messages (think: if the time to process messages varies heavily, then it is better to spread the processing-intensive messages across partitions rather than storing these messages within the same partition).</p>
<p>那么怎么在应用程序实例之前来保证处理的负载是平衡的，防止发生处理热点。工作负载是否平衡的决定因素是分区的数据有多平衡（比如你有两个分区，每个分区有一百万条消息要比一个分区有两百万条消息，而另外一个分区没有一条消息要好的多），以及消息的处理能力（比如处理消息的时间变化很大，那么将处理比较耗时的消息分散在多个分区，要比这些消息都存储在一个分区也要好得多）。</p>
<p>If your data happens to be heavily skewed(倾斜) in the way described above, some application instances may become processing hotspots (say, when most messages would end up being stored in only 1 of 10 partitions, then the application instance that is processing this one partition would be performing most of the work while other instances might be idle). You can minimize the likelihood of such hotspots by ensuring better data balancing across partitions (i.e. minimizing data skew at the point in time when data is being written to the input topics in Kafka) and by over-partitioning the input topics (think: use 50 or 100 partitions instead of just 10), which lowers the probability that a small subset of partitions will end up storing most of the topic’s data.</p>
<p>如果你的数据恰巧倾斜的很严重，有一些应用程序实例就会变成处理热点。你可以通过确保数据在分区之间有更好的平衡来最小化出现这种热点的可能性（比如当大部分的消息都只存储在10个分区中的一个时，那么处理这个分区的应用程序实例就会处理大部分的工作，而其他实例则可能很空闲），或者对输入主题采用更多的分区数（比如在数据写入到Kafka的输入主题是就尽量最小化数据的倾斜）来减少一个很小的分区字节存储了大部分主题数据的这种可能性。</p>
<h2 id="数据类型和序列化">数据类型和序列化</h2><h2 id="应用重置工具">应用重置工具</h2><p>The Application Reset Tool allows you to quickly reset an application in order to reprocess its data from scratch – think: an application “reset” button. Scenarios when would you like to reset an application include:</p>
<p>应用程序重置工具允许你快速地重置一个应用程序，然后重新处理数据，可以认为是应用程序的一个“重置”按钮。需要重置应用程序的场景包括：</p>
<ul>
<li>Development and testing 开发和测试时</li>
<li>Addressing bugs in production 在生产环境定位问题时</li>
<li>Demos 演示</li>
</ul>
<p>However, resetting an application manually is a bit involved. Kafka Streams is designed to hide many details about operator state, fault-tolerance, and internal topic management from the user (especially when using Kafka Streams DSL). In general this hiding of details is very desirable(值得要的，令人满意的) but it also makes manually resetting an application more difficult. The Application Reset Tool fills this gap and allows you to quickly reset an application.</p>
<p>不过，手动方式重置一个应用程序需要做很多工作。Kafka Streams提供了一个重置工具，帮你隐藏了很多细节，比如操作状态，故障容错，内部的主题管理。</p>
<p><strong>用户主题和内部主题</strong>  </p>
<p>在Kafka Streams中，我们会区分用户主题和内部主题，这两种都是普通的Kafka主题，不过对于内部主题，有一些特定的命名约定。</p>
<p>用户主题包括输入主题、输出主题、临时主题。这些主题是用户创建或管理的，包括应用程序的输入和输出主题，以及通过through()方法指定的临时主题，临时主题实际上同时既是输出也是输入主题。</p>
<p>内部主题是由Kafka Streams底层自动创建的。比如针对状态存储的变更日志主题就是一个内部主题。内部主题的命名约定是：<application.id>-<operatorname>-<suffix>。</suffix></operatorname></application.id></p>
<p>应用程序重置工具做的工作有：  </p>
<ol>
<li>对输入主题：重置应用程序所有分区的消费者提交偏移量到0</li>
<li>对临时主题：跳到主题的最后，比如设置应用程序的消费者提交偏移量到每个分区的logSize（实际上就是分区的最后位置）</li>
<li>对内部主题：除了重置偏移量到0，还要删除内部主题</li>
</ol>
<p>应用程序重置工具不做的：</p>
<ol>
<li>不会重置应用程序的输出主题。如果任何的输出主题或者临时主题被下游的应用程序消费，那么调整这些下游应用程序是你自己的责任</li>
<li>不会重置应用程序实例的本地环境。同样删除应用程序实例运行所在机器的本地状态，也是你自己的责任。</li>
</ol>
<p><strong>步骤1：运行重置工具</strong>  </p>
<p>执行<code>bin/kafka-streams-application-reset</code>命令，需要指定如下的参数：</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Option (* = required)         Description</span><br><span class="line">-<span class="ruby">--------------------         -----------</span><br><span class="line"></span>* --application-id &lt;id&gt;       The Kafka Streams application ID (application.id)</span><br><span class="line">-<span class="ruby">-bootstrap-servers &lt;urls&gt;    <span class="constant">Comma</span>-separated list of broker urls with <span class="symbol">format:</span> <span class="constant">HOST1</span><span class="symbol">:PORT1</span>,<span class="constant">HOST2</span><span class="symbol">:PORT2</span></span><br><span class="line"></span>                                (default: localhost:9092)</span><br><span class="line">-<span class="ruby">-intermediate-topics &lt;list&gt;  <span class="constant">Comma</span>-separated list of intermediate user topics</span><br><span class="line"></span>-<span class="ruby">-input-topics &lt;list&gt;         <span class="constant">Comma</span>-separated list of user input topics</span><br><span class="line"></span>-<span class="ruby">-zookeeper &lt;url&gt;             <span class="constant">Format</span><span class="symbol">:</span> <span class="constant">HOST</span><span class="symbol">:POST</span></span><br><span class="line"></span>                                (default: localhost:2181)</span><br></pre></td></tr></table></figure>
<p>You can combine the parameters of the script as needed. For example, if an application should only restart from an empty internal state but not reprocess previous data, simply omit the parameters –input-topics and –intermediate-topics.</p>
<p>你可以任意组合上面的参数，比如如果应用程序只会从空的内部状态重启，不会重新处理已有的数据，可以忽略输入主题和临时主题这两个参数。</p>
<p>On intermediate topics: In general, we recommend to manually delete and re-create any intermediate topics before running the application reset tool. This allows to free disk space in Kafka brokers early on. It is important to first delete and re-create intermediate topics before running the application reset tool.</p>
<p>Not deleting intermediate topics and only using the application reset tool is preferable:</p>
<ul>
<li>when there are external downstream consumers for the application’s intermediate topics</li>
<li>during development, where manually deleting and re-creating intermediate topics might be cumbersome and often unnecessary</li>
</ul>
<p>关于临时主题：通常我们推荐在运行重置工具之前，手动删除然后重建临时主题。这样可以尽早释放Kafka的集群磁盘空间。什么时候不需要删除临时主题：</p>
<ol>
<li>存在外部的下游消费者订阅了应用程序的临时主题</li>
<li>在开发环境，手动删除并重建主题可能很繁琐，而且通常没有必要这么做</li>
</ol>
<p><strong>步骤2：重置本地环境</strong>  </p>
<p>Running the application reset tool (step 1) ensures that your application’s state – as tracked globally in the application’s configured Kafka cluster – is reset. However, by design the reset tool does not modify or reset the local environment of your application instances, which includes the application’s local state directory.</p>
<p>For a complete application reset you must also delete the application’s local state directory on any machines on which an application instance was run prior to restarting an application instance on the same machines. You can either use the API method KafkaStreams#cleanUp() in your application code or manually delete the corresponding local state directory (default location: /var/lib/kafka-streams/<application.id>, cf. state.dir configuration parameter).</application.id></p>
<p>运行步骤1的应用程序重置工具确保你的应用程序状态被重置（应用程序的状态实际上是在应用程序配置的Kafka集群被全局地跟踪）。不过，重置工具被设计的时候，并不会修改或重置应用程序实例的本地环境，包括应用程序的本地状态目录。</p>
<p>如果要彻底重置应用程序，你必须删除应用程序的本地状态目录，而且任何之前运行过的应用程序实例所在的机器都需要在重启应用程序实例之前删除干净。你可以在应用程序中调用KafkaStreams#cleanUp()方法，或者手动删除对应的本地状态目录（默认的路径是： /var/lib/kafka-streams/<application.id>，即state.dir的配置参数）来清理。</application.id></p>
<p><strong>示例</strong>  </p>
<p>Let’s imagine you are developing and testing an application locally and want to iteratively improve your application via run-reset-modify cycles. You might have code such as the following:</p>
<p>假设你在本地开发和测试一个应用程序，并且通过运行-重置-修改的循环开发模式来不断迭代提升你的应用程序，你可能需要这样的代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> ResetDemo &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> throws Exception </span>&#123;</span><br><span class="line">    <span class="comment">// Kafka Streams configuration</span></span><br><span class="line">    Properties streamsConfiguration = <span class="keyword">new</span> Properties();</span><br><span class="line">    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"my-streams-app"</span>);</span><br><span class="line">    <span class="comment">// ...and so on...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Define the processing topology</span></span><br><span class="line">    KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line">    builder.stream(<span class="string">"my-input-topic"</span>)</span><br><span class="line">        .selectKey(...)</span><br><span class="line">        .through(<span class="string">"rekeyed-topic"</span>)</span><br><span class="line">        .countByKey(<span class="string">"global-count"</span>)</span><br><span class="line">        .to(<span class="string">"my-output-topic"</span>);</span><br><span class="line"></span><br><span class="line">    KafkaStreams app = <span class="keyword">new</span> KafkaStreams(builder, streamsConfiguration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Delete the application's local state.</span></span><br><span class="line">    <span class="comment">// <span class="doctag">Note:</span> In real application you'd call `cleanUp()` only under</span></span><br><span class="line">    <span class="comment">// certain conditions.  See tip on `cleanUp()` below.</span></span><br><span class="line">    app.cleanUp();</span><br><span class="line"></span><br><span class="line">    app.start();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// <span class="doctag">Note:</span> In real applications you would register a shutdown hook</span></span><br><span class="line">    <span class="comment">// that would trigger the call to `app.close()` rather than</span></span><br><span class="line">    <span class="comment">// using the sleep-then-close example we show here.</span></span><br><span class="line">    Thread.sleep(<span class="number">30</span> * <span class="number">1000L</span>);</span><br><span class="line">    app.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Calling cleanUp() is safe but do so judiciously(明智的): It is always safe to call KafkaStreams#cleanUp() because the local state of an application instance can be recovered from the underlying internal changelog topic(s). However, to avoid the corresponding recovery overhead it is recommended to not call cleanUp() unconditionally and every time an application instance is restarted/resumed. A production application would therefore use e.g. command line arguments to enable/disable the cleanUp() call as needed.</p>
<p>调用cleanUp()方法是安全的，不过要谨慎调用：调用KafkaStreams#cleanUp()总是安全的，因为应用程序实例的本队状态可以从底层的内部变更日志主题恢复过来。不过，为了防止由此产生的恢复开销，推荐不要盲目调用cleanUp()，或者说在每次应用程序重启/恢复的时候就调用cleanUp()。生产环境下的应用程序因此会使用命令行的参数来允许或禁止调用cleanUp()。</p>
</blockquote>
<p>然后你就可以执行如下的“run-reset-modify”循环：</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run your application</span></span><br><span class="line">$ bin/kafka-<span class="command">run</span>-<span class="type">class</span> io.confluent.examples.streams.ResetDemo</span><br><span class="line"></span><br><span class="line"><span class="comment"># After stopping all application instances, reset the application</span></span><br><span class="line">$ bin/kafka-streams-<span class="type">application</span>-reset <span class="comment">--application-id my-streams-app \</span></span><br><span class="line">                                      <span class="comment">--input-topics my-input-topic \</span></span><br><span class="line">                                      <span class="comment">--intermediate-topics rekeyed-topic</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Now you can modify/recompile as needed and then re-run the application again.</span></span><br><span class="line"><span class="comment"># You can also experiment, for example, with different input data without</span></span><br><span class="line"><span class="comment"># modifying the application.</span></span><br></pre></td></tr></table></figure>
<p>EOF. 翻译完毕 @2016.11.5</p>
<h2 id="重置流处理应用程序">重置流处理应用程序</h2><p>Confluent关于重置的实现翻译：<a href="https://www.confluent.io/blog/data-reprocessing-with-kafka-streams-resetting-a-streams-application/" target="_blank" rel="external">https://www.confluent.io/blog/data-reprocessing-with-kafka-streams-resetting-a-streams-application/</a>  </p>
<h3 id="示例">示例</h3><p>As a running example we assume you have the following Kafka Streams application, and we subsequently demonstrate (1) how to make this application “reset ready” and (2) how to perform an actual application reset.</p>
<p>下面的Kafka Streams应用程序，我们会展示两个步骤：1）怎么让应用程序开始准备重置，2）怎么执行真正的应用程序重置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ResetDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// Kafka Streams configuration</span></span><br><span class="line">        <span class="keyword">final</span> Properties streamsConfiguration = <span class="keyword">new</span> Properties();</span><br><span class="line">        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"my-streams-app"</span>);</span><br><span class="line">        <span class="comment">// make sure to consume the complete topic via "auto.offset.reset = earliest"</span></span><br><span class="line">        streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line">        <span class="comment">// ...and so on...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// define the processing topology</span></span><br><span class="line">        <span class="keyword">final</span> KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line">        builder.stream(<span class="string">"my-input-topic"</span>)</span><br><span class="line">            .selectKey(...)</span><br><span class="line">            .through(<span class="string">"rekeyed-topic"</span>)</span><br><span class="line">            .countByKey(<span class="string">"global-count"</span>)</span><br><span class="line">            .to(<span class="string">"my-output-topic"</span>);</span><br><span class="line"></span><br><span class="line">         <span class="comment">// ...run application...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>This application reads data from the input topic “my-input-topic”, then selects a new record key, and then writes the result into the intermediate topic “rekeyed-topic” for the purpose of data re-partitioning. Subsequently, the re-partitioned data is aggregated by a count operator, and the final result is written to the output topic “my-output-topic”.  Note that in this blog post we don’t put the focus on what this topology is actually doing — the point is to have a running example of a typical topology that has input topics, intermediate topics, and output topics.</p>
<p>这个应用程序会从输入主题“my-input-topic”读取数据，然后选择新的记录key，将结果写到临时的主题“rekeyed-topic”，目的是将数据重新分区。随后，重新分区的数据会通过count算子被聚合，最终的结果写到输出主题“my-output-topic”。本篇博客我们不会将重点放到拓扑是怎么工作的，重点是典型的拓扑，会有输入主题，临时主题，输出主题。</p>
<h4 id="Step_1:_Prepare_your_application_for_resets">Step 1: Prepare your application for resets</h4><p>The first step is to make your application “reset ready”. For this, the only thing you need to do is to include a call to KafkaStreams#cleanUp() in your application code (for details about cleanUp() see Section ”Application Reset Tool Details”).</p>
<p>Calling cleanUp() is required because resetting a Streams applications consists of two parts: global reset and local reset. The global reset is covered by the new application reset tool (see “Step 2”), and the local reset is performed through the Kafka Streams API. Because it is a local reset, it must be performed locally for each instance of your application. Thus, embedding it in your application code is the most convenient way for a developer to perform a local reset of an application (instance).</p>
<p>步骤1是让你的应用程序准备好重置，你需要做的是在你的应用程序代码包含对KafkaStreams#cleanUp()方法的调用。调用cleanUp()是必要的，因为重置一个流应用程序包括两个部分：全局的重置和本地的重置。全局的重置会通过新的应用程序重置工具完成，本地的重置是通过Kafka Streams的API完成的。因为它（即调用Kafka Streams API）是一个本地的重置，你的应用程序的每个实例都应该本地地执行。因此将它嵌入到应用程序代码中，对于开发者而言这是本队重置一个应用程序实例的最方便做法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ResetDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="comment">// ...prepare your application configuration and processing topology...</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> KafkaStreams streams = <span class="keyword">new</span> KafkaStreams(builder, streamsConfiguration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Delete the application's local state.</span></span><br><span class="line">        <span class="comment">// <span class="doctag">Note:</span> In real application you'd call `cleanUp()` only under certain conditions.</span></span><br><span class="line">        <span class="comment">// See Confluent Docs for more details:</span></span><br><span class="line">        <span class="comment">// http://docs.confluent.io/3.0.1/streams/developer-guide.html#step-2-reset-the-local-environments-of-your-application-instances</span></span><br><span class="line">        streams.cleanUp();</span><br><span class="line"></span><br><span class="line">        streams.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Add shutdown hook to respond to SIGTERM and gracefully close Kafka Streams</span></span><br><span class="line">        Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="annotation">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                streams.close();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>At this point the application is ready for being reset (when needed), and you’d start one or multiple instances of your application as usual, possibly on different hosts.</p>
<p>现在应用程序已经准备好重置了，你启动一个或多个应用程序的方式跟平常一样，可能会在不同的节点启动多个实例。</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run your application</span></span><br><span class="line"><span class="comment"># For the sake of this short blog post we use the Kafka helper script</span></span><br><span class="line"><span class="comment"># `kafka-run-class` here, but your mileage may vary.</span></span><br><span class="line"><span class="comment"># You could also, for example, call `java …` directly. Remember that</span></span><br><span class="line"><span class="comment"># an application that uses the Kafka Streams library is a standard Java application.</span></span><br><span class="line"><span class="variable">$ </span>bin/kafka-run-<span class="class"><span class="keyword">class</span> <span class="title">io</span>.<span class="title">confluent</span>.<span class="title">examples</span>.<span class="title">streams</span>.<span class="title">ResetDemo</span></span></span><br></pre></td></tr></table></figure>
<h4 id="Step_2:_Reset_the_application">Step 2: Reset the application</h4><p>So what would we need to do to restart this application from scratch, i.e., not resume the processing from the point the application was stopped before, but rather to reprocess all its input data again?</p>
<p>First you must stop all running application instances and make sure the whole consumer group is not active anymore (you can use bin/kafka-consumer-groups to list active consumer groups). Typically, the consumer group should become inactive one minute after you stopped all the application instances. This is important because the reset behavior is undefined if you use the reset tool while some application instances are still running — the running instances might produce wrong results or even crash.</p>
<p>那么我们怎么从头开始重启这个应用程序呢，比如不是从应用程序上次停止的地方继续回复处理，而是重新处理所有的输入数据？</p>
<p>首先，你必须要停止所有正在运行的应用程序实例，确保整个消费组是不活动的（你可以使用kafka-consumer-groups来列出仍然存活的消费组）。通常，消费组会在你停止所有的应用程序实例之后的一分钟状态才为不活动。这非常重要，如果你使用了重置工具，但同时有一些应用程序仍然在运行，重置这个动作是不确定的，因为正在运行的示例可能会产生错误的结果，甚至挂掉（实际上就是说先停止所有的应用程序实例，然后检查消费组不活动，最后才可以使用重置工具）。</p>
<p>Once all application instances are stopped you can call the application reset tool as follows:</p>
<p>当所有的应用程序实例都停止后，你可以像下面那样使用应用程序重置工具：</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># After stopping all application instances, reset the application</span></span><br><span class="line">$ bin/kafka-streams-<span class="type">application</span>-reset <span class="comment">--application-id my-streams-app \</span></span><br><span class="line">                                      <span class="comment">--input-topics my-input-topic \</span></span><br><span class="line">                                      <span class="comment">--intermediate-topics rekeyed-topic \</span></span><br><span class="line">                                      <span class="comment">--bootstrap-servers brokerHost:9092 \</span></span><br><span class="line">                                      <span class="comment">--zookeeper zookeeperHost:2181</span></span><br></pre></td></tr></table></figure>
<p>As you can see, you only need to provide the application ID (“my-streams-app”) as specified in your application configuration, and the names of all input and intermediate topics. Furthermore, you might need to specify Kafka connection information (bootstrap servers) and Zookeeper connection information. Both parameters have default values localhost:9092 and localhost:2181, respectively. Those are convenient to use during development with a local single Zookeeper/single broker setup. For production/remote usage you need to provide appropriate host:port values.</p>
<p>可以看到，你需要提供一个应用程序编号（“my-streams-app”，这是在你的应用程序配置中指定的），以及所有的输入主题和临时主题的名称。此外，你可能还需要指定Kafka以及ZooKeeper的连接信息。</p>
<p>Once the application reset tool has completed its run (and its internal consumer is not active anymore), you can restart your application as usual, and it will now reprocess its input data from scratch again. We’re done!</p>
<p>一旦应用程序重置工具完成后，你可以像平常那样重启你的应用程序了，现在它就会从头开始重新处理输入数据了！</p>
<p>It’s important to highlight that, to prevent possible collateral(并行，附属) damage, the application reset tool does not reset the output topics of an application. If any output (or intermediate) topics are consumed by downstream applications, it is your responsibility to adjust those downstream applications as appropriate when you reset the upstream application.</p>
<p>很重要的一点是，为了防止可能的附属损坏，应用程序重置工具并不会重置输出主题。如果有任何的输出主题（或者临时主题）被下游的应用程序所消费，那么这就是你的责任来调整这些下游的应用程序了（当你重置上游的应用程序时）。</p>
<p>Use application reset tool with care and double-check its parameters: If you provide wrong parameter values (e.g. typos in application.id) or specify parameters inconsistently (e.g. specifying the wrong input topics for the application), this tool might invalidate the application’s state or even impact other applications, consumer groups, or Kafka topics of your Kafka cluster.</p>
<p>使用应用程序重置工具要很小心，并且要仔细检查它的参数：如果你提供了一个错误的参数值（比如应用程序编号写错了），或者指定了不一致的参数（比如指定了这个应用程序错误的输入主题），该工具可能会使得应用程序的状态失效，甚至影响其他的应用程序，消费组，或者你的Kafka集群的主题。</p>
<p>As we have shown above, using the new application reset tool you can easily reprocess data from scratch with Kafka Streams. Perhaps somewhat surprisingly, there’s actually a lot going on behind the scenes to make application resets work as easily.  The following sections are a deep dive on these internals for the curious reader.</p>
<p>使用新的应用程序重置工具，你可以使用Kafka Streams很容易地从头开始重新处理数据。你可能会觉得有点奇怪，不过实际上为了让应用程序重置工作的很简单，后台做了很多的工作。</p>
<h3 id="Behind_the_Scenes_of_Kafka_Streams">Behind the Scenes of Kafka Streams</h3><p>In this second part of the blog post we discuss those Kafka Streams internals that are required to understand the details of a proper application reset. Figure 1 shows a Kafka Streams application before its first run. The topology has as single input topic with two partitions. The current offset of each partition is zero (or there is no committed offsets and parameter auto.offset.reset = earliest is used). Also the topology writes into a single output topic with two partitions which are both empty. No offsets are shown as output topics are not consumed by the application itself. Furthermore, the topology contains a call to through(), thus, it writes/reads into/from an additional (intermediate) topic with two empty partitions and it contains a stateful operator.</p>
<p>下图展示了一个Kafka Streams应用程序第一次运行之前的状态。这个拓扑只有一个输入主题（两个分区）。当前每个分区的偏移量是0（或者说没有提交偏移量，而且使用了参数auto.offset.reset = earliest，表示没有偏移量或者偏移量超出时，会重置到分区的最开始位置，即偏移量=0的位置）。同时拓扑还会写到一个输出主题，同样也有两个分区，现在都还是空的。还没有偏移量，因为输出主题还没有从应用程序中消费数据。另外，拓扑包括了through()调用，因此它会写入/读取临时主题，这个临时主题也有两个空的分区，并且包括了有状态的操作算子。</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/application-reset-01_before-first-run.png" alt=""></p>
<p>After the Kafka Streams application was executed and stopped, the application state changed as shown in Figure 2. In the following we discuss the relevant parts of Figure 2 with regard to reprocessing.</p>
<p>Figure 2: The application after it was stopped. The small vertical arrows denote committed consumer offsets for the input and intermediate topics (colors denote the corresponding sub-topology). You can see, for example, that sub-topology A has so far written more data to the intermediate topic than sub-topology B has been able to consume (e.g. the last message written to partition 1 has offset 7, but B has only consumed messages up to offset 4). Also, sub-topology performs stateful operations and thus has created a local state store and an accompanying(伴随) internal changelog topic for this state stores.</p>
<p>当Kafka Streams应用程序执行（一段时间后），然后停止，应用程序的状态改变如下图。（分区上的）垂直箭头表示输入主题和临时主题的消费者提交偏移量。可以看到子拓扑A写入到临时主题的数据要比子拓扑B已经消费者的数据要多很多（A的临时主题分区1最近写入消息的偏移量是7，但是B只消费到了偏移量4的位置）。同时，子拓扑因为执行了有状态的操作，所以创建了一个本地状态存储，伴随了针对这个状态存储的内部变更日志主题。</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/application-reset-02_after-stopped.png" alt=""></p>
<h4 id="reprocessing_Input_Topics">reprocessing Input Topics</h4><p>Kafka Streams builds upon existing Kafka functionality to provide scalability and elasticity, security, fault-tolerance, and more. For reprocessing input topics from scratch, one important concern is Kafka Streams’ fault-tolerance mechanism. If an application is stopped and restarted, per default it does not reread previously processed data again, but it resumes processing where it left off when it was stopped (cf. committed input topic offsets in Figure 2).<br>Internally, Kafka Streams leverages Kafka’s consumer client to read input topics and to commit offsets of processed messages in regular intervals (see commit.interval.ms). Thus, on restart an application does not reprocess the data from its previous run. In case that a topic was already processed completely, the application will start up but then be idle and wait until new data is available for processing. So one of the steps we need to take care of when manually resetting an application is to ensure that input topics are reread from scratch. However, this step alone is not sufficient to get a correct reprocessing result.</p>
<p>Kafka Streams构建在Kafka已有的功能之上来提供扩展性、伸缩性、安全性、故障容错等等。对于从头开始重新处理输入主题，一个重要的关注点是Kafka Streams的故障容错机制。如果一个应用程序停止并重启，默认情况下它不会重新读取已经处理过的数据，而是从上一次停止时离开的地方恢复处理（图2中为输入主题提交偏移量）。内部实现中，Kafka Streams利用了Kafka的消费者客户端读取输入主题，然后定期地为处理完的消息提交偏移量。因此应用程序重启的时候，不会重新处理上一次运行过的数据。如果有一种场景是：一个主题已经都被处理完成了，应用程序启动之后会空转，直到有新数据可用时才（应用程序才）会处理。所以当手动重置一个应用程序时，要非常小心的一个步骤是：确保输入主题从头开始重新读取。不过，单独这个步骤并不足以得到一个正确的重新处理的结果。</p>
<h4 id="Sub-Topologies_and_Internal_State_in_Kafka_Streams">Sub-Topologies and Internal State in Kafka Streams</h4><p>First, an application can consist of multiple sub-topologies that are connected via intermediate or internal topics (cf. Figure 2 with two sub-topologies A and B). In Kafka Streams, intermediate topics are user-specified topics that are used as both an input and an output topic within a single application (e.g., a topic that is used in a call to through()). Internal topics are those topics that are created by Kafka Streams “under the hood” (e.g., internal repartitioning topics which are basically internal intermediate topics).</p>
<p>首先，一个应用程序可以包含多个子拓扑，它们会通过临时或中间主题互相连接（比如图2中两个子拓扑A和B是通过临时主题相连接在一起的）。在Kafka Streams中，临时的主题是用户指定的，在一个应用程序中会同时作为输入和输出主题（通过调用through()方法定义的就是临时主题）。内部的主题是由Kafka Streams底层创建的（内部重新分区的主题，基本上是内部的临时主题）。</p>
<p>If there are multiple sub-topologies, it might happen that an upstream sub-topology produces records faster into intermediate topics than a downstream sub-topology can consume (cf. committed intermediate topic offsets in Figure 2).  Such a consumption delta (cf. the notion of consumer lag in Kafka) within an application would cause problems in the context of an application reset because, after an application restart, the downstream sub-topology would resume reading from intermediate topics from the point where it stopped before the restart. While this behavior is very much desired during normal operations of your application, it would lead to data inconsistencies when resetting an application. For a proper application reset we must therefore tell the application to skip to the very end of any intermediate topics.</p>
<p>如果有多个子拓扑，有可能会发生上游的子拓扑生产记录到临时主题，要比下游子拓扑消费的速度快（比如图2中临时主题的提交偏移量）。应用程序的这种消费差距（即Kafka中消费者的落后进度，用lag表示）在一个应用程序的重置场景下可能会导致出现问题，因为当一个应用程序重启后，下游的子拓扑会在重启之前的上一次停止位置，从临时主题恢复读取数据。虽然这种行为在应用程序正常的操作时是你非常想要的结果，但是在重置一个应用程序时，则可能会导致数据的不一致性。对于正确的应用程序重置方式，我们必须告诉应用程序跳到任何一个临时主题的最后位置。</p>
<p>Second, for any stateful operation like aggregations or joins, the internal state of these operations is written to a local state store that is backed by an internal changelog topic (cf. sub-topology B in Figure 2). On application restart, Kafka Streams “detects” these changelog topics and any existing local state  data, and it ensures that the internal state is fully built up and ready before the actual processing starts. To reset an application we must therefore also reset the application’s internal state, which means we must delete all its local state stores and their corresponding internal changelog topics.</p>
<p>其次，对于任何有状态的操作比如聚合或联合，这些操作的内部状态会被写入到一个临时状态存储中，这个存储也依赖于一个内部的变更日志主题（比如图2的子拓扑B）。在应用程序重启是，Kafka Streams会检测到这些变更日志流，以及已经存在的本地状态数据，它会确保内部状态被完整地构建起来，并且会在实际的处理开始之前准备完毕。所以为了重置一个应用程序，我们也必须要重置应用程序的内部状态，这意味着我们必须要删除所有的本地状态存储，以及对应的内部变更日志主题。</p>
<p>If you are interested in more details than we could cover in this blog post, please take a look at Kafka Streams: Internal Data Management in the Apache Kafka wiki.</p>
<h4 id="Resetting_a_Kafka_Streams_Application_Manually">Resetting a Kafka Streams Application Manually</h4><p>In order to reprocess topics from scratch, it is required to reset the application state that consists of multiple parts as described above:<br>为了从头开始重新处理主题，重置应用程序的状态要重置以下数据：</p>
<ol>
<li>committed offsets of input topics 输入主题的提交偏移量</li>
<li>committed offsets of intermediate topics 临时主题的提交偏移量</li>
<li>content and committed offsets of internal topics 内容，以及内部主题的提交偏移量</li>
<li>local state store 本地状态存储</li>
</ol>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/application-reset-03_after-reset_1.png" alt=""></p>
<p>Figure 3: The application after reset: (1) input topic offsets were reset to zero (2) intermediate topic offsets were advanced to end (3) internal topics were deleted (4) any local state stores were deleted (5) the output topic was not modified.</p>
<p>图3中应用程序在重置后：1）输入主题的偏移量被重置为0；2）临时主题的偏移量被跳跃到最后；3）内部主题被删除；4）任何的本地状态存储被删除；5）输出主题不会被修改</p>
<p><strong>Committed offsets of input topics</strong>: Internally, Kafka Streams leverages Kafka’s consumer client to read a topic and to commit offsets of processed messages in regular intervals (see commit.interval.ms). Thus, as a first step to reprocess data, the committed offsets need to be reset. This can be accomplished as follows: Write a special Kafka client application (e.g., leveraging Kafka’s Java consumer client or any other available language) that uses the application.id of your Kafka Streams application as its consumer group ID. The only thing this special client application does is to seek to offset zero for all partitions of all input topics and commit the offset (you should disable auto commit for this application). As this special application uses the same group ID as your Kafka Streams application (the application ID is used a consumer group ID internally), committing all offsets to zero allows your Streams application to consume its input topics from scratch when it is started again (cf. #1 in Figure 3).</p>
<p>输入主题的提交偏移量：在内部实现中，Kakfa Streams利用了Kafka消费者客户端来读取一个主题，并且定期地提交已经处理过的消息的偏移量。因此作为重新处理数据的第一个步骤，提交偏移量需要被重置。完成这一步可以这么做：编写一个特殊的Kafka消费者应用程序，使用流处理应用程序的<code>application.id</code>作为消费组名称。这个特殊的消费者唯一要做的事情是为所有输入主题的所有分区定位到偏移量=0，然后提交偏移量（这个客户端程序应该禁用自动提交偏移量）。由于这个特殊的消费者应用程序使用了和Kafka Streams应用程序相同的消费组编号（Kafka Streams在内部也会将application.id作为消费组编号），提交所有的偏移量到0，这样就允许你的流应用程序再次启动的时候可以从头开始消费输入主题（图3的步骤1）。</p>
<p><strong>Intermediate topics</strong>: For intermediate topics we must ensure to not consume any data from previous application runs. The simplest and recommended way is to delete and recreate those intermediate topics (recall that it is recommended to create user-specified topics manually before you run a Kafka Streams application). Additionally, it is required to reset the offsets to zero for the recreated topics (same as for input topics). Resetting the offsets is important because the application would otherwise pick up those invalid offsets on restart.</p>
<p>对于临时主题，我们要保证不能从上次应用程序运行的地方消费任何数据。最简单和推荐的方式是删除并重建这些临时主题（回顾下前面我们也说过推荐在运行Kafka Streams应用程序之前手动创建用户指定的主题）。另外，也需要将重新创建的主题的偏移量重置到0，这非常重要，否则应用程序在重启的时候可能会获得无效的偏移量。</p>
<p>As an alternative(供替代的选择), it is also possible to only modify the committed offsets for intermediate topics. You should consider this less invasive(侵入性) approach when there are other consumers for intermediate topics and thus deleting the topics is not possible. However, in contrast to modifying the offsets of input topics or deleted intermediate topic, the offsets for kept intermediate topics must be set to the largest value (i.e., to the current log-size) instead of zero, thus skipping over any not-yet consumed data. This ensure that, on restart, only data from the new run will be consumed by the application (cf. #2 in Figure 3). This alternative approach is used by the application reset tool.</p>
<p>除了删除和重建临时主题这个选择外，也可以值修改临时主题的提交偏移量。当临时主题有其他消费者时，可以采用这种方案较少侵入性的方案，因为（主题有消费者时）删除主题是不可能的。不过和修改输入主题、删除主题不同的是，临时主题的偏移量必须要设置到最大的值（当前的logSize），而不是0，这样就会跳过还没有被消费的任意数据。确保了在重启的时候，只有新的数据才会被应用程序消费（图3中的步骤2），应用程序重置工具使用的就是这种方法（修改临时主题的偏移量到最大）。总结下重置临时主题有两种方案：</p>
<ol>
<li>删除临时主题，重建临时主题，设置提交偏移量=0，提交偏移量</li>
<li>修改临时主题的提交偏移量到最大</li>
</ol>
<p><strong>Internal topics</strong>: Internal topics can simply be deleted (cf. #3 in Figure 3). As those are created automatically by Kafka Streams, the library can recreate them in the reprocessing case. Similar to deleting intermediate user topics, make sure that committed offsets are either deleted or set to zero.</p>
<p>内部主题可以简单地删除掉（图3的步骤3）。因为它们会被Kafka Streams自动创建，所以在重新处理的时候也会重新创建它们（内部主题）。和删除用户指定的临时主题一样，要确保提交偏移量要么删除，要么设置到0。</p>
<p>In order to delete those topics, you need to identify them. Kafka Streams creates two types of internal topics (repartitioning and state-backup) and uses the following naming convention (this naming convention could change in future releases however, which is one of the reasons we recommend the use of the application reset tool rather than manually resetting your applications):</p>
<p>为了删除内部主题，首先你要定位这些主题。Kafka Streams会创建两种类型的内部主题（重新分区和状态备份），并且会使用下面的命名约定（这个命名约定在未来的版本可能会变化，这也是推荐使用应用程序重置工具而不是手动重置应用程序的一个原因，因为应用程序重置工具可以帮我们隐藏这些内部细节，而手动重置则需要知道当前版本的命名约定）：</p>
<ul>
<li><applicationid>-<operatorname>-repartition 应用程序编号-操作符名称-repartition</operatorname></applicationid></li>
<li><applicationid>-<operatorname>-changelog 应用程序编号-操作符名称-changelog</operatorname></applicationid></li>
</ul>
<p>比如以我们前面一开始的示例为例，会创建内部主题：“my-streams-app-global-count-changelog”，因为countByKey()方法的操作符名称被指定为“global-count”。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ResetDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// Kafka Streams configuration</span></span><br><span class="line">        <span class="keyword">final</span> Properties streamsConfiguration = <span class="keyword">new</span> Properties();</span><br><span class="line">        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"my-streams-app"</span>);</span><br><span class="line">        <span class="comment">// make sure to consume the complete topic via "auto.offset.reset = earliest"</span></span><br><span class="line">        streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line">        <span class="comment">// ...and so on...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// define the processing topology</span></span><br><span class="line">        <span class="keyword">final</span> KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line">        builder.stream(<span class="string">"my-input-topic"</span>)</span><br><span class="line">            .selectKey(...)</span><br><span class="line">            .through(<span class="string">"rekeyed-topic"</span>)</span><br><span class="line">            .countByKey(<span class="string">"global-count"</span>)</span><br><span class="line">            .to(<span class="string">"my-output-topic"</span>);</span><br><span class="line"></span><br><span class="line">         <span class="comment">// ...run application...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Local State Stores</strong>: Similar to internal topics, local state stores can just be deleted (cf. #4 in Figure 3). They will be recreated automatically by Kafka Streams. All state of an application (instance) is stored in the application’s state directory (cf. parameter state.dir, with default value /var/lib/kafka-streams in Confluent Platform releases and /tmp/kafka-streams for Apache Kafka releases). Within the state store directory, each application has its own directory tree within a sub-folder that is named after the application ID. The simplest way to delete the state store for an application is therefore rm -rf <state.dir>/<application.id> (e.g., rm -rf /var/lib/kafka-streams/my-streams-app).</application.id></state.dir></p>
<p>本地状态存储：类似于内部主题，本地状态存储也可以删除（图3的步骤4），它们也会被Kafka Streams自动重新创建。一个应用程序实例的所有状态都存储在应用程序的状态目录下（参数state.dir，默认是/tmp/kafka-streams）。在状态存储目录中，每个应用程序都有自己的目录树，子目录的名称是应用程序的编号。删除一个应用程序的状态存储最简单的方式是直接执行命令：<code>rm -rf &lt;state.dir&gt;/&lt;application.id&gt;</code>（比如<code>rm -rf /tmp/kafka-streams/my-streams-app</code>）。</p>
<p>After this discussion, we see that manually resetting a Stream application is cumbersome(累赘) and error-prone. Thus, we developed the new “Application Reset Tool” to simplify this process.</p>
<p>可以看到，手动重置一个流应用程序非常繁琐并且容易出错。因此我们开发了应用程序重置工具来简化这个流程。</p>
<h3 id="Application_Reset_Tool_Details">Application Reset Tool Details</h3><p>In more detail, the application reset tool (bin/kafka-streams-application-reset) performs the following actions (cf. Figure 3):</p>
<p>具体来说，应用程序重置工具（bin/kafka-streams-application-reset）执行了一下的操作（图3）：</p>
<ol>
<li>for any specified input topic, it resets all offsets to zero 对任何指定的输入主题，重置所有的偏移量到0</li>
<li>for any specified intermediate topic, seeks to the end for all partitions 对任何指定的临时主题，跳到所有分区的末尾</li>
<li>for all internal topic 对所有的内部主题<br>3.1 resets all offsets to zero 重置所有的偏移量到0<br>3.2 deletes the topic 删除（内部）主题</li>
</ol>
<p>To use the script, as a minimum you need to specify the application ID. For this case, only internal topics will be deleted. Additionally, you can specify input topics and/or intermediate topics. More details about resetting a Streams application, can be found in the Confluent documentation.</p>
<p>使用这个脚本，你最少必须要指定应用程序编号，这样只有内部主题会被删除。当然你也可以指定输入主题和临时主题（这样就会重置相应主题的偏移量）。</p>
<p>Pay attention, that the application reset tool only covers the “global reset” part. Additionally, to the global reset, for each application instance a local reset of the application state directory is required, too. (cf. #4 in Figure 3) This can be done directly within your application using the method KafkaStreams#cleanUp(). Calling cleanUp() is only valid as long as the application instance is not running (i.e., before start() or after close()).</p>
<p>注意：应用程序重置工具只覆盖“全局重置”的部分，除了全局重置，每个应用程序实例也需要本地重置应用程序的状态目录（图3的步骤4）。这可以通过在应用程序中调用KafkaStreams#cleanUp()来完成。调用cleanUp()方法只有在应用程序实例还没有运行的时候才是有效的（在start()之前，或者在close()之后）。</p>
<p>Because resetting the local state store is embedded in your code, there is no additional work to do for local reset — local reset is included in restarting an application instance. For global reset, a single run of the application reset tool is sufficient.</p>
<p>因为重置本地状态存储是内嵌在你的代码中的，所以本地重置没有额外的工作，即本地重置包含在重启应用程序实例的过程中。对于全局重置，只需要运行一次应用程序重置工具即可。</p>
<h4 id="重置时指定了新的应用程序编号会发生什么？">重置时指定了新的应用程序编号会发生什么？</h4><p>Before we close we want to discuss what happens when you configure your Kafka Streams application to use a new application ID. Until now this has been a common workaround(变通方法) for resetting an application manually.</p>
<p>在结束本篇博文之前，我们想要讨论下当你配置Kafka Streams应用程序时使用了一个新的应用程序编号会发生什么事。目前为止，这种方式实际上也是手动重置应用程序的一种变通方法。</p>
<p>On the positive side, renaming the application ID does cause your application to reprocess its input data from scratch. Why? When a new application ID is used, the application does not have any committed offsets, internal topics, or local state associated with itself, because all of those use the application ID in some way to get linked to a Kafka Streams application. Of course, you also need to delete and recreate all intermediate user topics.</p>
<p>实际上，重新命名应用程序编号确实会让你的应用程序会从头开始重新处理输入数据。当使用了新的应用程序编号，新的应用没有任何的提交偏移量、内部主题、或相关联的本地状态，因为所有这些（数据）都使用应用程序编号的某种方式来和一个Kafka Streams应用程序相关联。当然，你还是需要删除并重建用户指定的所有内部主题。</p>
<p>So why all the fuss(小题大作) about resetting a Kafka Streams application if we could use this workaround?</p>
<p>那么如果我们可以使用这种变通的方法（重命名应用程序编号）来重置Kafka Streams应用程序，为什么还要小题大做（开发一个应用程序重置工具呢）？</p>
<p>First, resetting an application is more than just enabling it to reprocess its input data. An important part of the reset is to also clean-up all internal data that is created by a running application in the background. For example, all the internal topics (that are no longer used) consume storage space in your Kafka cluster if nobody deletes them. Second, the same clean-up must be performed for data written to the local state directories. If not explicitly deleted, disk storage is wasted on those machines that hosted an application instance. Last but not least there is all kind of metadata like topic names, consumer groups, committed offsets that are not used any more and linger around(游荡，苟延残喘). For those reasons, using a new application ID is considered nothing more than a crude workaround to reset a Kafka Streams application, and we wouldn’t recommend its use for production scenarios.</p>
<p>首先，重置一个应用程序并不仅仅是为了让它能重新处理输入数据。重置的一个重要部分是清理后台运行的应用程序创建的所有内部数据。比如所有不会再被使用的内部主题，如果没有人去删除它们，就会消耗Kafka集群的存储空间。其次，同样的清理工作也必须针对写入到本地状态存储目录的数据。如果没有显示删除，就会浪费运行应用程序实例的所在机器的磁盘。最后，有很多的元数据比如主题名称，消费组，提交偏移量，这些都不会再被使用了。基于这些理由使用新的应用程序编号被认为是重置应用程序的一个粗糙方案，因此我们不推荐在生产环境中使用这个种方式。</p>
<p>One more hint at the end: if you do not use a Kafka Streams application anymore (e.g., it gets replaced by a new version of the application or is just not longer needed), we recommend to run the reset tool once to clean-up all the left-overs(剩余) of the retired application.</p>
<p>最后再提示一点：如果你不再使用一个Kafka Streams应用程序了（比如应用程序有新的版本替换了，或者就只是不用了），推荐运行一次重置工具来清理所有剩余的退役应用。</p>
<h1 id="KIP-28:Add_a_processor_client">KIP-28:Add a processor client</h1><p>翻译：<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-28+-+Add+a+processor+client" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/KAFKA/KIP-28+-+Add+a+processor+client</a>  </p>
<h2 id="动机">动机</h2><p>A common use case for Kafka is real-time processes that transform data from input topics to output topics. Today there are a couple of options available for users to process such data:</p>
<p>使用Kafka的一个典型用例是实时处理，从输入主题中转换数据到输出主题。现在用户有两种方式来处理这样的数据：</p>
<p>1.使用Kafka的生产者和消费者API，自己定义处理逻辑，比如</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create a producer and a consumer</span></span><br><span class="line">KafkaProducer producer = <span class="keyword">new</span> KafkaProducer(configs);</span><br><span class="line">KafkaConsumer consumer = <span class="keyword">new</span> KafkaConsumer(configs);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// start a thread with a producer and consumer client</span></span><br><span class="line"><span class="comment">// for data IO and execute processing logic</span></span><br><span class="line"><span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable &#123;</span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">      <span class="comment">// read some data from up-stream Kafka</span></span><br><span class="line">      List&lt;Message&gt; inputMessages = consumer.poll();</span><br><span class="line"> </span><br><span class="line">      <span class="comment">// do some processing..</span></span><br><span class="line"> </span><br><span class="line">      <span class="comment">// send the output to the down-stream Kafka</span></span><br><span class="line">      producer.send(outputMessages);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;).start()</span><br></pre></td></tr></table></figure>
<p>2.使用成熟的流处理系统比如Storm、Samza、Spark Streaming、或者Flink，将Kafka作为它们的源/目标的流数据存储。</p>
<p>Both of those approaches have some downsides. Downsides of using the first option are that the producer and consumer APIs used for writing transformations are somewhat low level; simple examples are relatively simple, but any kind of more complex transformation tends to be a bit complicated. The opportunities(因素，机会) for a richer client to add value beyond what the producer and consumer do would be:</p>
<p>这两种方式都有优缺点，第一种方案的缺点是：用生产者和消费者API来写转换操作有些低级，简单的例子可能还好，稍微复杂的转换则不可取。提供一个富客户端相对原始的生产者和消费者有以下好处：</p>
<ol>
<li>Manage multi-threading and parallelism within a process. 在一个进程中管理多线程和并行</li>
<li>Manage partitioning assignment to processes / threads. 管理分区如何分配给进程或现场</li>
<li>Manage journaled local state storage. 管理本地状态存储（文件存储系统）</li>
<li>Manage offset commits and “exactly-once” guarantees as appropriate features are added in Kafka to support this. 管理偏移量的提交和正好一次的保证</li>
</ol>
<p>The second option, i.e. using a full stream processing framework can be a good solution but a couple of things tend to(趋向，易于) make it a bit heavy-weight (a brief and still-going survey can be found here):</p>
<p>第二种选项，使用流处理框架可能会是一个好的方案，不过下面这些附带产物很容易让它们变得非常重量级。</p>
<ol>
<li>These frameworks are poorly integrated with Kafka (different concepts, configuration, monitoring, terminology). For example, these frameworks only use Kafka as its stream data source / sink of the whole processing topology, while using their own in-memory format for storing intermediate data (RDD, Bolt memory map, etc). If users want to persist these intermediate results to Kafka as well, they need to break their processing into multiple topologies that need to be deployed separately, increasing operation and management costs.</li>
<li>These frameworks either duplicate or force the adoption(采用) of a packaging, deployment, and clustering solution. For example, in Storm you need to run a Storm cluster which is a separate thing that has to be monitored and operated. In an elastic environment like AWS, Mesos, YARN, etc this is sort of silly since then you have a Storm cluster inside the YARN cluster vs just directly running the jobs in Mesos; similarly Samza is tied up with YARN. </li>
<li><p>These frameworks can’t be integrated with existing services or applications. For example, you can’t just embed a light transformation library inside an existing app, but rather the entire framework that runs as a service.</p>
</li>
<li><p>这些框架与Kafka的集成很贫乏（不同的概念，配置，监控，术语）。比如这些框架都只会使用Kafka作为它们的整个处理拓扑中的流数据源或目标，但同时也会使用它们自己的内存格式来存储内部数据（RDD，Bolt内存字典）。如果用户想要持久化这些临时结果到Kafka中，他们需要将流处理分成多个部署独立的拓扑，而这显然增加了操作和维护的成本。</p>
</li>
<li>这些框架针对打包、部署、集群的方案会有重复或强制采用。比如在Storm中你需要运行一个独立的Storm集群，并需要监控和管理这个集群。</li>
<li>这些框架不能和已有的服务或应用程序继承。比如你不能简单地将一个轻量级的转换客户端库嵌入到已有的程序中，而是让整个框架作为一个服务运行。</li>
</ol>
<h2 id="Processor客户端提议">Processor客户端提议</h2><p>We want to propose another standalone “processor” client besides the existing producer and consumer clients for processing data consumed from Kafka and storing results back to Kafka. </p>
<p>除了已经存在的生产者和消费者客户端，我们想要提供一个新的标准“Processor”客户端，它会处理从Kafka消费的数据，然后将结果存储回Kafka。</p>
<p><strong>Data Processing 数据处理</strong>  </p>
<p>A processor computes on a stream of messages, with each message composed as a key-value pair.<br>Processor receives one message at a time and does not have access to the whole data set at once.</p>
<ol>
<li>Per-message processing: this is the basic function that can be triggered once a new message has arrived from the stream.</li>
<li>Time-triggered processing: this function can be triggered whenever a specified time period has elapsed. It can be used for windowing computation, for example.</li>
</ol>
<p>一个Processor会在消息流上做计算，每个消息由键值对组成。Processor一次只接收一条消息，并不需要一次访问所有的数据。</p>
<ol>
<li>每条消息处理：这是一个最基本的功能，当一条心的消息从流中到达时，应该触发一次处理逻辑</li>
<li>时间触发处理：当一个指定的时间间隔过去后，这个函数应该被触发。比如，它可以用在窗口计算</li>
</ol>
<p><strong>Compossible Processing 共存处理</strong>  </p>
<p>Multiple processors should be able to chained up to form a DAG (i.e. the processor topology) for complex processing logic.<br>Users can define such processor topology in a exploring REPL manner: make an initial topology, deploy and run, check the results and intermediate values, and pause the job and edit the topology on-the-fly.</p>
<p>针对复杂的处理逻辑，多个Processor应该被链接在一起，形成一个DAG（Processor处理拓扑）。用户可以定义采用REPL的方式定义这样的处理拓扑：创建和初始化一个拓扑，部署和运行，检查结果和中间数据，暂停作业，编辑拓扑。</p>
<p><strong>Local State Storage 本地状态存储</strong>  </p>
<p>Users can create state storage inside a processor that can be accessed locally.<br>For example, a processor may retain a (usually most recent) subset of data for a join, aggregation / non-monolithic operations.</p>
<p>用户可以在一个Processor中创建状态存储，并且只能在本地访问（所以叫做本地状态存储）。比如一个Processor可能会保存数据的子集（通常是最近的数据）用在join操作，聚合操作。</p>
<h3 id="Processor接口">Processor接口</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ProcessorContext</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">send</span><span class="params">(String topic, Object key, Object value)</span></span>;  <span class="comment">// send the key value-pair to a Kafka topic</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">schedule</span><span class="params">(<span class="keyword">long</span> timestamp)</span></span>;                      <span class="comment">// repeatedly schedule the punctuation function for the period</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">commit</span><span class="params">()</span></span>;                                      <span class="comment">// commit the current state, along with the upstream offset and the downstream sent data</span></span><br><span class="line">    <span class="function">String <span class="title">topic</span><span class="params">()</span></span>;                                     <span class="comment">// return the Kafka record's topic of the current processing key-value pair</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">()</span></span>;                                    <span class="comment">// return the Kafka record's partition id of the current processing key-value pair</span></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">offset</span><span class="params">()</span></span>;                                      <span class="comment">// return the Kafka record's offset of the current processing key-value pair</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Processor</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt;  </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessorContext context)</span></span>;           <span class="comment">// initialize the processor</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">(K1 key, V1 value)</span></span>;                <span class="comment">// process a key-value pair</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">punctuate</span><span class="params">()</span></span>;                              <span class="comment">// process when the the scheduled time has reached</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;                                  <span class="comment">// close the processor</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ProcessorDef</span> </span>&#123;</span><br><span class="line">    <span class="function">Processor <span class="title">instance</span><span class="params">()</span></span>;                          <span class="comment">// create a new instance of the processor from its definition</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopologyBuilder</span> </span>&#123;</span><br><span class="line">    <span class="comment">// add a source node to the topology which generates incoming traffic with the specified Kafka topics</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> TopologyBuilder <span class="title">addSource</span><span class="params">(String name, String... topics)</span> </span>&#123; ... &#125;  </span><br><span class="line">    <span class="comment">// add a sink node to the topology with the specified parent nodes that sends out-going traffic to the specified Kafka topics</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> TopologyBuilder <span class="title">addSink</span><span class="params">(String name, String topic, String... parentNames)</span> </span>&#123; ... &#125; </span><br><span class="line">    <span class="comment">// add a processor node to the topology with the specified parent nodes</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> TopologyBuilder <span class="title">addProcessor</span><span class="params">(String name, ProcessorDef definition, String... parentNames)</span> </span>&#123; ... &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>用户可以使用创建的Processor拓扑创建处理作业：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessorJob</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessorDef</span> <span class="keyword">implements</span> <span class="title">ProcessorDef</span> </span>&#123;</span><br><span class="line">        <span class="annotation">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Processor&lt;String, Integer&gt; instance() &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> Processor&lt;String, Integer&gt;() &#123;</span><br><span class="line">                <span class="keyword">private</span> ProcessorContext context;</span><br><span class="line">                <span class="keyword">private</span> KeyValueStore&lt;String, Integer&gt; kvStore;</span><br><span class="line"> </span><br><span class="line">                <span class="annotation">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessorContext context)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">this</span>.context = context;</span><br><span class="line">                    <span class="keyword">this</span>.context.schedule(<span class="keyword">this</span>, <span class="number">1000</span>);</span><br><span class="line">                    <span class="keyword">this</span>.kvStore = <span class="keyword">new</span> InMemoryKeyValueStore&lt;&gt;(<span class="string">"local-state"</span>, context);</span><br><span class="line">                &#125;</span><br><span class="line"> </span><br><span class="line">                <span class="annotation">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key, Integer value)</span> </span>&#123;</span><br><span class="line">                    Integer oldValue = <span class="keyword">this</span>.kvStore.get(key);</span><br><span class="line">                    <span class="keyword">if</span> (oldValue == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="keyword">this</span>.kvStore.put(key, value);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="keyword">int</span> newValue = oldValue + value;</span><br><span class="line">                        <span class="keyword">this</span>.kvStore.put(key, newValue);</span><br><span class="line">                    &#125;</span><br><span class="line">                    context.commit();</span><br><span class="line">                &#125;</span><br><span class="line"> </span><br><span class="line">                <span class="annotation">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">punctuate</span><span class="params">(<span class="keyword">long</span> streamTime)</span> </span>&#123;</span><br><span class="line">                    KeyValueIterator&lt;String, Integer&gt; iter = <span class="keyword">this</span>.kvStore.all();</span><br><span class="line">                    <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">                        Entry&lt;String, Integer&gt; entry = iter.next();</span><br><span class="line">                        System.out.println(<span class="string">"["</span> + entry.key() + <span class="string">", "</span> + entry.value() + <span class="string">"]"</span>);</span><br><span class="line">                        context.forward(entry.key(), entry.value());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"> </span><br><span class="line">                <span class="annotation">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">this</span>.kvStore.close();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamingConfig config = <span class="keyword">new</span> StreamingConfig(<span class="keyword">new</span> Properties());</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// build topology</span></span><br><span class="line">        TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line">        builder.addSource(<span class="string">"SOURCE"</span>, <span class="string">"topic-source"</span>);</span><br><span class="line">               .addProcessor(<span class="string">"PROCESS"</span>, <span class="keyword">new</span> MyProcessorDef(), <span class="string">"SOURCE"</span>);</span><br><span class="line">               .addSink(<span class="string">"SINK"</span>, <span class="string">"topic-sink"</span>, <span class="string">"PROCESS"</span>);</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// start process</span></span><br><span class="line">        KafkaStreaming streaming = <span class="keyword">new</span> KafkaStreaming(builder, config);</span><br><span class="line">        streaming.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的API示例了低级消费者/生产者接口的抽象，比如consumer.poll() / commit(), producer.send(callback), producer.flush()。</p>
<h3 id="High-level_Stream_DSL">High-level Stream DSL</h3><p>除了Processor API，我们也会引入高级Stream DSL，覆盖了常见的处理实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">KStream</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">//Creates a new stream consists of all elements of this stream which satisfy a predicate</span></span><br><span class="line">    KStream&lt;K, V&gt; filter(Predicate&lt;K, V&gt; predicate);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Creates a new stream by transforming key-value pairs by a mapper to all elements of this stream</span></span><br><span class="line">    &lt;K1, V1&gt; KStream&lt;K1, V1&gt; map(KeyValueMapper&lt;K, V, K1, V1&gt; mapper);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Creates a new stream by transforming valuesa by a mapper to all values of this stream</span></span><br><span class="line">    &lt;V1&gt; KStream&lt;K, V1&gt; mapValues(ValueMapper&lt;V, V1&gt; mapper);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Creates a new stream by applying a flat-mapper to all elements of this stream</span></span><br><span class="line">    &lt;K1, V1&gt; KStream&lt;K1, V1&gt; flatMap(KeyValueMapper&lt;K, V, K1, ? extends Iterable&lt;V1&gt;&gt; mapper);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Creates a new stream by applying a flat-mapper to all values of this stream</span></span><br><span class="line">    &lt;V1&gt; KStream&lt;K, V1&gt; flatMapValues(ValueMapper&lt;V, ? extends Iterable&lt;V1&gt;&gt; processor);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Creates a new windowed stream using a specified window instance.</span></span><br><span class="line">    KStreamWindowed&lt;K, V&gt; with(Window&lt;K, V&gt; window);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Creates an array of streams from this stream. Each stream in the array corresponds to a predicate in supplied predicates in the same order.</span></span><br><span class="line">    KStream&lt;K, V&gt;[] branch(Predicate&lt;K, V&gt;... predicates);</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">sendTo</span><span class="params">(String topic)</span></span>;   <span class="comment">//Sends key-value to a topic.</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Sends key-value to a topic, also creates a new stream from the topic.</span></span><br><span class="line">    <span class="comment">//This is mostly used for repartitioning and is equivalent to calling sendTo(topic) and from(topic).</span></span><br><span class="line">    KStream&lt;K, V&gt; through(String topic);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Processes all elements in this stream by applying a processor.</span></span><br><span class="line">    &lt;K1, V1&gt; KStream&lt;K1, V1&gt; process(KafkaProcessor&lt;K, V, K1, V1&gt; processor);</span><br><span class="line">    <span class="comment">// .. more operators</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">KStreamWindowed</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">KStream</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/**</span><br><span class="line">     * Creates a new stream by joining this windowed stream with the other windowed stream.</span><br><span class="line">     * Each element arrived from either of the streams is joined with elements with the same key in another stream.</span><br><span class="line">     * The resulting values are computed by applying a joiner.</span><br><span class="line">     */</span></span><br><span class="line">    &lt;V1, V2&gt; KStream&lt;K, V2&gt; join(KStreamWindowed&lt;K, V1&gt; other, ValueJoiner&lt;V, V1, V2&gt; joiner);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">/**</span><br><span class="line">     * Creates a new stream by joining this windowed stream with the other windowed stream.</span><br><span class="line">     * Each element arrived from either of the streams is joined with elements with the same key in another stream</span><br><span class="line">     * if the element from the other stream has an older timestamp.</span><br><span class="line">     * The resulting values are computed by applying a joiner.</span><br><span class="line">     */</span></span><br><span class="line">    &lt;V1, V2&gt; KStream&lt;K, V2&gt; joinPrior(KStreamWindowed&lt;K, V1&gt; other, ValueJoiner&lt;V, V1, V2&gt; joiner);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用高级接口，用户的程序可以很简单：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KStreamJob</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamingConfig config = <span class="keyword">new</span> StreamingConfig(props);</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// build the topology</span></span><br><span class="line">        KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line"> </span><br><span class="line">        KStream&lt;String, String&gt; stream1 = builder.from(<span class="string">"topic1"</span>);</span><br><span class="line"> </span><br><span class="line">        KStream&lt;String, Integer&gt; stream2 =</span><br><span class="line">            stream1.map((key, value) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(key, <span class="keyword">new</span> Integer(value)))</span><br><span class="line">                   .filter(((key, value) -&gt; <span class="keyword">true</span>));</span><br><span class="line"> </span><br><span class="line">        KStream&lt;String, Integer&gt;[] streams = stream2</span><br><span class="line">            .branch((key, value) -&gt; value &gt; <span class="number">10</span>,</span><br><span class="line">                    (key, value) -&gt; value &lt;= <span class="number">10</span>);</span><br><span class="line">  </span><br><span class="line">        streams[<span class="number">0</span>].sendTo(<span class="string">"topic2"</span>);</span><br><span class="line">        streams[<span class="number">1</span>].sendTo(<span class="string">"topic3"</span>);</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// start the process</span></span><br><span class="line">        KafkaStreaming kstream = <span class="keyword">new</span> KafkaStreaming(builder, config);</span><br><span class="line">        kstream.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="架构设计">架构设计</h2><p>下面我们会总结一些重要的架构设计要点：</p>
<p><img src="http://img.blog.csdn.net/20161106115808194" alt="kstream arch"></p>
<h3 id="分区分布">分区分布</h3><p>As shown in the digram above, each KStream process could have multiple threads (#.threads configurable in the properties), with each thread having a separate consumer and producer. So the first question is how can we distribute the partitions of the subscribed topics in the source processor among all the processes / threads. There are a couple of common cases for partition management in KStream:</p>
<p>如上图所示，每个KStream进程允许有多个线程（配置文件指定的线程数量），每个线程都有一个独立的消费者和生产者。所以第一个问题是：对于源处理算子订阅主题的分区，我们怎么分布这些分区。在KStream中有一些通用的分区管理场景：</p>
<ol>
<li>Co-partitioning: for windowed-joins.</li>
<li>Sticky partitioning: for stateful processing, users may want to have a static mapping from stream partitions to process threads.</li>
<li><p>N-way partitioning: when we have stand-by processor instances, users may want to assign a single stream partition to multiple process threads.</p>
</li>
<li><p>协调分区，针对窗口的join</p>
</li>
<li>粘性分区：对于有状态的操作，从流分区到处理线程，用户可能会用静态的映射方式</li>
<li>多路分区：当我们有备用的Processor实例，用户可能想要将一个流应用程序分配到多个处理线程上</li>
</ol>
<p>These use cases would require more flexible assignments than today’s server-side strategies, so we need to extend the consumer coordinator protocol in the way that:</p>
<ol>
<li>Consumers send JoinGroup with their subscribed topics, and receive the JoinGroup responses with the list of members in the group and the list of topic-partitions.</li>
<li>All consumers will get the same lists, and they can execute the same deterministic partition assignment algorithm to get their assigned topic-partitions.</li>
</ol>
<p>这些用例都需要比现有的服务端策略有更加灵活的分配方式，所以我们需要扩展消费者的协调协议：</p>
<ol>
<li>消费者发送带有订阅主题的JoinGroup，接收到带有消费组成员的JoinGroup响应，以及主题分区列表</li>
<li>所有消费者接收到相同的列表，执行相同的分区分配算法，来得到属于它们自己的主题分区</li>
</ol>
<p>With this new assignment protocol (details of this change can be found here), we distribute the partitions among worker thread as the following:</p>
<p>使用新的分配协议，我们会用下面的方式将分区在所有工作线程上进行分布：</p>
<p>0.Upon starting the KStream process, user-specified number of KStream threads will be created. There is no shared variables between threads and no synchronization barriers as well hence these threads will execute completely asynchronously. Hence we will describe the behavior of a single thread in all the following steps.  </p>
<p>在启动KStream进程时，指定数量的KStream线程会被创建。线程之间没有共享的变量，也没有同步的屏障，所以这些线程会完全异步地执行。所以在下面的步骤中我们只会描述一个线程的行为，其他线程都是类似的。</p>
<p>1.Thread constructs the user-specified processor topology without initializing it just in order to extract the list of subscribed topics from the topology.   </p>
<p>线程会构造用户指定的处理拓扑，但是不会初始化它，仅仅只是为了从拓扑中抽取中订阅的主题列表</p>
<p>2.Thread uses its consumer’s partitionsFor() to fetch the metadata for each of the subscribed topics to get a information of topic -&gt; #.partitions.   </p>
<p>线程使用消费者的partitionsFor()方法获取订阅的每个主题的元数据，得到topic和分区数量的信息</p>
<p>3.Thread now triggers consumer’s subscribe() with the subscribed topics, which will then applies the new rebalance protocol. The join-group request will be instantiated as follows (for example):  </p>
<p>现在线程会调用消费者的subscribe()方法，传递订阅的主题，然后会运用新的平衡协议。JoinGroup请求实例化对象如下：</p>
<figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JoinGroupRequest =&gt;</span><br><span class="line">  GroupId                 =&gt; <span class="string">"KStream-[JobName]"</span></span><br><span class="line">  GroupType               =&gt; <span class="string">"KStream"</span></span><br></pre></td></tr></table></figure>
<p>And the assignor interface is implemented as follows: 分配分区的接口如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">List&lt;TopicPartition&gt; <span class="title">assign</span><span class="params">(String consumerId, //消费者编号</span><br><span class="line">                            Map&lt;String, Integer&gt; partitionsPerTopic,  //每个主题有多少个分区</span><br><span class="line">                            List&lt;ConsumerMetadata&lt;T&gt;&gt; consumers)</span> </span>&#123; <span class="comment">//所有的消费者元数据</span></span><br><span class="line"> </span><br><span class="line">   <span class="comment">// 1. trigger user-customizable grouping function to group the partitions into groups. 将分区进行分组</span></span><br><span class="line">   <span class="comment">// 2. assign partitions to consumers at the granularity of partition-groups. 以分区分组的粒度将分区分配给消费者</span></span><br><span class="line">   <span class="comment">// 3*. persist the assignment result using commit-offset to Kafka. 持久化分配信息</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The interface of the grouping function is the following, it is very similar to the assign() interface above, with the only difference that it does not have the consumer-lists. </p>
<p>分组函数的接口如下，它和上面的assign()接口方法很类似，唯一的区别是没有消费者列表</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">PartitionGrouper</span> </span>&#123;</span><br><span class="line">  <span class="comment">//Group partitions into partition groups</span></span><br><span class="line">  List&lt;Set&lt;TopicPartition&gt;&gt; group(Map&lt;String, Integer&gt; partitionsPerTopic);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>So after the rebalance completes, each partition-group will be assigned as a whole to the consumers, i.e. no partitions belonging to the same group will be assigned to different consumers. The default grouping function maps partitions with the same id across topics to into a group (i.e. co-partitioning). </p>
<p>在平衡完成后，每个分区分组都会作为一个整体分配给消费者，也就是说不会有一个分区，在同一个组中，但被分配给不同的消费者（同一组中的分区总是分配给同一个消费者）。默认的分组方法会在多个主题之间，将相同的分区编号映射到同一个组中（比如协调分区）。</p>
<p>4.Upon getting the partition-groups, thread creates one task for each partition-group. And for each task, constructs the processor topology AND initializes the topology with the task context.</p>
<p>在得到分区分组后，线程会为每个分区分组创建一个任务。对每个任务，都会构造处理拓扑，然后使用任务的上下文信息初始化拓扑</p>
<pre><code>a. Initialization <span class="keyword">process</span> will trigger Processor.init() <span class="keyword">on</span> each processor <span class="keyword">in</span> the topology following topology<span class="attribute">'s</span> DAG order.
b. <span class="keyword">All</span> user-specified local states will also be created during the initialization <span class="keyword">process</span> (we will talk about this later <span class="keyword">in</span> the later sections).
c. Creates the <span class="keyword">record</span> queue <span class="keyword">for</span> each one <span class="keyword">of</span> the task<span class="attribute">'s</span> associated partition-<span class="keyword">group</span><span class="attribute">'s</span> partitions, so that <span class="keyword">when</span> consumers fetches <span class="keyword">new</span> messages, it will put them into the corresponding queue.

a.初始化过程会在拓扑的每个Processor上调用rocessor.init()方法，每个Processor按照拓扑的DAG顺序依次初始化  
b.所有用户指定的本地状态也会在初始化过程中被创建  
c.为每个任务关联的分区分组的每个分区创建一个记录队列（每个分区都有一个对了），这样当消费者拉取新消息时，可以将它们放到对应的队列中
</code></pre><p>Hence all tasks’ topologies have the same “skeleton” but different processor / state instantiated objects; in addition, partitions are also synchronized at the tasks basis (we will talk about partition-group synchronization in the next section).</p>
<p>所以所有任务的拓扑都有相同的“骨架”，但是有不同的处理/状态实例化对象；另外，分区也会在任务的基础上进行同步。</p>
<p>5.When rebalance is triggered, the consumers will read its last persisted partition assignment from Kafka and checks if the following are true when comparing with the new assignment result:</p>
<p>当平衡触发时，消费者会从Kafka中读取最近持久化的分区分配，并检查下面的条件，和新的分配结果比较是否成立</p>
<pre><code><span class="operator">a</span>. Existing partitions are still assigned <span class="built_in">to</span> <span class="operator">the</span> same partition-groups.
b. New partitions are assigned <span class="built_in">to</span> <span class="operator">the</span> existing partition-groups instead <span class="operator">of</span> creating <span class="built_in">new</span> groups.
c. Partition groups are assigned <span class="built_in">to</span> <span class="operator">the</span> specific consumers instead <span class="operator">of</span> randomly / <span class="built_in">round</span>-robin.

<span class="operator">a</span>.已经存在的分区仍然分配给相同的分区组  
b.新的分区分配给已经存在的分区组，而不是创建新的组  
c.分区组分配给指定的消费者，而不是采用随机或者轮询方式
</code></pre><p>For a), since the partition-group’s associated task-id is used as the corresponding change-log partition id, if a partition gets migrated from one group to another during the rebalance, its state will be no longer valid; for b) since we cannot (yet) dynamically change the #.partitions from the consumer APIs, dynamically adding more partition-groups (hence tasks) will cause change log partitions possibly not exist yet. for c) there are some more advanced partitioning setting such as sticky-partitioning / consistent hashing that we want to support in the future, which may then require additionally.</p>
<p>对于a)，由于分区组关联的任务编号作为对应的变更日志主题的分区编号，如果在平衡式，一个分区从一组迁移到另一个组，它的状态可能不再有效；对于b），由于我们不能在消费者API中动态地修改分区数量，如果动态地添加更多的分区组（以及任务），会导致变更日志主题的分区可能还不存在；对于c），未来我们想要支持一些更加高级的分区方式比如粘性分区/一致性哈希，这可能就需要我们将分区组分配给指定的消费者。</p>
<h3 id="流时间和同步">流时间和同步</h3><p>Time in the stream processing is very important. Windowing operations (join and aggregation) are defined by time. Since Kafka can replay stream, wall-clock based time (system time) may not make sense due to delayed messages / out-of-order messages. Hence we need to define a “time” for each stream according to its progress. We call it stream time.</p>
<p>流处理中的时间非常重要。窗口函数操作（比如聚合和联合）都是通过时间定义的。由于Kafka可以重放流，基于系统时间的时钟对于延迟的、无需的消息可能没有多大意义。所以我们需要为每个流根据它的处理进度定义一个时间，这个时间叫做流的时间。</p>
<p><strong>Stream Time</strong>  </p>
<p>A stream is defined to abstract all the partitions of the same topic within a task, and its name is the same as the topic name. For example if a task’s assigned partitions are {Topic1-P1, Topic1-P2, Topic1-P3, Topic2-P1, Topic2-P2}, then we treat this task as having two streams: “Topic1” and “Topic2” where “Topic1” represents three partitions P1 P2 and P3 of Topic1, and stream “Topic2” represents two partitions P1 and P2 of Topic2.</p>
<p>一个流的定义是在一个任务中，对相同主题的所有分区的抽象，它的名称和主题的名称相同。比如一个任务分配的分区有：{Topic1-P1, Topic1-P2, Topic1-P3, Topic2-P1, Topic2-P2}，那么我们就会认为这个任务有两个流：“Topic1”和“Topic2”，其中流“Topic1”代表了Topic1主题的三个分区P1，P2和P3，而流“Topic2”代表了Topic2主题的两个分区P1和P2。</p>
<p>Each message in a stream has to have a timestamp to perform window based operations and punctuations. Since Kafka message does not have timestamp in the message header, users can define a timestamp extractor based on message content that is used in the source processor when deserializing the messages. This extractor can be as simple as always returning the current system time (or wall-clock time), or it can be an Avro decoder that gets the timestamp field specified in the record schema. </p>
<p>流中的每条消息都必须有一个时间撮，才可以执行基于窗口的操作。由于Kafka消息头中没有时间撮（在新版本中其实已经有时间撮了），用户在序列化消息时，源处理算子会基于消息内容定义一个时间撮解析器。这个解析器可以简单地返回当前的系统时间（即时钟时间），或者是一个能够从记录的Schema获取时间撮字段的Avro解码器。</p>
<p>In addition, since Kafka supports multiple producers sending message to the same topic, brokers may receive messages in order that is not strictly following their timestamps (i.e. out-of-order messages). Therefore, we cannot simply define the “stream time” as the timestamp of the currently processed message in the stream hence that time can move back and forth.</p>
<p>此外，由于Kafka支持多个生产者发送消息到同一个主题，Broker接收到消息的顺序可能并不是严格按照它们的时间撮（即乱序的消息）。因此我们不能简单地将流中当前处理过的消息的时间撮作为“流时间”，因为那个时间可能会来回地移动。</p>
<p>We can define the “stream time” as a monotonically increasing value as the following:</p>
<ol>
<li>For each assigned partition, the thread maintains a record queue for buffering the fetched records from the consumer.</li>
<li>Each message has an associated timestamp that is extracted from the timestamp extractor in the message content.</li>
<li>The partition time is defined as the lowest message timestamp value in its buffer.<br> a. When the lowest timestamp corresponding record gets processed by the thread, the partition time possibly gets advanced.<br> b. The partition time will NOT gets reset to a lower value even if a later message was put in a buffer with a even lower timestamp.</li>
<li>The stream time is defined as the lowest partition timestamp value across all its partitions in the task:<br> a. Since partition times are monotonically increasing, stream times are also monotonically increasing.</li>
<li>Any newly created streams through the upstream processors inherits the stream time of the parents; for joins, the bigger parent’s stream time is taken.</li>
</ol>
<p>我们可以定义“流时间”是一个单调递增的值：</p>
<ol>
<li>对每个分配的分区，线程维护了一个记录队列，用来缓冲从消费者拉取到的记录</li>
<li>每条消息都有一个关联的时间撮，它是从消息内容中用时间撮解析器抽取出来的</li>
<li>分区的时间会被定义为缓冲区中最低的消息时间撮<br>3.1 当最低时间撮对应的记录被线程处理后，分区的时间可能会增长<br>3.2 分区时间不会被重置为一个更低的值，即使一条迟到的消息放到缓冲区，而它的时间撮比分区时间还要低  </li>
<li>流时间被定义为任务中所有分区的最低的分区时间<br>4.1 由于分区时间是单调递增的，所以流时间也是单调递增的  </li>
<li>任何通过上游处理节点新创建的流都继承了所有父节点的流时间。对于join操作而言，会选择所有父节点中最大的流时间作为它的流时间</li>
</ol>
<p><strong>Stream Synchronization</strong></p>
<p>When joining two streams, their progress need to be synchronized. If they are out of sync, a time window based join becomes faulty. Say a delay of one stream is negligible(微不足道的) and a delay of the other stream is one day, doing join over 10 minutes window does not make sense. To handle this case, we need to make sure that the consumption rates of all partitions within each task’s assigned partition-group are “synchronized”. Note that each thread may have one or more tasks, but it does not need to synchronize the partitions across tasks’ partition-groups.</p>
<p>当联合两个流时，它们的进度需要被同步。如果它们状态不同步，基于时间窗口的join就会出错。比如一个流的延迟很小，但是另一个流的延迟有一天，在做10分钟的窗口join时就没有意义了。为了处理这种场景，我们要确保每个任务分配的分区组中所有分区的消费速率是同步的。注意：由于每个线程可能有多个任务，但是并不需要在任务的分区组之间同步。</p>
<p>Work thread synchronizes the consumption within each one of such groups through consumer’s pause / resume APIs as following:</p>
<ol>
<li>When one un-paused partition is a head of time (partition time defined as above) beyond some defined threshold with other partitions, notify the corresponding consumer to pause.</li>
<li>When one paused partition is a head of time below some defined with other partitions, notify the corresponding consumer to un-pause.</li>
</ol>
<p>工作线程同步分区组中每个分区的消费进度，是通过消费者的pause/resume API完成的：</p>
<ol>
<li>当一个还没暂停的分区比其他分区的时间（这个时间指的是分区的时间）<strong>超前</strong>定义的阈值，通知对应的消费者暂停</li>
<li>当一个暂停的分区比其他分区的时间<strong>落后</strong>定义的阈值，通知对应的消费者不要暂停（即恢复）</li>
</ol>
<p>Two streams that are joined together have to be in the same task, and their represented partition lists have to match each other. That is, for example, a stream representing P1, P2 and P3 can be joined with another stream also representing P1, P2 and P3.</p>
<p>两个join的流必须在同一个任务中，它们对应的分区列表必须互相匹配。举例一个流有P1,P2,P3三个分区，可以和另外一个也有三个分区的流进行join（如果另外一个流的分区不是3个，就无法join）。</p>
<h3 id="本地状态管理">本地状态管理</h3><p>Users can create one or more state stores during their processing logic, and each task will have a state manager that keeps an instance of each specified store inside the task. Since a single store instance will not be shared across multiple partition groups, and each partition group will only be processed by a single thread, this guarantees any store will not be accessed concurrently by multiple thread at any given time.</p>
<p>用户可以在流处理逻辑中创建一个或多个状态存储，每个任务在其任务内部都有一个状态管理器，保存了每个指定存储的实例。由于一个单一的存储不会在多个分区组中共享，而且每个分区组都只会被一个线程处理，这就保证了在任何时间，都不会有多个线程并发地访问任何的存储（所以访问存储是线程安全的）。</p>
<p><strong>Log-backed State Storage</strong>  </p>
<p>Each state store will be backed up by a different Kafka change log topic, and each instance of the store correlates to one partition of the topic, such that:</p>
<p>每个状态存储后台都是一个不同的Kafka变更日志主题，每个存储实例都会存储主题的一个分区</p>
<figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#. tasks == #. partition groups == #. store instances <span class="keyword">for</span> <span class="keyword">each</span> state store == #.partitions <span class="keyword">of</span> the change log <span class="keyword">for</span> <span class="keyword">each</span> state store</span><br></pre></td></tr></table></figure>
<p>For example, if a processor instance consumes from upstream Kafka topic “topic-A” with 4 partitions, and creates two stores, namely store1 and store2, and user groups the 4 partitions into {topic-A-p1, topic-A-p2} and {topic-A-p3, topic-A-p4}; then two change log topics, for example namely “topic-store1-changelog” and “topic-store2-changelog”, need to be created beforehand, each with two partitions.</p>
<p>比如有一个Processor实例从上游有4个分区的Kafka主题“topic-A”消费数据，并创建了两个存储即store1和store2，用户将这4个分区分成{topic-A-p1, topic-A-p2}和{topic-A-p3, topic-A-p4}，那么就需要事先创建两个主题：”topic-store1-changelog”和”topic-store2-changelog”，每个主题有两个分区。</p>
<p>After processor writes to a store instance, it first sends the change message to its corresponding changelog topic partition. When user calls commit() in his processor, KStream needs to flush both the store instance as well as the producer sending to the changelog, as well as committing the offset in the upstream Kafka. If these three operations cannot be done atomically, then if there is a crash in between this operations duplicates could be generated since the upstream Kafka committing offset is executed in the last step; if there three operations can be done atomically, then we can guarantee “exactly-once” semantics.</p>
<p>当Processor写入存储实例，它首先会将变更消息发送到对应的变更日志主题分区中。当用户在Processor中调用commit()方法时，KStream需要同时刷新存储实例、生产者发送到变更日志的消息（之前的发送不一定真正写入，只有flush时才会确保消息真正写入）、提交上游Kafka的偏移量。如果这三个操作不能以原子操作完成，那么如果在这些步骤中发生崩溃，就会生成重复的数据，因为上游的Kafka提交偏移量是在最后一步执行的（和消费者的处理类似，最后提交偏移量，只能保证至少一次，但数据可能会重复）；如果这三个步骤能原子地完成，我们就可以保证“正好一次”的语义了。</p>
<p><strong>Persisting and Restoring State</strong>  </p>
<p>When we close a KStream instance, the following steps are executed:</p>
<ol>
<li>Flush all store’s state as mentioned above.</li>
<li>Write the change log offsets for all stores into a local offset checkpoint file. The existence of the offset checkpoint file indicates if the instance was cleanly shutdown.</li>
</ol>
<p>当我们关闭一个KStream示例时，会执行下面的步骤：</p>
<ol>
<li>刷新上面提到的所有状态存储</li>
<li>将所有存储的变更日志偏移量写到本地的偏移量检查点文件中。是否存在偏移量检查点文件，可以用力爱判断实例是否关闭的很干净（如果没有，说明KStream没有被彻底关闭）。</li>
</ol>
<p>Upon (re-)starting the KStream instance:</p>
<ol>
<li>Try to read the local offset checkpoint file into memory, and delete the file afterwards.</li>
<li>Check the offset of the corresponding change log partition read from the checkpoint file.<br> a. If the offset is read successfully, load the previously flushed state and replay the change log from the read offset up to the log-end-offset.<br> b. Otherwise, do not load the previously flushed state and replay the change log from the beginning up to the log-end-offset.</li>
</ol>
<p>重启KStream实例时：</p>
<ol>
<li>尝试读取本地的偏移量检查点文件到内存中，然后删除删除这个文件</li>
<li>读取检查点文件，检查对应的变更日志分区的偏移量<br>2.1 如果成功读取了偏移量，加载之前（关闭时）刷新的状态，从读取的偏移量到日志文件的最后，重放变更日志<br>2.2 否则，不要加载之前刷新的状态，也不需要重放变更日志</li>
</ol>
<h3 id="工作流程总结">工作流程总结</h3><p>下面总结Kafka Streams处理的步骤：</p>
<p><strong>启动</strong>  </p>
<p>Upon user calling KafkaStreaming.start(), the process instance creates the worker threads given user specified #.threads. In each worker thread:</p>
<ol>
<li>Construct the producer and consumer client, extract the subscription topic names from the topology.</li>
<li>Let the consumer to subscribe to the topics and gets the assigned partitions.</li>
<li>Trigger the grouping function with the assigned partitions get the returned list of partition-groups (hence tasks) with associated ids.</li>
<li>Initialize each task by:<br> a. Creates a record queue for buffering the fetched records for each partition.<br> b. Initialize a topology instance for the task from the builder with a newly created processor context.<br> c. Initialize the state manager of the task and constructs / resumes user defined local states.  </li>
<li>Runs the loop at its own pace until notified to be shutdown: there is no synchronization between these threads. In each iteration of the loop:<br> a. Thread checks if the record queues are empty / low, and if yes calls consumer.poll(timeout) / consumer.poll(0) to re-fill the buffer.<br> b. Choose one record from the queues and process it through the processor topology.<br> c. Check if some of the processors’ punctuate functions need to be triggered, and if yes, execute the function.<br> d. Check if user calls commit() during the processing of this records; if yes commit the offset / flush the local state / flush the producer.</li>
</ol>
<p>在调用KafkaStreaming.start()时，Processor实例会创建指定数量的工作线程，每个线程中：  </p>
<ol>
<li>构造生产者和消费者客户端，从拓扑中解析订阅的主题名称</li>
<li>消费者订阅主题，并得到分配的分区</li>
<li>使用分配的分区触发分组方法，返回分区分组列表以及对应的编号（任务）</li>
<li>初始化每个任务<br>4.1 为每个分区创建一个记录队列，用来缓冲每个分区的拉取记录<br>4.2 从Builder中为任务初始化拓扑实例<br>4.3 初始化任务的状态管理器，构造或恢复用户定义的本地状态  </li>
<li>以自己的步伐运行循环，直到收到关闭的通知，在这些线程中不需要同步，在循环的每次迭代中：<br>5.1 线程检查记录队列空了或者记录很少，则调用onsumer.poll(timeout) / consumer.poll(0)重新填充缓冲区<br>5.2 从队列中选择一条记录，并将其放入处理拓扑中处理<br>5.3 检查一些Processor的punctuate方法是否需要触发，如果需要则执行函数<br>5.4 在处理记录时，检查是否可以调用commit()，如果是，则提交偏移量、刷新本地状态、刷新生产者</li>
</ol>
<blockquote>
<p>这里的Processor实例指的是拓扑中的处理节点，而不是拓扑本身，也不是指应用程序实例</p>
</blockquote>
<p><strong>关闭</strong>  </p>
<p>Upon user calling KafkaStreaming.shutdown(), the following steps are executed:</p>
<ol>
<li>Commit / flush each partition-group’s current processing state as described in the local state management section.</li>
<li>Close the embedded producer and consumer clients.</li>
</ol>
<p>当用户调用KafkaStreaming.shutdown()，会执行以下步骤：</p>
<ol>
<li>提交或刷新每个分区组的当前处理状态</li>
<li>关闭内置的生产者和消费者客户端</li>
</ol>
<p>一些重要的类：</p>
<ol>
<li>PartitionGroup: a set of partitions along with their queuing buffers and timestamp extraction logic. 分区的集合，联通它们的队列缓冲区，以及时间撮解析逻辑</li>
<li>ProcessorStateManager: the manager of the local states within a task. 任务的本地状态管理器</li>
<li>ProcessorTopology: the instance of the topology generated by the TopologyBuilder. 通过TopologyBuilder生成的拓扑实例</li>
<li>StreamTask: the task of the processing tasks unit, which include a ProcessorTopology, a ProcessorStateManager and a PartitionGroup. 处理任务的单元，包括前面三个概念</li>
<li>StreamThread: contains multiple StreamTasks, a Consumer and a Producer client. 包括多个流任务，一个生产者、消费者客户端</li>
<li>KStreamFilter/Map/Branch/…: implementations of high-level KStream topology builder operators. 实现高级KStream拓扑构造器的算子</li>
</ol>
<h1 id="KIP-67:Queryable_state_for_Kafka_Streams">KIP-67:Queryable state for Kafka Streams</h1><p>翻译：<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-67%3A+Queryable+state+for+Kafka+Streams" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/KAFKA/KIP-67%3A+Queryable+state+for+Kafka+Streams</a>  </p>
<p>Today a Kafka Streams application will implicitly create state. This state is used for storing intermediate data such as aggregation results. The state is also used to store KTable’s data when they are materialized. The problem this document addresses is that this state is hidden from application developers and they cannot access it directly. The DSL allows users to make a copy of the data (using the through operator) but this leads to a doubling in the amount of state that is kept. In addition, this leads to extra IOs to external databases/key value stores that could potentially slow down the entire pipeline. Here is a simple example that illustrates the problem:</p>
<p>一个Kafka Streams应用程序通常都会在后台隐式地创建状态。这个状态用来存储临时数据，比如聚合的结果。状态也会被用在当物化KTable时存储KTable数据。这篇文档要解决的问题是状态对于应用开发者是隐藏的，他们不能直接访问状态。DSL操作允许用户使用through操作符复制数据，但是导致了需要保存的状态数量翻倍。另外，也导致了和外部数据库/键值存储产生额外的IO开销，这可能会降低整个数据管道的响应。下面模拟了这个问题：</p>
<figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> KTable&lt;String, Long&gt; wordCounts = textLine</span><br><span class="line"><span class="number">2</span>    .flatMapValues(value<span class="function"> -&gt;</span>Arrays.asList(value.toLowerCase().split(<span class="string">"\\W+"</span>)))</span><br><span class="line"><span class="number">3</span>    .map<span class="function"><span class="params">((key, word) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(word, word)</span><br><span class="line"><span class="number">4</span>    .countByKey(<span class="string">"StoreName"</span>)</span><br><span class="line"><span class="number">5</span> wordCounts.<span class="keyword">to</span>(Serdes.String(), Serdes.Long(), <span class="string">"streams-wordcount-output"</span>);</span></span></span><br></pre></td></tr></table></figure>
<p>In line 4, the aggregation already maintains state in a store called StoreName, however that store cannot be directly queried by the developer. Instead, the developer makes a copy of the data in that store into a topic called streams-wordcount-output. Subsequently, the developer might instantiate its own database after reading the data from that topic (this step is not shown above). This is shown in illustration (a):</p>
<p>在第四行中，聚合操作维护了一个状态存储叫做“StoreName”，不过这个存储不能直接被开发者用来查询。相反，开发者会在这个存储中复制数据到一个叫做“streams-wordcount-output”的主题。然后，开发者可能会在从这个主题读到数据后，实例化自己的数据库（这里没有展示出用法）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Confluent Kafka Streams Documentation 中文翻译：  &lt;a href=&quot;http://docs.confluent.io/3.0.1/streams/introduction.html&quot;&gt;http://docs.confluent.io/3.0.1/streams/introduction.html&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>译：Kafka权威指南-第五章-内部实现</title>
    <link href="http://github.com/zqhxuyuan/2016/10/29/Kafka-Definitive-Guide-ch05/"/>
    <id>http://github.com/zqhxuyuan/2016/10/29/Kafka-Definitive-Guide-ch05/</id>
    <published>2016-10-28T16:00:00.000Z</published>
    <updated>2016-10-29T10:07:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka权威指南中文翻译：<a href="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch05.html" target="_blank" rel="external">https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch05.html</a><br><a id="more"></a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kafka权威指南中文翻译：&lt;a href=&quot;https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch05.html&quot;&gt;https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch05.html&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="mq" scheme="http://github.com/zqhxuyuan/categories/mq/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>译：Kafka交互式查询和流处理的统一</title>
    <link href="http://github.com/zqhxuyuan/2016/10/29/Kafka-Interactive-Query/"/>
    <id>http://github.com/zqhxuyuan/2016/10/29/Kafka-Interactive-Query/</id>
    <published>2016-10-28T16:00:00.000Z</published>
    <updated>2016-10-27T05:52:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>Unifying Stream Processing and Interactive Queries in Apache Kafka<br><a href="http://www.confluent.io/blog/unifying-stream-processing-and-interactive-queries-in-apache-kafka/" target="_blank" rel="external">http://www.confluent.io/blog/unifying-stream-processing-and-interactive-queries-in-apache-kafka/</a><br><a id="more"></a></p>
<p>Interactive Queries allows you to get more than just processing from streaming. It allows you to treat the stream processing layer as a lightweight, embedded database and directly query the state of your stream processing application, without needing to materialize that state to external databases or storage. Apache Kafka maintains and manages that state and guarantees high availability and fault tolerance. As such, this new feature enables the hyper-convergence of processing and storage into one easy-to-use application that uses the Apache Kafka’s Streams API.</p>
<blockquote>
<p>交互式查询（Interactive Queries）并不仅仅是对流数据进行处理（stream processing），它相当于将流处理这一层看做是一个轻量级的、内置的数据库、直接查询流处理应用程序的状态，它不需要将状态（数据）物化（materialize）到外部数据库或存储系统中。Kafka会自己维护和管理状态，并且保证了高可用、容错。这个新特性使得处理逻辑变得高度聚合，并且可以使用Kafka的Streams API存储（状态）到一个易用的应用程序中。  </p>
</blockquote>
<p>The idea behind Interactive Queries is not new; a similar concept actually originated in traditional databases where it’s often known as “materialized views.” Though materialized views are very useful, the way they are implemented in databases is not suitable for modern application development – they force you to write your code all in SQL and deploy it into the database server. With our database background combined with our stream processing experience, we visited a key question: can the concept behind materialized views be applied to a modern stream processing engine to create a powerful and general purpose construct for building stateful applications and microservices? In this blog post, we show how Apache Kafka, through Interactive Queries, helps us do exactly that.</p>
<blockquote>
<p>交互式查询背后的思想并不新鲜，在传统的数据库中相似的概念是“物化视图”，尽管物化视图非常有用，但是数据库的这种实现方式对现代的应用程序开发并不适合（因为他们会强制你在SQL中编写所有的代码，并部署到服务端）。结合数据库的背景知识和流处理的体验，我们在思考一个关键的问题：物化视图是否如果应用到现代的流处理引擎，就可以构建出一个强壮的、通用的，有状态的应用程序（或者微服务），本篇博文我们会向你展示Apache Kafka如何通过交互式查询帮助我们实现了这个目标。  </p>
</blockquote>
<p>When we set out to design the stream processing API for Apache Kafka – Kafka Streams – a key motivation was to rethink the existing solution space for stream processing. Here, our vision has been to move stream processing out of the big data niche and make it available as a mainstream application development model. For us, the key to executing that vision is to radically simplify how users can process data at any scale – small, medium, large – and in fact, one of our mantras is “Build Applications, Not Clusters!” In the past, we wrote about three ways Apache Kafka simplifies the stream processing architecture – by eliminating the need for a separate cluster, having good abstractions for streams and tables and keeping the overall architecture simple. Interactive Queries is another feature to enable this vision.</p>
<blockquote>
<p>在为Kafka设计流式处理API时（即Kafka Streams），一个重要的动机是重新思考已有的流处理系统的局限性，我们的焦点已经从大数据领域转到了流处理，并且将其作为可用的主流的应用程序开发模型。执行这个愿景的关键是从根本上简化用户如何处理数据的扩展性问题（不管是小批量的数据、中等规模的、大规模的数据），实际上我们的一个口号是：“构建应用程序，不要集群！”。之前的博文中我们写了Kafka简化流处理的<a href="http://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/" target="_blank" rel="external">三种模式</a>：不需要单独的集群、对Streams和Table进行良好的抽象、保持整体架构的简洁。现在，交互式查询是这个愿景的另一个特性。</p>
</blockquote>
<p>In this blog post, we’ll start by digging deeper into the motivation behind Interactive Queries through a concrete example that outlines its applicability. Then we will describe how Interactive Queries works under the hood and provide a summary of related resources for further reading.</p>
<blockquote>
<p>本篇文章我们会先通过一个示例深入研究交互式查询的背后动机，然后会描述交互式是如何工作的。</p>
</blockquote>
<h2 id="示例：实时风险管理">示例：实时风险管理</h2><p>Let’s use an end-to-end example to pick up where we left off in our previous article on why stream processing applications need state. In that article, we described some simple stateful operations, e.g., if you are grouping data by some field and counting, then the state you maintain would be the counts that have accumulated so far. Or if you are joining two streams, the state would be the rows in each stream waiting to find a match in the other stream.</p>
<blockquote>
<p>在<a href="https://www.oreilly.com/ideas/why-local-state-is-a-fundamental-primitive-in-stream-processing" target="_blank" rel="external">流处理为什么需要状态</a>文章中，我们描述了一些简单的状态操作，比如根据一些字段分组和计数，那么你维护的状态就是目前为止收集到的所有数据的数量。或者如果将两个流进行合并，状态就会是在两个流中找出互相匹配的一行记录（由于流数据的顺序性，一个流要等待另一个流，才能最终获得完整的合并数据）。</p>
</blockquote>
<p>Now as a driving example in this blog, consider a financial institution, like a wealth management firm or a hedge fund, that maintains positions in assets held by the firm and/or its client investors. Maintaining positions means that the bank needs to keep track of the risk associated with those particular assets. The bank continuously collects business events and other data that could potentially influence the risk associated with a given position. This data includes market data fluctuations on the price of the asset, foreign exchange rates, research, or even news information that could influence the reputation of people involved with the asset. Any time this data changes, the risk position needs to be recalculated in order to keep a real-time view of the risk associated with each individual asset as well as on entire portfolios of investments.</p>
<p>Real-time risk management is an example of a stateful application. At a minimum, state is needed to keep track of the latest position for every asset. State is also needed inside the stream processing engine to keep track of various aggregate statistics, like the number of times an asset is traded in a day and the average bid/ask spread. The collected state needs to be continuously updated and queried.</p>
<blockquote>
<p>本篇博文举例的是一个金融场景，比如财富管理机构或者对冲基金（投资者）会维护资产的仓位。维护仓位意味着银行会跟踪这些资产的风险。银行会持续地收集商业事件以及其他可能会影响指定仓位风险的数据。这些数据包括资产的市场波动，外汇交换比率，研究机构，甚至是有声望的名人的新闻。任何时候只要数据变化，风险的仓位就需要被重新计算，这样才能对单独的资产，甚至整个投资组合都能够保持实时的风险视图。<br>实时风控管理是有状态的应用程序的一个示例，最小的需求是：状态需要能够跟踪每种资产的最近仓位。状态在流处理引擎中为了跟踪不同的聚合逻辑也是必须的，比如一天中资产的交易次数，平均出价(bid/ask)速度。收集到的状态需要被持续地更新和查询。  </p>
</blockquote>
<h2 id="现在的做法">现在的做法</h2><p>在没有Kafka Streams之前，典型的架构图如下：<br><img src="http://www.confluent.io/wp-content/uploads/2016/10/IQueries-Blog-Diagrams-2-1-1024x381.png" alt="arch before"></p>
<p>图中业务事件会被Kafka捕获，上部分是Hadoop/Spark批处理系统，下部分是流处理系统，这种混合部署模式也被称作Lambda架构，它的缺点也很明显：需要维护监控两套系统，总结现在的通用做法：  </p>
<ol>
<li>一个额外的Hadoop集群重复处理数据</li>
<li>在流处理层也要维护存储，因为流处理需要查询和聚合</li>
<li>流处理作业和批处理作业的输出都需要维护存储和数据库</li>
<li>写放大无法避免，</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Unifying Stream Processing and Interactive Queries in Apache Kafka&lt;br&gt;&lt;a href=&quot;http://www.confluent.io/blog/unifying-stream-processing-and-interactive-queries-in-apache-kafka/&quot;&gt;http://www.confluent.io/blog/unifying-stream-processing-and-interactive-queries-in-apache-kafka/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>译：Kafka权威指南-第四章-消费者</title>
    <link href="http://github.com/zqhxuyuan/2016/10/27/Kafka-Definitive-Guide-cn-04/"/>
    <id>http://github.com/zqhxuyuan/2016/10/27/Kafka-Definitive-Guide-cn-04/</id>
    <published>2016-10-26T16:00:00.000Z</published>
    <updated>2016-10-29T10:15:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka权威指南中文翻译：<a href="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch04.html" target="_blank" rel="external">https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch04.html</a><br><a id="more"></a></p>
<p>Applications that need to read data from Kafka use KafkaConsumer to subscribe to Kafka topics and receive messages from these topics. Reading data from Kafka is a bit different than reading data from other messaging systems and there are few unique concepts and ideas involved. It is difficult to understand how to use the consumer API without understanding these concepts first. So we’ll start by explaining some of the important concepts, and then we’ll go through some examples that show the different ways the consumer APIs can be used to implement applications with different requirements.  </p>
<blockquote>
<p>应用程序为了从Kafka中读取数据，会使用KafkaConsumer订阅Kafka的主题，然后就可以从这些主题中接收到消息。从Kafka读取数据和其他消息读取数据有点不同，有一些概念需要事先弄清楚，否则就对如何使用消费者API不知所措。下面我们会先解释一些重要的概念，然后通过示例的方式展示消费API的不同用法，从而实现不同的需求。  </p>
</blockquote>
<h2 id="概念">概念</h2><h3 id="消费者和消费组">消费者和消费组</h3><p>Suppose you have an application that needs to read messages from a Kafka topic, run some validations against them and write the results to another data store. In this case your application will create a consumer object, subscribe to the appropriate topic and start receiving messages, validating them and writing the results. This can work well for a while, but what if the rate at which producers write messages to the topic exceed the rate at which your application can validate them? If you are limited to a single consumer reading and processing the data, your application may fall farther and farther behind, unable to keep up with the rate of incoming messages. Obviously there is a need to scale consumption from topics. Just like multiple producers can write to the same topic, we need to allow multiple consumers to read from the same topic, splitting the data between them.</p>
<p>Kafka consumers are typically part of a consumer group. When multiple consumers are subscribed to a topic and belong to the same consumer group, then each consumer in the group will receive messages from a different subset of the partitions in the topic.</p>
<blockquote>
<p>假设你的应用程序要从Kafka的一个主题中读取消息，对消息进行验证，然后将结果写入到其他存储系统中。你的做法会是：创建一个消费者对象，订阅指定的主题，然后开始接收消息、数据验证、结果输出。这种做法在一段时间内可能工作的很好，但是如果生产者写入消息的速度超过应用程序执行验证逻辑的速度怎么办？如果你只有一个消费者负责读取和处理数据，消费者的读取进度最终会越来越跟不上生产者的写入进度，很显然我们需要对主题的消费进行扩展。就像多个生产者可以写到同一个主题一样，我们应该允许多个消费者同时从一个主题读取数据：通过将数据进行分离，每个消费者只负责一部分数据，达到负载均衡的目的。<br>Kafka的消费者通常都属于某一个消费组的一部分，当多个消费者订阅了一个主题并且属于同一个消费组，那么消费组中的每个消费者都会接收到主题的不同子集分区（一个主题分成多个分区，每个消费者分配到了不同的分区）。</p>
</blockquote>
<p>假设主题t1有4个分区，刚开始我们创建了一个消费者c1，并且它是消费组g1的唯一成员，c1订阅了主题t1。消费者c1会获取到t1所有4个分区的消息。  </p>
<p>如果添加了新的消费者c1到消费组g1，现在每个消费者（c1、c2）只会各自得到两个分区的消息，比如分区0和分区2的消息会到c1，分区1和分区3的消息会到c2。<br><img src="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/assets/ch04_consumer_group_2_consumers.jpg" alt=""></p>
<p>如果消费组g1有4个消费者，每个消费者都会读取一个分区的消息。<br><img src="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/assets/ch04_consumer_group_4_consumers.jpg" alt=""></p>
<p>如果再添加更多的消费者，消费者的数量比分区数量还要多，那么有一些消费者就会空闲而得不到任何消息。<br><img src="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/assets/ch04_consumer_group_5_consumers.jpg" alt=""></p>
<p>线性扩展消费Kafka主题的数据的主要解决方式是为消费组增加更多的消费者。Kafka消费者通常会做一些延迟较高的操作，比如写入数据库或者HDFS、做一些耗时的数据计算。这种情况下，单一的消费者无法跟上数据流入Kafka主题的速度，所以增加更多的消费者一起共享负载，并且每个消费者只拥有分区和消息的子集，才是扩展消费能力解决之道。因此为主题创建很多分区是一个好的设计，它允许在负载增加的时候可以随时增加更多的消费者（来均衡负载）。不过注意，消费者的数量不能超过主题的分区数，否则有些消费者永远处于空闲状态。  </p>
<p>除了通过添加消费者来扩展单一的应用程序（的处理能力），多个应用程序需要从同一个主题中读取数据也是很常见。实际上Kafka的一个设计目标就是确保数据生产到Kafka的主题后，对多个应用场景都是可用的。这种情况下我们希望每个业务场景对应的应用程序都能够得到所有的消息，而不是一部分消息子集。为了确保一个应用程序得到主题的所有消息，你要确保每个应用程序有单独的消费组。和其他消息系统不同的是，Kafka可以在不牺牲性能的前提下大规模扩展地消费者和消费组。  </p>
<p>上面的示例中，如果添加了只有一个消费者的新消费组g2，这个消费者就会得到主题t1的所有消息，而它和消费组g1在做什么事情毫无关系。就像消费组g1一样，消费组g2也可以有多个消费者，每个消费者也可以获得所有分区的子集，从整体上来说消费组g2仍然会得到所有的消息，而不会受其他消费组的影响。<br><img src="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/assets/ch04_two_consumer_groups.jpg" alt=""></p>
<p>总结下上面的操作过程，你为每个需要读取一个或多个主题所有消息的应用程序都创建了新的消费组，然后为已有的消费组添加消费者来动态地扩展从主题中读取和处理消息的能力，每个新增加的消费者都只会得到消息的子集。  </p>
<h3 id="消费组平衡">消费组平衡</h3><p>As we’ve seen in the previous section, consumers in a consumer group share ownership of the partitions in the topics they subscribe to. When we add a new consumer to the group it starts consuming messages from partitions which were previously consumed by another consumer. The same thing happens when a consumer shuts down or crashes, it leaves the group, and the partitions it used to consume will be consumed by one of the remaining consumers. Reassignment of partitions to consumers also happen when the topics the consumer group is consuming are modified, for example if an administrator adds new partitions.</p>
<blockquote>
<p>上面我们看到了消费组中的消费者共享了它们所订阅的主题的分区。当为消费组添加新消费者时，它会从之前其他消费者消费的分区开始消费消息。同样当发生消费者关闭、进程挂掉、离开消费组，它所使用的分区就会被其他剩余的消费者所消费。为消费者重新分配分区同样也会发生在消费组订阅的主题被修改时，比如管理员会添加一个新的分区。  </p>
</blockquote>
<p>The event in which partition ownership is moved from one consumer to another is called a rebalance. Rebalances are important since they provide the consumer group with both high-availability and scalability (allowing us to easily and safely add and remove consumers), but in the normal course of events they are fairly undesirable. During a rebalance, consumers can’t consume messaged, so a rebalance is in effect a short window of unavailability on the entire consumer group. In addition, when partitions are moved from one consumer to another the consumer loses its current state, if it was caching any data, it will need to refresh its caches - slowing down our application until the consumer sets up its state again. Throughout this chapter we will discuss how to safely handle rebalances and how to avoid unnecessary rebalances.</p>
<blockquote>
<p>分区的所有权从一个消费者转移给另一个消费者，这个事件叫做平衡（rebalance）。平衡操作的重要性不言而喻，因为它提供了消费组的高可用、可扩展性（允许我们添加或删除消费者变得简单和安全），但对于事件的处理则有点不受欢迎。在平衡期间，消费者不能消费消息，所以平衡实际上造成了消费组短暂的不可用窗口。另外，当分区从一个消费者转移到另一个消费者时会丢失当前的状态，如果它缓存了数据的话，就需要重新刷新缓存，这会使得我们的应用程序响应变慢，知道消费者重新恢复到正常的状态。本章我们会讨论如何安全地处理平衡操作，并且怎么避免不必要的平衡。  </p>
</blockquote>
<p>The way consumers maintain their membership in a consumer group and their ownership on the partitions assigned to them is by sending heartbeats to a Kafka broker designated as the Group Coordinator (note that this broker can be different for different consumer groups). As long the consumer is sending heartbeats in regular intervals, it is assumed to be alive, well and processing messages from its partitions. In fact, the act of polling for messages is what causes the consumer to send those heartbeats. If the consumer stops sending heartbeats for long enough, its session will time out and the group coordinator will consider it dead and trigger a rebalance. Note that if a consumer crashed and stopped processing messages, it will take the group coordinator few seconds without heartbeats to decide it is dead and trigger the rebalance. During those seconds, no messages will be processed from the partitions owned by the dead consumer. When closing a consumer cleanly, the consumer will notify the group coordinator that it is leaving, and the group coordinator will trigger a rebalance immediately, reducing the gap in processing. Later in this chapter we will discuss configuration options that control heartbeat frequency and session timeouts and how to set those to match your requirements.</p>
<blockquote>
<p>消费者为了维护它们在消费组中的成员地位，以及分配给它们的分区所有权，是通过发送心跳给被指定为消费组协调者（Group Coordinator）的一个Kafka代理（Broker），注意这个代理对于不同的消费组都是不同的。只要消费者能够在正常的时间间隔内发送心跳，它就会被认为是存活的、运行良好，就可以处理分区的消息。实际上消费者轮询消息的动作就是消费者发送心跳的原因。如果消费者很长时间没有发送心跳，它的会话会超时（服务端的协调者会保持每个消费者的连接会话），协调者就会认为消费者挂掉，从而触发一次平衡操作。注意如果消费者自己崩溃并且停止处理消息，协调者会在数秒之后判断消费者没有心跳，才决定它挂掉了并且触发平衡。在这数秒的时间段内，被挂掉消费者拥有的分区上不会处理任何消息。而如果是优雅地关闭一个消费者时，消费者会通知消费组说它正在离开，协调者就会立即触发平衡，从而减少了消息无法被处理的间隙。本章的后面我们会讨论一些关于控制心跳频率、会话超时的配置，以及如何设置它们来匹配我们的需求。</p>
</blockquote>
<p>HOW DOES THE PROCESS OF ASSIGNING PARTITIONS TO BROKERS WORK?<br>When a consumer wants to join a group, it sends a JoinGroup request to the group coordinator. The first consumer to join the group becomes the group leader. The leader receives a list of all consumers in the group from the group coordinator (this will include all consumers that sent a heartbeat recently and are therefore considered alive) and it is responsible for assigning a subset of partitions to each consumer. It uses an implementation of PartitionAssignor interface to decide which partitions should be handled by which consumer. Kafka has two built-in partition assignment policies, which we will discuss in more depth in the configuration section. After deciding on the partition assignment, the consumer leader sends the list of assignments to the GroupCoordinator which sends this information to all the consumers. Each consumer only sees his own assignment - the leader is the only client process that has the full list of consumers in the group and their assignments. This process repeats every time a rebalance happens.</p>
<blockquote>
<p>为消费者分配Partition是怎么工作的？<br>当一个消费者想要加入一个消费组，它会发送JoinGroup请求给消费组的协调者，第一个加入消费组的消费者会成为组的领导者（Leader）。领导者会从协调者接收到所有的消费者（包括最近发送了心跳，被认为是存活的所有消费者），并且负责为每个消费者分配分区子集。它会使用PartitionAssignor接口来决定哪个消费者应用处理哪些分区。Kafka内置了两种分区分配策略，后面在配置部分会详细介绍。在决定了分区分配之后，领导者发送每个消费者的分配列表给协调者，协调者会发送这些分配信息给所有的消费者。每个消费者只会看到它自己的分配结果。领导者是唯一有所有消费者列表和它们的分配信息的客户端进程。上面这个过程在每次平衡操作发生时都会重复执行。  </p>
</blockquote>
<h2 id="创建一个新的消费者">创建一个新的消费者</h2><p>消费者开始消费记录的第一步是创建一个<code>KafkaConsumer</code>实例，创建<code>KafkaConsumer</code>类似于创建<code>KafkaProducer</code>，首先创建一个<code>Properties</code>实例，传递消费者的配置属性。本章后面我们会讨论所有的属性，这里只需要三个必须的属性：<code>bootstrap.servers</code>、<code>key.deserializer</code>和<code>value.deserializer</code>。  </p>
<p>第一个属性<code>bootstrap.servers</code>指向Kafka集群的连接地址，它和<code>KafkaProducer</code>的使用方式一样。剩余的两个属性<code>key.deserializer</code>和<code>value.deserializer</code>和生产者的<code>serializers</code>类似。</p>
<p>还有一个属性<code>group.id</code>不是必须的，但现在我们假设它是必须的。<code>group.id</code>指定了<code>KafkaConsumer</code>实例所属的消费组。虽然创建不属于任何消费组的消费者也是可行的，但这种情况很少见，所以本章我们都会假设消费者是消费组的一部分。下面的代码实例了如何创建一个KafkaConsumer：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"broker1:9092,broker2:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"CountryCounter"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, String&gt;(props);</span><br></pre></td></tr></table></figure>
<p>如果你读过第三章创建生产者的代码，你会发现这里看到的大部分代码都很熟悉。我们打算消费的消息格式都是字符串类型（反序列化：Kafka中存储的是二进制字节类型，Kafka内部会负责将字节类型转换为字符串类型，所以消费者读取到的消息是字符串类型），所以我们使用了内置的<code>StringDeserializer</code>反序列化器，最后创建的KafkaConsumer泛型类型也是字符串（KafkaConsumer类上的两个泛型类型分别表示消息的键值类型）。只有<code>group.id</code>这个属性可能你没见过，它表示的是这个消费者（作为消费组的一部分）所属的消费组名称。  </p>
<h2 id="订阅主题">订阅主题</h2><p>一旦创建完消费组实例，下一步是让消费者订阅一个或多个主题。<code>subscribe()</code>方法会将多个主题的列表作为一个参数，使用起来非常简单，下面的代码创建了只有一个元素的列表，订阅的主题名称叫做”customerCountries”。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Collections.singletonList(<span class="string">"customerCountries"</span>));</span><br></pre></td></tr></table></figure>
<p>也可以使用正则表达式调用<code>subscribe()</code>方法，如果有人创建了和正则表达式匹配的新主题，平衡操作基本上会立即发生，消费者就会从新主题中立即开始消费。这种方式对于需要从多个主题消费消息的应用程序非常有用，这样就可以处理不同主题包含的不同类型的数据。为了订阅所有的<code>test</code>主题，调用方式如下：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(<span class="string">"test.*"</span>);</span><br></pre></td></tr></table></figure>
<h2 id="轮询循环">轮询循环</h2><p>消费者应用程序编程接口（API）的核心是一个简单的循环，会负责从服务端拉取更多的数据。一旦消费者订阅了主题，轮询循环会处理所有的协调细节、分区平衡、心跳、数据获取。返回给开发者的只是很简洁的API，仅仅返回分配分区的可用数据。消费者客户端代码的主体如下：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">while</span> (<span class="keyword">true</span>) &#123; <span class="comment">//1</span></span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>); <span class="comment">//2</span></span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123; <span class="comment">//3</span></span><br><span class="line">      log.debug(<span class="string">"topic=%s,partition=%s,offset=%d,customer=%s,country=%s\n"</span>,</span><br><span class="line">        record.topic(), record.partition(), </span><br><span class="line">        record.offset(), record.key(), record.value());</span><br><span class="line"></span><br><span class="line">      <span class="keyword">int</span> updatedCount = <span class="number">1</span>;</span><br><span class="line">      <span class="keyword">if</span> (custCountryMap.countainsValue(record.value())) &#123;</span><br><span class="line">        updatedCount = custCountryMap.get(record.value()) + <span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      custCountryMap.put(record.value(), updatedCount)</span><br><span class="line"></span><br><span class="line">      JSONObject json = <span class="keyword">new</span> JSONObject(custCountryMap);</span><br><span class="line">      System.out.println(json.toString(<span class="number">4</span>)) <span class="comment">//4</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  consumer.close(); <span class="comment">//5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>这里确实是一个死循环，消费者通常都是长时间运行的应用程序，会持续从Kafka中拉取数据。后面我们会展示如何干净地退出循环，并且关闭消费者。</li>
<li>这一行是本章最重要的一行代码。就像鲨鱼要么保持不断游动要么死亡，消费者必须一直轮询Kafka，否则就会被认为挂掉了，就会导致消费的分区被组中其他的消费者继续处理。</li>
<li><code>poll()</code>方法返回一批记录集。每条记录会包含这条记录来源于哪个主题和分区、这条记录在分区中的偏移量、当然还有这条记录的键值。通常我们会迭代列表，并且处理每一条单机的记录。<code>poll()</code>方法也可以接受一个超时时间参数，表示执行轮询最多花费多长时间，不管轮询的结果有没有数据。这个超时值通常由应用程序是否需要快速响应来决定，也就是你要在轮询之后多快返回对主线程的控制（消费者的轮询是单线程阻塞的，所以如果想要尽快在拉取到消息后马上处理，可以缩短超时时间，当时间超过后，轮询结束，就可以执行消息处理逻辑）。</li>
<li>消息处理通常最后会写入到数据存储系统或者更新已有的记录。本例的目标是为了跟踪每个国家的顾客数量，所以我们更新了字典表然后将结果打印为JSON字符串，实际应用中一般会将更新记录写入到存储系统中。</li>
<li>总是在退出时执行<code>close()</code>方法。这会关闭网络连接和Socket，并且会立即触发平衡操作，而不是让协调者来发现消费者可能因为挂掉而没有及时发送心跳，那样会等待更长的时间，也会导致分区子集的消息在更长的时间内不能被任何消费者所消费。</li>
</ol>
<p>轮询操作不仅仅做了获取数据这个工作。当新的消费者第一次调用<code>poll()</code>方法时，它会负责找到协调者、加入消费组、接收到分配的分区。如果需要做平衡操作，也是在<code>poll()</code>方法中处理的。当然用来表示消费者存活状态的心跳请求也是在<code>poll()</code>中发送的。基于这些原因，我们要确保迭代处理消息时要足够快速和高效。</p>
<p>注意：你不能在一个线程中拥有属于同一个消费者的多个消费者，而且也不能在同一个消费者中使用多线程。一个线程对应一个消费者是最基本的原则（这里的线程指的是主线程，而不是消费者中的拉取线程，一个消费者实际上是可以有多个拉取线程的）。</p>
<blockquote>
<p>在一个应用程序中如果要处理同一个消费组的多个消费者，你需要保证每个消费者运行在自己的线程中。通常将消费者逻辑保证成自定义的对象，然后使用Java的<code>ExecutorService</code>来启动各自的消费者线程。</p>
</blockquote>
<h2 id="提交和偏移量">提交和偏移量</h2><p>无论什么时候我们调用<code>poll()</code>方法时，它会返回已经写入到Kafka，但是消费组的消费者还没有读取的记录。这就意味着我们需要有一种方式来跟踪消费者读取到了哪条记录。前面讨论过，Kafka不同于其他消息系统的一个独有特性是，它不会从消费者中跟踪应答。相反，它允许消费者使用Kafka来跟踪每个分区的位置（偏移量）。我们把更新分区中当前位置这个动作叫做提交（commit）。  </p>
<p>那么消费者如何提交偏移量呢？它会往Kafka的一个特殊主题<code>__consumer_offsets</code>生产消息，这个主题保存了每个分区的提交位置。只要你的消费者是存活的、正在运行，也不会对它有任何影响。但是如果消费者挂了或者新消费者加入消费组，就会触发平衡。在平衡过后，每个消费者可能会被分配到和之前所处理的不同的新分区集合。为了明确要从哪里开始工作，消费者会读取每个分区最近的提交位置，然后从那个位置继续。  </p>
<p>如果分区的提交位置比消费者客户端处理的最近一条消息的位置要小，那么在最近处理消息的位置和提交位置之间的消息就会被（消费者）处理两次。<br><img src="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/assets/ch04_offsets_01.jpg" alt=""></p>
<p>相反如果提交位置比消费者实际处理的最近一条消息位置大，那么这两个位置中间的所有消息就都不会被消费组处理了（当然我们要尽量避免这种丢数据的场景）。<br><img src="https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/assets/ch04_offsets_02.jpg" alt=""></p>
<p>可见对偏移量（即上文说的位置）的管理对客户端应用程序而言影响很大。<code>KafkaConsumer</code>的使用接口提供了多种提交偏移量的方式。  </p>
<h3 id="自动提交偏移量">自动提交偏移量</h3><p>提交偏移量最简单的方式是让消费者为你做这件事情。如果你设置了<code>enable.auto.commit=true</code>，那么每隔5秒消费者就会提交客户端轮询结果的最大偏移量。5秒是一个默认值，通过配置项<code>auto.commit.interval.ms</code>控制。就像消费者的其他逻辑一样，自动提交偏移量也是由<code>poll()</code>轮询驱动的。当你调用<code>poll()</code>轮询时，消费者会检查是否可以开始提交，如果需要提交，就会在最近一次轮询返回时提交偏移量。不过，在使用这个简便的选项之前，理解这种方式的一些不良后果是非常重要的。  </p>
<p>Consider that by defaults automatic commit occurs every 5 seconds. Suppose that we are 3 seconds after the most recent commit and a rebalance is triggered. After the rebalancing all consumers will start consuming from the last offset committed. In this case the offset is 3 seconds old, so all the events that arrived in those 3 seconds will be processed twice. It is possible to configure the commit interval to commit more frequently and reduce the window in which records will be duplicated, but it is impossible to completely eliminate them.</p>
<p>Note that with auto-commit enabled, a call to poll will always commit the last offset returned by the previous poll. It doesn’t know which events were actually processed, so it is critical to always process all the events returned by poll before calling poll again (or before calling close(), it will also automatically commit offsets). This is usually not an issue, but pay attention when you handle exceptions or otherwise exit the poll loop prematurely.</p>
<blockquote>
<p>考虑默认场景下自动提交会每隔5秒发生一次。假设在最近一次提交过后3秒发生了一次平衡操作。平衡过后所有的消费者都会从上次最近提交的偏移量开始消费。这种场景下偏移量是3秒之前的了，这就会导致在这3秒内到达的所有事件都会被处理两次。尽管可以设置更短的提交间隔，以便更频繁地提交，减少消息被重复处理的窗口，但还是无法从根本上彻底解决数据重复处理的问题。  </p>
</blockquote>
<p>注意在开启自动提交时，调用<code>poll</code>总是会提交上一次<code>poll</code>的最近偏移量。但是它并不知道都实际处理了哪些事件，所以客户端应该总是要保证在调用新的<code>poll</code>之前要处理上一次<code>poll</code>返回的所有事件（或者在调用close之前，也会自动提交偏移量）。通常这不是一个严重的问题，但在处理异常或者过早地退出轮询循环时需要特别注意。  </p>
<p>自动提交偏移量非常方便，但是它的缺点也很明显：它不能够给予开发者足够的控制权来避免消息的重复处理。  </p>
<h3 id="手动提交偏移量">手动提交偏移量</h3><p>Most developers use to exercise more control over the time offsets are committed. Both to eliminate the possibility of missing messages and to reduce the number of messages duplicated during rebalancing. Te consumer API has the option of committing the current offset at a point that makes sense to the application developer rather than based on a timer.</p>
<p>By setting auto.commit.offset = false, offsets will only be committed when the application explicitly chooses to do so. The simplest and most reliable of the commit APIs is commitSync(). This API will commit the latest offset returned by poll() and return once the offset is committed, throwing an exception if commit fails for some reason.</p>
<p>It is important to remember that commitSync() will commit the latest offset returned by poll(), so make sure you call commitSync() after you are done processing all the records in the collection, or you risk missing messages as described above. Note that when rebalance is triggered, all the messages from the beginning of the most recent batch until the time of the rebalance will be processed twice.</p>
<p>Here is how we would use commitSync to commit offsets once we finished processing the latest batch of messages:</p>
<blockquote>
<p>大多数开发者都希望在定时提交偏移量上能够对偏移量有更多足够的掌控能力，他们的想法不仅仅是要消除丢失数据的可能性，也希望在平衡发生时减少消息的重复处理数量。消费者API提供了在某个点上提交当前偏移量的选项，相比基于定时器的自动提交方式，这种方式对应用程序开发者而言更有意义。  </p>
</blockquote>
<p>通过设置<code>auto.commit.offset=false</code>，偏移量只会在应用程序显示调用时才会被提交。最简单和可靠的提交API是<code>commitSync()</code>方法。该API会提交<code>poll()</code>返回时的最近偏移量，并且一旦偏移量被提交后就会返回，如果因为某种原因提交失败了则会抛出异常。  </p>
<p>注意<code>commitSync()</code>提交的会对<code>poll()</code>返回时提交这批数据的最近偏移量，所以要确保处理完集合中的所有记录后才调用<code>commitSync()</code>，否则你就会面临前面提到的丢失数据问题（先提交偏移量然后才处理记录，如果处理某条记录失败了，这条失败的记录以及之后的记录都不会有机会被处理）。当触发平衡时，从最近一批记录的最开始直到发生平衡这个时间点的所有消息都会被处理两次（消息重复处理仍然不可避免，比如处理顺序为：提交偏移量=5，拉取一批消息共10条，然后开始处理记录，假设只处理了3条就发生了平衡，平衡之后，消费者会重新从位置5开始处理，所以这批消息的开始到发生平衡时的3条记录就会被重新处理）。  </p>
<p>下面的代码示例中，一旦在处理完最近的一批数据后，使用<code>commitSync()</code>方法提交偏移量：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">  <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">    println(<span class="string">"topic=%s,partition=%s,offset=%d,customer=%s,country=%s"</span>,</span><br><span class="line">      record.topic(), record.partition(), </span><br><span class="line">      record.offset(), record.key(), record.value()); <span class="comment">//1</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    consumer.commitSync(); <span class="comment">//2</span></span><br><span class="line">  &#125; <span class="keyword">catch</span> (CommitFailedException e) &#123;</span><br><span class="line">    log.error(<span class="string">"commit failed"</span>, e) <span class="comment">//3</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>这里假设打印一条记录的内容表示已经处理完了该记录。你实际的应用程序肯定比这要复杂，你应该根据你的实际用例决定什么时候记录被处理完成。</li>
<li>一旦处理完了这一批的所有记录，我们调用了<code>commitSync()</code>提交这一批的最近偏移量，并且这个操作发生在下一次轮询拉取新消息之前。</li>
<li>在没有不可恢复的错误下，调用<code>commitSync()</code>提交偏移量失败应该重试，这里只记录错误日志。</li>
</ol>
<h3 id="异步提交偏移量">异步提交偏移量</h3><p>One drawback of manual commit is that the application is blocked until the broker responds to the commit request. This will limit the throughput of the application. Throughput can be improved by committing less frequently, but then we are increasing the number of potential duplicates that a rebalance will create.</p>
<blockquote>
<p>手动提交的一个缺点是应用程序会在代理返回提交请求的响应之前一直被阻塞（客户端向Kafka的代理节点发送提交偏移量请求，在这期间，应用程序无法执行其他业务逻辑），这会直接限制了应用程序的吞吐量。虽然可以通过频度更少的提交来提高吞吐量，但代价是在发生平衡时增加了重复处理的可能性。  </p>
</blockquote>
<p>另外一个选项是异步提交API，我们发送完提交请求后继续后续的业务逻辑处理，而不需要等待代理节点返回提交响应。<code>commitAsync()</code>方法会提交偏移量，然后继续向下运行（即开始新的轮询）。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">  <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">    <span class="comment">//处理记录，比如简单的打印，略</span></span><br><span class="line">  &#125;</span><br><span class="line">  consumer.commitAsync(); <span class="comment">// 1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The drawback is that while commitSync() will retry the commit until it either succeeds or encounters a non-retriable failure, commitAsync() will not retry. The reason it does not retry is that by the time commitAsync() receives a response from the server, there may have been a later commit which was already successful. Imagine that we sent a request to commit offset 2000. There is a temporary communication problem, so the broker never gets the request and therefore never respond. Meanwhile, we processed another batch and successfully committed offset 3000. If commitAsync() now retries the previously failed commit, it may succeed in committing offset 2000 after offset 3000 was already processed and committed. In case of a rebalance, this will cause more duplicates.</p>
<p>We are mentioning this complication and the importance of correct order of commits, because commitAsync() also gives you an option to pass in a callback that will be triggered when the broker responds. It is common to use the callback to log commit errors or to count them in a metric, but if you want to use the callback for retries, you need to be aware of the problem with commit order.</p>
<blockquote>
<p>同步的<code>commitSync()</code>方法缺点是如果提交成功会一直尝试提交，或者在遇到无法重试的场景下才会结束，但异步的<code>commitAsync()</code>方法不管有没有执行成功都不会重试。不需要重试的原因是当<code>commitAsync()</code>接收到服务端的响应时，可能已经存在一个更新的提交已经成功了。假设我们发送了一个要提交偏移量为2000的请求，但是由于临时的通信问题，代理节点从来就没有机会收到这个请求，所以也就不会发送响应。同时我们处理了新的一批数据，并且成功地提交了偏移量=3000。如果第一次的<code>commitAsync()</code>由于提交失败现在执行重试，它这是可能也会成功地提交了偏移量=2000，但实际上偏移量=3000已经完成并提交成功了，在平衡的场景下，这种场景会导致更多的重复处理。  </p>
</blockquote>
<p>我们提到的这种混乱主要是为了让大家知道提交顺序的重要性，<code>commitAsync()</code>方法还提供了自定义的处理作为回调函数传入，自定义的回调函数会在代理返回响应时触发执行。通常会使用回调函数记录提交错误或者在监控系统中计数，但如果你要用回调来做重试的话，你就需要担心提交顺序的问题。下面的代码中我们发送了提交请求，然后继续，但如果提交发生失败，会记录失败的偏移量，以及异常原因。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">  <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">    <span class="comment">//处理记录，比如简单的打印，略</span></span><br><span class="line">  &#125;</span><br><span class="line">  consumer.commitAsync(<span class="keyword">new</span> OffsetCommitCallback() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span><br><span class="line">                           Exception exception)</span> </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (e != <span class="keyword">null</span>) log.error(<span class="string">"Commit failed for offsets &#123;&#125;"</span>, offsets, e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;); <span class="comment">//1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>A simple pattern to get commit order right for asynchronous retries is to use a monotonically increasing sequence number. Increase the sequence number every time you commit and the sequence number at the time of the commit to the asyncCommit callback. When you’re getting ready to send a retry, check if the commit sequence number the callback got is equal to the instance variable, if it is - there was no newer commit and it is safe to retry. If the instance sequence number is higher, don’t retry since a newer commit was already sent.</p>
<p>保证异步重试时提交顺序的正确性，可以使用单调递增的序号。每次提交时增加序号，并且在异步提交的回调里执行成功时也增加序号。当你准备重试时，检查回调总的提交序列号是否和实例变量的相同，如果相同，说明没有新的提交，那么就可以安全地重试。但如果实例变量比回调的序号要搞，就不需要重试，因为新的提交已经发送出去了。  </p>
</blockquote>
<h3 id="结合同步和异步提交">结合同步和异步提交</h3><p>Normally, occasional failures to commit without retrying are not a huge problem, since if the problem is temporary the following commit will be successful. But if we know that this is the last commit before we close the consumer, or before a rebalance, we want to make extra sure that the commit succeeds.</p>
<blockquote>
<p>通常，偶然性出现提交故障并不是一个大问题，因为如果这个问题是短暂的，后续的提交也会成功。但如果在关闭消费者之前，我们确切地知道这就是最后一次提交了，或者在发生平衡之前，我们都要确保提交必须成功。</p>
</blockquote>
<p>Therefore a common pattern is to combine commitAsync with commitSync just before shutdown. Here is how it works (We will discuss how to commit just before rebalance when we get to the section about rebalance listeners):  </p>
<blockquote>
<p>因此一种普遍的做法是在关闭之前结合使用<code>commitAsync()</code>和<code>commitSync</code>（后面我们会在谈到平衡监听器时讨论怎么在平衡之前提交），做法如下：  </p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">      <span class="comment">//处理记录，略</span></span><br><span class="line">    &#125;</span><br><span class="line">    consumer.commitAsync(); <span class="comment">//1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">  log.error(<span class="string">"Unexpected error"</span>, e);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    consumer.commitSync(); <span class="comment">//2</span></span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>当一切工作的很正常时，我们使用异步的<code>commitAsync</code>，它很快，而且如果一次提交失败了，下一次会重试</li>
<li>在关闭消费者时，不会有下一次提交了，我们就调用同步的<code>commitSync</code>，它会重试直到提交成功</li>
</ol>
<h3 id="提交指定的偏移量">提交指定的偏移量</h3><p>Committing the latest offset only allows you to commit as often as you finish processing batches. But what if you want to commit more frequently than that? What if poll() returns a huge batch and you want to commit offsets in the middle of the batch to avoid having to process all those rows again if a rebalance occures? You can’t just call commitSync() or commitAsync() - this will commit the last offset returned, which you didn’t get to process yet.</p>
<p>Fortunately, the consumer API allows you to call commitSync() and commitAsync() and pass a map of partitions and offsets that you wish to commit. If you are in the middle of processing a batch of records, and the last message you got from partition 3 in topic “customers” has offset 5000, you can call commitSync() to commit offset 5000 for partition 3 in topic “customers”. Since your consumer may be consuming more than a single partition, you will need to track offsets on all of them, so moving to this level of precision in controlling offset commits adds complexity to your code.</p>
<blockquote>
<p>提交最近的偏移量只会允许你在处理完批记录后才会提交，但如果你想要更频繁地提交呢？如果说<code>poll()</code>返回的是一批很大的记录集，你想要在这批记录集的中间某个位置提交偏移量，避免在平衡发生时不得不重新处理这些所有的记录？你不能仅仅调用<code>commitSync()</code>或者<code>commitAsync()</code>，它们只会提交返回的最近偏移量，而返回的这些记录你都还没有执行。</p>
</blockquote>
<p>幸运的是，消费者API允许你调用<code>commitSync()</code>和<code>commitAsync()</code>时传递一个你希望提交的分区和偏移量的字典。如果你已经处理了一批记录的中间，并且你从主题为“客户端”的分区3得到的最近一条消息的偏移量=5000，你可以立即调用<code>commitSync()</code>来提交主题为“客户端”分区3的偏移量。由于你的消费者可能会消费多个分区，你需要跟踪所有分区的偏移量，所以用这种更细粒度的方式控制偏移量的提交会增加你的代码的复杂性。下面是提交指定偏移量的代码片段：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets; <span class="comment">//1</span></span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">....</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">  <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">    <span class="comment">//处理逻辑，略 //2</span></span><br><span class="line">    currentOffsets.put(<span class="keyword">new</span> TopicPartition(record.topic(),record.partition()),</span><br><span class="line">                       record.offset()); <span class="comment">//3</span></span><br><span class="line">    <span class="keyword">if</span> (count % <span class="number">1000</span> == <span class="number">0</span>)   <span class="comment">//4</span></span><br><span class="line">      consumer.commitAsync(currentOffsets); <span class="comment">//5</span></span><br><span class="line">    count++;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>我们使用这个Map字典结构用来手动跟踪偏移量</li>
<li>这里用打印记录的方式代替实际的业务处理</li>
<li>读取完每条记录后，用最近的偏移量更新偏移量字典</li>
<li>这里我们决定每隔1000条记录提交一次，实际应用中可以根据时间提交甚至是记录内容</li>
<li>这里选择调用<code>commitAsync</code>，不过<code>commitSync</code>也同样有效。当然提交指定的偏移量，也仍然需要处理前面章节中提到的错误。</li>
</ol>
<h2 id="平衡监听器">平衡监听器</h2><p>As we mentioned in previous section about committing offsets, a consumer will want to do some cleanup work before exiting and also before partition rebalancing.</p>
<p>If you know your consumer is about to lose ownership of a partition, you will want to commit offsets of the last event you’ve processed. If your consumer maintained a buffer with events that it only processes occasionally (for example, the currentRecords map we used when explaining pause() functionality), you will want to process the events you accumulated before losing ownership of the partition. Perhaps you also need to close file handles, database connections and such.</p>
<blockquote>
<p>前面章节中说过提交偏移量时，消费者会在<strong>分区平衡之前</strong>或者<strong>退出时</strong>执行一些清理工作。如果你知道消费者即将失去一个分区的所有权，你应当要提交已处理完最近事件的偏移量。如果你的消费者维护了一个事件缓冲区，并且偶尔才会处理一次（比如在使用<code>pause()</code>功能时会使用currentRecords字典暂存记录），你也应当在失去分区的所有权之前处理目前为止收集的所有事件。也许还需要做其他的工作比如关闭文件句柄，释放数据库连接等等。  </p>
</blockquote>
<p>消费者API允许你在消费者所属的分区被添加和移除时，运行自定义的代码逻辑。可以通过在调用<code>subscribe()</code>方法时传递一个<code>ConsumerRebalanceListener</code>监听器来完成，该监听器接口有两个需要的方法：  </p>
<ul>
<li><code>public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions)</code>会在平衡开始之前以及消费者停止消费消息之后调用。在这里通常要提交偏移量，这样无论下一个消费者是谁，它获得到分区后，就知道要从哪里开始。</li>
<li><code>public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions)</code>会在分区重新分配给消费者之后，在消费者开始消费消息之前调用。</li>
</ul>
<p>下面的示例展示了如何使用<code>onPartitionsRevoked()</code>方法在失去一个分区的所有权之前提交偏移量。后面我们会展示同时模拟使用了<code>onPartitionsAssigned()</code>方法的更复杂示例。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">HandleRebalance</span> <span class="keyword">implements</span> <span class="title">ConsumerRebalanceListener</span> </span>&#123; <span class="comment">//1</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span>&#123;<span class="comment">//2</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">    consumer.commitSync(currentOffsets); <span class="comment">//3</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  consumer.subscribe(topics, <span class="keyword">new</span> HandleRebalance()); <span class="comment">//4</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">      <span class="comment">//处理记录，略</span></span><br><span class="line">      currentOffsets.put(</span><br><span class="line">            <span class="keyword">new</span> TopicPartition(record.topic(), record.partition()),</span><br><span class="line">            record.offset());</span><br><span class="line">    &#125;</span><br><span class="line">    consumer.commitAsync(currentOffsets);</span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (WakeupException e) &#123;</span><br><span class="line">  <span class="comment">// ignore, we're closing</span></span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">  log.error(<span class="string">"Unexpected error"</span>, e);</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    consumer.commitSync(currentOffsets);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>In this example we don’t need to do anything when we get a new partition, we’ll just start consuming messages.</p>
<p>3<br>However, when we are about to lose a partition due to rebalancing, we need to commit offsets. Note that we are committing the latest offsets we’ve processed, not the latest offsets in the batch we are still processing. This is because a partition could get revoked while we are still in the middle of a batch. We are committing offsets for all partitions, not just the partitions we are about to lose - since the offsets are for events that were already processed, there is no harm in that. Last, note that we are using syncCommit() to make sure the offsets are committed before the rebalance proceeds.</p>
<p>4<br>The most important part - pass the ConsumerRebalanceListener to subscribe() method so it will get invoked by the consumer.</p>
<ol>
<li>我们从实现<code>ConsumerRebalanceListener</code>监听器开始</li>
<li>本例中，在分配到新分区之后我们没有做任何事情，接下来只是消费消息而已</li>
<li>然而，由于平衡导致失去分区的控制权时，需要提交偏移量。注意我们提交的是已经处理完的消息的最近偏移量，而不是当前一批仍然在处理的最近偏移量，因为在处理一批记录的中间也有可能分区被取消（这样可以最大限度地减少平衡之后重复处理的数据量，但还是不可避免数据重复）。同时我们会提交所有分区的偏移量（属于当前消费者的），而不是我们即将失去的某些分区，因为<code>currentOffsets</code>字典针对的是所有已经处理完的事件，所以这并没有什么大的影响。最后，我们使用了同步的<code>syncCommit</code>来确保在平衡发生时成功地提交了偏移量。</li>
<li>最重要的一部分，将步骤1创建的监听器传递给<code>subscribe()</code>方法，这样就可以被消费者调用</li>
</ol>
<h2 id="正好一次的处理语义">正好一次的处理语义</h2><p>So far we’ve seen how to use poll() to start consuming messages from the last committed offset in each partition and to proceed in processing all messages in sequence. However, sometimes you want to start reading at a different offset.</p>
<p>If you want to start reading all messages from the beginning of the partition, or you want to skip all the way to the end of the partition and start consuming only new messages, there are APIs specifically for that: seekToBeginning(TopicPartition tp) and seekToEnd(TopicPartition tp).</p>
<blockquote>
<p>目前为止我们已经看到了如何使用<code>poll()</code>从每个分区的最近提交位置开始消费消息，并且顺序地处理所有的消息。不过有时候，你可能想要在一个不同的任务位置开始读取（而不总是从最近的位置）。  </p>
</blockquote>
<p>如果你想要从分区的最开始位置读取所有的消息，或者你想要跳过中间所有的数据直接到达分区的最末尾，只想要消费新的消息。也确实有这样的API供你使用：<code>seekToBeginning()</code>和<code>seekToEnd()</code>分别满足了上面两种需求。  </p>
<p>However, the Kafka API also lets you seek to a specific offset. This ability can be used in a variety of ways, for example to go back few messages or skip ahead few messages (perhaps a time-sensitive application that is falling behind will want to skip ahead to more relevant messages), but the most exciting use-case for this ability is when offsets are stored in a system other than Kafka.</p>
<p>Think about this common scenario: Your application is reading events from Kafka (perhaps a clickstream of users in a website), processes the data (perhaps clean up clicks by robots and add session information) and then store the results in a database, NoSQL store or Hadoop. Suppose that we really don’t want to lose any data, nor do we want to store the same results in the database twice.</p>
<blockquote>
<p>不过Kafka API还允许你定位到指定的位置（在谈到提交时我们会说提交偏移量，在谈到定位时我们会说位置，位置这个概念用在现实生活中表示要到哪个地方，而偏移量更多表示的是处于一种什么状态，提交时主要关注的是状态数据，当然你不需要纠结这么多，位置和偏移量其实是相同的概念）。这种特性可以用在很多地方，比如回退几个消息重新处理，或者跳过一些消息（也许是一个时间敏感的应用程序，如果数据处理的进度落后太多时，你会想跳到最近的时间点，因为这些消息更能表示相关的当前状态）。但这种特性最令人兴奋的一个用例是：将偏移量存储到其他系统而不是Kafka中。  </p>
</blockquote>
<p>考虑下面的通用场景：应用程序从Kafka中读取事件（也许是一个网站的用户点击流）、处理数据（也许是清理机器点击，添加会话信息），然后存储结果到数据库、NoSQL或者Hadoop。假设我们真的不希望丢失任何数据，也不希望存储两份相同的数据。这种场景下，消费者的循环会是这样的：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">  <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">    currentOffsets.put(</span><br><span class="line">        <span class="keyword">new</span> TopicPartition(record.topic(), record.partition()),</span><br><span class="line">        record.offset());</span><br><span class="line">    processRecord(record); <span class="comment">//处理每一条记录</span></span><br><span class="line">    storeRecordInDB(record); <span class="comment">//存储记录到数据库、NoSQL或Hadoop</span></span><br><span class="line">    consumer.commitAsync(currentOffsets); <span class="comment">//提交偏移量</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Note that we are very paranoid, so we commit offsets after processing each record. However, there is still a chance that our application will crash after the record was stored in the database but before we committed offsets, causing the record to be processed again and the database to contain duplicates.</p>
<p>This could be avoided if there was only a way to store both the record and the offset in one atomic action. Either both the record and the offset are committed, or neither of them are committed. As long as the records are written to a database and the offsets to Kafka, this is impossible.</p>
<p>But what if we wrote both the record and the offset to the database, in one transaction? Then we’ll know that either we are done with the record and the offset is committed or we are not, and the record will be reprocessed.</p>
<p>Now the only problem is: if the record is stored in a database and not in Kafka, how will our consumer know where to start reading when it is assigned a partition? This is exactly what seek() can be used for. When the consumer starts or when new partitions are assigned, it can look up the offset in the database and seek() to that location.</p>
<p>请注意，我们非常偏执，在处理完每条记录都执行一次提交偏移量的动作。但是即使如此，应用程序仍然有可能会在存储到数据库之后，并且在提交偏移量之前挂掉，从而导致记录会被重新处理，最终数据库中的记录仍然会有重复的。  </p>
<p>如果存在一种方式只有以原子操作的方式同时存储记录和偏移量，就可以避免上面的问题了。原子的意思是：记录和偏移量要么同时都提交了，要么都没有提交。只要是记录写到数据库，而偏移量写到Kafka中，这就不可能做到原子操作（毕竟写两个不同的存储系统是没有事务保证的）。  </p>
<p>但如果我们在一个事务中同时写入记录和偏移量到数据库中呢？那么我们就会知道要么我们处理完了这条记录并且偏移量也成功提交了，要么这两个操作都没有发生，后者就会重新处理记录（因为记录和偏移量都没有成功存储，所以重复处理并不会使得存储系统有重复的数据）。</p>
<p>现在问题只有一个了：如果将记录存储在数据库而不是Kafka，我们的应用程序怎么知道要从分配分区的哪里开始读取？这就是<code>seek()</code>方法发挥作用的地方。当消费者启动或者分配到新的分区，可以先去数据库中查询分区的最近偏移量，然后通过<code>seek()</code>方法定位到这个位置。  </p>
<p>下面的代码是这种做法的基本骨架，我们使用了<code>ConsumerRebalanceLister</code>监听器和<code>seek()</code>方法，来确保从数据库中存储的偏移量开始处理。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//消费者平衡的监听器</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SaveOffsetsOnRebalance</span> <span class="keyword">implements</span> <span class="title">ConsumerRebalanceListener</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span>&#123;</span><br><span class="line">    commitDBTransaction(); <span class="comment">//1</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(TopicPartition partition: partitions)</span><br><span class="line">      consumer.seek(partition, getOffsetFromDB(partition)); <span class="comment">//2</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//消费者主逻辑</span></span><br><span class="line">consumer.subscribe(topics, <span class="keyword">new</span> SaveOffsetOnRebalance(consumer));</span><br><span class="line">consumer.poll(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (TopicPartition partition: consumer.assignment())</span><br><span class="line">  consumer.seek(partition, getOffsetFromDB(partition));   <span class="comment">//3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">  <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">    processRecord(record);</span><br><span class="line">    storeRecordInDB(record);</span><br><span class="line">    storeOffsetInDB(record.topic(),record.partition(),record.offset());<span class="comment">//4</span></span><br><span class="line">  &#125;</span><br><span class="line">  commitDBTransaction();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>我们使用了一个虚构的方法来保证提交事务操作到数据库中。这里的考虑是在处理记录的时候会插入记录和偏移量到数据库中，所以我们只需要在即将事务分区的所有权时提交这个事务，来确保这些信息被持久化。</li>
<li>我们还有一个虚构的方法会从数据库中读取分区的偏移量，然后在获得新分区的所有权时通过消费者的<code>seek()</code>方法定位到这些记录。</li>
<li>当消费者订阅主题并第一次启动时，立即调用一次无阻塞的<code>poll(0)</code>方法，来确保加入消费组，并且得到分配的分区。然后紧接着调用<code>seek()</code>定位到分配给我们（当前消费者）的分区的正确位置。注意<code>seek()</code>仅仅更新了我们要从哪里开始消费的位置，所以下一次调用<code>poll()</code>才会开始拉取正确的消息。如果在<code>seek()</code>时发生错误（比如偏移量不存在），调用<code>poll()</code>时就会抛出异常。</li>
<li>又一个虚构的方法，这次我们更新了数据库中存储偏移量的一张表。这里我们假设更新记录的操作很快就完成了，所以我们在每条记录上都执行了更新操作。不过提交偏移量是比较慢的，所以我们只在一批数据都处理完成后才执行提交操作。不过这里面仍然有很多优化的方法。</li>
</ol>
<p>通过将偏移量和数据一起存储在外部存储系统中，有多种方式可以实现正好一次的语义，不过所有的这些方法都需要使用<code>ConsumerRebalanceListener</code>和<code>seek()</code>一起协调，来确保偏移量及时地被存储，这样消费者就可以从正确的位置开始读取消息（只有发生平衡后分配到新的分区才需要从数据库中读取偏移量）。</p>
<h2 id="如何退出">如何退出</h2><p>When you decide to exit the poll loop, you will need another thread to call consumer.wakeup(). If you are running the consumer loop in the main thread, this can be done from a ShutdownHook. Note that consumer.wakeup() is the only consumer method that is safe to call from a different thread. Calling wakeup will cause poll() to exit with WakeupException, or if consumer.wakeup() was called while the thread was not waiting on poll, the exception will be thrown on the next iteration when poll is called. The WakeupException doesn’t need to be handled, it was just a way of breaking out of the loop, but it is important that before exitting the thread, you will call consumer.close(), this will do any last commits if needed and will send the group coordinator a message that the consumer is leaving the group, so rebalancing will be triggered immediately and you won’t need to wait for the session to time out.</p>
<blockquote>
<p>本章开始当我们在讨论轮询循环时，我们说过不用担心消费者的轮询是在一个无限死循环里，这里我们来讨论下如何干净地退出循环。当你决定要退出轮询循环时，你（当前消费者线程）需要其他的线程（其他消费者线程）调用<code>consumer.wakeup()</code>（这里的consumer是第一个消费者的线程）。如果你在主线程中正在运行消费者的循环，可以通过<code>ShutdownHook</code>钩子的方式来完成（退出时会在主线程中执行钩子函数）。注意<code>consumer.wakeup()</code>是唯一一个可以在其他线程中被安全调用的消费者方法。调用<code>wakeup()</code>方法会使得被阻塞的<code>poll()</code>方法退出，并且附带了一个<code>WakeupException</code>异常。或者如果在调用<code>consumer.wakeup()</code>时，主线程并没有被阻塞在<code>poll()</code>方法上时，异常会在下次调用<code>poll()</code>的时候才抛出。  </p>
</blockquote>
<p>不需要特别地处理<code>WakeupException</code>异常，它只是表示退出循环的一种方式而已。不过重要的是在退出线程之前，你应该调用<code>consumer.close()</code>方法。必要的话还应该做最后的一次提交，并且向协调者通知消费者正在离开消费组。这样平衡就会被立即触发，你就不需要等到会话超时时才会被协调者检测出来。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread() &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    System.out.println(<span class="string">"准备退出消费者客户端..."</span>);</span><br><span class="line">    consumer.wakeup(); <span class="comment">//1</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      mainThread.join();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="comment">// 循环直到通过Ctrl-C中断客户端，关闭钩子会在退出时执行清理工作</span></span><br><span class="line">  <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = movingAvg.consumer.poll(<span class="number">1000</span>);</span><br><span class="line">    System.out.println(System.currentTimeMillis() + <span class="string">"--等待新数据..."</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">      System.out.printf(<span class="string">"offset:%d,key:%s,value:%s\n"</span>,</span><br><span class="line">        record.offset(),record.key(),record.value());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (TopicPartition tp: consumer.assignment())</span><br><span class="line">      System.out.println(<span class="string">"提交偏移量的位置:"</span> + consumer.position(tp));</span><br><span class="line">    movingAvg.consumer.commitSync();</span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> (WakeupException e) &#123;</span><br><span class="line">    <span class="comment">// ignore for shutdown  //2</span></span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close(); <span class="comment">//3</span></span><br><span class="line">    System.out.println(<span class="string">"消费者关闭成功，结束！"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>关闭钩子运行在独立的线程（不同于主线程），所以唯一安全的操作是调用<code>wakeup()</code>中断轮询循环。</li>
<li>其他线程调用<code>wakeup()</code>会导致<code>poll()</code>调用抛出<code>WakeupException</code>异常。你应该捕获这个异常，确保应用程序不会以非预期的方式退出，不过这里不需要做任何事情。</li>
<li>在退出消费者之前，确保干净地关闭消费者。</li>
</ol>
<h2 id="反序列化">反序列化</h2><p>As discussed in the previous chapter, Kafka Producers require serializers to convert objects into byte arrays that are then sent to Kafka. Similarly, Kafka Consumers require deserializers to convert byte arrays recieved from Kafka into Java objects. In previous examples, we just assumed that both the key and the value of each message are Strings and we used the default StringDeserializer in the Consumer configuration.</p>
<p>In the previous chapter about the Kafka Producer, we’ve seen how to serialize custom types and how to use Avro and AvroSerializers to generate Avro objects from schema definitions and then serialize them when producing messages to Kafka. We will now look at how to create custom deserializers for your own objects and how to use Avro and its deserializers.</p>
<blockquote>
<p>上一章中Kafka的生产者要求将对象序列化为字节数组以便发送给Kafka。类似的Kafka的消费者要求从Kafka中读出的二进制字节数组反序列化为Java对象。前面的示例中我们在消费者的配置中只是假设了使用了默认的<code>StringDeserializer</code>反序列化器。  </p>
</blockquote>
<p>在上一章关于生产者，我们看到了如何序列化自定义的类型，怎么使用Avro和<code>AvroSerializers</code>序列化器从模式定义生成Avro对象，最后经过序列化后向Kafka生产消息。现在我们会看下怎么对你的对象使用自定义的反序列化器，以及如何使用Avro和它的反序列化器。  </p>
<p>It should be obvious that the serializer that was used in producing events to Kafka must match the deserializer that will be used when consuming events. Serializing with IntSerializer and then deserializing with StringDeserializer will not end well. This means that as a developer you need to keep track of which serializers were used to write into each topic, and make sure each topic only contains data that the deserializers you use can interpret. This is one of the benefits of using Avro and the Schema Repository for serializing and deserializing - the AvroSerializer can make sure that all the data written to a specific topic is compatible with the schema of the topic, which means it can be deserialized with the matching deserializer and schema. Any errors in compatibility - in the producer or the consumer side will be caught easily with an appropriate error message, which means you will not need to try to debug byte arrays for serialization errors.</p>
<blockquote>
<p>显然，用在向Kafka生产事件的序列化器，必须和消费事件使用的反序列化器是匹配的。如果用<code>IntSerializer</code>但却用<code>StringDeserializer</code>反序列化是不行的。这意味着作为开发者，你需要跟踪写入每个主题使用的序列化器，并且确保每个主题只能包含你使用的反序列化器可以解释通过的数据。这就是使用Avro和模式存储作为序列化和反序列化的好处之一，<code>AvroSerializer</code>序列化器可以确保所有写入指定主题的数据和这个主题的模式是兼容的，这也意味着这些数据也能够被匹配的反序列化器和模式用来反序列化。对于兼容性的任何错误问题，在生产者或者消费者端都可以很容易地使用合适的错误类型捕获，这还意味着你不需要为序列化错误调试字节数组，程序开发变得更加简单高效。  </p>
</blockquote>
<p>下面我们会展示如何快速地编写自定义的反序列化器，尽管这不是推荐的做法，然后我们会接着使用一个Avro示例来反序列化消息的键值。</p>
<h3 id="自定义反序列化">自定义反序列化</h3><p>我们以第三章中序列化时使用的一样的自定义对象作为示例，并且为它编写一个反序列化器。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Customer</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> customerID;</span><br><span class="line">  <span class="keyword">private</span> String customerName;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Customer</span><span class="params">(<span class="keyword">int</span> ID, String name)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.customerID = ID;</span><br><span class="line">    <span class="keyword">this</span>.customerName = name;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getID</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> customerID;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> customerName;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>自定义序列化器的代码如下：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.common.errors.SerializationException;</span><br><span class="line"><span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomerDeserializer</span> <span class="keyword">implements</span> <span class="title">Deserializer</span>&lt;<span class="title">Customer</span>&gt; </span>&#123; <span class="comment">//1</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map configs, <span class="keyword">boolean</span> isKey)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// nothing to configure</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Customer <span class="title">deserialize</span><span class="params">(String topic, <span class="keyword">byte</span>[] data)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> id;</span><br><span class="line">    <span class="keyword">int</span> nameSize;</span><br><span class="line">    String name;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (data == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">      ByteBuffer buffer = ByteBuffer.wrap(data);</span><br><span class="line">      id = buffer.getInt();</span><br><span class="line">      String nameSize = buffer.getInt();</span><br><span class="line"></span><br><span class="line">      <span class="keyword">byte</span>[] nameBytes = <span class="keyword">new</span> Array[Byte](nameSize);</span><br><span class="line">      buffer.get(nameBytes);</span><br><span class="line">      name = <span class="keyword">new</span> String(nameBytes, <span class="string">'UTF-8'</span>);</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> Customer(id, name); <span class="comment">//2</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Error when serialize Customer to byte[] "</span>+e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// nothing to close</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>自定义的<code>CustomerDeserializer</code>实现了<code>Deserializer</code>接口，泛型为<code>Customer</code>对象，注意类（Customer）和序列化器（CustomerDeserializer）需要在生产者和消费者应用程序中完全匹配。在一个规模庞大的组织机构中，有很多的生产者和消费者应用程序会共享Kafka的数据，对于这种约定而言是一个很大的挑战。</li>
<li>我们这里仅仅是对序列化逻辑的逆向处理，从字节数组中获取顾客编号和名称，并且使用它们来构造自定义的对象。</li>
</ol>
<p>使用反序列化器的消费者看起来是这样的：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"broker1:9092,broker2:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"CountryCounter"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, </span><br><span class="line">  <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, </span><br><span class="line">  <span class="string">"org.apache.kafka.common.serialization.CustomerDeserializer"</span>);</span><br><span class="line">KafkaConsumer&lt;String, Customer&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">consumer.subscribe(<span class="string">"customerCountries"</span>);</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  ConsumerRecords&lt;String, Customer&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">  <span class="keyword">for</span> (ConsumerRecord&lt;String, Customer&gt; record : records) &#123;</span><br><span class="line">  System.out.println(<span class="string">"current customer Id: "</span> + record.value().getId() + </span><br><span class="line">    <span class="string">" and current customer name: "</span> + record.value().getName());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Again, it is important to note that implementing custom serializer and deserializer is not a recommended practice. It tightly couples producers and consumers and is fragile and error-prone. A better solution would be to use a standard message format such as Thrift, Protobuf or Avro. We’ll now see how to use Avro deserializers with the kafka consumer. For background on Apache Avro, its schemas and schema-compatibility capabilities, please refer back to Chapter 3.</p>
<blockquote>
<p>再次强调，自己实现定制的序列化器和反序列化器是不被推荐。它紧紧地把生产者和消费者绑定在一起，导致程序非常脆弱并且很容易出问题。一种更好的方式是使用标准的消息格式，比如Thrift、Protobuf或者Avro。下面我们会看到如何在Kafka消费者中结合使用Avro反序列化器。如果想巩固Apache Avro的背景知识、它的模式、模式匹配功能，可以复习下第三章的相关内容。  </p>
</blockquote>
<h3 id="Avro反序列化">Avro反序列化</h3><p>Lets assume we are using the implementation of Customer class in Avro that was shown in Chapter 3. In order to consume those objects from Kafka, you want to implement a consuming application similar to this:</p>
<p>假设我们使用了第三章中展示的，用Avro实现的<code>Customer</code>类，为了从Kafka中消费这些对象，你的消费者应用程序会是这样的：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"broker1:9092,broker2:9092"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"CountryCounter"</span>);</span><br><span class="line">props.put(<span class="string">"key.serializer"</span>, </span><br><span class="line">  <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.serializer"</span>, </span><br><span class="line">  <span class="string">"io.confluent.kafka.serializers.KafkaAvroDeserializer"</span>); <span class="comment">//1</span></span><br><span class="line">props.put(<span class="string">"schema.registry.url"</span>, schemaUrl); <span class="comment">//2</span></span><br><span class="line">String topic = <span class="string">"customerContacts"</span></span><br><span class="line">KafkaConsumer consumer = <span class="keyword">new</span> KafkaConsumer(props);</span><br><span class="line">consumer.subscribe(Collections.singletonList(topic));</span><br><span class="line">System.out.println(<span class="string">"Reading topic:"</span> + topic);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  ConsumerRecords&lt;String, Customer&gt; records = consumer.poll(<span class="number">1000</span>); <span class="comment">//3</span></span><br><span class="line">  <span class="keyword">for</span> (ConsumerRecord&lt;String, Customer&gt; record: records) &#123;</span><br><span class="line">    System.out.println(<span class="string">"Current customer name is: "</span> </span><br><span class="line">      + record.value().getName()); <span class="comment">//4</span></span><br><span class="line">  &#125;</span><br><span class="line">  consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>我们使用了<code>KafkaAvroDeserializer</code>来反序列化Avro消息</li>
<li><code>schema.registry.url</code>是一个新的参数，它仅仅表示存储模式的地址。这样消费者就可以使用生产者序列化消息时注册的模式。</li>
<li>指定生成的<code>Customer</code>类，作为记录的值类型</li>
<li><code>record.value()</code>返回值是一个<code>Customer</code>实例，于是我们就可以直接使用它</li>
</ol>
<h2 id="消费者配置">消费者配置</h2><p>目前为止我们主要专注于学习消费者API的使用方法，但我们只看到了很少的配置属性，即只有那些必须的<code>bootstrap.servers</code>、<code>group.id</code>、<code>key.deserializer</code>和<code>value.deserializer</code>。在Apache Kafka的官方文档中列出了所有的消费者配置，大部分配置参数都有一个合理的默认值，并且不需要修改，不过有些参数对消费者的性能和可用性影响较大，让我们看下这些比较重要的配置。</p>
<p><strong>fetch.min.bytes</strong><br>This property allows a consumer to specify the minimum amount of data that it wants to receive from the broker when fetching records. If a Broker receives a request for records from a Consumer but the new records amount to fewer bytes than min.fetch.bytes, the broker will wait until more messages are available before sending the records back to the consumer. This reduces the load on both the Consumer and the Broker as they have to handle fewer back-and-forward messages in cases where the topics don’t have much new activity (or for lower activity hours of the day). You will want to set this parameter higher than the default if the Consumer is using too much CPU when there isn’t much data available, or to reduce load on the brokers when you have large number of consumers.</p>
<blockquote>
<p>消费者拉取记录时从服务端代理节点接收的最小数据大小（单位为字节）。如果一个代理节点接收到一个消费者的拉取记录请求，但是新的记录字节大小小于<code>min.fetch.bytes</code>，代理节点在发送记录给消费者之前会等待直到有更多可用的消息（直到满足条件才发送记录给消费者）。通过这种方式可以减少消费者端和代理节点端的负载，因为如果主题没有太多的活动数据（或者一天中的业务低峰期），他们处理的消息来回次数更少。如果消费者没有太多的数据但使用了太多的CPU，可以设置该参数为一个更大的值，在有很多消费者的场景下，也可以减少服务端的负载（假设这个值很低的话，有很多消费者都向服务端拉取记录，只要有一点点新数据，服务端就要立即返回给消费者，对于同样的数据量，如果这个值比较高的话，网络的来回次数更少）。</p>
</blockquote>
<p><strong>fetch.max.wait.ms</strong><br>By setting fetch.min.bytes you tell Kafka to wait until it has enough data to send before responding to the consumer. fetch.max.wait.ms lets you control how long to wait. By default Kafka will wait up to 500ms. This results in up to 500ms of extra latency in case there is not enough data flowing to the Kafka topic to satisfy the minimum amount of data to return. If you want to limit the potential latency (usually due to SLAs controlling the maximum latency of the application), you can set fetch.max.wait.ms to lower value. If you set fetch.max.wait.ms to 100ms and fetch.min.bytes to 1MB, Kafka will recieve a fetch request from the consumer and will respond with data either when it has 1MB of data to return or after 100ms, whichever happens first.  </p>
<blockquote>
<p>上面通过设置<code>fetch.min.bytes</code>告诉Kafka在没有足够的数据之前不要发送响应给消费者。<code>fetch.max.wait.ms</code>则允许你在客户端自己控制最多等待多长的时间。默认值是500ms，即如果Kafka在500ms内还没有收集到<code>fetch.min.bytes</code>字节的消息，也要返回响应给消费者。消费者客户端在某些场景下最多只会有500ms的延迟：没有足够的数据流向Kafka，来满足要返回给客户端大小为<code>fetch.min.bytes</code>的数据量。如果你想要限制可能的延迟（通常是由应用程序的SLA控制最大的延迟时间），你需要设置<code>fetch.max.wait.ms</code>为一个更小的值（以便更快地收到响应）。举例你设置了<code>fetch.max.wait.ms</code>为100ms，<code>fetch.min.bytes</code>为1MB，那么Kafka接收到消费者的拉取请求后，当数据量有1MB后，或者超过100ms时，这两种情况无论哪种先发生都会返回目前收集的记录给消费者客户端。  </p>
</blockquote>
<p><strong>max.partition.fetch.bytes</strong><br>This property controls the maximum number of bytes the server will return per partition. The default is 1MB, which means that when KafkaConsumer.poll() returns ConsumerRecords, the record object will use at most max.partition.fetch.bytes per partition assigned to the Consumer. So if a topic has 20 partitions, and you have 5 consumers, each consumer will need to have 4MB of memory available for ConsumerRecords. In practice, you will want to allocate more memory as each consumer will need to handle more partitions if other consumers in the group fail.max.partition.fetch.bytes must be larger than the largest message a broker will accept (max.message.size property in the broker configuration), or the broker may have messages that the consumer will be unable to consumer, in which case the consumer will hang trying to read them. </p>
<p>Another important consideration when setting max.partition.fetch.bytes is the amount of time it takes the consumer to process data. As you recall, the consumer must call poll() frequently enough to avoid session timeout and subsequent rebalance. If the amount of data a single poll() returns is very large, it may take the consumer longer to process, which means it will not get to the next iteration of the poll loop in time to avoid a session timeout. If this occures the two options are either to lower max.partition.fetch.bytes or to increase the session timeout.</p>
<blockquote>
<p>这个配置控制了服务端为每个分区返回的最大字节。默认值是1MB，这就意味着当调用<code>KafkaConsumer.poll()</code>返回<code>ConsumerRecords</code>，这个记录集对象中最多每个分区（分配给消费者的分区）占用1MB。举例一个主题有20个分区，你有5个消费者，每个消费者存储<code>ConsumerRecords</code>对象需要有4M的可用内存。实际中你应该为每个消费者分配更多的内存，因为消费组中的其他消费者失败时，当前消费者要处理更多的分区。<code>max.partition.fetch.bytes</code>需要比服务端能接受的最大消息大小（服务端的<code>max.message.size</code>配置，表示一条消息不能超过该值，否则不会被服务端接受）还要大，否则代理节点有些消息就无法被消费者所消费，就会导致消费者在尝试读取比较大的消息时被暂停掉（比如拉取的最大分区大小=1M，而一条消息最大可以有2M，由于客户端拉取的分区最大只有1M，超过1M以上的消息就没办法被拉取到了）。</p>
</blockquote>
<p>设置<code>max.partition.fetch.bytes</code>的另一个重要的衡量标准是消费者处理数据花费的时间。回忆下之前的知识点，消费者必须足够频繁地调用<code>poll()</code>方法来避免会话超时和随后的平衡（因为都是在一个线程里完成所有的IO操作）。如果调用一次<code>poll()</code>返回的数据非常大，消费者可能需要花费更长的时间才能处理完成，这就是说它无法及时地迭代下一次轮询循环来避免会话超时。如果发生这种情况，有两种解决办法，降低<code>max.partition.fetch.bytes</code>或者增加会话的超时时间。  </p>
<p><strong>session.timeout.ms</strong><br>The amount of time a consumer can be out of contact with the brokers while still considered alive, defaults to 3 seconds. If a consumer goes for more than session.timeout.ms without sending a heartbeat to the group coordinator, it is considered dead and the group coordinator will trigger a rebalance of the consumer group to allocate partitions from the dead consumer to the other consumers in the group. This property is closely related to heartbeat.interval.ms. heartbeat.interval.ms controls how frequently the KafkaConsumer poll() method will send a heartbeat to the group coordinator, while session.timeout.ms controls how long can a consumer go without sending a heartbeat. </p>
<p>Therefore, thoese two properties are typically modified together - heatbeat.interval.ms must be lower than session.timeout.ms, and is usually set to a 1/3 of the timeout value. So if session.timeout.ms is 3 seconds, heartbeat.interval.ms should be 1 second. Setting session.timeout.ms lower than default will allow consumer groups to detect and recover from failure sooner, but may also cause unwanted rebalances as result of consumers taking longer to complete the poll loop or garbage collection. Setting session.timeout.ms higher will reduce the chance of accidental rebalance, but also means it will take longer to detect a real failure.</p>
<blockquote>
<p>该配置表示消费者在一段时间内不需要和代理节点联系也仍然认为是存活的，默认值是3秒。如果消费者超过<code>session.timeout.ms</code>时间还没有发送心跳给协调者，消费者就会被认为挂掉。协调者会触发一次消费组的平衡，并将属于挂掉的消费者的分区分配给消费组中其他的消费者。该配置和<code>heartbeat.interval.ms</code>密切相关。<code>heartbeat.interval.ms</code>会控制每隔多长时间消费者的<code>poll()</code>方法会发送一次心跳给协调者，而<code>session.timeout.ms</code>则控制消费者在没有发送心跳时多久会离开消费组。因此这两个配置通常一起修改，而且<code>heatbeat.interval.ms</code>必须要比<code>session.timeout.ms</code>低，通常心跳间隔会设置为超时时间的1/3。比如<code>session.timeout.ms</code>设置为3秒，那么<code>heartbeat.interval.ms</code>就应该设置为1秒。</p>
</blockquote>
<p>将<code>session.timeout.ms</code>设置的比默认值要低时，允许消费组更快地检测和恢复故障。但同时也会造成不必要的平衡，比如消费者可能花费比较长的时间完成一次轮询调用或者正在发送垃圾回收，而并没有真正挂掉，但是因为会话超时时间很短，导致发生更频繁的平衡。如果设置<code>session.timeout.ms</code>太大了，虽然可以减少偶然的平衡，但同时也意味着要花费更长的时间才能检测到真正的错误。  </p>
<p><strong>auto.offset.reset</strong><br>This property controls the behavior of the consumer when it starts reading a partition for which it doesn’t have a committed offset or if the committed offset it has is invalid (usually because the consumer was down for so long that the record with that offset was already aged out of the broker). The default is “latest”, which means that lacking a valid offset the consumer will start reading from the newest records (records which were written after the consumer started running). The alternative is “earliest”, which means that lacking a valid offset the consumer will read all the data in the partition, starting from the very beginning.</p>
<blockquote>
<p>该配置控制了消费者在一个没有提交的偏移量，或者偏移量是无效（通常是因为消费者挂掉太长的时间，带有偏移量的那条记录对于代理节点而言太旧了）的分区上，如何开始读取数据的行为。默认值是“最近的”（latest），表示在没有有效偏移量的情况下，消费者会从最新的记录开始读取（在消费者开始运行之后才被写入的记录，如果说消费者还没正式运行，即使写入记录，在消费者正式启动之后也不会被读取到，只从消费者正式启动的那一刻之后的记录才会被读取）。另外一种选择方式是“最早的”（“earliest”），表示在没有有效的偏移量时，消费者会从分区的最开始读取所有的数据。</p>
</blockquote>
<p><strong>enable.auto.commit</strong><br>We discussed the different options for committing offsets earlier in this chapter. This parameter controls whether the consumer will commit offsets automatically and defaults to true. Set it to false if you prefer to control when offsets are committed, which is necessary to minimize duplicates and avoid missing data. If you set enable.auto.commit to true then you may also want to control how frequently offsets will be committed using auto.commit.interval.ms.</p>
<blockquote>
<p>本章我们已经讨论了提交偏移量的不同方式。这个参数控制了消费者是否会自动提交偏移量，默认值为true。如果你想要自己控制什么时候提交偏移量，可以设置为false。这对于减少数据的重复处理以及避免丢失数据是非常有必要的。如果你设置了<code>enable.auto.commit</code>为true，你通常也会使用<code>auto.commit.interval.ms</code>来控制多长时间提交一次偏移量。</p>
</blockquote>
<p><strong>partition.assignment.strategy</strong><br>We learned that partitions are assigned to consumers in a consumer group. A PartitionAssignor is a class that, given consumers and topics they subscribed to, decides which partitions will be assigned to which consumer. By default Kafka has two assignment strategies: * Range - which assigns to each consumer a consecutive subset of partitions from each topic it subscribes to. So if consumers C1 and C2 are subscribed to two topics, T1 and T2 and each of the topics has 3 partitions. Then C1 will be assigned partitions 0 and 1 from topics T1 and T2, while C2 will be assigned partition 2 from those topics. Note that because each topic has uneven number of partitions and the assignment is done for each topic independently, the first consumer ended up with more partitions than the second. This happens whenever Range assignment is used and the number of consumers does not divide the number of partitions in each topic neatly. </p>
<ul>
<li>RoundRobin - which takes all the partitions from all subscribed topics and assigns them to consumers sequentially, one by one. If C1 and C2 described above would use RoundRobin assignment, C1 would have partitions 0 and 2 from topic T1 and partition 1 from topic T2. C2 would have partition 1 from topic T1 and partitions 0 and 2 from topic T2. In general, if all consumers are subscribed to the same topics (a very common scenario), RoundRobin assignment will end up with all consumers having the same number of partitions (or at most 1 partition difference). partition.assignment.strategy allows you to choose a partition assignment strategy. The default is org.apache.kafka.clients.consumer.RangeAssignor which implements the Range strategy described above. You can replace it with org.apache.kafka.clients.consumer.RoundRobinAssignor. A more advanced option will be to implement your own assignment strategy, in which case partition.assignment.strategy should point to the name of your class.</li>
</ul>
<blockquote>
<p>我们知道分区被分配给一个消费组中的所有消费者。<code>PartitionAssignor</code>类的作用是：给定消费者和它们订阅的主题，它来决定哪个分区应该被分配给哪个消费者。默认Kafka有两种分配策略：平行（Range）：分配给每个消费者的是，从它订阅的每个主题中连续的分区子集。举例消费者C1和C2都订阅了两个主题：T1和T2，这两个主题每个都有3个分区。那么消费者C1会分配到T1和T2主题的分区0和分区1，而消费者C2只会分配到这两个主题的分区2。注意由于每个主题的分区数量是奇数的，而且每个主题的分配都是独立的，所以第一个消费者会比第二个消费者分配到更多的分区。使用<code>Range</code>分配方式，当消费者的数量不能整除每个主题的分区数时就会发生这种情况。  </p>
</blockquote>
<p>轮询分配（RoundRobin）：从所有订阅主题的所有分区中一个接一个顺序分配地分配给所有的消费者。以上面的两个消费者C1和C2为例，使用<code>RoundRobin</code>分配策略时，消费者C1会得到主题T1的分区0和分区2，以及主题T2的分区1；而消费者C2会得到主题T1的分区1，以及主题T2的分区0和分区2。通常情况下，如果所有的消费者订阅了相同的主题（这是很常见的场景），采用轮询分配的方式时，最终所有消费者都会分配到相同数量的分区（或者说最多有一个分区是不同的）。</p>
<p><code>partition.assignment.strategy</code>配置允许你选择一个分区的分配策略。默认是实现了Range策略的<code>org.apache.kafka.clients.consumer.RangeAssignor</code>类，你也可以替换成<code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code>类。当然你也可以实现自定义的分配策略，这种情况下应该将<code>partition.assignment.strategy</code>指向你自定义的类路径。  </p>
<p><strong>client.id</strong><br>This can be any string, and will be used by the brokers to identify messages sent from the client. It is used in logging, metrics and for quotas.</p>
<p>该设置可以是任何的字符串，它会被用在代理节点定位消息是从哪个消费者客户端发送的，通常用在日志记录，性能监控，限额等。</p>
<h2 id="单机消费者（不使用消费组）">单机消费者（不使用消费组）</h2><p>So far we discussed consumer groups, where partitions are assigned automatically to consumers and are rebalanced automatically when consumers are added or removed from the group. Typically, this behavior is just what you want, but in some cases you want something much simpler. Sometimes you know you have a single consumer that always needs to read data from all the partitions in a topic, or from a specific partition in a topic. In this case there is no reason for groups or rebalances, just subscribe to specific topic and/or partitions, consume messages and commit offsets on occasion.</p>
<p>目前为止我们讨论了消费组的功能是：分区会自动地分配给所有的消费者，当消费者添加到消费组或者从消费组中移除时会自动平衡。这种场景通常是你需要的，不过有些情况下你可能想要的更加简单。比如你知道你的一个简单的消费者总是需要读取一个主题所有分区的数据，或者是一个主题的某个指定的分区。那么你没有理由需要使用消费组来平衡，你只需要订阅指定的主题或分区，偶尔地消费消息或者提交偏移量就可以了（不需要定时提交或者复杂的逻辑，因为我们不想要其他的消费者来代替这个消费者）。</p>
<p>If this is the case, you don’t subscribe to a topic, instead you assign yourself few partitions. Here is an example of how a consumer can assign itself all partitions of a specific topic and consume from them:</p>
<p>这种场景下你不需要订阅一个主题，而是直接分配一些分区给消费者就可以了。下面的示例展示了怎么将一个指定主题的所有分区分配给一个消费者，然后消费它们（分区的消息）。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">List&lt;PartitionInfo&gt; partitionInfos=consumer.partitionsFor(<span class="string">"topic"</span>); <span class="comment">//1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (partitionInfos != <span class="keyword">null</span>) &#123;</span><br><span class="line">  <span class="keyword">for</span> (PartitionInfo partition : partitionInfos)</span><br><span class="line">    partitions.add(<span class="keyword">new</span> TopicPartition(partition.topic(),partition.partition()));</span><br><span class="line">  consumer.assign(partitions); <span class="comment">//2</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">1000</span>); <span class="comment">//3</span></span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record: records) &#123;</span><br><span class="line">      <span class="comment">//处理记录，略</span></span><br><span class="line">    &#125;</span><br><span class="line">    consumer.commitSync();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>我们向集群请求获取指定主题的所有可用分区，如果你只需要消费某个特定的分区，可以跳过这一步。</li>
<li>当我们知道了想要的哪些分区，调用<code>assign()</code>方法时，传递分区列表即可。</li>
</ol>
<p>Note that other than the lack of rebalances and the need to manually find the partitions, everything looks normal. Just remember that if someone adds new partitions to the topic, the consumer will not be notified. So either handle this by checking consumer.partitionsFor() periodically or keep in mind that if an admin add partitions, the applications will require bouncing. Also note that a consumer can either subscribe to topics (and be part of a consumer group), or assign itself partitions, but not both at the same time.</p>
<blockquote>
<p>注意，除了没有平衡，以及手动获取分区的需要外，其他一切看起来都很正常。不过要记住如果有人为主题添加了新的分区，这个消费者不会被通知到。所以要么定期地调用<code>consumer.partitionsFor()</code>，或者记住在管理员添加分区的时候，应用程序需要做一些变动。还要记住：消费者可以订阅主题（作为消费组的一部分），也可以分配分区给它自己，但是不能同时使用这两个功能。</p>
</blockquote>
<h2 id="旧的消费者API">旧的消费者API</h2><p>In this chapter we discussed the Java KafkaConsumer client that is part of org.apache.kafka.clients package. At the time of writing this chapter, Apache Kafka still has two older clients written in Scala that are part of kafka.consumer package which is part of the core Kafka module. These consumers are called SimpleConsumer (which is not very simple. It is a thin wrapper around the Kafka APIs that allow you to consume from specific partitions and offsets) and the High Level Consumer, also known as ZookeeperConsumerConnector, which is somewhat similar to the current consumer in that it has consumer groups and it rebalances partitions - but it uses Zookeeper to manage consumer groups and it does not give you the same control over commits and rebalances as we have now.</p>
<p>Because the current consumer supports both behaviors and gives much more reliability and control to the developer, we will not discuss the older APIs. If you are interested in using them, please think twice and then refer to Apache Kafka documentation to learn more.</p>
<blockquote>
<p>本章我们讨论了Java版本的<code>KafkaConsumer</code>客户端，它属于<code>org.apache.kafka.clients</code>包下的一部分。本章写作的时候，Apache Kafka仍然还有另外两种使用Scala编写的旧客户端（高级API和低级API），它是Kafka核心模块中<code>kafka.consumer</code>的一部分。这两个消费者分别被叫做<code>SimpleConsumer</code>（实际上一点都不简单，它是对Kafka APIs的简单封装，允许你从指定的分区消费和提交偏移量，类似于新消费者的手动分配分区API）和高级<code>Consumer</code>，后者也被叫做<code>ZookeeperConsumerConnector</code>，它和当前版本有消费组语义的消费者有点类似，它也会平衡分区，不过使用ZooKeeper来管理消费组，但是并没有在提交和平衡上提供一样的控制能力。</p>
</blockquote>
<p>Because the current consumer supports both behaviors and gives much more reliability and control to the developer, we will not discuss the older APIs. If you are interested in using them, please think twice and then refer to Apache Kafka documentation to learn more.</p>
<p>因为当前的消费者都支持这两种行为，并且能给予开发者更多的可靠性和控制权，所以我们这里不会讨论旧的APIs。如果你打算使用它们，建议你三思而后行，然后再去Apache Kafka的官方文档学习更多的知识。</p>
<p>EOF. 2016.10.29</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kafka权威指南中文翻译：&lt;a href=&quot;https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch04.html&quot;&gt;https://www.safaribooksonline.com/library/view/kafka-the-definitive/9781491936153/ch04.html&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="mq" scheme="http://github.com/zqhxuyuan/categories/mq/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Cassandra源码分析-存储引擎</title>
    <link href="http://github.com/zqhxuyuan/2016/10/19/Cassandra-Code-StorageEngine/"/>
    <id>http://github.com/zqhxuyuan/2016/10/19/Cassandra-Code-StorageEngine/</id>
    <published>2016-10-18T16:00:00.000Z</published>
    <updated>2016-10-21T08:12:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>Cassandra-2.2/3.0 源码分析：存储引擎<br><a id="more"></a></p>
<h2 id="DecoratedKey、Token">DecoratedKey、Token</h2><p>对字节形式的key进行修饰后的DecoratedKey会用在很多地方，比如读写：</p>
<blockquote>
<p>StorageService.getPartitioner()获取唯一的Partitioner。注意一个集群只允许配置一个Partitioner，<br>不允许配置多个Partitioner，否则会有冲突。而且服务端配置的Partitioner，客户端也必须使用相同的Partitioner，<br>如果说服务器使用Murmur3，而客户端使用Random，客户端启动时会报错。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Read</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Row <span class="title">getRow</span><span class="params">(Keyspace keyspace)</span> </span>&#123;</span><br><span class="line">    DecoratedKey dk = StorageService.getPartitioner().decorateKey(key);</span><br><span class="line">    <span class="keyword">return</span> keyspace.getRow(<span class="keyword">new</span> QueryFilter(dk, cfName, filter, timestamp));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20161019102825548" alt="c-decorated key"></p>
<p>Murmur3Partitioner对key进行装饰后，最终得到某个Token，这个Token是无状态的数据，所以新的key会创建新的Token对象。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Murmur3Partitioner</span> <span class="keyword">implements</span> <span class="title">IPartitioner</span></span><br><span class="line">    <span class="title">public</span> <span class="title">static</span> <span class="title">final</span> <span class="title">Murmur3Partitioner</span> <span class="title">instance</span> </span>= <span class="keyword">new</span> Murmur3Partitioner();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> DecoratedKey <span class="title">decorateKey</span><span class="params">(ByteBuffer key)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span>[] hash = getHash(key);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> PreHashedDecoratedKey(getToken(key, hash), key, hash[<span class="number">0</span>], hash[<span class="number">1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> LongToken <span class="title">getToken</span><span class="params">(ByteBuffer key, <span class="keyword">long</span>[] hash)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> LongToken(normalize(hash[<span class="number">0</span>]));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">LongToken</span> <span class="keyword">extends</span> <span class="title">Token</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> token;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">LongToken</span><span class="params">(<span class="keyword">long</span> token)</span> </span>&#123; <span class="keyword">this</span>.token = token; &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> IPartitioner <span class="title">getPartitioner</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> instance; &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> Object <span class="title">getTokenValue</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> token; &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Token</span> <span class="keyword">implements</span> <span class="title">RingPosition</span>&lt;<span class="title">Token</span>&gt;, <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> TokenSerializer serializer = <span class="keyword">new</span> TokenSerializer();</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenFactory</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> ByteBuffer <span class="title">toByteArray</span><span class="params">(Token token)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Token <span class="title">fromByteArray</span><span class="params">(ByteBuffer bytes)</span></span>;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> String <span class="title">toString</span><span class="params">(Token token)</span></span>; <span class="comment">// serialize as string, not necessarily human-readable</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Token <span class="title">fromString</span><span class="params">(String string)</span></span>; <span class="comment">// deserialize</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">abstract</span> <span class="keyword">public</span> IPartitioner <span class="title">getPartitioner</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">abstract</span> <span class="keyword">public</span> Object <span class="title">getTokenValue</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>DecoratedKey抽象类包括了key内容本身和Token，实现类有内存的BufferDecoratedKey和native的NativeDecoratedKey。  </p>
<blockquote>
<p>可见在支持native对象时，最底层的key对象已经开始用native方式分配内存了</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">DecoratedKey</span> <span class="keyword">implements</span> <span class="title">RowPosition</span>, <span class="title">FilterKey</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Token token;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Token <span class="title">getToken</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> token; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> ByteBuffer <span class="title">getKey</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最底层的接口其实还不是DecoratedKey，而是RingPosition</p>
<p><img src="http://img.blog.csdn.net/20161019114157487" alt="c-ringposition"></p>
<p>Token是对key进行hash得到的一个数值，因此可能产生hash冲突，即同一个hash值可能有多个key对应。<br>所以Key和Token不是一一对应的，根据key可以得到唯一的Token，但是根据Token不一定有唯一的key。  </p>
<h3 id="Native">Native</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NativeDecoratedKey</span> <span class="keyword">extends</span> <span class="title">DecoratedKey</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> peer;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">NativeDecoratedKey</span><span class="params">(Token token, NativeAllocator allocator, OpOrder.Group writeOp, ByteBuffer key)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(token);</span><br><span class="line">        <span class="keyword">int</span> size = key.remaining();</span><br><span class="line">        <span class="keyword">this</span>.peer = allocator.allocate(<span class="number">4</span> + size, writeOp);</span><br><span class="line">        MemoryUtil.setInt(peer, size);</span><br><span class="line">        MemoryUtil.setBytes(peer + <span class="number">4</span>, key);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ByteBuffer <span class="title">getKey</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> MemoryUtil.getByteBuffer(peer + <span class="number">4</span>, MemoryUtil.getInt(peer));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BufferDecoratedKey</span> <span class="keyword">extends</span> <span class="title">DecoratedKey</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ByteBuffer key;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BufferDecoratedKey</span><span class="params">(Token token, ByteBuffer key)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(token);</span><br><span class="line">        <span class="keyword">this</span>.key = key;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ByteBuffer <span class="title">getKey</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> key; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="http://normanmaurer.me/blog/2013/10/28/Lesser-known-concurrent-classes-Part-1/" target="_blank" rel="external">http://normanmaurer.me/blog/2013/10/28/Lesser-known-concurrent-classes-Part-1/</a><br><img src="http://img.blog.csdn.net/20161019112929825" alt="c-atomic update"><br>左图使用Atomic，右图使用volatile（总共占用500M）和AtomicUpdater（136M）</p>
</blockquote>
<h2 id="TokenMetadata">TokenMetadata</h2><h2 id="DataModel">DataModel</h2><p>OnDiskAtom：在盘原子（OnDisk + Atom原子，磁盘上的原子变量），有两个实现类：RangeTombstone和Cell，<br>Cell也有多种接口：AbstractCell、CounterCell、ExpiringCell、DeletedCell。<br>这里已经把删除相关的几种实现都覆盖了：TTL为ExpiringCell，删除命令为DeletedCell，删除多个为RangeTombstone。<br>普通操作的抽象类是AbstractCell，有两种大类实现：BufferCell和AbstractNativeCell，分别代表内存和Offheap中的Cell。  </p>
<p>Cell有多种实现，除了几种删除相关的Cell外，普通Cell又分为BufferCell和NativeCell。<br>其中BufferCell在内存中，而NativeCell在OffHeap中。</p>
<p><img src="http://img.blog.csdn.net/20160928232515893" alt="c-cell"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Row</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> DecoratedKey key;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> ColumnFamily cf;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ColumnFamily</span> <span class="keyword">implements</span> <span class="title">Iterable</span>&lt;<span class="title">Cell</span>&gt;, <span class="title">IRowCacheEntry</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> CFMetaData metadata;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ArrayBackedSortedColumns</span> <span class="keyword">extends</span> <span class="title">ColumnFamily</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> DeletionInfo deletionInfo;</span><br><span class="line">    <span class="keyword">private</span> Cell[] cells;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> size;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> sortedSize;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Cell</span> <span class="keyword">extends</span> <span class="title">OnDiskAtom</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> CellName <span class="title">name</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ByteBuffer <span class="title">value</span><span class="params">()</span></span>;    </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BufferCell</span> <span class="keyword">extends</span> <span class="title">AbstractCell</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> CellName name;  <span class="comment">//Cell名称</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> ByteBuffer value; <span class="comment">//Cell值</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> <span class="keyword">long</span> timestamp; <span class="comment">//时间撮，每个Cell都有一个时间撮，用来防止冲突</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Keyspace</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;UUID, ColumnFamilyStore&gt; columnFamilyStores = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> KSMetaData metadata;    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Keyspace和ColumnFamily的定义都有元数据，分别用KSMetaData和CFMetadata表示ks和表级别的配置信息。</p>
<p>数据库：Keyspace<br>表：ColumnFamily<br>主键：DecoratedKey<br>行：Row<br>列：Cell  </p>
<p>Row由Key和ColumnFamily组成，即主键和列族（很多个列）。ColumnFamily是Column的Family，Column也叫做Cell。<br>ColumnFamily可以用一个双层Map表示：<code>Map&lt;RowKey, SortedMap&lt;ColumnKey, ColumnValue&gt;&gt;</code><br>因为是Map结构，所以查询Map中的指定key非常快，列是有序存储的，所以扫描多个列或者定位某个列也很高效。</p>
<p><img src="http://www.bodhtree.com/blog/wp-content/uploads/2013/12/clip_image0022.jpg" alt="column-family"></p>
<h2 id="SSTableWriter">SSTableWriter</h2><p>SSTable构建在SequenceFile上，它在磁盘的数据存储是有序的，SSTable包括数据文件和索引文件。除此之外，为了加快文件的读取，<br>还有BloomFilter、IndexSummary，注意索引文件会存储每个key在数据文件中的索引位置，而IndexSummary文件则存储部分key，<br>每隔一定key的数量才在summary文件中存储一个条目。通常summary文件比较小，所以可以直接以MMap的形式映射到内存中。  </p>
<p>SSTable分成SSTableWriter和SSTableReader，具体的文件操作接口实现是：BigTableWriter和BigTableReader。  </p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SSTable</span><br><span class="line">  <span class="string">|-- SSTableWriter </span></span><br><span class="line">        <span class="string">|-- BigTableWriter </span></span><br><span class="line">  <span class="string">|-- SSTableReader</span></span><br><span class="line">        <span class="string">|-- BigTableReader</span></span><br></pre></td></tr></table></figure>
<p><code>BigTableWriter.append()</code>应该是实际的写入一行记录方法，看看调用链：当Memtable刷写时，会把内存中有序的数据追加到BigTableWriter。  </p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">BigTableWriter.append(DecoratedKey, ColumnFamily)</span><br><span class="line">  <span class="string">|-- Memtable.writeSortedContents(File) </span></span><br><span class="line">        <span class="string">|-- Memtable.flush()</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>append方法第一个参数DecoratedKey表示row key，那么第二个参数ColumnFamily表示的是这个key对应的所有Column家族。<br>ColumnFamily是Column数据的集合，Column包括ColumnName和ColumnValue，有了row key，column name，column value，数据也就准备完毕。  </p>
</blockquote>
<h3 id="BigTableWriter">BigTableWriter</h3><p>BigTableWriter针对索引文件和数据文件的写入分别是：IndexWriter和SequentialWriter。后者负责data文件，<br>而前者除了Index文件，还有BloomFilter文件、Summary文件都一起完成。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BigTableWriter</span> <span class="keyword">extends</span> <span class="title">SSTableWriter</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> IndexWriter iwriter; <span class="comment">//勤劳的Index，还要负责BF、IndexSummary</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> SequentialWriter dataFile; <span class="comment">//孤傲的数据文件</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">(DecoratedKey decoratedKey, ColumnFamily cf)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> startPosition = beforeAppend(decoratedKey); <span class="comment">//写文件，要知道开始位置</span></span><br><span class="line">    RowIndexEntry entry = rawAppend(cf, startPosition, decoratedKey, dataFile.stream); <span class="comment">//返回索引条目</span></span><br><span class="line">    <span class="keyword">long</span> endPosition = dataFile.getFilePointer();</span><br><span class="line">    afterAppend(decoratedKey, endPosition, entry);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">beforeAppend</span><span class="params">(DecoratedKey decoratedKey)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (lastWrittenKey == <span class="keyword">null</span>) ? <span class="number">0</span> : dataFile.getFilePointer();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">afterAppend</span><span class="params">(DecoratedKey decoratedKey, <span class="keyword">long</span> dataEnd, RowIndexEntry index)</span> </span>&#123;</span><br><span class="line">    lastWrittenKey = decoratedKey;</span><br><span class="line">    <span class="keyword">if</span> (first == <span class="keyword">null</span>) first = lastWrittenKey;</span><br><span class="line">    iwriter.append(decoratedKey, index, dataEnd); <span class="comment">//索引文件，以及其他BF、IndexSummary都在这里完成</span></span><br><span class="line">    dbuilder.addPotentialBoundary(dataEnd);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="IndexWriter索引文件">IndexWriter索引文件</h3><p>先来看IndexWriter怎么写入索引文件以及BF、IndexSummary等。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IndexWriter</span> <span class="keyword">extends</span> <span class="title">AbstractTransactional</span> <span class="keyword">implements</span> <span class="title">Transactional</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> SequentialWriter indexFile; <span class="comment">//Index文件</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">final</span> SegmentedFile.Builder builder;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">final</span> IndexSummaryBuilder summary; <span class="comment">//IndexSummary文件</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">final</span> IFilter bf; <span class="comment">//Bloom Filter文件</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">(DecoratedKey key, RowIndexEntry indexEntry, <span class="keyword">long</span> dataEnd)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    bf.add(key); <span class="comment">//添加到Bloom Filter中，BF类似于一个List集合</span></span><br><span class="line">    <span class="keyword">long</span> indexStart = indexFile.getFilePointer();</span><br><span class="line">    ByteBufferUtil.writeWithShortLength(key.getKey(), indexFile.stream);</span><br><span class="line">    rowIndexEntrySerializer.serialize(indexEntry, indexFile.stream); <span class="comment">//序列化到IndexFile文件中</span></span><br><span class="line">    <span class="keyword">long</span> indexEnd = indexFile.getFilePointer();</span><br><span class="line">    summary.maybeAddEntry(key, indexStart, indexEnd, dataEnd); <span class="comment">//也许需要追加索引条目到Summary文件中</span></span><br><span class="line">    builder.addPotentialBoundary(indexStart);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Write_Data(Column_Index)">Write Data(Column Index)</h3><p>key和ColumnFamily已经足够可以代表要写入的数据了。ColumnIndex字面意思是Column索引，为什么要在列上加索引，<br>因为Cassandra是宽表，一行数据可能有很多列（最多2亿个列，可想而知，对列做索引也可以提高性能）。<br>这里的out参数是dataFile的输出流，所以接下来的文件写入都是会写到Data数据文件中的。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> RowIndexEntry <span class="title">rawAppend</span><span class="params">(ColumnFamily cf, <span class="keyword">long</span> startPosition, DecoratedKey key, DataOutputPlus out)</span> </span>&#123;</span><br><span class="line">  ColumnIndex.Builder builder = <span class="keyword">new</span> ColumnIndex.Builder(cf, key.getKey(), out);</span><br><span class="line">  ColumnIndex index = builder.build(cf); <span class="comment">//这里会由dataFile写文件内容</span></span><br><span class="line">  out.writeShort(END_OF_ROW); <span class="comment">//一行数据的结束标记位</span></span><br><span class="line">  <span class="keyword">return</span> RowIndexEntry.create(startPosition, cf.deletionInfo().getTopLevelDeletion(), index); <span class="comment">//返回RowIndex</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>IndexInfo的组成：Composite lastName、firsetName（Block索引块的第一个列名和最后一个列名），offset、width（索引块的偏移量和长度）。<br>一个ColumnIndex包括了多个IndexInfo，因此ColumnIndex表示的是所有Column组成在一起的最终索引结果。<br><img src="http://img.blog.csdn.net/20161020165624490" alt="c column index"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ColumnIndex</span> </span>&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">final</span> List&lt;IndexHelper.IndexInfo&gt; columnsIndex;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Help to create an index for a column family based on size of columns, and write said columns to disk.</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ColumnIndex result;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> DataOutputPlus output;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ByteBuffer key;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ColumnIndex <span class="title">build</span><span class="params">(ColumnFamily cf)</span></span>&#123;</span><br><span class="line">      <span class="keyword">for</span>(Cell c : cf) add(c); <span class="comment">//不考虑Tombstone等，非常简单！     </span></span><br><span class="line">      ColumnIndex index = build();</span><br><span class="line">      <span class="keyword">return</span> index; </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ColumnIndex实际上并不是单个Column，或者说仅仅表示一个Column的Index，它表示的真正含义是一行的所有Column。<br>一行记录有很多Column，这些Column会每隔blockSize生成一个IndexInfo关于列的索引条目。<br><img src="http://img.blog.csdn.net/20161020165013403" alt="c index info"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(OnDiskAtom column)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (firstColumn == <span class="keyword">null</span>) &#123;  <span class="comment">//第一列，在一行中只会执行一次。。错误！因为一行有多个Block， 每个Block都会执行一次</span></span><br><span class="line">    firstColumn = column;</span><br><span class="line">    startPosition = endPosition;</span><br><span class="line">    endPosition += tombstoneTracker.writeOpenedMarkers(firstColumn.name(), output, atomSerializer);</span><br><span class="line">    blockSize = <span class="number">0</span>; </span><br><span class="line">    maybeWriteRowHeader(); <span class="comment">//第一个列，需要添加RowHeader</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (tombstoneTracker.update(column, <span class="keyword">false</span>)) &#123;</span><br><span class="line">    <span class="keyword">long</span> size = tombstoneTracker.writeUnwrittenTombstones(output, atomSerializer);</span><br><span class="line">    size += atomSerializer.serializedSizeForSSTable(column);</span><br><span class="line">    endPosition += size;</span><br><span class="line">    blockSize += size; <span class="comment">//增加block大小，最后才可以判断是否需要添加列索引</span></span><br><span class="line">    atomSerializer.serializeForSSTable(column, output);  <span class="comment">//序列化Column！！！</span></span><br><span class="line">  &#125;</span><br><span class="line">  lastColumn = column; <span class="comment">//最近一个列</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// if we hit the column index size that we have to index after, go ahead and index it.</span></span><br><span class="line">  <span class="keyword">if</span> (blockSize &gt;= DatabaseDescriptor.getColumnIndexSize()) &#123; <span class="comment">//间隔blockSize，添加一个列索引</span></span><br><span class="line">    IndexHelper.IndexInfo cIndexInfo = <span class="keyword">new</span> IndexHelper.IndexInfo(</span><br><span class="line">      firstColumn.name(), column.name(), indexOffset + startPosition, endPosition - startPosition);</span><br><span class="line">    result.columnsIndex.add(cIndexInfo);</span><br><span class="line">    firstColumn = <span class="keyword">null</span>; <span class="comment">//重置firstColumn为null，这样下一个Block会重新执行开头的if条件</span></span><br><span class="line">    lastBlockClosing = column;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后build的时候，第一个if条件表示在add过程中都没产生IndexInfo，第二个条件是说即使add过程有IndexInfo，<br>可能剩余的Column不够一个完整的Block，也要新建一个IndexInfo不足以产生一个完整的Block。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ColumnIndex <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (result.columnsIndex.isEmpty() || lastBlockClosing != lastColumn) &#123;</span><br><span class="line">    IndexHelper.IndexInfo cIndexInfo = <span class="keyword">new</span> IndexHelper.IndexInfo(</span><br><span class="line">      firstColumn.name(), lastColumn.name(), indexOffset + startPosition, endPosition - startPosition);</span><br><span class="line">    result.columnsIndex.add(cIndexInfo);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ColumnIndex会被用于创建RowIndexEntry，如果索引块（IndexInfo）只有一个，则直接创建RowIndexEntry（不需要把IndexInfo传进去），<br>否则创建IndexedEntry，会把ColumnIndex的所有IndexInfo都传入。<br><img src="http://img.blog.csdn.net/20161020170822505" alt="c-index entry"></p>
<blockquote>
<p>position其实就是Row的起始位置，知道了起始位置，就可以构建row key索引的条目了。后续的IndexWriter我们已经分析过了。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> RowIndexEntry&lt;IndexHelper.IndexInfo&gt; create(<span class="keyword">long</span> position, DeletionTime deletionTime, ColumnIndex index) &#123;</span><br><span class="line">    <span class="keyword">if</span> (index.columnsIndex.size() &gt; <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> IndexedEntry(position, deletionTime, index.columnsIndex);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> RowIndexEntry&lt;&gt;(position); <span class="comment">//如果只有一个columnsIndex，直接用RowIndexEntry</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后回到IndexWriter流程，看看索引文件中RowIndex条目的序列化：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//RowIndexEntry.Serializer</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">serialize</span><span class="params">(RowIndexEntry&lt;IndexHelper.IndexInfo&gt; rie, DataOutputPlus out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  out.writeLong(rie.position);</span><br><span class="line">  out.writeInt(rie.promotedSize(idxSerializer));</span><br><span class="line">  <span class="keyword">if</span> (rie.isIndexed()) &#123; <span class="comment">//如果有多个IndexInfo</span></span><br><span class="line">      DeletionTime.serializer.serialize(rie.deletionTime(), out);</span><br><span class="line">      out.writeInt(rie.columnsIndex().size());</span><br><span class="line">      <span class="keyword">for</span> (IndexHelper.IndexInfo info : rie.columnsIndex()) <span class="comment">//每个IndexInfo都要序列化</span></span><br><span class="line">          idxSerializer.serialize(info, out);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Tombstone">Tombstone</h3><blockquote>
<p><a href="http://stackoverflow.com/questions/27776337/what-types-of-tombstones-does-cassandra-support" target="_blank" rel="external">http://stackoverflow.com/questions/27776337/what-types-of-tombstones-does-cassandra-support</a><br><a href="http://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html" target="_blank" rel="external">http://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html</a></p>
</blockquote>
<p>Tombstone标记可以作用在A Column（d）、A Range of Columns（e）、A Whole Row。<br>单单DELETE语法就有多种：删除表、删除指定行、删除指定行的指定列。下面列举了几种Tombstone类型：  </p>
<table>
<thead>
<tr>
<th>Tombstone类型</th>
<th>SQL</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>column tombstone</td>
<td><code>delete id from ts1 WHERE col1 = &#39;3131&#39;;</code></td>
<td>{“key”: “3131”,”columns”: [[“id”,”54822130”,1417814320400000,”d”]]},</td>
</tr>
<tr>
<td>row tombstone</td>
<td><code>delete from ts1 WHERE col1 = &#39;31&#39;;</code></td>
<td>{“key”: “31”,”metadata”: {“deletionInfo”: {“markedForDeleteAt”:1417814302304000,”localDeletionTime”:1417814302}},”columns”: []}</td>
</tr>
<tr>
<td>list tombstone</td>
<td><code>insert into flights (id, destinations) values (&#39;BA1234&#39;, [&#39;ORD&#39;, &#39;LHR&#39;]);</code></td>
<td>[“1381316637599609:45787829:tags:_”,”1381316637599609:45787829:tags:!”,1438264650252000,”t”,1438264650],</td>
</tr>
</tbody>
</table>
<h4 id="Mutation-delete">Mutation.delete</h4><p>Mutation中有三个关于delete的方法（虽然参数中都没有row key，不过Mutation一定是具体到key的），<br>cfName表示ColumnFamilyName，即表名。下面的示例中cfName=tableX，CellName=col1。  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">delete</span> <span class="keyword">from</span> tableX <span class="keyword">where</span> rowkey=<span class="keyword">key</span>;</span>      #<span class="operator"><span class="keyword">delete</span>(<span class="keyword">String</span> cfName, <span class="keyword">long</span> <span class="keyword">timestamp</span>)</span><br><span class="line"><span class="keyword">delete</span> col1 <span class="keyword">from</span> tableX <span class="keyword">where</span> rowkey=<span class="keyword">key</span>;</span> #<span class="operator"><span class="keyword">delete</span>(<span class="keyword">String</span> cfName, CellName <span class="keyword">name</span>, <span class="keyword">long</span> <span class="keyword">timestamp</span>)</span></span><br></pre></td></tr></table></figure>
<p>三种delete方法，调用的是不同的方法，分别是:delete，addTombstone，addAtom。其中删除列没有使用DeletionInfo。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Mutation</span> <span class="keyword">implements</span> <span class="title">IMutation</span></span><br><span class="line">  //1.删除行</span><br><span class="line">  <span class="title">public</span> <span class="title">void</span> <span class="title">delete</span>(<span class="title">String</span> <span class="title">cfName</span>, <span class="title">long</span> <span class="title">timestamp</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> localDeleteTime = (<span class="keyword">int</span>) (System.currentTimeMillis() / <span class="number">1000</span>);</span><br><span class="line">    addOrGet(cfName).delete(<span class="keyword">new</span> DeletionInfo(timestamp, localDeleteTime));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//2.删除行中的某一列</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">(String cfName, CellName name, <span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">    addOrGet(cfName).addTombstone(name, localDeleteTime, timestamp);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//3.删除行的多个列，范围</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteRange</span><span class="params">(String cfName, Composite start, Composite end, <span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">    addOrGet(cfName).addAtom(<span class="keyword">new</span> RangeTombstone(start, end, timestamp, localDeleteTime));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ColumnFamily</span> <span class="keyword">implements</span> <span class="title">Iterable</span>&lt;<span class="title">Cell</span>&gt;, <span class="title">IRowCacheEntry</span></span><br><span class="line">  //2.删除行中的某一列</span><br><span class="line">  <span class="title">public</span> <span class="title">void</span> <span class="title">addTombstone</span>(<span class="title">CellName</span> <span class="title">name</span>, <span class="title">int</span> <span class="title">localDeletionTime</span>, <span class="title">long</span> <span class="title">timestamp</span>) </span>&#123;</span><br><span class="line">    addColumn(<span class="keyword">new</span> BufferDeletedCell(name, localDeletionTime, timestamp));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//3.删除行的多个列，范围</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addAtom</span><span class="params">(OnDiskAtom atom)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (atom <span class="keyword">instanceof</span> Cell) &#123;</span><br><span class="line">      addColumn((Cell)atom);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">assert</span> atom <span class="keyword">instanceof</span> RangeTombstone;</span><br><span class="line">      delete((RangeTombstone)atom); <span class="comment">//RangeTombstone实现了OnDiskAtom，所以可以强转</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">(DeletionInfo info)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="DeletionInfo、DeletionTime">DeletionInfo、DeletionTime</h4><p><strong>列级别</strong>的删除采用的数据结构是BufferDeletedCell，只有<strong>行级别和RangeTombstone</strong>的删除才会使用DeletionInfo。  </p>
<blockquote>
<p>TopLevel字面意思是最高级别，在所有Column之上的是Row，Row叫做TopLevel。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A combination of a top-level (or row) tombstone and range tombstones describing the deletions within a &#123;@link ColumnFamily&#125; (or row).</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DeletionInfo</span> <span class="keyword">implements</span> <span class="title">IMeasurableMemory</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> DeletionTime topLevel; <span class="comment">//代表row级别的删除</span></span><br><span class="line">    <span class="keyword">private</span> RangeTombstoneList ranges; <span class="comment">//A list of range tombstones within the row.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//判断一个Cell是否能够被DeletionInfo删除</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isDeleted</span><span class="params">(Cell cell)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (isLive()) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (cell.timestamp() &lt;= topLevel.markedForDeleteAt) <span class="keyword">return</span> <span class="keyword">true</span>; <span class="comment">//Cell的时间撮比markedForDeleteAt小，可以删除</span></span><br><span class="line">        <span class="keyword">return</span> ranges != <span class="keyword">null</span> &amp;&amp; ranges.isDeleted(cell); <span class="comment">//如果存在RangeTombstone，则用RangeTombstoneList判断</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>序列化DeletionInfo的两个时间撮到文件中：  </p>
<ol>
<li>localDeletionTime：什么时候会创建这个Tombstone，它的目的仅仅是为了经过gc_grace_seconds后删除Tombstone。</li>
<li>markedForDeleteAt：标记记录什么时候可以被删除，通常用来作为判断条件，如果为MIN，则表示这行记录不会被删除</li>
</ol>
<blockquote>
<p>通常使用DELETE命令删除行，会立即生成Tombstone，这时localDeletionTime是当前系统时间撮。如果使用TTL方式，则localDeletionTime是在TTL后的系统时间。<br>当创建Tombstone之后，为了不让Tombstone一直保存在磁盘中，再经过gc_grace_seconds后要把Tombstone删除掉，注意只有在创建Tombstone之后的gc才删除。<br>假设是TTL方式创建一条记录，并不是说在创建记录之后经过gc时间删除Tombstone；而是经过TTL时间+gc时间才删除Tombstone，因为经过TTL时间才开始创建Tombstone。  </p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//DeletionTime</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DeletionTime</span> <span class="keyword">implements</span> <span class="title">Comparable</span>&lt;<span class="title">DeletionTime</span>&gt;, <span class="title">IMeasurableMemory</span> </span>&#123;</span><br><span class="line">  <span class="comment">//A special DeletionTime that signifies that there is no top-level (row) tombstone.</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> DeletionTime LIVE = <span class="keyword">new</span> DeletionTime(Long.MIN_VALUE, Integer.MAX_VALUE);</span><br><span class="line">  <span class="comment">//A timestamp after which data should be considered deleted. If set to Long.MIN_VALUE, this implies that the data has not been marked for deletion at all.</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">long</span> markedForDeleteAt;</span><br><span class="line">  <span class="comment">//The local server timestamp, at which this tombstone was created. This is only used for purposes of purging the tombstone after gc_grace_seconds have elapsed.</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> localDeletionTime;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">DeletionTime</span><span class="params">(<span class="keyword">long</span> markedForDeleteAt, <span class="keyword">int</span> localDeletionTime)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.markedForDeleteAt = markedForDeleteAt;</span><br><span class="line">    <span class="keyword">this</span>.localDeletionTime = localDeletionTime;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isLive</span><span class="params">()</span> </span>&#123; <span class="comment">//Returns whether this DeletionTime is live, that is deletes no columns.</span></span><br><span class="line">    <span class="keyword">return</span> markedForDeleteAt == Long.MIN_VALUE &amp;&amp; localDeletionTime == Integer.MAX_VALUE;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isDeleted</span><span class="params">(OnDiskAtom atom)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> atom.timestamp() &lt;= markedForDeleteAt; <span class="comment">//可以看到比较某个Atom能够被删除，用的是markedForDeleteAt</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="序列化RowKey和TopLevel_Tombstone">序列化RowKey和TopLevel Tombstone</h3><p>回头重新看下ColumnIndex添加原始数据中关于Tombstone的逻辑（前面忽略了Tombstone）。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ColumnIndex <span class="title">build</span><span class="params">(ColumnFamily cf)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="comment">// cf has disentangled the columns and range tombstones, we need to re-interleave them in comparator order</span></span><br><span class="line">  Comparator&lt;Composite&gt; comparator = cf.getComparator();</span><br><span class="line">  DeletionInfo.InOrderTester tester = cf.deletionInfo().inOrderTester();</span><br><span class="line">  Iterator&lt;RangeTombstone&gt; rangeIter = cf.deletionInfo().rangeIterator();</span><br><span class="line">  RangeTombstone tombstone = rangeIter.hasNext() ? rangeIter.next() : <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">for</span> (Cell c : cf) &#123;</span><br><span class="line">    <span class="keyword">while</span> (tombstone != <span class="keyword">null</span> &amp;&amp; comparator.compare(c.name(), tombstone.min) &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// skip range tombstones that are shadowed by partition tombstones</span></span><br><span class="line">      <span class="keyword">if</span> (!cf.deletionInfo().getTopLevelDeletion().isDeleted(tombstone)) add(tombstone);</span><br><span class="line">      tombstone = rangeIter.hasNext() ? rangeIter.next() : <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!tester.isDeleted(c)) add(c);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> (tombstone != <span class="keyword">null</span>) &#123; <span class="comment">//添加所有的Tombstone，DeletionInfo中有RangeTombstoneList</span></span><br><span class="line">    add(tombstone);</span><br><span class="line">    tombstone = rangeIter.hasNext() ? rangeIter.next() : <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  finishAddingAtoms(); <span class="comment">//writeUnwrittenTombstones，主要是序列化RangeTombstone</span></span><br><span class="line">  ColumnIndex index = build(); <span class="comment">//构建ColumnIndex最后一个IndexInfo，其他IndexInfo在add方法中已经添加过了</span></span><br><span class="line">  maybeWriteEmptyRowHeader(); <span class="comment">//序列化RowKey和TomLevel(Row) Tombstone</span></span><br><span class="line">  <span class="keyword">return</span> index;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>maybeWriteEmptyRowHeader有两种调用场景</strong>  </p>
<p>因为DeletionInfo中用DeletionTime字段表示<code>A top-level (row) tombstone</code>。即DeletionTime代表的是行级别的，所以会和RowKey同一个级别一起序列化。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ColumnIndex的Builder内部类</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">maybeWriteEmptyRowHeader</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (!deletionInfo.isLive()) maybeWriteRowHeader(); <span class="comment">//存在Tombstone</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">maybeWriteRowHeader</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (lastColumn == <span class="keyword">null</span>) &#123; <span class="comment">//如果lastColumn为空，说明没有任何列</span></span><br><span class="line">      ByteBufferUtil.writeWithShortLength(key, output); <span class="comment">//写入RowKeyLength和RowKey内容</span></span><br><span class="line">      DeletionTime.serializer.serialize(deletionInfo.getTopLevelDeletion(), output);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//DeletionTime的序列化内部类</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">serialize</span><span class="params">(DeletionTime delTime, DataOutputPlus out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    out.writeInt(delTime.localDeletionTime);</span><br><span class="line">    out.writeLong(delTime.markedForDeleteAt);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>maybeWriteRowHeader</code>不仅仅在最后调用一次，实际上在add(Atom)中每个Collumn Block也都会调用一次。也就是说每个Column Block都会分别创建一个IndexInfo，<br>也会调用一次maybeWriteRowHeader。add方法中会把这两个操作放在一起执行，而最后一个不足的Block，在build中创建IndexInfo，然后手动调用maybeWriteRowHeader。  </p>
<p>Wait!Wait!Wait!注意到maybeWriteRowHeader序列化RowKey的条件是<code>lastColumn=null</code>，通常情况下只要有列，lastColulmn都不会为空！<br>不过<strong>即使没有任何列，也需要写入RowKey和TopLevel Tombstone</strong>。所以<code>maybeWriteRowHeader</code>有两个调用场景：<br><img src="http://img.blog.csdn.net/20161021085510867" alt="c-write row"></p>
<ol>
<li>没有任何列的情况下会被调用，并且只会被调用一次</li>
<li>有列，不管有多少个Column Block，也只会调用一次，因此第一个Block时lastColumn=null，后续的Block则lastColumn!=null</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(OnDiskAtom column)</span></span>&#123;  <span class="comment">//如果没有任何列，不会调用add方法，lastColumn=null，也需要写入RowKey和TopLevel Tombstone</span></span><br><span class="line">    <span class="keyword">if</span> (firstColumn == <span class="keyword">null</span>) &#123;       <span class="comment">//一旦调用该add方法，说明有Column列</span></span><br><span class="line">      maybeWriteRowHeader();  <span class="comment">//只有lastColumn=null时，才写入RowKey和TopLevel Tombstone</span></span><br><span class="line">    &#125;</span><br><span class="line">    lastColumn = column;      <span class="comment">//只要有一个Column被添加，lastColumn就不为空</span></span><br><span class="line">    <span class="keyword">if</span> (blockSize &gt; ...) &#123;</span><br><span class="line">      firstColumn = <span class="keyword">null</span>;     <span class="comment">//新的Block，重置firstColumn，但是不会重置lastColumn哦</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="序列化Column">序列化Column</h3><p>RowKey和Tombstone要序列化，当然Column肯定也需要序列化，在添加每个Column时，先序列化Column大小，再序列化Column值。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">add</span><span class="params">(OnDiskAtom column)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (tombstoneTracker.update(column, <span class="keyword">false</span>)) &#123;</span><br><span class="line">    <span class="keyword">long</span> size = tombstoneTracker.writeUnwrittenTombstones(output, atomSerializer); <span class="comment">//序列化Column的SIZE</span></span><br><span class="line">    size += atomSerializer.serializedSizeForSSTable(column);</span><br><span class="line">    endPosition += size;</span><br><span class="line">    blockSize += size;</span><br><span class="line">    atomSerializer.serializeForSSTable(column, output);  <span class="comment">//序列化Column的内容</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>先来看下<code>atomSerializer</code>的构造，它和ColumnFamily的<code>Comparator</code>有关，而Comparator来源于CFMetaData</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ColumnIndex的Builder内部类</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> OnDiskAtom.Serializer atomSerializer; </span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">Builder</span><span class="params">(ColumnFamily cf, ByteBuffer key, DataOutputPlus output)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.key = key;</span><br><span class="line">    deletionInfo = cf.deletionInfo();</span><br><span class="line">    <span class="keyword">this</span>.indexOffset = rowHeaderSize(key, deletionInfo);</span><br><span class="line">    <span class="keyword">this</span>.result = <span class="keyword">new</span> ColumnIndex(<span class="keyword">new</span> ArrayList&lt;IndexHelper.IndexInfo&gt;());</span><br><span class="line">    <span class="keyword">this</span>.output = output;</span><br><span class="line">    <span class="keyword">this</span>.tombstoneTracker = <span class="keyword">new</span> RangeTombstone.Tracker(cf.getComparator());</span><br><span class="line">    <span class="keyword">this</span>.atomSerializer = cf.getComparator().onDiskAtomSerializer();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//CFMetadata</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">CFMetaData</span> </span>&#123;</span><br><span class="line">  <span class="comment">//必选</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">volatile</span> CellNameType comparator;          <span class="comment">// bytes, long, timeuuid, utf8, etc.</span></span><br><span class="line">  <span class="comment">//可选</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">volatile</span> AbstractType&lt;?&gt; defaultValidator = BytesType.instance;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">volatile</span> AbstractType&lt;?&gt; keyValidator = BytesType.instance;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="ColumnFamily的比较类型(CellNameType)">ColumnFamily的比较类型(CellNameType)</h4><p>CFMetaData字面意思是ColumnFamily的元数据，实际上是表级别的设置。下面举例看下表级别的Comparator到底是什么东西。<br>CellNameType的类继承体系如下，主要分为Compound和Simple，然后又根据Sparse和Dense，最终由四种组合类型。  </p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">CellNameType</span></span><br><span class="line">  AbstractCellNameType </span><br><span class="line">      AbstractCompoundCellNameType</span><br><span class="line">          CompoundDenseCellNameType </span><br><span class="line">          CompoundSparseCellNameType</span><br><span class="line">      AbstractSimpleCellNameType</span><br><span class="line">          SimpleSparseCellNameType</span><br><span class="line">          SimpleDenseCellNameType</span><br></pre></td></tr></table></figure>
<p>CellName是一个组合，主要是为了CQL3而设计的，<code>a CellName has first a number of clustering components, followed by the CQL3 column name, 
and then possibly followed by a collection element part.</code>，首先是一些排序组件，然后是（普通的）CQL3列名，最后可能还有集合类型。  </p>
<blockquote>
<p>The sparse ones are CellName where one of the component (the last or second-to-last for collections) is used to store the CQL3 column name.<br>In other words, we have 4 types of CellName/CellNameType which correspond to the 4 type of table layout that we need to distinguish:  </p>
<ol>
<li>Simple (non-truly-composite) dense: this is the dynamic thrift CFs whose comparator is not composite.  </li>
<li>Simple (non-truly-composite) sparse: this is the thrift static CFs (that don’t have a composite comparator).  </li>
<li>Composite dense: this is the dynamic thrift CFs with a CompositeType comparator.  </li>
<li>Composite sparse: this is the CQL3 layout (note that this is the only one that support collections).  </li>
</ol>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">CellNameType</span> <span class="keyword">extends</span> <span class="title">CType</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isDense</span><span class="params">()</span></span>;  <span class="comment">// Whether or not the cell names for this type are dense.</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">clusteringPrefixSize</span><span class="params">()</span></span>;  <span class="comment">// The number of clustering columns for the table this is the type of.</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> CBuilder <span class="title">prefixBuilder</span><span class="params">()</span></span>;  <span class="comment">// A builder for the clustering prefix.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>clustering prefix表示列名中，clustering key的值是作为列名的前缀。当然也可以不需要clustering key，这时候就只有普通的PartitionKey作为主键了。<br>注意如果是CQL3的<code>COMPACT STORAGE</code>，则不会在列名中存储CQL3列名（即普通的列名），这种类型叫做<code>dense</code>，所以CellNameType继承体系的依据是：  </p>
<ol>
<li>是否是COMPACT STORAGE(或者thrift)，是为：Dense，不是为：Sparse</li>
<li>ParitionKey是否有多个，是为Compound，不是为：Simple</li>
</ol>
<table>
<thead>
<tr>
<th>COMPACT STORAGE</th>
<th>Partition Key多个</th>
<th>类型</th>
</tr>
</thead>
<tbody>
<tr>
<td>N</td>
<td>Y</td>
<td>SimpleDenseCellNameType</td>
</tr>
<tr>
<td>N</td>
<td>N</td>
<td>SimpleSparseCellNameType</td>
</tr>
<tr>
<td>Y</td>
<td>Y</td>
<td>CompoundDenseCellNameType</td>
</tr>
<tr>
<td>Y</td>
<td>N</td>
<td>CompoundSparseCellNameType</td>
</tr>
</tbody>
</table>
<blockquote>
<p>COMPACT STORAGE和thrift是等价的，Dense译为密集，表示存储比较紧密，Spark译为稀疏，则存储的列比较多。</p>
</blockquote>
<h4 id="CellNameType示例">CellNameType示例</h4><p>下面示例的Partition Key有四个字段，Clustering Key有一个字段（sequence_id），并且排序方式是<strong>DESC</strong>（即<strong>ReversedType</strong>），并且还有两个普通字段（event、timestamp）。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE forseti.velocity_app (</span><br><span class="line">    attribute text,</span><br><span class="line">    partner_code text,</span><br><span class="line">    app_name text,</span><br><span class="line">    type text,</span><br><span class="line">    sequence_id text,</span><br><span class="line">    event text,</span><br><span class="line">    timestamp bigint,</span><br><span class="line">    PRIMARY KEY ((attribute, partner_code, app_name, type), sequence_id)</span><br><span class="line">) WITH CLUSTERING ORDER BY (sequence_id DESC)</span><br><span class="line">    AND bloom_filter_fp_chance = 0.1</span><br><span class="line">    AND caching = '&#123;"keys":"ALL", "rows_per_partition":"ALL"&#125;'</span><br><span class="line">    AND comment = ''</span><br><span class="line">    AND compaction = &#123;'unchecked_tombstone_compaction': 'true', 'tombstone_threshold': '0.1', 'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'&#125;</span><br><span class="line">    AND compression = &#123;'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'&#125;</span><br><span class="line">    AND dclocal_read_repair_chance = 0.0</span><br><span class="line">    AND default_time_to_live = 0</span><br><span class="line">    AND gc_grace_seconds = 0</span><br><span class="line">    AND max_index_interval = 2048</span><br><span class="line">    AND memtable_flush_period_in_ms = 0</span><br><span class="line">    AND min_index_interval = 128</span><br><span class="line">    AND read_repair_chance = 0.1</span><br><span class="line">    AND speculative_retry = '99.0PERCENTILE';</span><br><span class="line"></span><br><span class="line">insert into velocity_app(attribute, partner_code, app_name, type,timestamp,event,sequence_id)values('zqhxuyuan','tongdun','tongdun_app','login',1111111111,'&#123;jsondata&#125;','1111111111-1');</span><br><span class="line">select * from velocity_app where attribute='zqhxuyuan' and type='login' and partner_code='tongdun' and app_name='tongdun_app';</span><br><span class="line"></span><br><span class="line">#schema_columnfamilies实际上对应的就是CFMetadata</span><br><span class="line">cqlsh:system&gt; select * from system.schema_columnfamilies where keyspace_name='forseti' and columnfamily_name='velocity_app';</span><br><span class="line"> keyspace_name | columnfamily_name | bloom_filter_fp_chance | caching                                     | cf_id                                | comment | compaction_strategy_class                                       | compaction_strategy_options | comparator                                                                                                                                                                     | compression_parameters                                                   | default_time_to_live | default_validator                         | dropped_columns | gc_grace_seconds | is_dense | key_validator                                                                                                                                                                                                      | local_read_repair_chance | max_compaction_threshold | max_index_interval | memtable_flush_period_in_ms | min_compaction_threshold | min_index_interval | read_repair_chance | speculative_retry | subcomparator | type</span><br><span class="line">---------------+-------------------+------------------------+---------------------------------------------+--------------------------------------+---------+-----------------------------------------------------------------+-----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------+-------------------------------------------+-----------------+------------------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------+--------------------------+--------------------+-----------------------------+--------------------------+--------------------+--------------------+-------------------+---------------+----------</span><br><span class="line">       forseti |      velocity_app |                   0.01 | &#123;"keys":"ALL", "rows_per_partition":"NONE"&#125; | 763248f0-8f88-11e6-a6b6-71d72bc0ba41 |         | org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy |                          &#123;&#125; | org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.UTF8Type),org.apache.cassandra.db.marshal.UTF8Type) | &#123;"sstable_compression":"org.apache.cassandra.io.compress.LZ4Compressor"&#125; |                    0 | org.apache.cassandra.db.marshal.BytesType |            null |           864000 |    False | org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type) |                      0.1 |                       32 |               2048 |                           0 |                        4 |                128 |                  0 |    99.0PERCENTILE |          null | Standard</span><br><span class="line"></span><br><span class="line">cqlsh:system&gt; select * FROM  schema_columns where keyspace_name='forseti' and columnfamily_name='velocity_app';</span><br><span class="line"> keyspace_name | columnfamily_name | column_name  | component_index | index_name | index_options | index_type | type           | validator</span><br><span class="line">---------------+-------------------+--------------+-----------------+------------+---------------+------------+----------------+----------------------------------------------------------------------------------------</span><br><span class="line">       forseti |      velocity_app |     app_name |               2 |       null |          null |       null |  partition_key |                                               org.apache.cassandra.db.marshal.UTF8Type</span><br><span class="line">       forseti |      velocity_app |    attribute |               0 |       null |          null |       null |  partition_key |                                               org.apache.cassandra.db.marshal.UTF8Type</span><br><span class="line">       forseti |      velocity_app |        event |               1 |       null |          null |       null |        regular |                                               org.apache.cassandra.db.marshal.UTF8Type</span><br><span class="line">       forseti |      velocity_app | partner_code |               1 |       null |          null |       null |  partition_key |                                               org.apache.cassandra.db.marshal.UTF8Type</span><br><span class="line">       forseti |      velocity_app |  sequence_id |               0 |       null |          null |       null | clustering_key | org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.UTF8Type)</span><br><span class="line">       forseti |      velocity_app |    timestamp |               1 |       null |          null |       null |        regular |                                               org.apache.cassandra.db.marshal.LongType</span><br><span class="line">       forseti |      velocity_app |         type |               3 |       null |          null |       null |  partition_key |                                               org.apache.cassandra.db.marshal.UTF8Type</span><br></pre></td></tr></table></figure>
<p>下表总结了<code>schema_columnfamilies</code>的几种比较类型，其中第一行comparator，即是表级别的Comparator。<br>这是一个组合类型，第一个字段是倒序的sequence_id，那么第二个字段到底表示的是哪个字段。<br>首先不可能是PartitionKey，因为PartitionKey有四个字段，那么就只剩下两个普通字段了，<br>但是这两个普通字段的类型又不同，分别是bigint和text！实际上用UTF8类型代表这两个字段。<br><strong>所以comparator实际上是Column级别的比较器！因为物理存储时，列名为：ClusteringKey的Value：普通字段的名称。<br>即ReversedType(UTF8Type)=ClusteringKey的值（倒序排列），UTF8Type=普通列的名称。</strong>  </p>
<table>
<thead>
<tr>
<th>比较器</th>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>comparator</strong></td>
<td><strong>CompositeType(ReversedType(UTF8Type),UTF8Type)</strong></td>
<td>CellNameType</td>
</tr>
<tr>
<td>default_validator</td>
<td>BytesType</td>
</tr>
<tr>
<td>key_validator</td>
<td>CompositeType(UTF8Type,UTF8Type,UTF8Type,UTF8Type)</td>
<td>PartitionKey，不包括ClusteringKey </td>
</tr>
</tbody>
</table>
<blockquote>
<p>注意CQL的comparator和Thrift的Comparator有点不同，除非如果是Compact Storage的CQL才相同（因为COMPACT不会存储普通列名）。  </p>
</blockquote>
<h4 id="CellName与Type">CellName与Type</h4><p>CellNameType继承CType（左图），CellName继承Composite（右图）。CellName是列的名称，即列的数据内容，CellNameType是列的类型，即列数据是什么样的类型。<br>比如上面示例中，相同sequence_id有两个（普通）列：123456789-001:event，123456789-001:timestamp，列的值分别是：”{json evnet}”，123456789。  </p>
<table>
<thead>
<tr>
<th>ColumnName</th>
<th>123456789-001:event</th>
<th>123456789-001:timestamp</th>
</tr>
</thead>
<tbody>
<tr>
<td>ColumnValue</td>
<td>“{json event}”</td>
<td>123456789</td>
</tr>
</tbody>
</table>
<p>那么CellName分别是”123456789-001:event”和”123456789-001:timestamp”，而CellNameType只有一个：CompositeType(ReversedType(UTF8Type),UTF8Type)</p>
<p><img src="http://img.blog.csdn.net/20161021113351335" alt="c-ctype"> | <img src="http://img.blog.csdn.net/20161021113407601" alt="c-composite"></p>
<p><strong>列名存储列的数据内容，列类型用来表示列的一些特性，比如是不Dense的，前缀是什么，这样根据列的类型，可以做一些特殊的操作。可以把列类型看做是列的元数据。</strong>  </p>
<p><strong>列名</strong></p>
<p>CellName的最底层接口是Composite：A composite value can be though as a list of ByteBuffer。即列名就是ByteBuffer数据内容，<br>不管是SimpleSparseCellName还是CompoundSparseCellName，列名肯定是唯一的，用ColumnIdentifier表示：Represents an identifer for a CQL column definition。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">CellName</span> <span class="keyword">extends</span> <span class="title">Composite</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> ColumnIdentifier <span class="title">cql3ColumnName</span><span class="params">(CFMetaData metadata)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ColumnIdentifier</span> <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">cassandra</span>.<span class="title">cql3</span>.<span class="title">selection</span>.<span class="title">Selectable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> ByteBuffer bytes;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String text;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleSparseCellName</span> <span class="keyword">extends</span> <span class="title">AbstractComposite</span> <span class="keyword">implements</span> <span class="title">CellName</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ColumnIdentifier columnName;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CompoundSparseCellName</span> <span class="keyword">extends</span> <span class="title">CompoundComposite</span> <span class="keyword">implements</span> <span class="title">CellName</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> ColumnIdentifier columnName;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><em>列类型</em></p>
<p>根据PartitionKey的数量，一个是Simple，多个是Compound，那么CompoundSparseCellNameType表示多个PartitionKey，而且是正常的CQL表（不是COMPACT STORAGE）。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//是不是truly composite CType，是由多少个PartitionKey决定</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleCType</span> <span class="keyword">extends</span> <span class="title">AbstractCType</span> </span>&#123;  <span class="comment">// A not truly-composite CType.</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> AbstractType&lt;?&gt; type; <span class="comment">//一个PartitionKey，所以只有一个成员变量，代表PartitionKey的类型</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CompoundCType</span> <span class="keyword">extends</span> <span class="title">AbstractCType</span> </span>&#123; <span class="comment">// A truly-composite CType.</span></span><br><span class="line">    <span class="keyword">final</span> List&lt;AbstractType&lt;?&gt;&gt; types;  <span class="comment">//多个PartitionKey，所以是一个列表</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractCompoundCellNameType</span> <span class="keyword">extends</span> <span class="title">AbstractCellNameType</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> CompoundCType clusteringType;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> CompoundCType fullType;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CompoundSparseCellNameType</span> <span class="keyword">extends</span> <span class="title">AbstractCompoundCellNameType</span> </span>&#123;</span><br><span class="line">    <span class="comment">// For CQL3 columns, this is always UTF8Type. However, for compatibility with super columns, we need to allow it to be non-UTF8.</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> AbstractType&lt;?&gt; columnNameType;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">CompoundSparseCellNameType</span><span class="params">(List&lt;AbstractType&lt;?&gt;&gt; types)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(types, UTF8Type.instance); <span class="comment">//第二个参数是：columnNameType</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>虽然CFMetadata的comparator是<code>CellNameType</code>，但是上面的CQL示例中comparaor却是<code>CompositeType</code>。CompoundCType和CompositeType都表示复合类型，其成员变量都是List类型。  </p>
<blockquote>
<p>问题：为什么需要CompoundCType和CompositeType，Composite可以看做是CellName，CompositeType就是CelllNameType了，而CompoundCType也是CellNameType。两者到底有什么不同？</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CompoundCType</span> <span class="keyword">extends</span> <span class="title">AbstractCType</span> </span>&#123; <span class="comment">// A truly-composite CType.</span></span><br><span class="line">    <span class="keyword">final</span> List&lt;AbstractType&lt;?&gt;&gt; types;  <span class="comment">//多个PartitionKey，所以是一个列表</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CompositeType</span> <span class="keyword">extends</span> <span class="title">AbstractCompositeType</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> List&lt;AbstractType&lt;?&gt;&gt; types;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractCompositeType</span> <span class="keyword">extends</span> <span class="title">AbstractType</span>&lt;<span class="title">ByteBuffer</span>&gt; </span>&#123;..&#125;</span><br></pre></td></tr></table></figure>
<h4 id="序列化Cell">序列化Cell</h4><p>回到ColumnIndex序列化Column上，CellName只是Column的名称，而Cell实际上才是一个完整的列：包括列名和列值。Cell的最底层是OnDiskAtom。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">OnDiskAtom</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Composite <span class="title">name</span><span class="params">()</span></span>; <span class="comment">//列名，比如CellName继承了Composite</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">timestamp</span><span class="params">()</span></span>; <span class="comment">//每个Atom都有时间撮</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Serializer</span> <span class="keyword">implements</span> <span class="title">ISSTableSerializer</span>&lt;<span class="title">OnDiskAtom</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CellNameType type; <span class="comment">//列名的类型</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">serializeForSSTable</span><span class="params">(OnDiskAtom atom, DataOutputPlus out)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (atom <span class="keyword">instanceof</span> Cell) &#123;</span><br><span class="line">            type.columnSerializer().serialize((Cell)atom, out); <span class="comment">//Atom强转为Cell</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">assert</span> atom <span class="keyword">instanceof</span> RangeTombstone;</span><br><span class="line">            type.rangeTombstoneSerializer().serializeForSSTable((RangeTombstone)atom, out);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Cell</span> <span class="keyword">extends</span> <span class="title">OnDiskAtom</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> CellName <span class="title">name</span><span class="params">()</span></span>;       <span class="comment">//列名  --&gt;重写了OnDiskAtom的name方法，将返回类型从Composite转为CellName</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ByteBuffer <span class="title">value</span><span class="params">()</span></span>;    <span class="comment">//列值</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于Cell包括了列名和列值，当然每个列都有一个时间撮，所以序列化时要写入这三部分。ColumnSerializer是Cell的序列化类。  </p>
<ol>
<li>序列化CellName（cell.name方法）</li>
<li>如果是CounterCell或ExpiringCell，则还要写入相关的时间撮</li>
<li>序列化Cell的时间撮（cell.timestamp方法）</li>
<li>序列化Cell的值（cell.value方法）</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ColumnSerializer</span> <span class="keyword">implements</span> <span class="title">ISerializer</span>&lt;<span class="title">Cell</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> CellNameType type;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">serialize</span><span class="params">(Cell cell, DataOutputPlus out)</span> </span>&#123;</span><br><span class="line">    type.cellSerializer().serialize(cell.name(), out); <span class="comment">//序列化Cell的name，实际上就是序列化CellName</span></span><br><span class="line">    out.writeByte(cell.serializationFlags()); <span class="comment">//Flag标志位</span></span><br><span class="line">    <span class="keyword">if</span> (cell <span class="keyword">instanceof</span> CounterCell) &#123;</span><br><span class="line">        out.writeLong(((CounterCell) cell).timestampOfLastDelete());</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cell <span class="keyword">instanceof</span> ExpiringCell) &#123;</span><br><span class="line">        out.writeInt(((ExpiringCell) cell).getTimeToLive());</span><br><span class="line">        out.writeInt(cell.getLocalDeletionTime());</span><br><span class="line">    &#125;</span><br><span class="line">    out.writeLong(cell.timestamp());</span><br><span class="line">    ByteBufferUtil.writeWithLength(cell.value(), out);</span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下图总结了Data文件和索引文件的物理视图，其中最右边最细粒度的Column包括了四个字段，正好对应了上面ColumnSerializer的序列化过程。</p>
<p><img src="http://img.blog.csdn.net/20161021134751953" alt="c-storage layout"></p>
<h3 id="CQLSSTableWriter">CQLSSTableWriter</h3><p>除了数据写流程中Memtable的flush会通过SSTableWriter生成SSTable，也可以直接使用离线方式的CQLSSTableWriter直接生成SSTable。<br>后者不需要启动Cassandra，通常可以用在离线数据生成，如果要把数据导入Cassandra中，还需要通过NodeTool的Stream方式导入。</p>
<blockquote>
<p>Builder构建模式用在很多地方，比如添加Column数据的ColumnIndex，通过Build模式生成关于Column的索引块信息</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">String schema = <span class="string">"CREATE TABLE myKs.myTable (k int PRIMARY KEY,v1 text,v2 int)"</span>;</span><br><span class="line">String insert = <span class="string">"INSERT INTO myKs.myTable (k, v1, v2) VALUES (?, ?, ?)"</span>;</span><br><span class="line">CQLSSTableWriter writer = CQLSSTableWriter.builder()</span><br><span class="line">                                          .inDirectory(<span class="string">"path/to/directory"</span>)</span><br><span class="line">                                          .forTable(schema)</span><br><span class="line">                                          .using(insert).build();</span><br><span class="line">writer.addRow(<span class="number">0</span>, <span class="string">"test1"</span>, <span class="number">24</span>);</span><br><span class="line">writer.addRow(<span class="number">1</span>, <span class="string">"test2"</span>, <span class="keyword">null</span>);</span><br><span class="line">writer.addRow(<span class="number">2</span>, <span class="string">"test3"</span>, <span class="number">42</span>);</span><br><span class="line">writer.close();</span><br></pre></td></tr></table></figure>
<p><code>CQLSSTableWriter</code>只是提供了CQL方式的一种接口（工具类），实际上最后还是会以SSTableWriter的方式进入到写流程中。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CQLSSTableWriter</span> <span class="keyword">implements</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> AbstractSSTableSimpleWriter writer;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> UpdateStatement insert;   <span class="comment">//Writer只可能是insert语句，所以Statement是固定的UpdateStatement</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> List&lt;ColumnSpecification&gt; boundNames;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Builder</span></span><br><span class="line">      <span class="title">private</span> <span class="title">boolean</span> <span class="title">sorted</span> </span>= <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">public</span> Builder <span class="title">sorted</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          <span class="keyword">this</span>.sorted = <span class="keyword">true</span>;</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> CQLSSTableWriter <span class="title">build</span><span class="params">()</span> </span>&#123;</span><br><span class="line">          AbstractSSTableSimpleWriter writer = sorted ? <span class="keyword">new</span> SSTableSimpleWriter(directory, schema, partitioner)</span><br><span class="line">                                             : <span class="keyword">new</span> BufferedWriter(directory, schema, partitioner, bufferSizeInMB);</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">new</span> CQLSSTableWriter(writer, insert, boundNames);</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>insert语句会被解析成<code>List&lt;ColumnSpecification&gt;</code>，insert语句的<code>?</code>部分和<code>addRow</code>的参数是绑定在一起的（bound）。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> CQLSSTableWriter <span class="title">rawAddRow</span><span class="params">(List&lt;ByteBuffer&gt; values)</span> </span>&#123;</span><br><span class="line">  QueryOptions options = QueryOptions.forInternalCalls(<span class="keyword">null</span>, values);</span><br><span class="line">  List&lt;ByteBuffer&gt; keys = insert.buildPartitionKeyNames(options);</span><br><span class="line">  Composite clusteringPrefix = insert.createClusteringPrefix(options);</span><br><span class="line">  UpdateParameters params = <span class="keyword">new</span> UpdateParameters(insert.cfm, options, insert.getTimestamp(now, options), insert.getTimeToLive(options), Collections.&lt;ByteBuffer, CQL3Row&gt;emptyMap());</span><br><span class="line">  <span class="keyword">for</span> (ByteBuffer key : keys) &#123;</span><br><span class="line">    <span class="keyword">if</span> (writer.shouldStartNewRow() || !key.equals(writer.currentKey().getKey()))</span><br><span class="line">      writer.newRow(key);  <span class="comment">//如果是没有排序的，使用SSTableSimpleWriter</span></span><br><span class="line">    insert.addUpdateForKey(writer.currentColumnFamily(), key, clusteringPrefix, params, <span class="keyword">false</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>SSTableSimpleWriter底层使用了SSTableWriter，它的writeRow方法写入一行记录，调用SSTableWriter.append方法，剩余的流程就和前面分析的写入一样了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractSSTableSimpleWriter</span> <span class="keyword">implements</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">newRow</span><span class="params">(ByteBuffer key)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    writeRow(currentKey, columnFamily);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SSTableSimpleWriter</span> <span class="keyword">extends</span> <span class="title">AbstractSSTableSimpleWriter</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> SSTableWriter writer;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">writeRow</span><span class="params">(DecoratedKey key, ColumnFamily columnFamily)</span> </span>&#123;</span><br><span class="line">        writer.append(key, columnFamily);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>AbstractSSTableSimpleWriter的继承树如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">AbstractSSTableSimpleWriter (org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.io</span><span class="class">.sstable</span>)</span><br><span class="line">    SSTableSimpleWriter (org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.io</span><span class="class">.sstable</span>)</span><br><span class="line">    SSTableSimpleUnsortedWriter (org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.io</span><span class="class">.sstable</span>)</span><br><span class="line">        BufferedWriter <span class="keyword">in</span> CQLSSTableWriter (org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.io</span><span class="class">.sstable</span>)</span><br></pre></td></tr></table></figure>
<p>客户端使用CQLSSTableWriter工具生成SSTable的流程图如下：<br><img src="http://img.blog.csdn.net/20161021144358933" alt="c-writer flow"></p>
<h4 id="Spark_Cassandra_Connector的Writer">Spark Cassandra Connector的Writer</h4><h2 id="ColumnFamily">ColumnFamily</h2><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">ColumnFamily</span></span><br><span class="line">    AtomicBTreeColumns</span><br><span class="line">    ArrayBackedSortedColumns</span><br></pre></td></tr></table></figure>
<p>ColumnFamily提供了添加原子变量的几种方式，比如添加Tombstone、Atom、Column等。该抽象类还定义了很多获取Column的方法，比如迭代器，对Columns进行排序等。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ColumnFamily</span> <span class="keyword">implements</span> <span class="title">Iterable</span>&lt;<span class="title">Cell</span>&gt;, <span class="title">IRowCacheEntry</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> CFMetaData metadata;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addTombstone</span><span class="params">(CellName name, <span class="keyword">int</span> localDeletionTime, <span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">        addColumn(<span class="keyword">new</span> BufferDeletedCell(name, localDeletionTime, timestamp));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addAtom</span><span class="params">(OnDiskAtom atom)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (atom <span class="keyword">instanceof</span> Cell) &#123;</span><br><span class="line">            addColumn((Cell)atom);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            delete((RangeTombstone)atom);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addColumn</span><span class="params">(CellName name, ByteBuffer value, <span class="keyword">long</span> timestamp, <span class="keyword">int</span> timeToLive)</span> </span>&#123;</span><br><span class="line">        Cell cell = AbstractCell.create(name, value, timestamp, timeToLive, metadata());</span><br><span class="line">        addColumn(cell);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">addColumn</span><span class="params">(Cell cell)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ArrayBackedSortedColumns</span> <span class="keyword">extends</span> <span class="title">ColumnFamily</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Cell[] cells;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="SSTableReader">SSTableReader</h2><h3 id="BigTableReader">BigTableReader</h3><h2 id="3-0新存储引擎">3.0新存储引擎</h2><blockquote>
<p><a href="http://thelastpickle.com/blog/2016/03/04/introductiont-to-the-apache-cassandra-3-storage-engine.html" target="_blank" rel="external">http://thelastpickle.com/blog/2016/03/04/introductiont-to-the-apache-cassandra-3-storage-engine.html</a><br>Starting with the 3.x storage engine Partitions, Rows, and Clustering are natively supported.<br>A Partition is a collection of Rows that share the same Partition Key(s) that are ordered,<br>within the Partition, by their Clustering Key(s). Rows are then by globally identified by<br>their Primary Key: the combination of Partition Key and Clustering Key.<br>The important change is that the 3.x storage engine now knows about these ideas,<br>it may seem strange but previously it did not know about the Rows in a Partition.<br>The new storage engine was created specifically to handle these concepts in a way<br>that reduces storage requirements and improves performance.</p>
<p><a href="http://www.datastax.com/2015/12/storage-engine-30" target="_blank" rel="external">http://www.datastax.com/2015/12/storage-engine-30</a><br>2.0 maps of (ordered) maps of binary data：<code>Map&lt;byte[], SortedMap&lt;byte[], Cell&gt;&gt;</code><br>The top-level keys of that map are the partition keys,<br>and each partition (identified by its key) is a sorted key/value map.<br>The inner values of that partition map is called a <code>Cell</code> mostly because<br>it contains both a binary value and the timestamp that is used for conflict resolution  </p>
<p>3.0 <code>Map&lt;byte[], SortedMap&lt;Clustering, Row&gt;&gt;</code><br>At the top-level, a table is still a map of partitions indexed by their partition key.<br>And the partition is still a sorted map, but it is one of rows indexed by their “clustering”.<br>The Clustering holds the values for the clustering columns of the CQL row it represents.<br>And the Row object represents, well, a given CQL row, associating to each column their value and timestamp.</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Cassandra-2.2/3.0 源码分析：存储引擎&lt;br&gt;
    
    </summary>
    
      <category term="cassandra" scheme="http://github.com/zqhxuyuan/categories/cassandra/"/>
    
    
      <category term="cassandra" scheme="http://github.com/zqhxuyuan/tags/cassandra/"/>
    
  </entry>
  
  <entry>
    <title>Cassandra源码分析-Network</title>
    <link href="http://github.com/zqhxuyuan/2016/10/13/Cassandra-Code-Network/"/>
    <id>http://github.com/zqhxuyuan/2016/10/13/Cassandra-Code-Network/</id>
    <published>2016-10-12T16:00:00.000Z</published>
    <updated>2016-10-20T04:53:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>Cassandra-2.2 源码分析：Netty客户端/服务端、请求处理、消息服务<br><a id="more"></a></p>
<h2 id="CassandraDaemon">CassandraDaemon</h2><p>启动日志，代表了各个组件的启动顺序</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">INFO  [main] 2016-10-11 15:39:47,410 ColumnFamilyStore.java:382 - Initializing system.sstable_activity</span><br><span class="line">INFO  [main] 2016-10-11 15:39:48,950 CacheService.java:111 - Initializing key <span class="operator"><span class="keyword">cache</span> <span class="keyword">with</span> <span class="keyword">capacity</span> <span class="keyword">of</span> <span class="number">49</span> MBs.</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">48</span>,<span class="number">967</span> CacheService.<span class="keyword">java</span>:<span class="number">133</span> - Initializing <span class="keyword">row</span> <span class="keyword">cache</span> <span class="keyword">with</span> <span class="keyword">capacity</span> <span class="keyword">of</span> <span class="number">0</span> MBs</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">48</span>,<span class="number">972</span> CacheService.<span class="keyword">java</span>:<span class="number">162</span> - Initializing counter <span class="keyword">cache</span> <span class="keyword">with</span> <span class="keyword">capacity</span> <span class="keyword">of</span> <span class="number">24</span> MBs</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">48</span>,<span class="number">974</span> CacheService.<span class="keyword">java</span>:<span class="number">173</span> - Scheduling counter <span class="keyword">cache</span> <span class="keyword">save</span> <span class="keyword">to</span> every <span class="number">7200</span> seconds (going <span class="keyword">to</span> <span class="keyword">save</span> all <span class="keyword">keys</span>).</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">49</span>,<span class="number">080</span> ColumnFamilyStore.<span class="keyword">java</span>:<span class="number">382</span> - Initializing <span class="keyword">system</span>.hints</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">49</span>,<span class="number">089</span> ColumnFamilyStore.<span class="keyword">java</span>:<span class="number">382</span> - Initializing <span class="keyword">system</span>.......</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">302</span> ColumnFamilyStore.<span class="keyword">java</span>:<span class="number">382</span> - Initializing demo.<span class="keyword">test</span></span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">51</span>,<span class="number">716</span> <span class="keyword">Index</span>.<span class="keyword">java</span>:<span class="number">93</span> - Initializing Lucene <span class="keyword">index</span></span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">52</span>,<span class="number">405</span> <span class="keyword">Index</span>.<span class="keyword">java</span>:<span class="number">101</span> - <span class="keyword">Initialized</span> <span class="keyword">index</span> demo.<span class="keyword">test</span>.idx</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">52</span>,<span class="number">413</span> ColumnFamilyStore.<span class="keyword">java</span>:<span class="number">382</span> - Initializing demo.tweets</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">52</span>,<span class="number">419</span> AutoSavingCache.<span class="keyword">java</span>:<span class="number">163</span> - Completed loading (<span class="number">1</span> ms;</span> 21 keys) KeyCache <span class="operator"><span class="keyword">cache</span></span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">52</span>,<span class="number">497</span> CommitLog.<span class="keyword">java</span>:<span class="number">168</span> - Replaying <span class="keyword">bin</span>/../<span class="keyword">data</span>/commitlog/CommitLog-<span class="number">5</span>-<span class="number">1474959171115.</span><span class="keyword">log</span>, ....</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">52</span>,<span class="number">739</span> CommitLog.<span class="keyword">java</span>:<span class="number">170</span> - <span class="keyword">Log</span> replay <span class="keyword">complete</span>, <span class="number">135</span> replayed mutations</span><br><span class="line"></span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">52</span>,<span class="number">969</span> StorageService.<span class="keyword">java</span>:<span class="number">600</span> - Cassandra <span class="keyword">version</span>: <span class="number">2.2</span><span class="number">.6</span></span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">52</span>,<span class="number">969</span> StorageService.<span class="keyword">java</span>:<span class="number">601</span> - Thrift API <span class="keyword">version</span>: <span class="number">20.1</span><span class="number">.0</span></span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">52</span>,<span class="number">969</span> StorageService.<span class="keyword">java</span>:<span class="number">602</span> - CQL supported <span class="keyword">versions</span>: <span class="number">3.3</span><span class="number">.1</span> (<span class="keyword">default</span>: <span class="number">3.3</span><span class="number">.1</span>)</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">53</span>,<span class="number">010</span> IndexSummaryManager.<span class="keyword">java</span>:<span class="number">85</span> - Initializing <span class="keyword">index</span> summary manager <span class="keyword">with</span> a <span class="keyword">memory</span> pool <span class="keyword">size</span> <span class="keyword">of</span> <span class="number">49</span> MB <span class="keyword">and</span> a <span class="keyword">resize</span> <span class="built_in">interval</span> <span class="keyword">of</span> <span class="number">60</span> minutes</span><br><span class="line"></span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">53</span>,<span class="number">013</span> StorageService.<span class="keyword">java</span>:<span class="number">621</span> - Loading persisted ring state</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">53</span>,<span class="number">056</span> StorageService.<span class="keyword">java</span>:<span class="number">794</span> - <span class="keyword">Starting</span> up <span class="keyword">server</span> gossip</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">53</span>,<span class="number">247</span> MessagingService.<span class="keyword">java</span>:<span class="number">540</span> - <span class="keyword">Starting</span> Messaging Service <span class="keyword">on</span> localhost/<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">7000</span> (lo0)</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">53</span>,<span class="number">318</span> StorageService.<span class="keyword">java</span>:<span class="number">968</span> - <span class="keyword">Using</span> saved tokens [-<span class="number">1036061867878377743</span>, -<span class="number">1049032071638556980</span>, ]</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">53</span>,<span class="number">425</span> StorageService.<span class="keyword">java</span>:<span class="number">1937</span> - Node localhost/<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> state jump <span class="keyword">to</span> <span class="keyword">NORMAL</span></span><br><span class="line"></span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">53</span>,<span class="number">785</span> <span class="keyword">Server</span>.<span class="keyword">java</span>:<span class="number">151</span> - Netty <span class="keyword">using</span> <span class="keyword">Java</span> NIO <span class="keyword">event</span> <span class="keyword">loop</span></span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">53</span>,<span class="number">970</span> <span class="keyword">Server</span>.<span class="keyword">java</span>:<span class="number">185</span> - <span class="keyword">Starting</span> listening <span class="keyword">for</span> CQL clients <span class="keyword">on</span> localhost/<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">9042.</span>..</span><br><span class="line">INFO  [<span class="keyword">main</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">15</span>:<span class="number">39</span>:<span class="number">54</span>,<span class="number">159</span> CassandraDaemon.<span class="keyword">java</span>:<span class="number">439</span> - <span class="keyword">Not</span> <span class="keyword">starting</span> RPC <span class="keyword">server</span> <span class="keyword">as</span> requested. <span class="keyword">Use</span> JMX (StorageService-&gt;startRPCServer()) <span class="keyword">or</span> nodetool (enablethrift) <span class="keyword">to</span> <span class="keyword">start</span> it</span></span><br></pre></td></tr></table></figure>
<p>停止Cassandra，会停止CassandraDaemon、Server（nativeServer）、Gossiper。因为默认没有启动ThriftServer，所以就不需要停止它了。</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">INFO</span>  <span class="attr_selector">[RMI TCP Connection(5)-127.0.0.1]</span> 2016<span class="tag">-10-11</span> 15<span class="pseudo">:49</span><span class="pseudo">:05</span>,275 <span class="tag">CassandraDaemon</span><span class="class">.java</span><span class="pseudo">:451</span> <span class="tag">-</span> <span class="tag">Cassandra</span> <span class="tag">shutting</span> <span class="tag">down</span>...</span><br><span class="line"><span class="tag">INFO</span>  <span class="attr_selector">[RMI TCP Connection(5)-127.0.0.1]</span> 2016<span class="tag">-10-11</span> 15<span class="pseudo">:49</span><span class="pseudo">:05</span>,286 <span class="tag">Server</span><span class="class">.java</span><span class="pseudo">:218</span> <span class="tag">-</span> <span class="tag">Stop</span> <span class="tag">listening</span> <span class="tag">for</span> <span class="tag">CQL</span> <span class="tag">clients</span></span><br><span class="line"><span class="tag">INFO</span>  <span class="attr_selector">[StorageServiceShutdownHook]</span> 2016<span class="tag">-10-11</span> 15<span class="pseudo">:49</span><span class="pseudo">:05</span>,292 <span class="tag">Gossiper</span><span class="class">.java</span><span class="pseudo">:1448</span> <span class="tag">-</span> <span class="tag">Announcing</span> <span class="tag">shutdown</span></span><br></pre></td></tr></table></figure>
<p>CassandraDaemon启动类，有三个主要的服务类：</p>
<ol>
<li>StorageService：存储相关的服务</li>
<li>ThriftServer：Thrift协议</li>
<li>Server：native网络传输通信服务器</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> CassandraDaemon instance = <span class="keyword">new</span> CassandraDaemon();</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    instance.activate();</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">activate</span><span class="params">()</span></span>&#123;</span><br><span class="line">    setup();</span><br><span class="line">    start();</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">()</span></span>&#123;</span><br><span class="line">    StorageService.instance.initServer();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> rpcPort = DatabaseDescriptor.getRpcPort();</span><br><span class="line">    <span class="keyword">int</span> nativePort = DatabaseDescriptor.getNativeTransportPort();</span><br><span class="line">    thriftServer = <span class="keyword">new</span> ThriftServer(rpcAddr, rpcPort, listenBacklog);</span><br><span class="line">    nativeServer = <span class="keyword">new</span> org.apache.cassandra.transport.Server(nativeAddr, nativePort);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    nativeServer.start();</span><br><span class="line">    thriftServer.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>配置文件中端口和对应的实现类：</p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">storage_port</span>: <span class="string">7000              --&gt; StorageService</span></span><br><span class="line"><span class="attribute">native_transport_port</span>: <span class="string">9042     --&gt; nativeServer</span></span><br><span class="line"><span class="attribute">rpc_port</span>: <span class="string">9160                  --&gt; ThriftServer</span></span><br><span class="line"></span><br><span class="line"><span class="livescript"><span class="attribute">start_native_transport</span>: <span class="literal">true</span><span class="function">    --&gt;</span> 默认开启<span class="keyword">native</span>协议</span><br><span class="line"><span class="attribute">start_rpc</span>: <span class="literal">false</span><span class="function">                --&gt;</span> 默认关闭thrift协议</span></span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20161013140045454" alt="c-ports"><br>图片来源：<a href="http://stackoverflow.com/questions/2359159/cassandra-port-usage-how-are-the-ports-used" target="_blank" rel="external">http://stackoverflow.com/questions/2359159/cassandra-port-usage-how-are-the-ports-used</a></p>
<p>CassandraDaemon的内部类Server有两个实现类，用于Thrift协议的o.a.c.thrift.ThriftServer，以及用于native二进制协议的o.a.c.transport.Server。</p>
<h2 id="ThriftServer">ThriftServer</h2><p>cassandra.thrift文件在安装包的interface下，主要分为</p>
<ol>
<li>data structures（Column、SuperColumn等）</li>
<li>service的struct数据结构：ConsistencyLevel、ColumnParent、ColumnPath、SliceRange、KeyRange、KeySlice、Deletion、Mutation、TokenRange、ColumnDef、CfDef、KsDef、ColumnSlice等</li>
<li>service的api服务方法：get、get_slice、multiget_slice、get_range_slices、insert、add、remove、batch_mutate、get_multi_slice等</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ThriftServer</span> <span class="keyword">implements</span> <span class="title">CassandraDaemon</span>.<span class="title">Server</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        CassandraServer iface = getCassandraServer();</span><br><span class="line">        server = <span class="keyword">new</span> ThriftServerThread(address, port, backlog, getProcessor(iface), getTransportFactory());</span><br><span class="line">        server.start();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ThriftServerThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> TServer serverEngine;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">ThriftServerThread</span><span class="params">(...)</span> </span>&#123;</span><br><span class="line">            serverEngine = <span class="keyword">new</span> TServerCustomFactory(DatabaseDescriptor.getRpcServerType()).buildTServer(args);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            serverEngine.serve();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomTThreadPoolServer</span> <span class="keyword">extends</span> <span class="title">TServer</span> </span>&#123;    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">serve</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        serverTransport_.listen();</span><br><span class="line">        stopped = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">while</span> (!stopped) &#123;</span><br><span class="line">            TTransport client = serverTransport_.accept();</span><br><span class="line">            processorFactory_.getProcessor(client_).process(input,output)</span><br><span class="line">        &#125;</span><br><span class="line">        executorService.shutdown();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以get查询为例：cassandra.thrift的服务定义了get方法需要主键key、列路径ColumnPath、一致性级别</p>
<figure class="highlight thrift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ColumnPath</span> </span>&#123;</span><br><span class="line">    <span class="number">3</span>: <span class="keyword">required</span> <span class="built_in">string</span> column_family,</span><br><span class="line">    <span class="number">4</span>: <span class="keyword">optional</span> <span class="built_in">binary</span> super_column,</span><br><span class="line">    <span class="number">5</span>: <span class="keyword">optional</span> <span class="built_in">binary</span> column,</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">service</span> <span class="title">cassandra</span> </span>&#123;</span><br><span class="line">  ColumnOrSuperColumn get(<span class="number">1</span>:<span class="keyword">required</span> <span class="built_in">binary</span> key,</span><br><span class="line">                          <span class="number">2</span>:<span class="keyword">required</span> ColumnPath column_path,</span><br><span class="line">                          <span class="number">3</span>:<span class="keyword">required</span> ConsistencyLevel consistency_level=ConsistencyLevel.ONE)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>服务端的处理方法在interface/thrift/gen-java/o.a.c.thrift.Cassandra类的TProcessor中</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">get</span>&lt;<span class="title">I</span> <span class="keyword">extends</span> <span class="title">Iface</span>&gt; <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">thrift</span>.<span class="title">ProcessFunction</span>&lt;<span class="title">I</span>, <span class="title">get_args</span>&gt; </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> get_result <span class="title">getResult</span><span class="params">(I iface, get_args args)</span> <span class="keyword">throws</span> org.apache.thrift.TException </span>&#123;</span><br><span class="line">    get_result result = <span class="keyword">new</span> get_result();</span><br><span class="line">    result.success = iface.get(args.key, args.column_path, args.consistency_level);</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最终会调用o.a.c.thrift.CassandraServer的get方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ColumnOrSuperColumn <span class="title">get</span><span class="params">(ByteBuffer key, ColumnPath column_path, ConsistencyLevel consistency_level)</span> </span>&#123;</span><br><span class="line">    ThriftClientState cState = state();</span><br><span class="line">    String keyspace = cState.getKeyspace();</span><br><span class="line">    cState.hasColumnFamilyAccess(keyspace, column_path.column_family, Permission.SELECT);</span><br><span class="line"></span><br><span class="line">    CFMetaData metadata = ThriftValidation.validateColumnFamily(keyspace, column_path.column_family);</span><br><span class="line">    org.apache.cassandra.db.ConsistencyLevel consistencyLevel = ThriftConversion.fromThrift(consistency_level);</span><br><span class="line"></span><br><span class="line">    SortedSet&lt;CellName&gt; names = <span class="keyword">new</span> TreeSet&lt;CellName&gt;(metadata.comparator);</span><br><span class="line">    names.add(metadata.comparator.cellFromByteBuffer(column_path.column));</span><br><span class="line">    IDiskAtomFilter filter = <span class="keyword">new</span> NamesQueryFilter(names);</span><br><span class="line"></span><br><span class="line">    ReadCommand command = ReadCommand.create(keyspace, key, column_path.column_family, now, filter);</span><br><span class="line">    Map&lt;DecoratedKey, ColumnFamily&gt; cfamilies = readColumnFamily(Arrays.asList(command), consistencyLevel, cState);</span><br><span class="line">    ColumnFamily cf = cfamilies.get(StorageService.getPartitioner().decorateKey(command.key));</span><br><span class="line"></span><br><span class="line">    List&lt;ColumnOrSuperColumn&gt; tcolumns = thriftifyColumnFamily(cf, metadata.isSuper() &amp;&amp; column_path.column != <span class="keyword">null</span>, <span class="keyword">false</span>, now);</span><br><span class="line">    <span class="keyword">return</span> tcolumns.get(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>根据客户端构造好的ReadCommand查询发生在readColumnFamily，并通过StorageProxy代理类完成读操作</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> Map&lt;DecoratedKey, ColumnFamily&gt; readColumnFamily(List&lt;ReadCommand&gt; commands, ConsistencyLevel consistency_level, ClientState cState) &#123;</span><br><span class="line">    Map&lt;DecoratedKey, ColumnFamily&gt; columnFamilyKeyMap = <span class="keyword">new</span> HashMap&lt;DecoratedKey, ColumnFamily&gt;();</span><br><span class="line">    schedule(DatabaseDescriptor.getReadRpcTimeout());</span><br><span class="line">    List&lt;Row&gt; rows = StorageProxy.read(commands, consistency_level, cState);</span><br><span class="line">    <span class="keyword">for</span> (Row row: rows) &#123;</span><br><span class="line">        columnFamilyKeyMap.put(row.key, row.cf);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> columnFamilyKeyMap;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Java_Driver（Netty）">Java Driver（Netty）</h2><p>DataStax的<a href="https://github.com/datastax/java-driver/">Java客户端</a>使用Netty实现，Cassandra的native服务端协议也采用Netty实现。<br>所以先了解客户端怎么发送数据，才能知道服务端怎么接收数据。使用Driver，<a href="https://github.com/datastax/java-driver/blob/3.x/driver-examples/src/main/java/com/datastax/driver/examples/basic/ReadCassandraVersion.java">读取Cassandra版本的简单示例</a>如下：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Cluster cluster = Cluster.builder()</span><br><span class="line">        .addContactPoints(CONTACT_POINTS).withPort(PORT)</span><br><span class="line">        .build();</span><br><span class="line">Session session = cluster.connect();</span><br><span class="line">ResultSet rs = session.execute(<span class="string">"select release_version from system.local"</span>);</span><br><span class="line">Row row = rs.one();</span><br><span class="line">String releaseVersion = row.getString(<span class="string">"release_version"</span>);</span><br></pre></td></tr></table></figure>
<p>Session是客户端建立的和服务端的会话连接对象，当connect连接建立成功后，实际上客户端和服务端的网络通道已经都打通了。Connection是客户端和服务端节点实际的连接处理对象。  </p>
<p><img src="http://img.blog.csdn.net/20161013102442935" alt="c-connection-init"></p>
<p>DataStax的Driver是Netty的客户端，Cassadra的nativeServer是Netty的服务端。所以Driver采用Bootstrap连接服务端，服务端采用ServerBootstrap接受客户端的连接。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Connection</span> </span>&#123;</span><br><span class="line">    <span class="function">ListenableFuture&lt;Void&gt; <span class="title">initAsync</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Bootstrap bootstrap = factory.newBootstrap();</span><br><span class="line">        ProtocolOptions protocolOptions = factory.configuration.getProtocolOptions();</span><br><span class="line">        bootstrap.handler(</span><br><span class="line">                <span class="keyword">new</span> Initializer(<span class="keyword">this</span>, protocolVersion, protocolOptions.getCompression().compressor(), protocolOptions.getSSLOptions(),</span><br><span class="line">                        factory.configuration.getPoolingOptions().getHeartbeatIntervalSeconds(),</span><br><span class="line">                        factory.configuration.getNettyOptions(),</span><br><span class="line">                        factory.configuration.getCodecRegistry()));</span><br><span class="line"></span><br><span class="line">        ChannelFuture future = bootstrap.connect(address);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>客户端实际的Handler主要是Initializer，而其中处理请求的是Connection.Dispatcher</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Initializer</span> <span class="keyword">extends</span> <span class="title">ChannelInitializer</span>&lt;<span class="title">SocketChannel</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">// Stateless handlers</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Message.ProtocolDecoder messageDecoder = <span class="keyword">new</span> Message.ProtocolDecoder();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Message.ProtocolEncoder messageEncoderV4 = <span class="keyword">new</span> Message.ProtocolEncoder(ProtocolVersion.V4);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Frame.Encoder frameEncoder = <span class="keyword">new</span> Frame.Encoder();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Connection connection;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> FrameCompressor compressor;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> NettyOptions nettyOptions;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ChannelHandler idleStateHandler;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> CodecRegistry codecRegistry;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">initChannel</span><span class="params">(SocketChannel channel)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// set the codec registry so that it can be accessed by ProtocolDecoder</span></span><br><span class="line">        channel.attr(Message.CODEC_REGISTRY_ATTRIBUTE_KEY).set(codecRegistry);</span><br><span class="line">        ChannelPipeline pipeline = channel.pipeline();</span><br><span class="line">        pipeline.addLast(<span class="string">"frameDecoder"</span>, <span class="keyword">new</span> Frame.Decoder());</span><br><span class="line">        pipeline.addLast(<span class="string">"frameEncoder"</span>, frameEncoder);</span><br><span class="line">        <span class="keyword">if</span> (compressor != <span class="keyword">null</span>) &#123;</span><br><span class="line">            pipeline.addLast(<span class="string">"frameDecompressor"</span>, <span class="keyword">new</span> Frame.Decompressor(compressor));</span><br><span class="line">            pipeline.addLast(<span class="string">"frameCompressor"</span>, <span class="keyword">new</span> Frame.Compressor(compressor));</span><br><span class="line">        &#125;</span><br><span class="line">        pipeline.addLast(<span class="string">"messageDecoder"</span>, messageDecoder);</span><br><span class="line">        pipeline.addLast(<span class="string">"messageEncoder"</span>, messageEncoderFor(protocolVersion));</span><br><span class="line">        pipeline.addLast(<span class="string">"idleStateHandler"</span>, idleStateHandler);</span><br><span class="line">        pipeline.addLast(<span class="string">"dispatcher"</span>, connection.dispatcher);</span><br><span class="line">        nettyOptions.afterChannelInitialized(channel);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Dispatcher看起来只是负责读取消息的响应结果</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dispatcher</span> <span class="keyword">extends</span> <span class="title">SimpleChannelInboundHandler</span>&lt;<span class="title">Message</span>.<span class="title">Response</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">channelRead0</span><span class="params">(ChannelHandlerContext ctx, Message.Response response)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> streamId = response.getStreamId();</span><br><span class="line">        ResponseHandler handler = pending.remove(streamId);</span><br><span class="line">        handler.cancelTimeout();</span><br><span class="line">        handler.callback.onSet(Connection.<span class="keyword">this</span>, response, System.nanoTime() - handler.startTime, handler.retryCount);</span><br><span class="line">        <span class="keyword">if</span> (isClosed()) tryTerminate(<span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那么客户端在哪里发送数据呢？我们从示例的session.execute看看能不能找到发送消息的线索。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ResultSetFuture <span class="title">executeAsync</span><span class="params">(<span class="keyword">final</span> Statement statement)</span> </span>&#123;</span><br><span class="line">    DefaultResultSetFuture future = <span class="keyword">new</span> DefaultResultSetFuture(<span class="keyword">this</span>, cluster.manager.protocolVersion(), makeRequestMessage(statement, <span class="keyword">null</span>));</span><br><span class="line">    <span class="keyword">new</span> RequestHandler(<span class="keyword">this</span>, future, statement).sendRequest();</span><br><span class="line">    <span class="keyword">return</span> future;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>makeRequestMessage会创建请求，那么sendRequest就会真正地发送请求了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RequestHandler</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">sendRequest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        startNewExecution();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startNewExecution</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">//future就是callback，因为future中会makeRequestMessage，所以这里可以获取callback的Request</span></span><br><span class="line">        Message.Request request = callback.request(); </span><br><span class="line">        SpeculativeExecution execution = <span class="keyword">new</span> SpeculativeExecution(request, position);</span><br><span class="line">        runningExecutions.add(execution);</span><br><span class="line">        execution.sendRequest(); <span class="comment">//发送请求，request封装在execution中</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>SpeculativeExecution是推测执行，其中QueryPlan是查询计划（根据客户端设置的负载均衡策略，路由客户端请求到不同的host节点，这个host就是传说中的Coordinator）。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpeculativeExecution</span> <span class="keyword">implements</span> <span class="title">Connection</span>.<span class="title">ResponseCallback</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Message.Request request;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">sendRequest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Host host;</span><br><span class="line">        <span class="keyword">while</span> (!isDone.get() &amp;&amp; (host = queryPlan.next()) != <span class="keyword">null</span> &amp;&amp; !queryStateRef.get().isCancelled()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (query(host)) <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        reportNoMoreHosts(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">query</span><span class="params">(<span class="keyword">final</span> Host host)</span> </span>&#123;</span><br><span class="line">        HostConnectionPool currentPool = manager.pools.get(host);</span><br><span class="line">        <span class="keyword">if</span> (allowSpeculativeExecutions &amp;&amp; nextExecutionScheduled.compareAndSet(<span class="keyword">false</span>, <span class="keyword">true</span>))</span><br><span class="line">            scheduleExecution(speculativeExecutionPlan.nextExecution(host));</span><br><span class="line">        Connection connection = currentPool.borrowConnection(manager.configuration().getPoolingOptions().getPoolTimeoutMillis(), TimeUnit.MILLISECONDS);</span><br><span class="line">        write(connection, <span class="keyword">this</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Connection connection, Connection.ResponseCallback responseCallback)</span> <span class="keyword">throws</span> ConnectionException, BusyConnectionException </span>&#123;</span><br><span class="line">        connectionHandler = connection.write(responseCallback, statement.getReadTimeoutMillis(), <span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>query方法看起来把request对象丢了，不过write(connection, this)传递了this对象，仍然有机会取出request对象。<br>write方法继续传递responseCallback对象，可以看到callback.request()起死回生了，我们的请求对象request并没有丢失。<br>channel.writeAndFlush(request)是Netty写数据的方法，即客户端把请求对象发送给了服务端。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ResponseHandler <span class="title">write</span><span class="params">(ResponseCallback callback, <span class="keyword">long</span> statementReadTimeoutMillis, <span class="keyword">boolean</span> startTimeout)</span> <span class="keyword">throws</span> ConnectionException, BusyConnectionException </span>&#123;</span><br><span class="line">    ResponseHandler handler = <span class="keyword">new</span> ResponseHandler(<span class="keyword">this</span>, statementReadTimeoutMillis, callback);</span><br><span class="line">    dispatcher.add(handler);</span><br><span class="line">    Message.Request request = callback.request().setStreamId(handler.streamId);</span><br><span class="line">    <span class="keyword">if</span> (DISABLE_COALESCING) &#123; <span class="comment">//直接写，不缓存</span></span><br><span class="line">        channel.writeAndFlush(request).addListener(writeHandler(request, handler));</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">//缓存</span></span><br><span class="line">        flush(<span class="keyword">new</span> FlushItem(channel, request, writeHandler(request, handler)));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> handler;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="nativeServer（Netty）">nativeServer（Netty）</h2><p>native服务器使用Netty，ServerBootstrap绑定的Initializer添加了多种Handler组成ChannelPipeline：</p>
<ol>
<li>Frame解码、编码</li>
<li>消息解码、编码</li>
<li>消息分发（Dispatcher）</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Server</span> <span class="keyword">implements</span> <span class="title">CassandraDaemon</span>.<span class="title">Server</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> EventLoopGroup workerGroup;</span><br><span class="line">    <span class="keyword">private</span> EventExecutor eventExecutorGroup;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        eventExecutorGroup = <span class="keyword">new</span> RequestThreadPoolExecutor();</span><br><span class="line">        <span class="keyword">boolean</span> hasEpoll = enableEpoll ? Epoll.isAvailable() : <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (hasEpoll) &#123;</span><br><span class="line">            workerGroup = <span class="keyword">new</span> EpollEventLoopGroup();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            workerGroup = <span class="keyword">new</span> NioEventLoopGroup();</span><br><span class="line">        &#125;</span><br><span class="line">        ServerBootstrap bootstrap = <span class="keyword">new</span> ServerBootstrap()</span><br><span class="line">                                    .group(workerGroup)</span><br><span class="line">                                    .channel(hasEpoll ? EpollServerSocketChannel.class : NioServerSocketChannel.class)</span><br><span class="line">                                    .childOption(ChannelOption.TCP_NODELAY, <span class="keyword">true</span>)</span><br><span class="line">                                    .childOption(ChannelOption.SO_LINGER, <span class="number">0</span>)</span><br><span class="line">                                    .childOption(ChannelOption.SO_KEEPALIVE, DatabaseDescriptor.getRpcKeepAlive())</span><br><span class="line">                                    .childOption(ChannelOption.ALLOCATOR, CBUtil.allocator)</span><br><span class="line">                                    .childOption(ChannelOption.WRITE_BUFFER_HIGH_WATER_MARK, <span class="number">32</span> * <span class="number">1024</span>)</span><br><span class="line">                                    .childOption(ChannelOption.WRITE_BUFFER_LOW_WATER_MARK, <span class="number">8</span> * <span class="number">1024</span>);</span><br><span class="line">        bootstrap.childHandler(<span class="keyword">new</span> Initializer(<span class="keyword">this</span>));</span><br><span class="line">        bootstrap.bind(socket);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Initializer</span> <span class="keyword">extends</span> <span class="title">ChannelInitializer</span> </span>&#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> Server server;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">initChannel</span><span class="params">(Channel channel)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">            ChannelPipeline pipeline = channel.pipeline();</span><br><span class="line">            pipeline.addFirst(<span class="string">"connectionLimitHandler"</span>, <span class="keyword">new</span> ConnectionLimitHandler()); <span class="comment">//连接限制</span></span><br><span class="line">            pipeline.addLast(<span class="string">"frameDecoder"</span>, <span class="keyword">new</span> Frame.Decoder(server.connectionFactory)); <span class="comment">//Frame解码</span></span><br><span class="line">            pipeline.addLast(<span class="string">"frameEncoder"</span>, <span class="keyword">new</span> Frame.Encoder()); <span class="comment">//Frame编码</span></span><br><span class="line">            pipeline.addLast(<span class="string">"frameDecompressor"</span>, <span class="keyword">new</span> Frame.Decompressor()); <span class="comment">//Frame解压缩</span></span><br><span class="line">            pipeline.addLast(<span class="string">"frameCompressor"</span>, <span class="keyword">new</span> Frame.Compressor()); <span class="comment">//Frame压缩</span></span><br><span class="line">            pipeline.addLast(<span class="string">"messageDecoder"</span>, <span class="keyword">new</span> Message.ProtocolDecoder()); <span class="comment">//消息内容解码</span></span><br><span class="line">            pipeline.addLast(<span class="string">"messageEncoder"</span>, <span class="keyword">new</span> Message.ProtocolEncoder()); <span class="comment">//消息内容编码</span></span><br><span class="line">            pipeline.addLast(server.eventExecutorGroup, <span class="string">"executor"</span>, <span class="keyword">new</span> Message.Dispatcher()); <span class="comment">//消息分发</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编解码和请求、响应是对应的，比如服务端收到请求，将客户端发送的请求进行解码（ProtocolDecoder），服务端处理完毕后，将响应内容编码发送到客户端（ProtocolEncoder）。  </p>
<p>CQL协议和Thrift协议一样，都需要事先定义好数据结构、服务方法等，CQL协议的说明文档在doc文件夹下，Frame的中文翻译是框架，所以它定义了消息内容的格式，其中Header消息头一共9个字节（40+32=72bits/8=9byte），消息内容是不定长的。Message是建立在Frame之上的消息类型（所以你可以看到Initializer构建ChannelPipeline是先Frame，然后是Message，最后是Message的Dispatcher，这跟请求的处理也是类型的：服务端先接收请求，然后解析出对应的请求类型，最后才处理请求）。  </p>
<p>消息类型有多种：ERROR、STARTUP、QUERY、RESULT、PREPARE、EXECUTE、EVENT、BATCH，每种消息类型都指定了是Request还是Response。比如ERROR、RESULT、EVENT是Response，其他都是Request。  </p>
<h3 id="Dispatcher">Dispatcher</h3><p>服务端的Dispatcher处理器会接收请求、执行请求、返回响应结果。这里的flush和Netty客户端中发送请求时采用缓存形式的flush类似，<br>不过最终的目的都是发送数据给对端（客户端发送请求给服务端，服务端发送响应结果给客户端）。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Dispatcher</span> <span class="keyword">extends</span> <span class="title">SimpleChannelInboundHandler</span>&lt;<span class="title">Request</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">channelRead0</span><span class="params">(ChannelHandlerContext ctx, Request request)</span> </span>&#123;</span><br><span class="line">        ServerConnection connection = (ServerConnection)request.connection();</span><br><span class="line">        logger.trace(<span class="string">"Received: &#123;&#125;, v=&#123;&#125;"</span>, request, connection.getVersion());</span><br><span class="line"></span><br><span class="line">        Response response = request.execute(qstate); <span class="comment">//服务端执行请求</span></span><br><span class="line">        response.setStreamId(request.getStreamId());</span><br><span class="line">        response.attach(connection);</span><br><span class="line">        connection.applyStateTransition(request.type, response.type);</span><br><span class="line"></span><br><span class="line">        logger.trace(<span class="string">"Responding: &#123;&#125;, v=&#123;&#125;"</span>, response, connection.getVersion());</span><br><span class="line">        flush(<span class="keyword">new</span> FlushItem(ctx, response, request.getSourceFrame()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以CQL查询为例，trace日志如下：</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">TRACE [SharedPool-Worker-1] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>5 Message.java:506 - Received: QUERY select * from velocity_app where attribute='zqhxuyuan' and type='login' and partner_code='tongdun' and app_name='tongdun_app'<span class="comment">;, v=4</span></span><br><span class="line"></span><br><span class="line">TRACE [SharedPool-Worker-1] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>6 QueryProcessor.java:221 - Process org.apache.cassandra.cql3.statements.SelectStatement@<span class="number">37504b54</span> @CL.ONE</span><br><span class="line">DEBUG [SharedPool-Worker-1] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>6 SliceQueryPager.java:92 - Querying next page of slice query<span class="comment">; new filter: SliceQueryFilter [reversed=false, slices=[[, ]], count=100, toGroup = 1]</span></span><br><span class="line">TRACE [SharedPool-Worker-1] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>7 ReadCallback.java:76 - Blockfor is 1<span class="comment">; setting up requests to localhost/127.0.0.1</span></span><br><span class="line">TRACE [SharedPool-Worker-1] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>7 AbstractReadExecutor.java:118 - reading data locally</span><br><span class="line">TRACE [SharedPool-Worker-2] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>8 SliceQueryFilter.java:269 - collecting 0 of 100: <span class="number">1111111111-1</span><span class="number">::fa</span>lse:<span class="number">0@1476176498</span>640102</span><br><span class="line">TRACE [SharedPool-Worker-2] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>8 SliceQueryFilter.java:269 - collecting 1 of 100: <span class="number">1111111111-1</span>:event:false:<span class="number">10@1476176498</span>640102</span><br><span class="line">TRACE [SharedPool-Worker-2] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>8 SliceQueryFilter.java:269 - collecting 1 of 100: <span class="number">1111111111-1</span>:timestamp:false:<span class="number">8@1476176498</span>640102</span><br><span class="line">TRACE [SharedPool-Worker-1] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>8 StorageProxy.java:1444 - Read: 1 ms.</span><br><span class="line">DEBUG [SharedPool-Worker-1] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>9 AbstractQueryPager.java:95 - Fetched 1 live rows</span><br><span class="line">DEBUG [SharedPool-Worker-1] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>9 AbstractQueryPager.java:112 - Got result (1) smaller than page size (100), considering pager exhausted</span><br><span class="line">DEBUG [SharedPool-Worker-1] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>9 AbstractQueryPager.java:133 - Remaining rows to page: <span class="number">2147483646</span></span><br><span class="line"></span><br><span class="line">TRACE [SharedPool-Worker-1] <span class="number">2016-10-11</span> <span class="number">17:02:27,34</span>9 Message.java:525 - Responding: ROWS [attribute(forseti, velocity_app), org.apache.cassandra.db.marshal.UTF8Type][partner_code(forseti, velocity_app), org.apache.cassandra.db.marshal.UTF8Type][app_name(forseti, velocity_app), org.apache.cassandra.db.marshal.UTF8Type][type(forseti, velocity_app), org.apache.cassandra.db.marshal.UTF8Type][sequence_id(forseti, velocity_app), org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.UTF8Type)][event(forseti, velocity_app), org.apache.cassandra.db.marshal.UTF8Type][timestamp(forseti, velocity_app), org.apache.cassandra.db.marshal.LongType]</span><br><span class="line"> | zqhxuyuan | tongdun | tongdun_app | login | <span class="number">1111111111-1</span> | &#123;jsondata&#125; | <span class="number">1111111111</span></span><br><span class="line">---, v=4</span><br></pre></td></tr></table></figure>
<p>如果开启tracing on，会显示查询语句在服务端的运行轨迹。</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Tracing session: 6eceb810-8f91-11e6-a2b4-dbe2eb0e3cb9</span><br><span class="line"></span><br><span class="line"> activity                                                                                                                                                     |<span class="string"> timestamp                  </span>|<span class="string"> source    </span>|<span class="string"> source_elapsed</span><br><span class="line">--------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------+-----------+----------------</span><br><span class="line">                                                                                                                                           Execute CQL3 query </span>|<span class="string"> 2016-10-11 17:02:27.345000 </span>|<span class="string"> 127.0.0.1 </span>|<span class="string">              0</span><br><span class="line"> Parsing select * from velocity_app where attribute='zqhxuyuan' and type='login' and partner_code='tongdun' and app_name='tongdun_app'; [SharedPool-Worker-1] </span>|<span class="string"> 2016-10-11 17:02:27.345000 </span>|<span class="string"> 127.0.0.1 </span>|<span class="string">            333</span><br><span class="line">                                                                                                                    Preparing statement [SharedPool-Worker-1] </span>|<span class="string"> 2016-10-11 17:02:27.346000 </span>|<span class="string"> 127.0.0.1 </span>|<span class="string">            730</span><br><span class="line">                                                                                       Executing single-partition query on velocity_app [SharedPool-Worker-2] </span>|<span class="string"> 2016-10-11 17:02:27.347000 </span>|<span class="string"> 127.0.0.1 </span>|<span class="string">           2236</span><br><span class="line">                                                                                                           Acquiring sstable references [SharedPool-Worker-2] </span>|<span class="string"> 2016-10-11 17:02:27.347000 </span>|<span class="string"> 127.0.0.1 </span>|<span class="string">           2357</span><br><span class="line">                                                                                                            Merging memtable tombstones [SharedPool-Worker-2] </span>|<span class="string"> 2016-10-11 17:02:27.347000 </span>|<span class="string"> 127.0.0.1 </span>|<span class="string">           2446</span><br><span class="line">                                                              Skipped 0/0 non-slice-intersecting sstables, included 0 due to tombstones [SharedPool-Worker-2] </span>|<span class="string"> 2016-10-11 17:02:27.347000 </span>|<span class="string"> 127.0.0.1 </span>|<span class="string">           2591</span><br><span class="line">                                                                                             Merging data from memtables and 0 sstables [SharedPool-Worker-2] </span>|<span class="string"> 2016-10-11 17:02:27.347000 </span>|<span class="string"> 127.0.0.1 </span>|<span class="string">           2661</span><br><span class="line">                                                                                                      Read 1 live and 0 tombstone cells [SharedPool-Worker-2] </span>|<span class="string"> 2016-10-11 17:02:27.348000 </span>|<span class="string"> 127.0.0.1 </span>|<span class="string">           3240</span><br><span class="line">                                                                                                                                             Request complete </span>|<span class="string"> 2016-10-11 17:02:27.349147 </span>|<span class="string"> 127.0.0.1 </span>|<span class="string">           4147</span></span><br></pre></td></tr></table></figure>
<p>可以看到<strong>日志文件</strong>第一行/最后一行打印的时间撮和<strong>tracing on</strong>的第一行/最后一行基本一致。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//日志文件</span></span><br><span class="line">TRACE [SharedPool-Worker-<span class="number">1</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">17</span>:<span class="number">02</span>:<span class="number">27</span>,<span class="number">345</span> Message.java:<span class="number">506</span> - Received: QUERY</span><br><span class="line">TRACE [SharedPool-Worker-<span class="number">1</span>] <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">17</span>:<span class="number">02</span>:<span class="number">27</span>,<span class="number">349</span> Message.java:<span class="number">525</span> - Responding: ROWS </span><br><span class="line"></span><br><span class="line"><span class="comment">//CQL tracing on</span></span><br><span class="line">       Execute CQL3 query | <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">17</span>:<span class="number">02</span>:<span class="number">27.345000</span></span><br><span class="line">         Request complete | <span class="number">2016</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">17</span>:<span class="number">02</span>:<span class="number">27.349147</span></span><br></pre></td></tr></table></figure>
<h3 id="QueryMessage">QueryMessage</h3><p>以o.a.c.transport.messages.QueryMessage请求为例，</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Message.<span class="function">Response <span class="title">execute</span><span class="params">(QueryState state)</span> </span>&#123;</span><br><span class="line">    Tracing.instance.begin(<span class="string">"Execute CQL3 query"</span>, state.getClientAddress(), builder.build());</span><br><span class="line">    Message.Response response = ClientState.getCQLQueryHandler().process(query, state, options, getCustomPayload());</span><br><span class="line">    <span class="keyword">return</span> response;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>CQLQueryHandler的处理器是QueryProcessor，</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ResultMessage <span class="title">process</span><span class="params">(String queryString, QueryState queryState, QueryOptions options)</span> </span>&#123;</span><br><span class="line">    ParsedStatement.Prepared p = getStatement(queryString, queryState.getClientState());</span><br><span class="line">    options.prepare(p.boundNames);</span><br><span class="line">    CQLStatement prepared = p.statement;</span><br><span class="line">    <span class="keyword">return</span> processStatement(prepared, queryState, options);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> ResultMessage <span class="title">processStatement</span><span class="params">(CQLStatement statement, QueryState queryState, QueryOptions options)</span> </span>&#123;</span><br><span class="line">    logger.trace(<span class="string">"Process &#123;&#125; @CL.&#123;&#125;"</span>, statement, options.getConsistency());</span><br><span class="line">    ClientState clientState = queryState.getClientState();</span><br><span class="line">    ResultMessage result = statement.execute(queryState, options);</span><br><span class="line">    <span class="keyword">return</span> result == <span class="keyword">null</span> ? <span class="keyword">new</span> ResultMessage.Void() : result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Response response = request.execute(qstate)：我们举例了request是QueryMessage（即Message.Request类型），返回结果是ResultMessage，正好是Message.Response类型。<br>这和我们说的消息类型中，QUERY是Request，RESULT是Response就对应上来了。</p>
</blockquote>
<p>现在从Message进入到Statement，以SelectStatement为例，我们终于看到了和thrift类似的StorageProxy代理调用</p>
<blockquote>
<p>通常消息类型也会对应不同的Statement，比如QueryMessage对应了SelectStatement，Execute或Batch消息对应不同的Statement。<br>请求对象的转换：Request - Statement - Command。比如查询请求 - SelectStatement - ReadCommands。  </p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> ResultMessage.<span class="function">Rows <span class="title">execute</span><span class="params">(QueryState state, QueryOptions options)</span> <span class="keyword">throws</span> RequestExecutionException, RequestValidationException </span>&#123;</span><br><span class="line">    ConsistencyLevel cl = options.getConsistency();</span><br><span class="line">    <span class="keyword">int</span> limit = getLimit(options);</span><br><span class="line">    Pageable command = getPageableCommand(options, limit, now);</span><br><span class="line">    <span class="keyword">int</span> pageSize = getPageSize(options);</span><br><span class="line">    <span class="keyword">if</span> (pageSize &lt;= <span class="number">0</span> || command == <span class="keyword">null</span> || !QueryPagers.mayNeedPaging(command, pageSize))</span><br><span class="line">        <span class="keyword">return</span> execute(command, options, limit, now, state); <span class="comment">//不分页查询</span></span><br><span class="line">    QueryPager pager = QueryPagers.pager(command, cl, state.getClientState(), options.getPagingState());</span><br><span class="line">    <span class="keyword">return</span> execute(pager, options, limit, now, pageSize); <span class="comment">//分页查询</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">private</span> ResultMessage.<span class="function">Rows <span class="title">execute</span><span class="params">(Pageable command, QueryOptions options, <span class="keyword">int</span> limit, <span class="keyword">long</span> now, QueryState state)</span> </span>&#123;</span><br><span class="line">    List&lt;Row&gt; rows = command <span class="keyword">instanceof</span> Pageable.ReadCommands</span><br><span class="line">             ? StorageProxy.read(((Pageable.ReadCommands)command).commands, options.getConsistency(), state.getClientState())</span><br><span class="line">             : StorageProxy.getRangeSlice((RangeSliceCommand)command, options.getConsistency());</span><br><span class="line">    <span class="keyword">return</span> processResults(rows, options, limit, now); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>不管是thrift协议的ThriftServer，还是二进制协议的Server，最终都会调用StorageProxy代理类。</p>
<p><img src="http://img.blog.csdn.net/20160928232623843" alt="c-storageproxy"></p>
<h2 id="StorageProxy">StorageProxy</h2><p>StorageProxy代理类的read方法根据一致性级别是不是Serial有两种：普通的读取和事务性的读取（Transaction）。</p>
<blockquote>
<p>Cassandra的事务支持使用Paxos实现，对应的读方法是：readWithPaxos</p>
<p>CQL或者Driver客户端发送请求，某个服务端的StorageProxy接收请求，这个服务器叫做Coordinator协调节点。<br>协调节点接收客户端请求，除了ReadCommand读取请求外，还有consistencyLevel用来决定要不要往其他节点继续发送请求。  </p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Performs the actual reading of a row out of the StorageService, fetching a specific set of column names from a given column family.</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;Row&gt; <span class="title">read</span><span class="params">(List&lt;ReadCommand&gt; commands, ConsistencyLevel consistencyLevel, ClientState state)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> consistencyLevel.isSerialConsistency()</span><br><span class="line">         ? readWithPaxos(commands, consistencyLevel, state)</span><br><span class="line">         : readRegular(commands, consistencyLevel);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> List&lt;Row&gt; <span class="title">readRegular</span><span class="params">(List&lt;ReadCommand&gt; commands, ConsistencyLevel consistencyLevel)</span> </span>&#123;</span><br><span class="line">    List&lt;Row&gt; rows = fetchRows(commands, consistencyLevel);</span><br><span class="line">    <span class="keyword">return</span> rows;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> List&lt;Row&gt; <span class="title">fetchRows</span><span class="params">(List&lt;ReadCommand&gt; initialCommands, ConsistencyLevel consistencyLevel)</span></span>&#123;</span><br><span class="line">    List&lt;Row&gt; rows = <span class="keyword">new</span> ArrayList&lt;&gt;(initialCommands.size());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// send out read requests</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; commands.size(); i++) &#123;</span><br><span class="line">        ReadCommand command = commands.get(i);</span><br><span class="line">        AbstractReadExecutor exec = AbstractReadExecutor.getReadExecutor(command, consistencyLevel);</span><br><span class="line">        exec.executeAsync(); <span class="comment">//异步线程执行</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// read results and make a second pass for any digest mismatches</span></span><br><span class="line">    <span class="keyword">for</span> (AbstractReadExecutor exec: readExecutors) &#123;</span><br><span class="line">        Row row = exec.get(); <span class="comment">//获取线程的执行结果</span></span><br><span class="line">        <span class="keyword">if</span> (row != <span class="keyword">null</span>) &#123;</span><br><span class="line">            row = exec.command.maybeTrim(row);</span><br><span class="line">            rows.add(row);  <span class="comment">//添加到结果集中</span></span><br><span class="line">        &#125;                </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> rows; <span class="comment">//最终的查询结果</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过StorageProxy得到的查询结果怎么发送会客户端，则是由前面Netty的Dispatcher完成，不属于StorageProxy的职责。  </p>
<p>读取的线程池有多种实现，比如不带推测的NeverSpeculatingReadExecutor。实际的读取线程还是被包装在ReadCommand中。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">NeverSpeculatingReadExecutor</span> <span class="keyword">extends</span> <span class="title">AbstractReadExecutor</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">executeAsync</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        makeDataRequests(targetReplicas.subList(<span class="number">0</span>, <span class="number">1</span>));</span><br><span class="line">        <span class="keyword">if</span> (targetReplicas.size() &gt; <span class="number">1</span>)</span><br><span class="line">            makeDigestRequests(targetReplicas.subList(<span class="number">1</span>, targetReplicas.size()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">makeRequests</span><span class="params">(ReadCommand readCommand, Iterable&lt;InetAddress&gt; endpoints)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (InetAddress endpoint : endpoints) &#123;</span><br><span class="line">        <span class="keyword">if</span> (isLocalRequest(endpoint)) &#123; <span class="comment">//请求的目的地包含本地节点</span></span><br><span class="line">            hasLocalEndpoint = <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        MessagingService.instance().sendRRWithFailure(message, endpoint, handler);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (hasLocalEndpoint) &#123; <span class="comment">//立即在本地执行，由于还有远程数据需要读取，所以需要callback/handler</span></span><br><span class="line">        StageManager.getStage(Stage.READ).maybeExecuteImmediately(<span class="keyword">new</span> LocalReadRunnable(command, handler));            </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在前面启动CassandraDaemon时，我们说每个Cassandra都会启动Thrift和native两种服务器。对应的StorageProxy作为代理类会接收客户端发送的各种请求（比如读和写）。<br>但是作为分布式系统，客户端发送请求，具体要交给哪些节点处理呢？Cassandra中有一个协调者的角色表示接收客户端的请求所在的节点，但这个节点可能并不是真正存储数据的节点，<br>它会将客户端的请求转发到其他真正应该需要存储数据的节点。读取和存储一样，如果数据没有存储在协调者节点上，也就无法从协调者读取数据，那么协调者也应该负责发送读取请求到<br>真正存储数据的节点，然后等待真实节点返回数据给协调者，再由协调者返回数据给客户端。  </p>
<p>这里接收请求的节点即协调者，就会负责makeRequests创建请求。如果说客户端的请求正好也会存储到当前协调者上，那么协调者就可以直接存储数据了。<br>所以如果满足isLocalRequest，就会在本地节点通过maybeExecuteImmediately立即执行命令。对于其他非本地的远程节点，则通过sendRRWithFailure把带有命令的请求发送出去（发送到哪个目标节点，由第二个参数endpoint决定）。</p>
<p>LocalReadRunnable封装了ReadCommand线程类和回调函数，实际的读取在command.getRow，最后返回Row一行记录。ReadCommand有两种实现：SliceFromReadCommand和SliceByNamesReadCommand。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">LocalReadRunnable</span> <span class="keyword">extends</span> <span class="title">DroppableRunnable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ReadCommand command;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ReadCallback&lt;ReadResponse, Row&gt; handler;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">runMayThrow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Keyspace keyspace = Keyspace.open(command.ksName);</span><br><span class="line">        Row r = command.getRow(keyspace);</span><br><span class="line">        ReadResponse result = ReadVerbHandler.getResponse(command, r);</span><br><span class="line">        handler.response(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们来看下客户端调用StorageProxy的命令（比如ReadCommand）是如何在服务端传输的</p>
<ol>
<li>AbstractReadExecutor（比如NeverSpeculatingReadExecutor），然后调用executeAsync执行线程池</li>
<li>LocalReadRunnable，调用maybeExecuteImmediately执行线程</li>
<li>在LocalReadRunnable里，runMayThrow会开始真正执行ReadCommand的getRow指令</li>
</ol>
<p><img src="http://img.blog.csdn.net/20160928232810333" alt="c-proxyread"></p>
<p>从客户端到服务端的StorageProxy执行读取请求，最终返回读取结果给客户端的流程如下：</p>
<p><img src="http://img.blog.csdn.net/20161013174049157" alt="c-network-flow"></p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Client</span>(<span class="type">Netty</span>)-&gt;<span class="type">Server</span>(<span class="type">Netty</span>): channel.write(request)</span><br><span class="line"><span class="type">Server</span>(<span class="type">Netty</span>)-&gt;<span class="type">Dispatcher</span>: pipeline</span><br><span class="line"><span class="type">Dispatcher</span>-&gt;<span class="type">Request</span>: execute</span><br><span class="line"><span class="type">Request</span>-&gt;<span class="type">Statement</span>: execute</span><br><span class="line"><span class="type">Statement</span>-&gt;<span class="type">StorageProxy</span>: read</span><br><span class="line"><span class="type">StorageProxy</span>-&gt;<span class="type">ReadExecutor</span>: executeAsync</span><br><span class="line"><span class="type">ReadExecutor</span>-&gt;<span class="type">LocalReadRunnable</span>: runMayThrow</span><br><span class="line"><span class="type">StorageProxy</span>-&gt;<span class="type">ReadExecutor</span>: get</span><br><span class="line"><span class="type">LocalReadRunnable</span>-&gt;<span class="type">ReadCommand</span>: getRow(keyspace): row</span><br><span class="line"><span class="type">LocalReadRunnable</span>-&gt;<span class="type">ReadVerbHandler</span>: getResponse(command,row): <span class="literal">result</span></span><br><span class="line"><span class="type">LocalReadRunnable</span>-&gt;<span class="type">ReadVerbHandler</span>: response(<span class="literal">result</span>)</span><br><span class="line"><span class="type">ReadExecutor</span>-&gt;<span class="type">MessageService</span>: sendRR(message,endpoint,handler)</span><br><span class="line"><span class="type">StorageProxy</span>-&gt;<span class="type">Dispatcher</span>: query <span class="literal">result</span> pass to dispatcher</span><br><span class="line"><span class="type">Dispatcher</span>-&gt;<span class="type">Client</span>(<span class="type">Netty</span>): channel.write(response)</span><br></pre></td></tr></table></figure>
<h4 id="协调节点选择目标节点">协调节点选择目标节点</h4><p>NeverSpeculatingReadExecutor异步方式执行读取请求，其中targetReplicas决定了要发送请求给哪些节点。<br>targetReplicas是一个列表，第一个节点发送Data请求，其他剩余节点发送Digest请求。targetReplicas定义在<br>父类AbstractReadExecutor中，并且在构造ReadExecutor实例对象的时候被赋值。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">NeverSpeculatingReadExecutor</span> <span class="keyword">extends</span> <span class="title">AbstractReadExecutor</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">NeverSpeculatingReadExecutor</span><span class="params">(ReadCommand command, ConsistencyLevel consistencyLevel, List&lt;InetAddress&gt; targetReplicas)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(command, consistencyLevel, targetReplicas);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">executeAsync</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        makeDataRequests(targetReplicas.subList(<span class="number">0</span>, <span class="number">1</span>));</span><br><span class="line">        <span class="keyword">if</span> (targetReplicas.size() &gt; <span class="number">1</span>)</span><br><span class="line">            makeDigestRequests(targetReplicas.subList(<span class="number">1</span>, targetReplicas.size()));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Collection&lt;InetAddress&gt; <span class="title">getContactedReplicas</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> targetReplicas;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>具体的AbstractReadExecutor实例，只有在执行请求的时候才开始被创建。targetReplicas目标节点的分布和数据模型的主键key相关，因为key决定了分布在哪些节点上。<br>RingPosition是一个具体的Token值，Cassandra每个节点都有个多个Token Range，当对key计算得到一个具体的Token值时，它是一定能够落在特定的节点上的。<br>当然为了保证数据的可靠性，会将同一份数据分布在多个节点上，根据副本策略，比如SimpleStrategy会顺时针依次存放到三个不同的节点上。其他策略会考虑机架等因素。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//StorageProxy处理读取请求，创建ReadExecutor线程</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> List&lt;Row&gt; <span class="title">fetchRows</span><span class="params">(List&lt;ReadCommand&gt; initialCommands, ConsistencyLevel consistencyLevel)</span> </span>&#123;</span><br><span class="line">    AbstractReadExecutor exec = AbstractReadExecutor.getReadExecutor(command, consistencyLevel);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//AbstractReadExecutor</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> AbstractReadExecutor <span class="title">getReadExecutor</span><span class="params">(ReadCommand command, ConsistencyLevel consistencyLevel)</span> <span class="keyword">throws</span> UnavailableException </span>&#123;</span><br><span class="line">    Keyspace keyspace = Keyspace.open(command.ksName);</span><br><span class="line">    List&lt;InetAddress&gt; allReplicas = StorageProxy.getLiveSortedEndpoints(keyspace, command.key); <span class="comment">//如何获取key的所有副本？？？</span></span><br><span class="line">    ReadRepairDecision repairDecision = Schema.instance.getCFMetaData(command.ksName, command.cfName).newReadRepairDecision();</span><br><span class="line">    List&lt;InetAddress&gt; targetReplicas = consistencyLevel.filterForQuery(keyspace, allReplicas, repairDecision);</span><br><span class="line">    consistencyLevel.assureSufficientLiveNodes(keyspace, targetReplicas); <span class="comment">//如果没有足够的副本，提前抛出UAE异常</span></span><br><span class="line"></span><br><span class="line">    ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(command.cfName);</span><br><span class="line">    RetryType retryType = cfs.metadata.getSpeculativeRetry().type;</span><br><span class="line">    <span class="comment">// Speculative retry is disabled *OR* there are simply no extra replicas to speculate. 禁用重试，或者没有可用的副本来满足重试</span></span><br><span class="line">    <span class="keyword">if</span> (retryType == RetryType.NONE || consistencyLevel.blockFor(keyspace) == allReplicas.size())</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> NeverSpeculatingReadExecutor(command, consistencyLevel, targetReplicas);</span><br><span class="line">    <span class="comment">//还有其他实现    </span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//StorageProxy，将key根据Partitioner包装成RingPosition。Partitioner是对key的hash计算方式</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;InetAddress&gt; <span class="title">getLiveSortedEndpoints</span><span class="params">(Keyspace keyspace, ByteBuffer key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> getLiveSortedEndpoints(keyspace, StorageService.getPartitioner().decorateKey(key));</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> List&lt;InetAddress&gt; <span class="title">getLiveSortedEndpoints</span><span class="params">(Keyspace keyspace, RingPosition pos)</span> </span>&#123;</span><br><span class="line">    List&lt;InetAddress&gt; liveEndpoints = StorageService.instance.getLiveNaturalEndpoints(keyspace, pos);</span><br><span class="line">    DatabaseDescriptor.getEndpointSnitch().sortByProximity(FBUtilities.getBroadcastAddress(), liveEndpoints);</span><br><span class="line">    <span class="keyword">return</span> liveEndpoints;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>为了获取指定searchPosition在TokenMetadata中的位置，给定所有已经排序好的sortedTokens，再根据二分查找就可以很快查询出对应的Token了。<br>如何根据指定的searchToken获取所有副本应该存储的节点位置，定义了一个抽象方法calculateNaturalEndpoints计算自然点，交给子类实现。  </p>
<blockquote>
<p>可见TokenMetadata是数据源，Cassandra节点在启动后，应该将所管理的Tokens都更新到TokenMetadata，<br>这个TokenMetadta在每个节点上上的数据应该是一样的，确保客户端不管请求哪个协调者都得到相同的元数据。  </p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//AbstractReplicationStrategy</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ArrayList&lt;InetAddress&gt; <span class="title">getNaturalEndpoints</span><span class="params">(RingPosition searchPosition)</span> </span>&#123;</span><br><span class="line">    Token searchToken = searchPosition.getToken();</span><br><span class="line">    Token keyToken = TokenMetadata.firstToken(tokenMetadata.sortedTokens(), searchToken);</span><br><span class="line">    ArrayList&lt;InetAddress&gt; endpoints = getCachedEndpoints(keyToken);</span><br><span class="line">    <span class="keyword">if</span> (endpoints == <span class="keyword">null</span>) &#123;</span><br><span class="line">        TokenMetadata tm = tokenMetadata.cachedOnlyTokenMap();</span><br><span class="line">        <span class="comment">// if our cache got invalidated, it's possible there is a new token to account for too</span></span><br><span class="line">        keyToken = TokenMetadata.firstToken(tm.sortedTokens(), searchToken);</span><br><span class="line">        endpoints = <span class="keyword">new</span> ArrayList&lt;InetAddress&gt;(calculateNaturalEndpoints(searchToken, tm));</span><br><span class="line">        cachedEndpoints.put(keyToken, endpoints);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ArrayList&lt;InetAddress&gt;(endpoints);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> List&lt;InetAddress&gt; <span class="title">calculateNaturalEndpoints</span><span class="params">(Token searchToken, TokenMetadata tokenMetadata)</span></span>;</span><br></pre></td></tr></table></figure>
<p>最简单的SimpleStrategy，通过ringIterator迭代TokenMetadata。TokenMetadata包含了所有的Token，每个Token都对应一个节点地址endpoint。<br>注意Cassandra使用VNodes，每个节点负责的TokenRange是不连续的。所以如果顺序分配Token的话，有可能都是同一个endpoint。<br>举例：[Token1:Node1, Token2:Node1, Token3:Node2, Token4:Node2, Token5:Node1, Token6:Node3, …]，<br>如果顺序分配Token=[Token1:Node1, Token2:Node1, Token3:Node2]，则只能找到两个节点=[Node1,Node2]，所以查找时还要去重节点信息。<br>实际上的选择策略是：[Token1:Node1, Token3:Node2, Token6:Node3]，最终选择的节点=[Node1,Node2,Node3]。  </p>
<blockquote>
<p>你可能会问假设key对应的是Token1，而上面却选择了[Token1, Token3, Token6]，而Token3和Token6根本就不等于Token1。<br>但实际上我们选择的是节点，而不是Token!</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//SimpleStrategy</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;InetAddress&gt; <span class="title">calculateNaturalEndpoints</span><span class="params">(Token token, TokenMetadata metadata)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> replicas = getReplicationFactor();</span><br><span class="line">    ArrayList&lt;Token&gt; tokens = metadata.sortedTokens();</span><br><span class="line">    List&lt;InetAddress&gt; endpoints = <span class="keyword">new</span> ArrayList&lt;InetAddress&gt;(replicas);</span><br><span class="line">    Iterator&lt;Token&gt; iter = TokenMetadata.ringIterator(tokens, token, <span class="keyword">false</span>);</span><br><span class="line">    <span class="keyword">while</span> (endpoints.size() &lt; replicas &amp;&amp; iter.hasNext()) &#123; </span><br><span class="line">        InetAddress ep = metadata.getEndpoint(iter.next());</span><br><span class="line">        <span class="keyword">if</span> (!endpoints.contains(ep))</span><br><span class="line">            endpoints.add(ep);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> endpoints;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Token、TokenMetadata会在《存储引擎》中详细介绍</p>
</blockquote>
<h2 id="StorageService">StorageService</h2><p>CassandraDaemon在启动thrift服务器和native服务器之前，先初始化了StorageService。刚启动的Cassandra会尝试加入集群，其中和网络相关的是MessagingService消息服务。  </p>
<blockquote>
<p>StorageService类似前面的ThriftServer和native netty Server，都是一种服务端的实现。只不过StorageService负责存储，而前两者负责消息传输、RPC调用。  </p>
<p>问题：StorageProxy可以看做是StorageService的前置代理类，客户端请求要先经过StorageProxy才能到达StorageService。还是说StorageProxy和StorageService是平等的关系？<br>实际上两者应该是平等的，我们并没有看到StorageProxy到StorageService的调用。再者，StorageService本身也是可以接收客户端请求的。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">initServer</span><span class="params">(<span class="keyword">int</span> delay)</span> </span>&#123;</span><br><span class="line">    prepareToJoin();</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">prepareToJoin</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Gossiper.instance.register(<span class="keyword">this</span>);</span><br><span class="line">    Gossiper.instance.start(SystemKeyspace.incrementAndGetGeneration(), appStates);</span><br><span class="line">    MessagingService.instance().listen();</span><br><span class="line">    LoadBroadcaster.instance.startBroadcasting();</span><br><span class="line">    HintedHandOffManager.instance.start();</span><br><span class="line">    BatchlogManager.instance.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StorageService除了消息的存储服务类MessagingService外，还有其他和消息存储相关的第三方类，这些类共同组成了Cassandra分布式的存储特性，包括：  </p>
<ol>
<li>Gossiper协议，用来保证集群、节点的一致性</li>
<li>HintedHandOffManager，在节点出现异常时，管理暂时失败的请求</li>
<li>BatchlogManager，提交日志管理类</li>
</ol>
<h3 id="MessagingService">MessagingService</h3><blockquote>
<p>StorageService中启动的MessagingService使用的是storage_port=7000端口（服务器和服务器之间的协议），<br>而使用Netty的nativeServer使用native_transport_port=9042端口（客户端和服务端的协议）。<br>为什么要有两种端口，而且这两种端口，最后的服务都会走到ReadCommand.getRow()上去，为什么不统一成一个端口？<br>这是因为客户端《==》服务器，服务器《==》服务器，两者的处理协议不同。服务器和服务器之间采用原始IO处理消息，不使用Netty。<br><img src="http://img.blog.csdn.net/20161013142808839" alt="c-port-protocol">    </p>
<p>在下面的分析中，你会看到MessageService的原始ServerSocket也能处理请求（节点之间），而前面分析的CQL等客户端(Netty)通过StorageProxy也可以处理请求。<br><img src="http://img.blog.csdn.net/20161013143359123" alt="c-read-native-message"></p>
</blockquote>
<p>消息服务采用原始的ServerSocket，启动服务端线程后，在SocketThread中开始接受客户端请求，客户端请求的类型包括stream和普通的消息。</p>
<blockquote>
<p>Streaming消息也包括多种类型，主要发生于节点之间数据的流式交换，比如sstableloader，nodetool repair都会产生streaming线程。<br><a href="http://www.datastax.com/dev/blog/streaming-in-cassandra-2-0" target="_blank" rel="external">http://www.datastax.com/dev/blog/streaming-in-cassandra-2-0</a></p>
<p>一个Cassandra服务端节点通常只有一个ServerSocket，作为服务端要接受客户端的连接，通常服务端会为每个连接的客户端建立一个Socket，即会在服务端维护很多Socket连接。<br>在<code>getServerSockets()</code>中会绑定服务端端口，这样在创建SocketThread时，就可以开始接受客户端连接了。  </p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> List&lt;ServerSocket&gt; <span class="title">getServerSockets</span><span class="params">(InetAddress localEp)</span> </span>&#123;</span><br><span class="line">  ServerSocketChannel serverChannel = serverChannel = ServerSocketChannel.open();</span><br><span class="line">  ServerSocket socket = serverChannel.socket();</span><br><span class="line">  InetSocketAddress address = <span class="keyword">new</span> InetSocketAddress(localEp, DatabaseDescriptor.getStoragePort());</span><br><span class="line">  socket.bind(address,<span class="number">500</span>);</span><br><span class="line">  <span class="keyword">return</span> ArrayList.asList(socket);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">listen</span><span class="params">(InetAddress localEp)</span> <span class="keyword">throws</span> ConfigurationException </span>&#123;</span><br><span class="line">  <span class="keyword">for</span> (ServerSocket ss : getServerSockets(localEp)) &#123; <span class="comment">//监听storage port端口，如果有加密，会有两个端口</span></span><br><span class="line">    SocketThread th = <span class="keyword">new</span> SocketThread(ss, <span class="string">"ACCEPT-"</span> + localEp);</span><br><span class="line">    th.start();</span><br><span class="line">    socketThreads.add(th);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> ServerSocket server;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">final</span> Set&lt;Closeable&gt; connections = Sets.newConcurrentHashSet();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (!server.isClosed()) &#123;       </span><br><span class="line">        Socket socket = server.accept();</span><br><span class="line">        DataInputStream in = <span class="keyword">new</span> DataInputStream(socket.getInputStream());</span><br><span class="line">        <span class="keyword">int</span> header = in.readInt();</span><br><span class="line">        <span class="keyword">boolean</span> isStream = MessagingService.getBits(header, <span class="number">3</span>, <span class="number">1</span>) == <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> version = MessagingService.getBits(header, <span class="number">15</span>, <span class="number">8</span>);</span><br><span class="line">        Thread thread = isStream ? <span class="keyword">new</span> IncomingStreamingConnection(version, socket, connections)</span><br><span class="line">                      : <span class="keyword">new</span> IncomingTcpConnection(version, MessagingService.getBits(header, <span class="number">2</span>, <span class="number">1</span>) == <span class="number">1</span>, socket, connections);</span><br><span class="line">        thread.start();</span><br><span class="line">        connections.add((Closeable) thread);</span><br><span class="line">    &#125;  </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    server.close(); <span class="comment">//关闭ServerSocket线程</span></span><br><span class="line">    <span class="keyword">for</span> (Closeable connection : connections) connection.close(); <span class="comment">//关闭TCP连接处理消息的线程</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>IncomingTcpConnection是一个后台线程类，会不停地读取并处理消息，然后交给MessagingService的实例处理。</p>
<blockquote>
<p>一个StorageService对应一个ServerSocket，每个ServerSocket只有一个SocketThread，SocketThread线程会循环接受客户端连接，TCP连接线程会循环处理消息。<br>如果Socket线程挂了，说明服务端节点挂掉了，那么所有已经连接的客户端也就丢失了连接，TCP连接线程也就不需要处理消息。<br><img src="http://img.blog.csdn.net/20161020101737355" alt="c-socket connection"></p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">    receiveMessages();</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">receiveMessages</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    DataInput in = <span class="keyword">new</span> DataInputStream(socket.getInputStream());</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</span><br><span class="line">        receiveMessage(in, version);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> InetAddress <span class="title">receiveMessage</span><span class="params">(DataInput input, <span class="keyword">int</span> version)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    MessageIn message = MessageIn.read(input, version, id);</span><br><span class="line">    MessagingService.instance().receive(message, id, timestamp, isCrossNodeTimestamp);</span><br><span class="line">    <span class="keyword">return</span> message.from;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>MessageIn根据输入流构造，其中最关键的是verb，用来决定是哪种类型的消息。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MessageIn</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> InetAddress from;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> T payload;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> Map&lt;String, <span class="keyword">byte</span>[]&gt; parameters;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> MessagingService.Verb verb;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> version; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>MessagingService.receive(MessageIn)接收到消息后会创建一个MessageDeliveryTask，每个Task会在不同Stage的ThreadPool中运行</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">receive</span><span class="params">(MessageIn message, <span class="keyword">int</span> id, <span class="keyword">long</span> timestamp, <span class="keyword">boolean</span> isCrossNodeTimestamp)</span> </span>&#123;</span><br><span class="line">    Runnable runnable = <span class="keyword">new</span> MessageDeliveryTask(message, id, timestamp, isCrossNodeTimestamp);</span><br><span class="line">    LocalAwareExecutorService stage = StageManager.getStage(message.getMessageType());</span><br><span class="line">    stage.execute(runnable, ExecutorLocals.create(state));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>MessageDeliveryTask也是一个线程，不过它是被线程池调度的，执行完了就完了，不像IncomingTcpConnection那样永远不会结束。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MessageDeliveryTask</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> MessageIn message;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        MessagingService.Verb verb = message.verb;</span><br><span class="line">        IVerbHandler verbHandler = MessagingService.instance().getVerbHandler(verb);</span><br><span class="line">        verbHandler.doVerb(message, id);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>先来看下线程的调度，是通过LocalAwareExecutorService，类似线程池。注意execute方法并没有真正执行任务，而是把Runnable的任务包装成FutureTask，并等待后续的某个时间才开始调度。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> class AbstractLocalAwareExecutorService implements LocalAwareExecutorService</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Runnable command, ExecutorLocals locals)</span> </span>&#123;</span><br><span class="line">        addTask(newTaskFor(command, <span class="keyword">null</span>, locals));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">protected</span> &lt;T&gt; <span class="function">FutureTask&lt;T&gt; <span class="title">newTaskFor</span><span class="params">(Runnable runnable, T result, ExecutorLocals locals)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (locals != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> LocalSessionFutureTask&lt;T&gt;(runnable, result, locals);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> FutureTask&lt;&gt;(runnable, result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>服务端只有一个ServerSocket，接受每个客户端连接，都会产生一个Socket，并不是说所有客户端只产生一个Socket（JAVA基础）。<br>每个Socket都会产生一个IncommingTcpConnection。Cassandra服务端之间的通信采用MessagingService（客户端和服务端采用Netty），<br>下图中假设其他Cassandra服务端[1]发送了三个请求给另一个Cassandra服务端[2]，所以在服务端2上只会有一个连接，而不是三个连接。<br>（如果是有三个Cassandra服务端[1,2,3]各自发送一个请求给节点4，则服务端4上会有三个连接）。<br>图中左上角是其他节点发送请求，其余的都是在当前节点上执行。<br><img src="http://img.blog.csdn.net/20161020103328627" alt="c-message service"></p>
<blockquote>
<p>前面我们说过协调者会将客户端请求转发到非本地节点，实际上使用的是OutboundTcpConnection，那么对于服务端接收客户端消息，则用的是StorageService的IncomingTcpConnection<br><img src="http://img.blog.csdn.net/20160928233634935" alt="c-msginout"></p>
<p>比较StorageProxy和StorageService的MessageService的一些共同点：</p>
<table>
<thead>
<tr>
<th>服务类</th>
<th>线程池</th>
<th>指令</th>
<th>线程</th>
<th>真正执行方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>StorageProxy</td>
<td>AbstractReadExecutor</td>
<td>ReadCommand</td>
<td>LocalReadRunnable</td>
<td>ReadCommand.getRow</td>
</tr>
<tr>
<td>MessageService</td>
<td>LocalAwareExecutorService</td>
<td>MessageIn</td>
<td>MessageDeliveryTask/ExecutorLocals</td>
<td>IVerbHandler.doVerb(messageIn)</td>
</tr>
</tbody>
</table>
</blockquote>
<p>JMC查看线程相关信息，包括服务端线程（一个SocketThread）、输入(Incoming)、输出(Outgoing)<br><img src="http://img.blog.csdn.net/20161020111001112" alt="c-network thread"> |<br><img src="http://img.blog.csdn.net/20161020111303475" alt="c-socket thread"></p>
<p>消息最终通过VerbHandler被处理，那么我们接着举例读写相关的Handler。</p>
<h3 id="IVerbHandler接口">IVerbHandler接口</h3><p>IVerbHandler和消息类型一样有多种实现类。</p>
<blockquote>
<p>思考下前面使用StorageProxy时，ReadCommand直接执行getRow方法，而用IVerbHandler，则对应使用ReadVerbHandler.doVerb(messageIn)，其中messageIn就是ReadCommand。<br>所以实际上ReadVerbHandler是ReadCommnad的一层封装而已，在ReadVerbHandler.doVerb中最终还是会调用到ReadCommand.getRow方法。<br>那么为什么要有ReadCommand和ReadVerbHandler两种实现呢，实际上ReadCommand仅仅是Read操作的处理方式，而ReadVerbHandler不仅包括要调用ReadCommand，还要负发送请求。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20160928232340339" alt="c-verbhandler"></p>
<h4 id="ReadVerbHandler">ReadVerbHandler</h4><p>Mutation是写操作，Read是读操作，读写操作都会返回响应给客户端。只不过读操作要将读取结果集Row对象封装到MessageOut中。读写的区别是message的payload，读是ReadCommand，而写是Mutation。这里的读是根据主键唯一查询，如果是根据主键进行能范围查询，则对应RangeSliceVerbHandler。</p>
<blockquote>
<p>MessageIn类似于StorageProxy的Message.Request，而MessageOut就等价于Message.Response。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadVerbHandler</span> <span class="keyword">implements</span> <span class="title">IVerbHandler</span>&lt;<span class="title">ReadCommand</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doVerb</span><span class="params">(MessageIn&lt;ReadCommand&gt; message, <span class="keyword">int</span> id)</span> </span>&#123;</span><br><span class="line">        ReadCommand command = message.payload;</span><br><span class="line">        Keyspace keyspace = Keyspace.open(command.ksName);</span><br><span class="line">        Row row = command.getRow(keyspace);</span><br><span class="line"></span><br><span class="line">        MessageOut&lt;ReadResponse&gt; reply = <span class="keyword">new</span> MessageOut&lt;ReadResponse&gt;(MessagingService.Verb.REQUEST_RESPONSE, getResponse(command, row), ReadResponse.serializer);</span><br><span class="line">        MessagingService.instance().sendReply(reply, id, message.from);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ReadResponse <span class="title">getResponse</span><span class="params">(ReadCommand command, Row row)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (command.isDigestQuery()) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> ReadResponse(ColumnFamily.digest(row.cf));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> ReadResponse(row);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RangeSliceVerbHandler</span> <span class="keyword">implements</span> <span class="title">IVerbHandler</span>&lt;<span class="title">AbstractRangeCommand</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doVerb</span><span class="params">(MessageIn&lt;AbstractRangeCommand&gt; message, <span class="keyword">int</span> id)</span> </span>&#123;</span><br><span class="line">        RangeSliceReply reply = <span class="keyword">new</span> RangeSliceReply(message.payload.executeLocally());</span><br><span class="line">        MessagingService.instance().sendReply(reply.createMessage(), id, message.from);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ReadVerbHandler会调用ReadCommand的实际业务处理getRow方法，而且还要将读取结果发送回源节点：MessageIn中不仅带有具体的操作指令，还有这些指令的来源。<br>比如Server1发送了ReadCommand给Server2（表示Server1要读取Server2），那么message中不仅有ReadCommand，还表示ReadCommand是从Server1过来的。<br>所以在Server2节点上，ReadVerbHandler执行完ReadCommand后，要将读取结果返回给Server1。  </p>
<p>ReadCommand有两个实现类：SliceFromReadCommand和SliceByNamesReadCommand，同样读操作会通过Keyspace-&gt;ColumnFamilyStore-&gt;ColumnFamily。  </p>
<blockquote>
<p>在StorageProxy.read中，最终也会到达ReadCommand。那么为什么有两种读取实现呢？其实通过IVerbHandler是以接收消息的形式，一旦节点接收到读命令后，接着读取keyspace。<br>而StorageProxy可以看做是协调节点，如果请求发送的目标endpoints中包含当前本地节点，也需要读取数据，这时不是以接收消息的形式，而是直接RPC的形式。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//SliceFromReadCommand</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Row <span class="title">getRow</span><span class="params">(Keyspace keyspace)</span> </span>&#123;</span><br><span class="line">    CFMetaData cfm = Schema.instance.getCFMetaData(ksName, cfName);</span><br><span class="line">    DecoratedKey dk = StorageService.getPartitioner().decorateKey(key);</span><br><span class="line">    <span class="keyword">return</span> keyspace.getRow(<span class="keyword">new</span> QueryFilter(dk, cfName, filter, timestamp));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Keyspace</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Row <span class="title">getRow</span><span class="params">(QueryFilter filter)</span> </span>&#123;</span><br><span class="line">    ColumnFamilyStore cfStore = getColumnFamilyStore(filter.getColumnFamilyName());</span><br><span class="line">    ColumnFamily columnFamily = cfStore.getColumnFamily(filter);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Row(filter.key, columnFamily);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>DecoratedKey会在《存储引擎》中详细介绍。</p>
</blockquote>
<h5 id="协调节点、MessageService">协调节点、MessageService</h5><p>StorageProxy协调节点如果含有请求的数据，本地线程<code>LocalReadRunnable</code>执行ReadCommand的getRow方法，通过handler.response处理读取结果。  </p>
<blockquote>
<p>注意这里并不是执行将读取结果返回给客户端的逻辑，因为发送响应并不是由StorageProxy完成（涉及到和客户端的交互都是用Netty完成）。  </p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">LocalReadRunnable</span> <span class="keyword">extends</span> <span class="title">DroppableRunnable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ReadCommand command;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ReadCallback&lt;ReadResponse, Row&gt; handler;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">runMayThrow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Keyspace keyspace = Keyspace.open(command.ksName);</span><br><span class="line">        Row r = command.getRow(keyspace);</span><br><span class="line">        ReadResponse result = ReadVerbHandler.getResponse(command, r);</span><br><span class="line">        handler.response(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StorageProxy如果本身没有数据，或者即使有数据，还要将请求发送给其他节点来满足一致性级别。<br>其他Cassandra节点使用7000对应的storage_port通过MessageService处理协调节点发送过来的请求。<br>MessageService接收协调节点转发的请求，执行ReadCommand.getRow和协调节点本地线程类似。  </p>
<p>执行ReadCommand.getRow总是会返回Row读取结果，不管是不是协调节点，都要封装出对应的响应对象ReadResponse。<br>但是后续的处理有点不同，协调节点调用handler.response，而非协调节点则通过MessageService将MessageOut返回给协调节点。  </p>
<blockquote>
<p>协调节点的Handler是ReadCallback，它会启动一个后台的digest比较工作，所以和请求处理关联不是很大，这里暂时省略。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadVerbHandler</span> <span class="keyword">implements</span> <span class="title">IVerbHandler</span>&lt;<span class="title">ReadCommand</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doVerb</span><span class="params">(MessageIn&lt;ReadCommand&gt; message, <span class="keyword">int</span> id)</span> </span>&#123;</span><br><span class="line">        ReadCommand command = message.payload;</span><br><span class="line">        Keyspace keyspace = Keyspace.open(command.ksName);</span><br><span class="line">        Row row = command.getRow(keyspace);</span><br><span class="line">        MessageOut&lt;ReadResponse&gt; reply = <span class="keyword">new</span> MessageOut&lt;ReadResponse&gt;(Verb.REQUEST_RESPONSE, getResponse(command, row), ReadResponse.serializer);</span><br><span class="line">        MessagingService.instance().sendReply(reply, id, message.from);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>比较协调节点和非协调节点的区别是：协调节点会转发客户端的请求给非协调节点，非协调节点在处理请求完成后要返回响应结果给协调节点。<br>两者的共同点是：在处理客户端的请求时，都是使用ReadCommand.getRow获取在自己本地节点的数据。最后协调节点因为要汇聚多个非协调<br>节点的数据，所以有一个Handler做一些回调相关的工作，但这个工作并不是发送响应结果返回给客户端，而是做一些校验、修复等。  </p>
<p><img src="http://img.blog.csdn.net/20161013175252455" alt="c-internode-resp"></p>
<h4 id="MutationVerbHandler">MutationVerbHandler</h4><p>Cassandra的Insert、Update、Delete都属于Mutation，所以MutationVerbHandler处理的是Mutation操作。<br>和读操作不同的是，读取数据时可能只会读取一个节点，其他节点读取的是Digest。而写操作要将写发送到每个副本上去。<br>当然MutationVerbHandler本身不会去实现副本复制。它只负责要么将Mutation存储到本地，要么将Mutation发送出去。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MutationVerbHandler</span> <span class="keyword">implements</span> <span class="title">IVerbHandler</span>&lt;<span class="title">Mutation</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doVerb</span><span class="params">(MessageIn&lt;Mutation&gt; message, <span class="keyword">int</span> id)</span>  <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">byte</span>[] from = message.parameters.get(Mutation.FORWARD_FROM);</span><br><span class="line">        InetAddress replyTo;</span><br><span class="line">        <span class="keyword">if</span> (from == <span class="keyword">null</span>) &#123;</span><br><span class="line">            replyTo = message.from;</span><br><span class="line">            <span class="keyword">byte</span>[] forwardBytes = message.parameters.get(Mutation.FORWARD_TO);</span><br><span class="line">            <span class="keyword">if</span> (forwardBytes != <span class="keyword">null</span>)</span><br><span class="line">                forwardToLocalNodes(message.payload, message.verb, forwardBytes, message.from);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            replyTo = InetAddress.getByAddress(from);</span><br><span class="line">        &#125;</span><br><span class="line">        message.payload.apply(); <span class="comment">//这里是重点</span></span><br><span class="line">        WriteResponse response = <span class="keyword">new</span> WriteResponse();</span><br><span class="line">        MessagingService.instance().sendOneWay(response.createMessage(), id, replyTo);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">forwardToLocalNodes</span><span class="params">(Mutation mutation, MessagingService.Verb verb, <span class="keyword">byte</span>[] forwardBytes, InetAddress from)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> (DataInputStream in = <span class="keyword">new</span> DataInputStream(<span class="keyword">new</span> FastByteArrayInputStream(forwardBytes))) &#123;</span><br><span class="line">            <span class="keyword">int</span> size = in.readInt();</span><br><span class="line">            <span class="comment">// tell the recipients who to send their ack to</span></span><br><span class="line">            MessageOut&lt;Mutation&gt; message = <span class="keyword">new</span> MessageOut&lt;&gt;(verb, mutation, Mutation.serializer).withParameter(Mutation.FORWARD_FROM, from.getAddress());</span><br><span class="line">            <span class="comment">// Send a message to each of the addresses on our Forward List</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">                InetAddress address = CompactEndpointSerializationHelper.deserialize(in);</span><br><span class="line">                <span class="keyword">int</span> id = in.readInt();</span><br><span class="line">                MessagingService.instance().sendOneWay(message, id, address);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Mutation的apply会将Mutation运用到Keyspace-&gt;ColumnFamilyStore，最终我们看到了分布式存储系统中熟悉的Memtable这个对象。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Mutation</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Keyspace ks = Keyspace.open(keyspaceName);</span><br><span class="line">    ks.apply(<span class="keyword">this</span>, ks.getMetadata().durableWrites);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Keyspace</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(Mutation mutation, <span class="keyword">boolean</span> writeCommitLog, <span class="keyword">boolean</span> updateIndexes)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> (OpOrder.Group opGroup = writeOrder.start()) &#123;</span><br><span class="line">        ReplayPosition replayPosition = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">if</span> (writeCommitLog) replayPosition = CommitLog.instance.add(mutation);</span><br><span class="line">        DecoratedKey key = StorageService.getPartitioner().decorateKey(mutation.key());</span><br><span class="line">        <span class="keyword">for</span> (ColumnFamily cf : mutation.getColumnFamilies()) &#123;</span><br><span class="line">            ColumnFamilyStore cfs = columnFamilyStores.get(cf.id());</span><br><span class="line">            SecondaryIndexManager.Updater updater = updateIndexes</span><br><span class="line">                                                  ? cfs.indexManager.updaterFor(key, cf, opGroup)</span><br><span class="line">                                                  : SecondaryIndexManager.nullUpdater;</span><br><span class="line">            cfs.apply(key, cf, updater, opGroup, replayPosition);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//ColumnFamilyStore</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(DecoratedKey key, ColumnFamily cf, SecondaryIndexManager.Updater indexer, OpOrder.Group opGroup, ReplayPosition replayPosition)</span> </span>&#123;</span><br><span class="line">    Memtable mt = data.getMemtableFor(opGroup, replayPosition);</span><br><span class="line">    mt.put(key, cf, indexer, opGroup);</span><br><span class="line">    maybeUpdateRowCache(key);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Response">Response</h4><p>MutationVerbHandler将Mutation运用到本地结束后，要返回结果给客户端。就像MessagingService接收请求后使用IncomingTcpConnection-&gt;MessageDeliveryTask线程操作读，返回结果会使用OutboundTcpConnection线程完成写。  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendOneWay</span><span class="params">(MessageOut message, <span class="keyword">int</span> id, InetAddress to)</span> </span>&#123;</span><br><span class="line">        OutboundTcpConnection connection = getConnection(to, message);</span><br><span class="line">        connection.enqueue(message, id); <span class="comment">//队列</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> OutboundTcpConnection <span class="title">getConnection</span><span class="params">(InetAddress to, MessageOut msg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> getConnectionPool(to).getConnection(msg);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> OutboundTcpConnectionPool <span class="title">getConnectionPool</span><span class="params">(InetAddress to)</span> </span>&#123;</span><br><span class="line">        OutboundTcpConnectionPool cp = connectionManagers.get(to);</span><br><span class="line">        <span class="keyword">if</span> (cp == <span class="keyword">null</span>) &#123;</span><br><span class="line">            cp = <span class="keyword">new</span> OutboundTcpConnectionPool(to);</span><br><span class="line">            OutboundTcpConnectionPool existingPool = connectionManagers.putIfAbsent(to, cp);</span><br><span class="line">            <span class="keyword">if</span> (existingPool != <span class="keyword">null</span>) cp = existingPool;</span><br><span class="line">            <span class="keyword">else</span> cp.start();</span><br><span class="line">        &#125;</span><br><span class="line">        cp.waitForStarted();</span><br><span class="line">        <span class="keyword">return</span> cp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个节点都会创建三个OutboundTcpConnection，启动OutboundTcpConnectionPool时会同时启动三个OutboundTcpConnection</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">OutboundTcpConnectionPool(InetAddress remoteEp) &#123;</span><br><span class="line">    smallMessages = <span class="keyword">new</span> OutboundTcpConnection(<span class="keyword">this</span>);</span><br><span class="line">    largeMessages = <span class="keyword">new</span> OutboundTcpConnection(<span class="keyword">this</span>);</span><br><span class="line">    gossipMessages = <span class="keyword">new</span> OutboundTcpConnection(<span class="keyword">this</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span></span>&#123;</span><br><span class="line">    smallMessages.start();</span><br><span class="line">    largeMessages.start();</span><br><span class="line">    gossipMessages.start();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>nodetool的netstats命令最后三行（Pool Name 连接池名称）对应上面的三种OutboundTcpConnection。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">➜  apache-cassandra-<span class="number">2.2</span><span class="number">.6</span> bin/nodetool -h <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span> netstats</span><br><span class="line">Mode: NORMAL</span><br><span class="line">Not sending any streams.</span><br><span class="line">Read Repair Statistics:</span><br><span class="line">Attempted: <span class="number">376745</span></span><br><span class="line">Mismatch (Blocking): <span class="number">0</span></span><br><span class="line">Mismatch (Background): <span class="number">0</span></span><br><span class="line">Pool Name                    Active   Pending      Completed</span><br><span class="line">Large messages                  n/a         <span class="number">0</span>              <span class="number">9</span></span><br><span class="line">Small messages                  n/a         <span class="number">0</span>        <span class="number">2495610</span></span><br><span class="line">Gossip messages                 n/a         <span class="number">0</span>        <span class="number">2390273</span></span><br></pre></td></tr></table></figure>
<p>OutboundTcpConnection的enqueue会将消息入队列，后台线程会从队列中取出消息执行write方法，将消息发送出去</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">enqueue</span><span class="params">(MessageOut&lt;?&gt; message, <span class="keyword">int</span> id)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (backlog.size() &gt; <span class="number">1024</span>)</span><br><span class="line">        expireMessages();</span><br><span class="line">    backlog.put(<span class="keyword">new</span> QueuedMessage(message, id));</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">final</span> List&lt;QueuedMessage&gt; drainedMessages = <span class="keyword">new</span> ArrayList&lt;&gt;(drainedMessageSize);</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        cs.coalesce(backlog, drainedMessages, drainedMessageSize);</span><br><span class="line">        <span class="keyword">for</span> (QueuedMessage qm : drainedMessages) &#123;</span><br><span class="line">            writeConnected(qm, count == <span class="number">1</span> &amp;&amp; backlog.isEmpty());</span><br><span class="line">        &#125;</span><br><span class="line">        drainedMessages.clear();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">writeConnected</span><span class="params">(QueuedMessage qm, <span class="keyword">boolean</span> flush)</span> </span>&#123;</span><br><span class="line">    writeInternal(qm.message, qm.id, timestampMillis);</span><br><span class="line">    completed++;</span><br><span class="line">    <span class="keyword">if</span> (flush) out.flush();</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">writeInternal</span><span class="params">(MessageOut message, <span class="keyword">int</span> id, <span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    out.writeInt(MessagingService.PROTOCOL_MAGIC);</span><br><span class="line">    out.writeInt(id);</span><br><span class="line">    out.writeInt((<span class="keyword">int</span>) timestamp);</span><br><span class="line">    message.serialize(out, targetVersion);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>OutboundTcpConnection线程实际上和StorageService用的都是Storage端口（7000），这也说明了：<br>涉及到节点之间数据发送，使用的是storage_port，比如协调节点转发客户端请求给其他节点，<br>或者其他节点在处理完协调节点转发的客户端请求后，要将响应结果先返回给协调节点。  </p>
<p><img src="http://img.blog.csdn.net/20160928232412715" alt="c-storageport"></p>
<hr>
<p>下面是读操作引起Java堆内存溢出的堆栈信息，有可能是读操作将数据不断放入内存，导致内存不足引起内存溢出。<br>最终调用的是OnDiskAtom的deserializeFromSSTable，即读取SSTable时反序列化的数据会写到内存中。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">ERROR <span class="number">19</span>:<span class="number">20</span>:<span class="number">50</span>,<span class="number">316</span> Exception <span class="keyword">in</span> thread Thread[ReadStage:<span class="number">63</span>,<span class="number">5</span>,main]</span><br><span class="line"></span><br><span class="line">java<span class="class">.lang</span><span class="class">.OutOfMemoryError</span>: Java heap space</span><br><span class="line">    at java<span class="class">.nio</span><span class="class">.ByteBuffer</span><span class="class">.wrap</span>(ByteBuffer<span class="class">.java</span>:<span class="number">350</span>)</span><br><span class="line">    at java<span class="class">.nio</span><span class="class">.ByteBuffer</span><span class="class">.wrap</span>(ByteBuffer<span class="class">.java</span>:<span class="number">373</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.io</span><span class="class">.util</span><span class="class">.RandomAccessReader</span><span class="class">.readBytes</span>(RandomAccessReader<span class="class">.java</span>:<span class="number">391</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.utils</span><span class="class">.ByteBufferUtil</span><span class="class">.read</span>(ByteBufferUtil<span class="class">.java</span>:<span class="number">392</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.utils</span><span class="class">.ByteBufferUtil</span><span class="class">.readWithShortLength</span>(ByteBufferUtil<span class="class">.java</span>:<span class="number">371</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.OnDiskAtom</span><span class="variable">$Serializer</span>.<span class="function"><span class="title">deserializeFromSSTable</span><span class="params">(OnDiskAtom.java:<span class="number">84</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.OnDiskAtom</span><span class="variable">$Serializer</span>.<span class="function"><span class="title">deserializeFromSSTable</span><span class="params">(OnDiskAtom.java:<span class="number">73</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.columniterator</span><span class="class">.IndexedSliceReader</span><span class="variable">$IndexedBlockFetcher</span>.<span class="function"><span class="title">getNextBlock</span><span class="params">(IndexedSliceReader.java:<span class="number">370</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.columniterator</span><span class="class">.IndexedSliceReader</span><span class="variable">$IndexedBlockFetcher</span>.<span class="function"><span class="title">fetchMoreData</span><span class="params">(IndexedSliceReader.java:<span class="number">325</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.columniterator</span><span class="class">.IndexedSliceReader</span><span class="class">.computeNext</span>(IndexedSliceReader<span class="class">.java</span>:<span class="number">151</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.columniterator</span><span class="class">.IndexedSliceReader</span><span class="class">.computeNext</span>(IndexedSliceReader<span class="class">.java</span>:<span class="number">48</span>)</span><br><span class="line">    at com<span class="class">.google</span><span class="class">.common</span><span class="class">.collect</span><span class="class">.AbstractIterator</span><span class="class">.tryToComputeNext</span>(AbstractIterator<span class="class">.java</span>:<span class="number">143</span>)</span><br><span class="line">    at com<span class="class">.google</span><span class="class">.common</span><span class="class">.collect</span><span class="class">.AbstractIterator</span><span class="class">.hasNext</span>(AbstractIterator<span class="class">.java</span>:<span class="number">138</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.columniterator</span><span class="class">.SSTableSliceIterator</span><span class="class">.hasNext</span>(SSTableSliceIterator<span class="class">.java</span>:<span class="number">90</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.filter</span><span class="class">.QueryFilter</span>$<span class="number">2</span>.<span class="function"><span class="title">getNext</span><span class="params">(QueryFilter.java:<span class="number">171</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.filter</span><span class="class">.QueryFilter</span>$<span class="number">2</span>.<span class="function"><span class="title">hasNext</span><span class="params">(QueryFilter.java:<span class="number">154</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.utils</span><span class="class">.MergeIterator</span><span class="variable">$Candidate</span>.<span class="function"><span class="title">advance</span><span class="params">(MergeIterator.java:<span class="number">143</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.utils</span><span class="class">.MergeIterator</span><span class="variable">$ManyToOne</span>.<span class="function"><span class="title">advance</span><span class="params">(MergeIterator.java:<span class="number">122</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.utils</span><span class="class">.MergeIterator</span><span class="variable">$ManyToOne</span>.<span class="function"><span class="title">computeNext</span><span class="params">(MergeIterator.java:<span class="number">96</span>)</span></span></span><br><span class="line">    at com<span class="class">.google</span><span class="class">.common</span><span class="class">.collect</span><span class="class">.AbstractIterator</span><span class="class">.tryToComputeNext</span>(AbstractIterator<span class="class">.java</span>:<span class="number">143</span>)</span><br><span class="line">    at com<span class="class">.google</span><span class="class">.common</span><span class="class">.collect</span><span class="class">.AbstractIterator</span><span class="class">.hasNext</span>(AbstractIterator<span class="class">.java</span>:<span class="number">138</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.filter</span><span class="class">.SliceQueryFilter</span><span class="class">.collectReducedColumns</span>(SliceQueryFilter<span class="class">.java</span>:<span class="number">157</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.filter</span><span class="class">.QueryFilter</span><span class="class">.collateColumns</span>(QueryFilter<span class="class">.java</span>:<span class="number">136</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.filter</span><span class="class">.QueryFilter</span><span class="class">.collateOnDiskAtom</span>(QueryFilter<span class="class">.java</span>:<span class="number">84</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.CollationController</span><span class="class">.collectAllData</span>(CollationController<span class="class">.java</span>:<span class="number">293</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.CollationController</span><span class="class">.getTopLevelColumns</span>(CollationController<span class="class">.java</span>:<span class="number">65</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.ColumnFamilyStore</span><span class="class">.getTopLevelColumns</span>(ColumnFamilyStore<span class="class">.java</span>:<span class="number">1357</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.ColumnFamilyStore</span><span class="class">.getColumnFamily</span>(ColumnFamilyStore<span class="class">.java</span>:<span class="number">1214</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.ColumnFamilyStore</span><span class="class">.getColumnFamily</span>(ColumnFamilyStore<span class="class">.java</span>:<span class="number">1126</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.Table</span><span class="class">.getRow</span>(Table<span class="class">.java</span>:<span class="number">347</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.SliceFromReadCommand</span><span class="class">.getRow</span>(SliceFromReadCommand<span class="class">.java</span>:<span class="number">70</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.service</span><span class="class">.StorageProxy</span><span class="variable">$LocalReadRunnable</span>.<span class="function"><span class="title">runMayThrow</span><span class="params">(StorageProxy.java:<span class="number">1052</span>)</span></span></span><br></pre></td></tr></table></figure>
<h2 id="Tracing_on">Tracing on</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Message.<span class="function">Response <span class="title">execute</span><span class="params">(QueryState state)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        UUID tracingId = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">if</span> (isTracingRequested()) &#123;</span><br><span class="line">            tracingId = UUIDGen.getTimeUUID();</span><br><span class="line">            state.prepareTracingSession(tracingId);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (state.traceNextQuery()) &#123;</span><br><span class="line">            state.createTracingSession();</span><br><span class="line">            ImmutableMap.Builder&lt;String, String&gt; builder = ImmutableMap.builder();</span><br><span class="line">            builder.put(<span class="string">"query"</span>, query);</span><br><span class="line">            Tracing.instance.begin(<span class="string">"Execute CQL3 query"</span>, state.getClientAddress(), builder.build());</span><br><span class="line">        &#125;</span><br><span class="line">        Message.Response response = ClientState.getCQLQueryHandler().process(query, state, options, getCustomPayload());</span><br><span class="line">        <span class="keyword">return</span> response;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        Tracing.instance.stopSession(); <span class="comment">//logger.trace("request complete");</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Gossip">Gossip</h2><p><a href="http://blog.csdn.net/FireCoder/article/details/5707539" target="_blank" rel="external">http://blog.csdn.net/FireCoder/article/details/5707539</a><br><a href="http://blog.csdn.net/zhangzhaokun/article/details/5859760" target="_blank" rel="external">http://blog.csdn.net/zhangzhaokun/article/details/5859760</a><br><a href="http://wiki.apache.org/cassandra/ArchitectureGossip" target="_blank" rel="external">http://wiki.apache.org/cassandra/ArchitectureGossip</a><br><a href="http://thelastpickle.com/blog/2011/12/15/Anatomy-of-a-Cassandra-Partition.html" target="_blank" rel="external">http://thelastpickle.com/blog/2011/12/15/Anatomy-of-a-Cassandra-Partition.html</a>  </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Cassandra-2.2 源码分析：Netty客户端/服务端、请求处理、消息服务&lt;br&gt;
    
    </summary>
    
      <category term="cassandra" scheme="http://github.com/zqhxuyuan/categories/cassandra/"/>
    
    
      <category term="cassandra" scheme="http://github.com/zqhxuyuan/tags/cassandra/"/>
    
  </entry>
  
  <entry>
    <title>图解Cassandra</title>
    <link href="http://github.com/zqhxuyuan/2016/08/20/Cassandra-Intro/"/>
    <id>http://github.com/zqhxuyuan/2016/08/20/Cassandra-Intro/</id>
    <published>2016-08-19T16:00:00.000Z</published>
    <updated>2016-08-22T01:32:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Cassandra In Pictures</p>
<a id="more"></a>
<h2 id="基础知识">基础知识</h2><p>## </p>
<p>Dynamic snitching is something we’ve done in Cassandra as far back as 0.6.5, but can still be a source of confusion for many. In this post, I’m going to do my best to demystify everything and anything you could want to know about it, as well as preview some of the improvements that have been made for our upcoming 1.2 release later this year.</p>
<p>To begin, let’s first answer the most obvious question: what is dynamic snitching? To understand this, we’ll first recall what a snitch does. A snitch’s function is to determine which datacenters and racks are both written to and read from. So, why would that be ‘dynamic?’ This comes into play on the read side only (there’s nothing to be done for writes since we send them all and then block to until the consistency level is achieved.) When doing reads however, Cassandra only asks one node for the actual data, and, depending on consistency level and read repair chance, it asks the remaining replicas for checksums only. This means that it has a choice of however many replicas exist to ask for the actual data, and this is where the dynamic snitch goes to work.</p>
<p>Since only one replica is sending the full data we need, we need to chose the best possible replica to ask, since if all we get back is checksums we have nothing useful to return to the user. The dynamic snitch handles this task by monitoring the performance of reads from the various replicas and choosing the best one based on this history.</p>
<p>In a modern Cassandra world however, read repair chance is lowered, since hints were made reliable. So, given this, can’t we maximize our cache capacity a bit when our consistency level is ONE? Yes! This is exactly what the badness threshold parameter to the dynamic snitch is designed to do. What this parameter defines is, as a percentage, how much worse the natural first replica must perform, in order to switch to a different one. Thus given replicas X, Y, and Z, the X replica will be preferred until it performs badness_threshold worse than Y or Z. This means that when everything is healthy, the cache capacity is maximized across the nodes, but if things gets worse, specifically badness_threshold worse, then Cassandra will continue to provide availability by using the other replicas at its disposal. The dynamic snitch doesn’t determine replica placement itself, though, that is what your chosen snitch does. The dynamic snitch simply wraps your snitch, and provides this adaptive behavior on reads. How is this accomplished? Originally, the dynamic snitch was modeled in a similar fashion to the failure detector, since the failure detector was also adaptive. It is fed latency information from reads from the nodes, and chooses which node is performing the fastest based on that information. Concessions had to be made though; if it spent too much CPU time calculating which host was best, that would become counterproductive, since it would be sacrificing overall read throughput, which is often CPU-bound. So, it adopted a two-pronged approach, and had two separate operations. One was cheap (receiving the updates) and one was more expensive (calculating the scores for each host.) Both were mitigated; the cheaper update phase had a maximum cap of updates that it would accept, and the calculation operation only ran every so often.</p>
<p>By default, the score calculation is set to a fairly reasonable 100 milliseconds. The updates are capped at a maximum of 10,000 per scoring interval, but this introduces some new problems and questions. First off, if we don’t read from a host because we determine it’s no longer performing sufficiently, how can we know that it has recovered? Without incoming information to influence this, we have to add a new rule, which (again by default) is to reset all the scores every 10 minutes. This way, we’ll have to sample a few reads after every reset interval, but it gives each replica a fair shot once again. Secondly, is 10,000 a good number? We can’t really know, since this could very well be dependent on the power of the machine Cassandra is running on. And finally, is latency the only and best source for deciding which machine to read data from?</p>
<p>In the next release of Cassandra, these latter two problems have been addressed. Instead of sampling a fixed amount of updates, we now use a statistically significant random sample, and weight more recent information heavier than past information. But wait, there’s more! Instead of relying purely on latency information from reads, we now also consider other factors, like whether or not a node is currently doing a compaction, since that can often penalize reads.</p>
<p>There is one final wrinkle left, however. In the absence of information, the dynamic snitch can’t react. The implication of this is that it needs to hit the rpc_timeout in order to get the ‘this host is bad now’ message, but we don’t want to have to wait that long to choose a new replica. How can we respond without anything to respond to? To solve this, we have to examine the broader scope of the situation. In actuality, we do have a signal we can respond to, and that is time itself. Thus, if a node suddenly becomes a black hole, we’ll only throw reads at it for one scoring interval, and when the next score is calculated we’ll consider latency, the node’s state (called a severity factor) and how long it has been since the node last replied, penalizing it so that we stop trying to read from it (badness_threshold permitting.)</p>
<p>Hopefully this post has explained more thoroughly the available settings for the dynamic snitch, why they exist, as well as giving you a better understanding of how it works in general. Dynamic snitching has long been a technique Cassandra has used to improve read performance, but has never been explained in great detail until now.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Cassandra In Pictures&lt;/p&gt;
    
    </summary>
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/categories/bigdata/"/>
    
    
      <category term="cassandra" scheme="http://github.com/zqhxuyuan/tags/cassandra/"/>
    
  </entry>
  
  <entry>
    <title>Azkaban动态修改作业属性</title>
    <link href="http://github.com/zqhxuyuan/2016/08/16/Azkaban/"/>
    <id>http://github.com/zqhxuyuan/2016/08/16/Azkaban/</id>
    <published>2016-08-15T16:00:00.000Z</published>
    <updated>2016-10-26T13:54:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>Aakaban动态修改作业属性</p>
<a id="more"></a>
<p>线上有个任务跑挂了</p>
<p><img src="http://img.blog.csdn.net/20161026214345680" alt="az0"></p>
<p>Project有一个可以跑历史任务，但是默认的运行时间是： </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">beg=<span class="number">2016</span>-<span class="number">6</span>-<span class="number">1</span></span><br><span class="line">end=<span class="number">2016</span>-<span class="number">6</span>-<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20161026214206566" alt="az1"></p>
<blockquote>
<p>一个Project中可以创建多个.job文件，比如上面的Project创建了4个.job文件。</p>
</blockquote>
<p>如果要跑其他天的任务，可以在Job属性中修改，不过前提是这个任务必须跑过一次后才可以修改，由于之前都没跑过，所以手动执行一次任务。</p>
<p><img src="http://img.blog.csdn.net/20161026214219051" alt="az2"></p>
<p>修改Job的参数的路径：点击上面的ExecutorID》进入到任务的详细页面，再点击Detail</p>
<p><img src="http://img.blog.csdn.net/20161026214247788" alt="az3"></p>
<p>从日志中可以看到参数是<strong>2016-6-1和2016-6-2</strong>，点击右上角的Job Properties</p>
<p><img src="http://img.blog.csdn.net/20161026214259801" alt="az5"></p>
<p>可以修改beg和end参数，然后保存。</p>
<p><img src="http://img.blog.csdn.net/20161026214311660" alt="az6"></p>
<p>最后回到任务页面，手动执行（前一个任务必须执行完，或者Kill掉，不允许暂停）。</p>
<p><img src="http://img.blog.csdn.net/20161026214321945" alt="az7"></p>
<p>再看任务的日志，可以看到参数变成了<strong>2016-10-8和2016-10-8</strong>，现在就可以重跑线上挂掉的那天的任务了。</p>
<p><img src="http://img.blog.csdn.net/20161026214333117" alt="az9"></p>
<p>附上azkaban的.job文件：  </p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span>=command</span><br><span class="line">beg=2016-6-1</span><br><span class="line">end=2016-6-2</span><br><span class="line">command=<span class="keyword">sh</span> <span class="keyword">cross</span>-partner-<span class="keyword">range</span>.<span class="keyword">sh</span> <span class="label">$&#123;beg&#125;</span> <span class="label">$&#123;end&#125;</span></span><br></pre></td></tr></table></figure>
<p>这里只有定义了beg和end参数，才可以在Job Properties中修改，如果说脚本是写死的比如：  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">type=command</span><br><span class="line">command=sh cross-partner-range.sh <span class="number">2016</span>-<span class="number">6</span>-<span class="number">1</span> <span class="number">2016</span>-<span class="number">6</span>-<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>那么就没办法做到动态修改作业的属性了，这样就只能重新创建一个.job文件，然后上传，执行。</p>
<p>再附上生成azkaban的zip包的方式：  </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/IdeaProjects/tongdun/vulcan/cross-partner</span><br><span class="line">mvn clean package</span><br><span class="line"></span><br><span class="line">azkabanProject=cross-partner</span><br><span class="line">rm -rf ~/Downloads/<span class="variable">$azkabanProject</span></span><br><span class="line">rm ~/Downloads/<span class="variable">$azkabanProject</span>.zip</span><br><span class="line">mkdir ~/Downloads/<span class="variable">$azkabanProject</span></span><br><span class="line"></span><br><span class="line">cp target/cross-partner-<span class="number">1.0</span>.<span class="number">0</span>-SNAPSHOT-jar-with-dependencies.jar ~/Downloads/<span class="variable">$azkabanProject</span>/</span><br><span class="line">cp azkaban/* ~/Downloads/<span class="variable">$azkabanProject</span>/</span><br><span class="line">zip -r ~/Downloads/<span class="variable">$azkabanProject</span>.zip ~/Downloads/<span class="variable">$azkabanProject</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Aakaban动态修改作业属性&lt;/p&gt;
    
    </summary>
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/categories/bigdata/"/>
    
    
      <category term="work" scheme="http://github.com/zqhxuyuan/tags/work/"/>
    
  </entry>
  
  <entry>
    <title>JVM GC</title>
    <link href="http://github.com/zqhxuyuan/2016/07/26/JVM/"/>
    <id>http://github.com/zqhxuyuan/2016/07/26/JVM/</id>
    <published>2016-07-25T16:00:00.000Z</published>
    <updated>2016-09-08T01:49:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>JVM GC<br><a id="more"></a></p>
<h2 id="JVM架构">JVM架构</h2><p>当一个程序启动之前，它的<code>class</code>会被<strong>类装载器</strong>装入<strong>方法区</strong>(Permanent区)，<strong>执行引擎</strong>读取方法区的<code>字节码</code>自适应解析，边解析边运行，然后<strong>pc寄存器</strong>指向了main函数所在位置，虚拟机开始为main函数在<strong>Java栈</strong>中预留一个栈帧（每个方法都对应一个栈帧），然后开始跑main函数，main函数里的代码被执行引擎映射成本地操作系统里相应的实现，然后调用<strong>本地方法接口</strong>，本地方法运行的时候，操纵系统会为本地方法分配本地方法栈，用来储存一些临时变量，然后运行本地方法，调用操作系统API。</p>
<p><img src="http://img.blog.csdn.net/20160727151628493" alt="jvm1"></p>
<p>执行引擎中的GC（垃圾收集器）主要作用域运行时数据区的方法区和堆。</p>
<h2 id="一些概念">一些概念</h2><h3 id="通用GC概念">通用GC概念</h3><p><strong><code>垃圾</code></strong>：Garbage（名词），在系统运行过程当中所产生的一些无用的对象，这些对象占据着一定的内存空间，如果长期不被释放，可能导致OOM。</p>
<p><strong><code>垃圾收集器</code></strong>：Garbage Collector（名词），负责回收垃圾对象的垃圾收集器</p>
<p><strong><code>垃圾回收</code></strong>：Garbage Collect（动词），垃圾收集器工作时，对垃圾进行回收</p>
<p><img src="http://img.blog.csdn.net/20160728102025667" alt="jvm9"></p>
<p><strong><code>垃圾回收算法/GC算法</code></strong>：不同的GC算法，它们的垃圾回收工作模式不同（比如串行、并行等）</p>
<ol>
<li>引用计数算法（Reference Counting）</li>
<li>标记-清除算法（Mark-Sweep）</li>
<li>复制算法（Copy）</li>
<li>标记-整理算法（Mark-Compact）</li>
<li>标记-清除-整理算法（Mark-Sweep-Compact）</li>
</ol>
<table>
<thead>
<tr>
<th>GC算法</th>
<th>优点</th>
<th>缺点</th>
<th>存活对象移动</th>
<th>内存碎片</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>引用计数</td>
<td>实现简单</td>
<td>不能处理循环引用</td>
</tr>
<tr>
<td>标记清除</td>
<td>不需要额外空间</td>
<td>两次扫描，耗时严重</td>
<td><code>N</code></td>
<td>Y</td>
<td>旧生代</td>
</tr>
<tr>
<td>复制</td>
<td>没有标记和清除</td>
<td>需要额外空间</td>
<td>Y</td>
<td>N</td>
<td>新生代</td>
</tr>
<tr>
<td>标记整理</td>
<td>没有内存碎片</td>
<td>需要移动对象的成本</td>
<td>Y</td>
<td>N</td>
<td>旧生代</td>
</tr>
</tbody>
</table>
<blockquote>
<p>这几种算法中存活对象没有移动的算法只有：标记-清除算法。复制算法会将存活对象移动到另一块内存区，标记整理算法会将存活对象移动到边界位置</p>
</blockquote>
<p><strong><code>垃圾回收线程/GC线程</code></strong>：垃圾收集器工作时的线程。</p>
<p>应用程序和GC都是一种线程，以Java的main方法为例：应用程序的线程指的是main方法的主线程，GC线程是JVM的内部线程。</p>
<p>在GC过程中，如果GC线程必须暂停应用程序线程（用户线程），则发生Stop the World。当然也可以允许GC线程和应用程序线程一起运行，即GC并不会暂停应用程序的线程。</p>
<p><strong><code>串行、并行、并发</code></strong>：串行和并行指的是垃圾收集器工作时暂停应用程序（发生Stop the World），使用单核CPU（串行）还是多核CPU（并行）。</p>
<ol>
<li>串行（Serial）：使用单核CPU串行地进行垃圾收集</li>
<li>并行（Parallel）：使用多CPU并行地进行垃圾收集，并行是GC线程有多个，但在运行GC线程时，用户线程是阻塞的</li>
<li>并发（Concurrent）：垃圾收集时不会暂停应用程序线程，大部分阶段用户线程和GC线程都在运行，我们称垃圾收集器和应用程序是并发运行的。</li>
</ol>
<table>
<thead>
<tr>
<th>概念</th>
<th>Stop the World</th>
<th>单线程/多线程</th>
</tr>
</thead>
<tbody>
<tr>
<td>串行</td>
<td>Y</td>
<td>GC线程是单线程</td>
</tr>
<tr>
<td>并行</td>
<td>Y</td>
<td>GC线程是多线程</td>
</tr>
<tr>
<td>并发</td>
<td>N</td>
<td>GC线程和应用程序线程是多线程</td>
</tr>
</tbody>
</table>
<p><img src="http://img.blog.csdn.net/20160728090327307" alt="jvm2"></p>
<blockquote>
<p>在Java中有并发编程的概念，并发编程中有多线程的概念。通常并发指的是不同类型的线程可以同时运行（比如GC线程和用户线程并发地运行），而并行指的是相同类型的线程采用多线程模式运行（比如GC线程使用多个CPU并行地运行）。</p>
</blockquote>
<p><strong><code>GC暂停/Stop The World/STW</code></strong>：不管选择哪种GC算法，Stop-the-world都是不可避免的。Stop-the-world意味着从应用中停下来并进入到GC执行过程中去。<strong>一旦Stop-the-world发生，除了GC所需的线程外，其他线程都将停止工作，中断了的线程直到GC任务结束才继续它们的任务。</strong>GC调优通常就是为了改善stop-the-world的时间（尽量减少STW对应用程序造成的暂停时间）。</p>
<p><img src="http://img.blog.csdn.net/20160728091903705" alt="jvm3"></p>
<p><strong><code>垃圾对象</code></strong>：对象如果没有在使用，认为是垃圾对象，那么怎么判定有没有在使用？</p>
<ol>
<li>一个在使用的对象（被引用的对象）：程序的某个部分依然维系者一个指向该对象的指针</li>
<li>一个没有使用的对象（未被引用的对象）：该对象不再被你程序的任何部分引用，所以被这些不再使用的对象占用的内存可以（被垃圾收集器）得到回收</li>
</ol>
<p><img src="http://img.blog.csdn.net/20160728093154913" alt="jvm4"></p>
<p>具体的实现方式：</p>
<ol>
<li>引用计数算法：对象的引用计数=0，表示没有对象引用它，可以作为垃圾对象（该方法无法处理循环引用）</li>
<li>根搜索算法：当前对象到根对象没有一条可达的路径，可以作为垃圾对象（JVM采用此方法）</li>
</ol>
<h3 id="引用计数算法和根搜索算法">引用计数算法和根搜索算法</h3><p><img src="http://img.blog.csdn.net/20160728112312948" alt="jvm19"><br><img src="http://img.blog.csdn.net/20160728112324058" alt="jvm20"><br><img src="http://img.blog.csdn.net/20160728112335787" alt="jvm21"><br><img src="http://img.blog.csdn.net/20160728112347974" alt="jvm22">  </p>
<h4 id="引用计数">引用计数</h4><p><strong><code>引用计数算法</code></strong>：每个对象都有一个引用计数器，当有对象引用它时，计数器+1；当引用失效时，计数器-1；任何时刻计数器为0时就是不可能再被使用的。</p>
<p>下图中左图是对象的引用关系，中图有一个引用失效，右图是清理引用计数器=0的对象后。</p>
<p><img src="http://img.blog.csdn.net/20160728095730587" alt="jvm5"></p>
<p>但是这种方式的缺点是：</p>
<ol>
<li>引用和去引用伴随加法和减法，影响性能</li>
<li>对于循环引用的对象无法进行回收</li>
</ol>
<p>下面的3个图中，最右图三个对象的循环引用的计数器都不为0，但是他们对于根对象都已经不可达了，但是无法释放。</p>
<p><img src="http://img.blog.csdn.net/20160728095740463" alt="jvm6"></p>
<h4 id="根搜索">根搜索</h4><p>解决循环引用的办法是：使用<strong><code>根搜索算法</code></strong>来判定对象是否需要被回收，只要对象没有一条到根对象的可达路径，就可以被回收。  </p>
<p><img src="http://img.blog.csdn.net/20160728120344793" alt="jvm33"></p>
<p>所以问题转换为：怎么定义根对象？在Java中可以作为GC Roots的对象：</p>
<ol>
<li>虚拟机栈（左上）的栈帧的局部变量表所引用的对象</li>
<li>本地方法栈（右中）的JNI所引用的对象</li>
<li>方法区（右下）的静态变量和常量所引用的对象</li>
</ol>
<h4 id="示例">示例</h4><p>循环引用的程序实例，如果采用引用计数，无法被垃圾收集器回收</p>
<p><img src="http://img.blog.csdn.net/20160728100151788" alt="jvm7"></p>
<p>使用根搜索算法，则可以正常回收</p>
<p><img src="http://img.blog.csdn.net/20160728100203725" alt="jvm8"></p>
<h3 id="Tracing_GC算法">Tracing GC算法</h3><p>Tracing GC算法主要包括：  </p>
<ol>
<li>标记清理/标记清除：标记垃圾对象，然后清理垃圾对象</li>
<li>复制算法：标记垃圾对象和非垃圾对象，将非垃圾对象移动到某个空闲的内存块</li>
<li>标记压缩/标记整理：标记垃圾对象和非垃圾对象，将非垃圾对象移动在一起</li>
</ol>
<blockquote>
<p>通常还会存在标记清理和标记压缩结合起来的：标记-清理-压缩算法</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20160728103618754" alt="jvm13"></p>
<p>结合根搜索算法定义的对象可达性，对应的垃圾收集算法如下（左图是垃圾收集前，右图是垃圾收集后）：</p>
<p><img src="http://img.blog.csdn.net/20160728103631861" alt="jvm14"></p>
<h4 id="A-标记-清理算法（Mark-Sweep_Collector）">A.标记-清理算法（Mark-Sweep Collector）</h4><p><img src="http://img.blog.csdn.net/20160728102912080" alt="jvm10"></p>
<p><code>步骤</code>：</p>
<ol>
<li>标记阶段：根据可达性分析对不可达对象进行标记，即标记出所有需要被回收的对象</li>
<li>清理阶段：标记完成后统一清理这些对象，即回收被标记的对象所占用的空间</li>
</ol>
<p><code>缺点</code>：</p>
<ol>
<li>标记和清理的效率都不算高（因为垃圾对象比较少，大部分对象都不是垃圾）</li>
<li>会产生大量的内存碎片，碎片太多可能会导致后续过程中需要为大对象分配空间时无法找到足够的空间而提前触发新的一次垃圾收集动作</li>
</ol>
<p><code>适用场景</code>：基于Mark-Sweep的GC多用于老年代。</p>
<h4 id="B-复制算法（Copy_Collector）">B.复制算法（Copy Collector）</h4><p><img src="http://img.blog.csdn.net/20160728102922133" alt="jvm11"></p>
<p><code>适用场景</code>：新生代GC</p>
<h4 id="C-标记-压缩算法（Mark-Compact_Collector）">C.标记-压缩算法（Mark-Compact Collector）</h4><p><img src="http://img.blog.csdn.net/20160728102931705" alt="jvm12"></p>
<p><code>步骤</code>：在完成标记之后，它不是直接清理可回收对象，而是将存活对象都向一端移动，然后清理掉端边界以外的内存</p>
<ol>
<li>在标记好待回收对象后，将存活的对象移至一端</li>
<li>然后对剩余的部分进行回收</li>
</ol>
<p><code>优点</code>：</p>
<ol>
<li>可以解决内存碎片的问题</li>
</ol>
<p><code>适用场景</code>：基于Mark-Compact的GC多用于老年代</p>
<h4 id="D-标记-清理-压缩算法（Mark-Sweep-Compact_Collector）">D.标记-清理-压缩算法（Mark-Sweep-Compact Collector）</h4><ol>
<li>结合使用标记清理算法（Mark-Sweep）和标记压缩算法（Mark-Compact）</li>
<li>并不是每次标记清理都会执行压缩，而是多次执行GC后，才会执行一次Compact</li>
</ol>
<p>优点：</p>
<ol>
<li>相对于标记清理和标记压缩算法，可以减少移动对象的成本（并不是说不会移动对象，只要有压缩就一定会移动对象，只不过压缩不是很频繁）</li>
</ol>
<h2 id="JVM_GC">JVM GC</h2><p>前面我们分析了垃圾收集器的几种算法，在Java中，因为对象创建在堆中，垃圾收集时，垃圾收集器就应该扫描堆中的对象，执行垃圾收集工作。</p>
<h3 id="基于分代理论的垃圾回收">基于分代理论的垃圾回收</h3><p>JVM的垃圾回收器基于以下两个假设：</p>
<ul>
<li><strong>大多数对象很快就会变得不可达</strong>，即很多对象的生存时间都很短</li>
<li>只有极少数情况会出现旧对象(老年代对象)持有新对象(新生代)的引用，即新生对象很少引用生存时间长的对象</li>
</ul>
<blockquote>
<p>问题1：到底是老年代对象引用新生代对象，还是新生代对象引用老年代对象？<br>问题2：引用和持有引用有什么关系，比如A引用了B，和A持有B的引用。  </p>
</blockquote>
<p>这两条假设被称为”弱分代假设”。为了证明此假设，在HotSpot VM中物理内存空间被划分为两部分：新生代(Young generation)和老年代(Old generation)。</p>
<p><strong>新生代</strong>：大部分新创建的对象分配在新生代。因为大部分对象很快就会变得不可达，所以它们被分配在新生代，然后消失不再。当对象从新生代移除时，我们称之为”Minor GC”。</p>
<p><strong>老年代</strong>：在新生代中存活的对象达到一定年龄阈值时会被复制到老年代。一般来说老年代的内存空间比新生代大，所以在老年代GC发生的频率较新生代低一些。当对象从老年代被移除时，我们称之为”Major GC”(或者Full GC)。</p>
<h3 id="概念">概念</h3><p><strong><code>分代</code></strong>：将JVM的堆内存<code>分</code>成多个<code>代</code>（generation）。</p>
<p><img src="http://img.blog.csdn.net/20160728104537718" alt="jvm15"></p>
<p><strong><code>新生代/年轻代</code></strong>：Java对象存活周期短命的对象放在新生代</p>
<ol>
<li>由Eden、两块相同大小的Survivor区构成，to总为空</li>
<li>一般在Eden分配对象，优化：<code>T</code>hread <code>L</code>ocal <code>A</code>llocation <code>B</code>uffer</li>
<li>保存80%-90%生命周期较短的对象，GC频率高，采用效率较高的复制算法</li>
</ol>
<p><img src="http://img.blog.csdn.net/20160728104549915" alt="jvm16"></p>
<p><strong><code>旧生代/老年代/年老代</code></strong>：Java对象存活周期长命的对象放在老年代</p>
<ol>
<li>存放新生代中经历多次GC仍然存活的对象</li>
<li>新建的对象也有可能直接在旧生代分配，取决于具体GC的实现</li>
<li>GC频率相对降低，标记(mark)、清理(sweep)、压缩(compaction)算法的各种结合和优化</li>
</ol>
<p><img src="http://img.blog.csdn.net/20160728104559493" alt="jvm17"></p>
<p><strong><code>Minor GC/Majar GC/Full GC</code></strong>  </p>
<ol>
<li>Minor GC 清理的是新生代空间，因此也叫做新生代GC</li>
<li>Major GC 清理的是老年代的空间，因此也叫做老年代GC</li>
<li>Full GC  清理的是整个堆：包括新生代、老年代空间</li>
</ol>
<p><img src="http://img.blog.csdn.net/20160728105422778" alt="jvm18"></p>
<p><strong><code>JVM的分代回收算法</code></strong>：根据不同代的特点采取最适合的收集算法，老年代的特点是每次垃圾收集时只有少量对象需要被回收，新生代的特点是每次垃圾回收时都有大量的对象需要被回收。</p>
<ol>
<li>新生代：由于新生代产生很多临时对象，大量对象需要进行回收，所以采用复制算法是最高效的：<strong>存活对象少，回收对象多</strong></li>
<li>老年代：回收的对象很少，都是经过几次标记后都不是可回收的状态转移到老年代的，所以仅有少量对象需要回收，故采用标记清除或者标记整理算法：<strong>存活对象多，回收对象少</strong></li>
</ol>
<p><strong><code>不同代的GC算法选择</code></strong>：把Java堆分为新生代和老年代：短命对象归为新生代，长命对象归为老年代。</p>
<ol>
<li>少量对象存活，适合复制算法：在新生代中，每次GC时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成GC。</li>
<li>大量对象存活，适合用标记-清理/标记-整理：在老年代中，因为对象存活率高、没有额外空间对他进行分配担保，就必须使用“标记-清理”/“标记-整理”算法进行GC。</li>
</ol>
<blockquote>
<p>新生代和老年代使用不同的GC算法（不同区中对象的存活特性不同），基于大多数新生对象都会在GC中被收回，新生代的GC使用复制算法<br>对象一般出生在Eden区，年轻代GC过程中，对象在2个幸存区之间移动，如果幸存区中的对象存活到适当的年龄，会被移动（提升）到老年代。<br>当对象在老年代死亡时，就需要更高级别的GC，更重量级的GC算法，复制算法不适用于老年代，因为没有多余的空间用于复制</p>
</blockquote>
<h3 id="Q&amp;A">Q&amp;A</h3><p><strong>GC需要完成的事情</strong>：</p>
<ol>
<li>哪些内存需要回收？</li>
<li>什么时候回收？</li>
<li>如何回收？</li>
</ol>
<p>JVM内存区域中的程序计数器、虚拟机栈、本地方法栈这3个区域随着线程而生，线程而灭；栈中的栈帧随着方法的进入和退出而有条不紊地执行着出栈和入栈的操作，每个栈帧中分配多少内存基本是在类结构确定下来时就已知的。在这几个区域不需要过多考虑回收的问题，因为方法结束或者线程结束时，内存自然就跟着回收了。</p>
<p>而Java堆和方法区则不同，一个接口中的多个实现类需要的内存可能不同，一个方法中的多个分支需要的内存也可能不一样，我们只有在程序处于运行期间时才能知道会创建哪些对象，这部分内存的分配和回收都是动态的，GC关注的也是这部分内存。</p>
<p><strong>Java的内存管理</strong>：Java的内存管理实际上就是对象的管理，其中包括对象的分配和释放。分配对象使用new关键字；释放对象时，只要将对象所有引用赋值为null，让程序不能够再访问到这个对象，我们称该对象为”不可达的”，GC将负责回收所有”不可达”对象的内存空间。</p>
<p>对于GC来说，当程序员创建对象时，GC就开始监控这个对象的地址、大小以及使用情况。通常，GC采用有向图的方式记录和管理堆中的所有对象。通过这种方式确定哪些对象是”可达的”，哪些对象是”不可达的”。当GC确定一些对象为”不可达”时，GC就有责任回收这些内存空间。</p>
<p><strong>为什么需要GC？</strong>：忘记或者错误的内存回收会导致程序或系统的不稳定甚至崩溃，Java语言没有提供释放已分配内存的显示操作方法，但是Java提供的GC功能可以自动监测对象是否超过作用域从而达到自动回收内存的目的。在使用Java时，不需要在程序代码中显式地释放内存空间，垃圾回收器会帮你找到不再需要的(垃圾)对象并把他们移出。</p>
<p><strong>为什么GC时需要暂停应用程序？</strong>：垃圾回收的时候，需要整个堆的引用状态保持不变，否则判定是垃圾，等稍后回收的时候它又被引用了，这就全乱套了。所以GC的时候，其他所有的程序执行处于暂停状态，卡住了。幸运的是，这个卡顿是非常短（尤其是新生代），对程序的影响微乎其微，所以GC的卡顿问题由此而来，也是情有可原，暂时无可避免。</p>
<p>以引用计数的方式回收垃圾对象为例，应用程序的线程需要被暂停才能完成回收，因为如果引用状态一直在变的话，垃圾收集器就无法准确地计数（统计对象的引用次数）。垃圾回收时要保证内存中所有对象的引用状态不变，所以GC时其他所有的程序处于暂停状态。</p>
<p><strong>增量式GC和普通GC的区别</strong>：GC在JVM中通常是由一个或一组进程来实现的，它本身也和用户程序一样占用heap空间，运行时也占用CPU。当GC进程运行时，应用程序停止运行。如果GC运行时间较长时，用户能够感到Java程序的停顿（超时）；如果GC运行时间太短，则可能对象回收率太低，这意味着还有很多应该回收的对象没有被回收，仍然占用大量内存。因此在设计GC的时候，就必须在停顿时间和回收率之间进行权衡。</p>
<p>增量式GC：通过一定的回收算法，把一个长时间的中断，划分为很多个小的中断，通过这种方式减少GC对用户程序的影响。虽然增量式GC在整体性能上可能不如普通GC的效率高，但是它能够减少程序的最长停顿时间。增量式GC的实现采用TrainGC算法，它的基本想法是：将堆中的所有对象按照创建和使用情况进行分组（分层），将使用频繁高和具有相关性的对象放在一队中，随着程序的运行，不断对组进行调整。当GC运行时，它总是先回收最老的（最近很少访问的）的对象，如果整组都为可回收对象，GC将整组回收。这样，每次GC运行只回收一定比例的不可达对象，保证程序的顺畅运行。</p>
<p><strong>为什么要分代</strong>：分代的垃圾回收策略，是基于这样一个事实：不同对象的生命周期是不一样的。因此，不同生命周期的对象可以采取不同的收集方式，以便提高回收效率。在Java程序运行的过程中，会产生大量的对象，其中有些对象是与业务信息相关，比如Http请求中的Session对象、线程、Socket连接，这类对象跟业务直接挂钩，因此生命周期比较长。还有一些对象主要是程序运行过程中生成的临时变量，这些对象生命周期会比较短，比如String对象，由于其不变类的特性，系统会产生大量的这些对象，有些对象甚至只用一次即可回收。</p>
<p>试想，在不进行对象存活时间区分的情况下，每次垃圾回收都是对整个堆空间进行回收，花费时间相对会长，同时，因为每次回收都需要遍历所有存活对象，但实际上，对于生命周期长的对象而言，这种遍历是没有效果的，因为可能进行了很多次遍历，但是他们依旧存在。因此，分代垃圾回收采用分治的思想，进行代的划分，把不同生命周期的对象放在不同代上，不同代上采用最适合它的垃圾回收方式进行回收。</p>
<p><strong>分代的好处</strong>：如果单纯从JVM的功能考虑（用简单粗暴的标记-清理删除垃圾对象），并不需要新生代，完全可以针对整个堆进行操作，但是每次GC都针对整个堆标记清理回收对象太慢了。把堆划分为新生代和老年代有2个好处：</p>
<ol>
<li>简化了新对象的分配（只在新生代分配内存），可以更有效的清除不再需要的对象（死对象）。</li>
<li>在新生代中，GC可以快速标记回收“死对象”，而不需要扫描整个堆中的存活一段时间的“老对象”。</li>
</ol>
<p><strong>什么情况下触发垃圾回收</strong>：由于对象进行了分代处理，因此垃圾回收区域、时间也不一样。GC有两种类型：Scavenge GC和Full GC。Scavenge GC：当新对象生成，并且在Eden申请空间失败时，就会触发Scavenge GC，对Eden区域进行GC，清除非存活对象，并且把尚且存活的对象移动到Survivor区。这种方式的GC是对年轻代的Eden区进行，不会影响到年老代。因为大部分对象都是从Eden区开始的，同时Eden区不会分配的很大，所以Eden区的GC会频繁进行。一般在这里需要使用速度快、效率高的算法，使Eden区能尽快空闲出来。</p>
<p>Full GC：对整个堆进行整理，包括Young、Tenured和Perm。Full GC因为需要对整个堆进行回收，所以比Scavenge GC要慢，因此应该尽可能减少Full GC的次数。在对JVM调优的过程中，很大一部分工作就是对于FullGC的调节。有如下原因可能导致Full GC：</p>
<ol>
<li>旧生代空间不足</li>
<li>持久代空间不足</li>
<li>CMS GC时出现了promotion failed和concurrent mode failure</li>
<li>统计得到新生代minor gc时晋升到旧生代的平均大小小于旧生代剩余空间</li>
<li>直接调用System.gc，可以DisableExplicitGC来禁止</li>
</ol>
<p><strong>分配担保</strong>：老年代的对象中，有一小部分是因为在新生代回收时，老年代做担保，进来的对象；绝大部分对象是因为很多次GC都没有被回收掉而进入老年代。</p>
<h2 id="新生代GC">新生代GC</h2><p><img src="http://img.blog.csdn.net/20160728113534443" alt="jvm23"></p>
<p>一句话总结：对象开始是创建在Eden区，然后经过在Survivor区域上的数次转移而存活下来的长寿对象最后会被移到老年代。</p>
<p><strong>Eden区</strong>：</p>
<p>当进行Eden区的回收时，垃圾回收器会从根对象开始遍历所有的可达对象，并将它们标记为存活状态。<br>标记完成后，所有存活对象会被复制到其中一个Survivor区。<br>于是整个Eden区便可认为是清空了，又可以重新用来分配对象了。<br>这个过程叫做标记复制（Mark and Copy）：存活对象先被标记，随后被复制到Survivor区中。  </p>
<p><strong>Survivor区</strong>：</p>
<p>紧挨着Eden的两个Survivor区其中一个Survivor区始终都是空的。<br>空的Survivor区会在下一次新生代GC的时候迎来它的居民。<br>整个新生代中的所有存活对象（Eden和from区）都会被复制到to区中。<br>一旦完成之后，对象便都跑到to区中，而Eden和from区则被清空了。<br>这时from区和to区两者的角色便会发生调转（下次GC时仍然从from到to）。  </p>
<p><strong>对象提升到老年代</strong>：</p>
<p>存活对象会不断地在两个存活区之间来回地复制，直到其中的一些对象被认为是已经成熟，足够老了。<br>在一轮GC完成后，每个分区中存活下来的对象的计数便会增加（如果刚刚从Eden存活下来其年龄=1），<br>当一个对象的年龄超过了一个特定的年老阈值之后，它便会被提升到老年代中。<br>出现对象提升的时候，这些对象则不会再被复制到另一个存活区，而是直接复制到老年代中。<br>如果存活区的大小不足以存放所有的新生代存活对象，则会出现过早提升。  </p>
<h3 id="步骤">步骤</h3><p>1.对象分配</p>
<p><img src="http://img.blog.csdn.net/20160728114616161" alt="jvm25"></p>
<p>2.填充到Eden区</p>
<p><img src="http://img.blog.csdn.net/20160728114626708" alt="jvm26"></p>
<p>3.将Eden区中存活的对象（引用对象）拷贝到其中一个存活区</p>
<p><img src="http://img.blog.csdn.net/20160728114637911" alt="jvm27"></p>
<p>4.年龄计数器：在Eden中存活的对象其年龄初始=1，从其他存活区存活下来年龄+1</p>
<p><img src="http://img.blog.csdn.net/20160728114647520" alt="jvm28"></p>
<p>5.增加年龄计数器，图中To存活区有三个对象来自于From存活区，一个对象来自Eden</p>
<p><img src="http://img.blog.csdn.net/20160728114657349" alt="jvm29"></p>
<p>6.对象提升，这里假设年龄阈值=8，发生GC时，From存活区中=8的对象提升到老年代，其他存活对象移动到To存活区</p>
<p><img src="http://img.blog.csdn.net/20160728114707380" alt="jvm30"></p>
<p>7.总结下对象提升的过程：对象在新生代分配，每当熬过一次YGC，对象的年龄计数器+1，当达到阈值时仍然存活，提升到老年代</p>
<p><img src="http://img.blog.csdn.net/20160728114716912" alt="jvm31"></p>
<p>8.总结下GC过程：对象在新生代分配并填充，当新生代满时发生YGC，当对象在存活区熬过一定年龄，提升到老年代</p>
<p><img src="http://img.blog.csdn.net/20160728114725911" alt="jvm32"></p>
<h3 id="新生代GC的特点：">新生代GC的特点：</h3><ol>
<li>只要JVM无法为新创建的对象分配空间，就肯定会触发新生代GC，比如Eden区满了。因此对象创建得越频繁，新生代GC肯定也更频繁</li>
<li>一旦内存池满了，它的所有内容就会被拷贝走，指针又将重新归零。因此和经典的标记、清除、整理的过程不同的是<br>Eden区和Survivor区的清理只涉及到标记和拷贝。在它们中是不会出现碎片的。写指针始终在当前使用区的顶部</li>
<li>在一次新生代GC事件中，通常不涉及到年老代。年老代到年轻代的引用被认为是GC的根对象。而在标记阶段中，从年轻代到年老代的引用则会被忽略掉</li>
<li>所有的新生代GC都会触发stop-the-world暂停，这会中断应用程序的线程。对绝大多数应用而言，暂停的时间是可以忽略不计的</li>
</ol>
<h3 id="合理设置新生代大小">合理设置新生代大小</h3><ol>
<li>新生代过小，会导致新生对象很快就晋升，到老年代中，在老年代中对象很难被回收</li>
<li>新生代过大，会发生过多的复制过程</li>
<li>我们的目标是：最小化短命对象晋升到老年代的数量，最小化新生代GC的次数和持续时间</li>
</ol>
<p><img src="http://img.blog.csdn.net/20160728113837814" alt="jvm24"></p>
<h3 id="JVM的新生代GC算法">JVM的新生代GC算法</h3><ol>
<li>Serial Copying：单CPU、新生代小、对暂停时间要求丌高的应用</li>
<li>Parallel Scavenge：多CPU、对暂停时间要求较短的应用</li>
<li>ParNew：Serial Copying的多线程版本</li>
</ol>
<p>均使用复制算法，在分配对象时，如果Eden空间不足触发新生代GC</p>
<h3 id="JVM的新生代GC优化">JVM的新生代GC优化</h3><p><strong>指针碰撞（bump-the-pointer）</strong>：Bump-the-pointer技术会跟踪在Eden上新创建的对象。由于新对象被分配在Eden空间的最上面，所以后续如果有新对象创建，只需要判断新创建对象的大小是否满足剩余的Eden空间。如果新对象满足要求，则其会被分配到Eden空间，同样位于Eden的最上面。所以当有新对象创建时，只需要判断此新对象的大小即可，因此具有更快的内存分配速度。  </p>
<p>然而，在多线程环境下，将会有别样的状况。为了满足多个线程在Eden空间上创建对象时的线程安全，不可避免的会引入锁，因此随着锁竞争的开销，创建对象的性能也大打折扣。</p>
<p><strong>线程局部分配缓冲区（Thread-Local Allocation Buffers）</strong>：</p>
<p>在HotSpot中正是通过TLABs解决了多线程问题。TLABs允许每个线程在Eden上有自己的小片空间，线程只能访问其自己的TLAB区域，因此bump-the-pointer能通过TLAB在不加锁的情况下完成快速的内存分配。</p>
<p><img src="http://img.blog.csdn.net/20160728134144149" alt="jvm34"></p>
<h2 id="老年代GC">老年代GC</h2><p>老年代GC算法：</p>
<ol>
<li>Serial MSC/<strong>Serial Old</strong>/Serial Mark Sweep Compact</li>
<li>Parallel Compacting/<strong>Parallel Old</strong></li>
<li>CMS</li>
</ol>
<p>下图是新生代GC和老年代GC的几种组合方式：</p>
<ol>
<li>新生代Serial+老年代Serial（Serial Old）</li>
<li>新生代ParNew+老年代Serial（Serial Old）</li>
<li>新生代Parallel Scavenge+老年代Parallel（Parallel Old）</li>
</ol>
<p><img src="http://img.blog.csdn.net/20160728144207044" alt="jvm37"></p>
<p>CMS收集器：</p>
<p><img src="http://img.blog.csdn.net/20160728144217623" alt="jvm38"></p>
<h3 id="CMS_GC工作流程">CMS GC工作流程</h3><p>CMS GC是针对老年代的GC算法，CMS采用标记清理算法，只不过并不是严格意义的标记清理</p>
<p>1.年轻代被分割成一个Eden区和两个survivor区，老年代是一片连续空间</p>
<p><img src="http://img.blog.csdn.net/20160728162057995" alt="jvm42"></p>
<p>2.你的JAVA程序运行一段时间后，对象分散地分布在老年代中，<br>使用CMS，老年代是对象消亡的地方，它们不会被移动，也不会被压缩，除非遇到一个Full GC</p>
<p><img src="http://img.blog.csdn.net/20160728162109095" alt="jvm43"></p>
<p>3.年轻代GC工作过程：<br>① 存活的对象从Eden区或一个Survivor区，移动到另外一个Survivor区<br>② 一些老的对象如果达到了晋升阈值，就会被提升到老年代中</p>
<p><img src="http://img.blog.csdn.net/20160728162120829" alt="jvm44"></p>
<p>4.年轻代GC后，Eden区和其中一个Survivor区会被清理，Survivor中存活对象如果超过晋升阈值，晋升到老年代</p>
<p><img src="http://img.blog.csdn.net/20160728162141142" alt="jvm45"></p>
<p>5.老年代GC采用标记清理算法</p>
<p><img src="http://img.blog.csdn.net/20160728162441094" alt="jvm46"></p>
<p>6.老年代GC经过清理阶段后，很多内存会被释放，但是仍然没有压缩</p>
<p><img src="http://img.blog.csdn.net/20160728162200918" alt="jvm47"></p>
<h3 id="CMS（Concurrent_Mark_Sweep）">CMS（Concurrent Mark Sweep）</h3><p>CMS收集器是一种以获取最短回收停顿时间为目标的收集器（尽可能降低停顿），它是基于“标记-清除”算法实现的，它的目标是尽量减少应用的暂停时间，减少Full GC发生的几率，利用和应用程序线程<strong>并发</strong>（允许垃圾回收线程和应用线程共享处理器资源，比如下面的步骤2、3并发两个阶段）的垃圾回收线程来<strong>标记清除</strong>年老代。整个收集过程大致分为4个步骤：</p>
<ol>
<li>初始标记(CMS-initial-mark)：从根对象开始标记直接关联的对象，会产生全局停顿</li>
<li>并发标记(CMS-concurrent-mark)：进行GC Root根搜索算法阶段，会判定对象是否存活</li>
<li>重新标记(CMS-remark)：由于并发标记时，用户线程依然运行，因此在正式清理前，再做修正。<br>这个阶段的停顿时间会被初始标记阶段稍长，但比并发标记阶段要短</li>
<li>并发清除(CMS-concurrent-sweep)：基于标记结果，直接清理对象（清理的是没有标记的对象）</li>
</ol>
<p><img src="http://img.blog.csdn.net/20160728140210892" alt="jvm35"></p>
<p>下图结合了其他类型的收集器，对比CMS。可以看到CMS只不过是把一段较长的Stop the World暂停所有应用程序，分成了两个较短的暂停。第一次暂停（初始标记）从GC Roots对象开始标记存活的对象；第二次暂停（重新标记）是在并发标记之后，重新标记并发标记阶段遗漏的对象（在并发标记阶段结束后对象状态的更新导致）。第一次暂停会比较短，第二次暂停通常会比较长，并且重新标记这个阶段可以并行标记（初始标记使用一个线程，重新标记使用多线程）。</p>
<p><img src="http://img.blog.csdn.net/20160728141921295" alt="jvm36"></p>
<p><strong>CMS收集器的优点</strong>：由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以整体来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。</p>
<p><strong>CMS收集器的缺点</strong>：</p>
<p><code>1.与其他GC相比，CMS GC要求更多的内存空间和CPU资源</code>。在并发阶段，虽然不会导致用户线程停顿，但是会占用CPU资源而导致引用程序变慢，总吞吐量下降。CMS默认启动的回收线程数是：(CPU数量+3)/4，可以通过<code>-XX:ParallelCMSThreads</code>设定CMS的线程数量。由于GC线程与应用抢占CPU，会影响系统整体吞吐量和性能：在用户线程运行过程中，分一半CPU去做GC，系统性能在GC阶段就下降一半。</p>
<p><img src="http://img.blog.csdn.net/20160728150711018" alt="jvm39"></p>
<p><code>2.清理不彻底，会产生浮动垃圾</code>：因为在并发清理阶段（步骤4），伴随程序的运行自然会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在本次收集中处理它们，只好留待下一次GC时将其清理掉。</p>
<p><img src="http://img.blog.csdn.net/20160728155616723" alt="jvm40"></p>
<p><code>3.因为GC的同时应用也在运行，不能在老年代空间快满时再清理</code>，Old区需要预留足够的内存空间给用户线程使用：否则如果在并发GC期间（步骤4），用户线程又申请了大量内存（即使产生的是垃圾对象，也没办法在本次清理），导致内存不够。如果预留的空间不够，就会出现Concurrent Mode Failure的错误。这时候虚拟机将启动后备预案：临时启用Serial Old收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了（因为Serial Old是单线程Stop-the-World的）。</p>
<p>因此CMS收集器不能像其他收集器（比如单线程的Serial Old GC或者多线程的Parallel Old GC都是等到Old满了才开始对Old老年代执行垃圾回收）那样等到老年代几乎完全被填满了再进行收集，需要预留一部分内存空间提供给<strong>并发收集时的程序运作</strong>使用，这个参数是<code>-XX:CMSInitiatingOccupancyFraction</code>，在较早的Java版本，默认设置下，CMS收集器在老年代使用了68%的空间时就会被激活；在较新的Java版本，这个阈值为75%。设置的过高将会很容易导致”Concurrent Mode Failure”失败，性能反而降低：因为阈值越高，越往后执行Old GC，而越往后，老年代占用的空间就会越大，当执行Old GC时，很可能老年代的空间不够，这时如果Survivor转移到Old，非常容易出现晋升失败，一旦失败，只好执行Full GC，性能就越差劲。  </p>
<p><img src="http://img.blog.csdn.net/20160728160847993" alt="jvm41"></p>
<p>假设Old GC时，老年代总内存=10G，占用了9G（由于碎片存在，不足分配新对象，发生Old GC）<br>在并行清理阶段，即使把垃圾都清理完，释放了3G，现在占用9-3=6G，但是如果在并发清理阶段<br>新产生的对象占用了4G，本次垃圾无法清理，导致内存占用=6+4=10G，老年代空间又满了！  </p>
<p>所以对于老年代的GC应该预留一定的内存空间给并发清理阶段产生的对象，默认值是68%。<br>假设老年代总内存=10G，当使用了6.8G时，就会触发老年代GC，而不是等到差不多占满才触发。<br>举例使用6.8G后，释放了3G，现在占用6.8-3=3.8G，即使新产生了4G不会在本次垃圾收集被清理<br>总的内存占用也只有3.8+4=7.8G，虽然又会触发一次老年代GC，但是不至于把老年代的内存用光。  </p>
<p>为了保证在CMS回收完堆之前还有空间分配给正在运行的应用程序，必须预留一部分空间。也就是说，CMS不会在老年代满的时候才开始收集，相反，它会尝试更早的开始收集，以避免在回收完成之前，堆没有足够空间分配！默认当老年代使用68%的时候，CMS就开始行动了。</p>
<p><code>4.CMS GC默认不提供内存压缩</code>，为了避免过多的内存碎片而需要执行压缩任务时，CMS GC会比任何其他GC带来更多的stop-the-world时间：因为整理过程是独占的，会引起停顿时间变长。不过CMS允许设置进行几次Full GC后，进行一次碎片整理。</p>
<p>CMS是基于“标记-清除”算法实现的收集器，使用“标记-清除”算法收集后，会产生大量碎片。空间碎片太多时，将会给对象分配带来很多麻烦，比如说大对象，内存空间找不到连续的空间来分配不得不提前触发一次Full  GC。为了解决这个问题，CMS收集器提供了一个-XX:UseCMSCompactAtFullCollection开关参数，用于在Full  GC之后增加一个碎片整理过程，还可通过-XX:CMSFullGCBeforeCompaction参数设置执行多少次不压缩的Full GC之后，跟着来一次碎片整理过程。</p>
<h3 id="Q&amp;A-1">Q&amp;A</h3><p><strong><code>为什么CMS要使用标记清除而不是标记压缩？</code></strong>：如果使用标记压缩，需要对（存活）对象的内存位置进行改变，这样程序就很难继续执行</p>
<p><strong><code>CMS采用标记清除有什么缺点？</code></strong>：标记清除会产生大量内存碎片，不利于内存分配</p>
<p><strong><code>CMS有没有暂停？</code></strong>：CMS并非没有暂停，而是用两次短暂停来替代串行标记整理算法的长暂停</p>
<p><strong><code>CMS中的C（Concurrent）并发什么意思</code></strong>：并发标记、并发清除、并发重设阶段的所谓并发，是指一个或者多个垃圾回收线程和应用程序线程并发地运行，垃圾回收线程不会暂停应用程序的执行，如果你有多于一个处理器，那么并发收集线程将与应用线程在不同的处理器上运行，显然，这样的开销就是会降低应用的吞吐量。Remark阶段的并行，是指暂停了所有应用程序后，启动一定数目的垃圾回收进程进行并行标记，此时的应用线程是暂停的。</p>
<h3 id="CMS参数">CMS参数</h3><table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
<th>默认值</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>-XX:+UseConcMarkSweepGC</td>
<td>使用CMS内存收集</td>
<td>默认是并行收集器</td>
</tr>
<tr>
<td>-XX:CMSFullGCsBeforeCompaction</td>
<td>多少次后进行内存压缩</td>
<td></td>
<td>由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生”碎片”，使得运行效率降低。此值设置运行多少次GC以后对内存空间进行压缩、整理</td>
</tr>
<tr>
<td>-XX:+CMSParallelRemarkEnabled</td>
<td>降低标记停顿</td>
</tr>
<tr>
<td>-XX+UseCMSCompactAtFullCollection</td>
<td>在Full GC的时候，对年老代的压缩</td>
<td></td>
<td>CMS是不会移动内存的，因此非常容易产生碎片，导致内存不够用。开启该参数，在Full GC时会进行内存压缩。增加这个参数是个好习惯，可能会影响性能，但是可以消除碎片</td>
</tr>
<tr>
<td>-XX:+UseCMSInitiatingOccupancyOnly</td>
<td>使用手动定义初始化定义开始CMS收集</td>
<td></td>
<td>禁止hostspot自行触发CMS GC</td>
</tr>
<tr>
<td>-XX:CMSInitiatingOccupancyFraction=70</td>
<td>老年代内置使用70％后开始CMS收集</td>
<td>低版本的JDK缺省值为68%，JDK6及以上版本缺省值则为92%</td>
<td>减少出现promotion failed错误的解决办法是提前出发CMS GC</td>
</tr>
<tr>
<td>-XX:CMSInitiatingPermOccupancyFraction</td>
<td>设置Perm Gen使用到达多少比率时触发</td>
</tr>
<tr>
<td>-XX:+CMSIncrementalMode</td>
<td>设置为增量模式</td>
<td></td>
<td>用于单CPU情况</td>
</tr>
<tr>
<td>-XX:ConcGCThreads</td>
<td>并发CMS过程运行时的线程数</td>
<td>ConcGCThreads=(ParallelGCThreads+3)/4</td>
<td>value=4意味着CMS周期的所有阶段都以4个线程来执行</td>
</tr>
</tbody>
</table>
<p>当堆满之后，并行收集器便开始进行垃圾收集，例如，当没有足够的空间来容纳新分配或提升的对象。对于CMS收集器，长时间等待是不可取的，因为在并发垃圾收集期间应用持续在运行(并且分配对象)。因此，为了在应用程序使用完内存之前完成垃圾收集周期，CMS收集器要比并行收集器更先启动。</p>
<p>因为不同的应用会有不同对象分配模式，JVM会收集实际的对象分配(和释放)的运行时数据，并且分析这些数据，来决定什么时候启动一次CMS垃圾收集周期。为了引导这一过程， JVM会在一开始执行CMS周期前作一些线索查找。该线索由<code>-XX:CMSInitiatingOccupancyFraction=&lt;value&gt;</code>来设置，该值代表老年代堆空间的使用率。比如，value=75意味着第一次CMS垃圾收集会在老年代被占用75%时被触发。通常CMSInitiatingOccupancyFraction的默认值为68。</p>
<h3 id="CMS_Failed">CMS Failed</h3><p>Promotion Failed：在进行Minor GC时，survivor space放不下、对象只能放入老年代，而此时老年代也放不下造成的</p>
<blockquote>
<p>这个问题的产生是由于救助空间不够，从而向年老代转移对象，年老代没有足够的空间来容纳这些对象，导致一次full gc的产生。解决这个问题的办法有两种完全相反的倾向：增大救助空间、增大年老代或者去掉救助空间。 增大救助空间就是调整-XX:SurvivorRatio参数，这个参数是Eden区和Survivor区的大小比值，默认是32，也就是说Eden区是 Survivor区的32倍大小，要注意Survivo是有两个区的，因此Surivivor其实占整个young genertation的1/34。调小这个参数将增大survivor区，让对象尽量在survitor区呆长一点，减少进入年老代的对象。去掉救助空 间的想法是让大部分不能马上回收的数据尽快进入年老代，加快年老代的回收频率，减少年老代暴涨的可能性，这个是通过将-XX:SurvivorRatio 设置成比较大的值（比如65536)来做到。</p>
</blockquote>
<p>Concurrent Mode Failure：在执行CMS GC的过程中同时有对象要放入老年代，而此时老年代空间不足造成的，比如CMS GC时浮动垃圾过多导致暂时性的空间不足触发Full GC</p>
<blockquote>
<p>Concurrent mode failed的产生是由于CMS回收年老代的速度太慢，导致年老代在CMS完成前就被沾满，引起full gc，避免这个现象的产生就是调小-XX:CMSInitiatingOccupancyFraction参数的值，让CMS更早更频繁的触发，降低年老代被沾满的可能。</p>
</blockquote>
<h3 id="CMS_GC日志">CMS GC日志</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.662</span>: [GC [<span class="number">1</span> CMS-initial-mark: <span class="number">28122</span>K(<span class="number">49152</span>K)] <span class="number">29959</span>K(<span class="number">63936</span>K), <span class="number">0.0046877</span> secs] [Times: user=<span class="number">0.00</span> sys=<span class="number">0.00</span>, real=<span class="number">0.00</span> secs] </span><br><span class="line"><span class="number">1.666</span>: [CMS-concurrent-mark-start]</span><br><span class="line"><span class="number">1.699</span>: [CMS-concurrent-mark: <span class="number">0.033</span>/<span class="number">0.033</span> secs] [Times: user=<span class="number">0.25</span> sys=<span class="number">0.00</span>, real=<span class="number">0.03</span> secs] </span><br><span class="line"><span class="number">1.699</span>: [CMS-concurrent-preclean-start]</span><br><span class="line"><span class="number">1.700</span>: [CMS-concurrent-preclean: <span class="number">0.000</span>/<span class="number">0.000</span> secs] [Times: user=<span class="number">0.00</span> sys=<span class="number">0.00</span>, real=<span class="number">0.00</span> secs] </span><br><span class="line"><span class="number">1.700</span>: [GC[YG occupancy: <span class="number">1837</span> K (<span class="number">14784</span> K)]</span><br><span class="line">   <span class="number">1.700</span>: [Rescan (parallel) , <span class="number">0.0009330</span> secs]</span><br><span class="line">   <span class="number">1.701</span>: [weak refs processing, <span class="number">0.0000180</span> secs] </span><br><span class="line">          [<span class="number">1</span> CMS-remark: <span class="number">28122</span>K(<span class="number">49152</span>K)] <span class="number">29959</span>K(<span class="number">63936</span>K), <span class="number">0.0010248</span> secs] </span><br><span class="line">          [Times: user=<span class="number">0.00</span> sys=<span class="number">0.00</span>, real=<span class="number">0.00</span> secs] </span><br><span class="line"><span class="number">1.702</span>: [CMS-concurrent-sweep-start]</span><br><span class="line"><span class="number">1.739</span>: [CMS-concurrent-sweep: <span class="number">0.035</span>/<span class="number">0.037</span> secs] [Times: user=<span class="number">0.11</span> sys=<span class="number">0.02</span>, real=<span class="number">0.05</span> secs] </span><br><span class="line"><span class="number">1.739</span>: [CMS-concurrent-reset-start]</span><br><span class="line"><span class="number">1.741</span>: [CMS-concurrent-reset: <span class="number">0.001</span>/<span class="number">0.001</span> secs] [Times: user=<span class="number">0.00</span> sys=<span class="number">0.00</span>, real=<span class="number">0.00</span> secs]</span><br></pre></td></tr></table></figure>
<p>日志解释：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">39.910</span>: [GC <span class="number">39.910</span>: [ParNew: <span class="number">261760</span>K-&gt;<span class="number">0</span>K(<span class="number">261952</span>K), <span class="number">0.2314667</span> secs] <span class="number">262017</span>K-&gt;<span class="number">26386</span>K(<span class="number">1048384</span>K), <span class="number">0.2318679</span> secs] </span><br><span class="line">新生代的GC执行。 新生代的占用量经过GC之后，从 <span class="number">261760</span>K 下降到了 <span class="number">0</span>K . 花费的时间 <span class="number">0.2314667</span>秒.</span><br><span class="line"></span><br><span class="line"><span class="number">40.146</span>: [GC [<span class="number">1</span> CMS-initial-mark: <span class="number">26386</span>K(<span class="number">786432</span>K)] <span class="number">26404</span>K(<span class="number">1048384</span>K), <span class="number">0.0074495</span> secs]</span><br><span class="line">老年代的GC. CMS会在这里进行打上初始标记.会有一次StopWholeWorld的暂停. 内存占用量从<span class="number">786432</span>K 下降到<span class="number">26386</span>K。</span><br><span class="line"></span><br><span class="line"><span class="number">40.154</span>: [CMS-concurrent-mark-start]</span><br><span class="line">开始并发标记阶段。</span><br><span class="line"></span><br><span class="line"><span class="number">40.683</span>: [CMS-concurrent-mark: <span class="number">0.521</span>/<span class="number">0.529</span> secs]</span><br><span class="line">Concurrent marking 花费 <span class="number">0.521</span>秒 <span class="number">0.529</span>秒中包含有yield其他线程的时间</span><br><span class="line"></span><br><span class="line"><span class="number">40.683</span>: [CMS-concurrent-preclean-start]</span><br><span class="line">开始PreClean 阶段. PreClean 也是并行发生的. 这个阶段进行的一些工作，可以为下一次的stop-the-world(remark阶段)减少一些工作</span><br><span class="line"></span><br><span class="line"><span class="number">40.701</span>: [CMS-concurrent-preclean: <span class="number">0.017</span>/<span class="number">0.018</span> secs] 开始进行一些pre-clean的工作。 这行日志表示其耗费了 <span class="number">0.017</span>秒</span><br><span class="line"></span><br><span class="line"><span class="number">40.704</span>: [GC40<span class="number">.704</span>: [Rescan (parallel) , <span class="number">0.1790103</span> secs]<span class="number">40.883</span>: [weak refs processing, <span class="number">0.0100966</span> secs] [<span class="number">1</span> CMS-remark: <span class="number">26386</span>K(<span class="number">786432</span>K)] <span class="number">52644</span>K(<span class="number">1048384</span>K), <span class="number">0.1897792</span> secs]</span><br><span class="line">Stop-the-world阶段，这个阶段重新扫描CMS堆里面剩余的对象。重新从root开始扫描并处理引用的对象。重新扫描花费了<span class="number">0.1790103</span>秒,处理软引用耗费了<span class="number">0.0100966</span>秒，而这整个阶段总归耗费了<span class="number">0.1897792</span>秒.</span><br><span class="line"></span><br><span class="line"><span class="number">40.894</span>: [CMS-concurrent-sweep-start]</span><br><span class="line">开始对以及消亡和未标记的对象进行sweeep。sweeping这个阶段与其他线程是并行运行的。</span><br><span class="line"></span><br><span class="line"><span class="number">41.020</span>: [CMS-concurrent-sweep: <span class="number">0.126</span>/<span class="number">0.126</span> secs]</span><br><span class="line">sweep阶段耗费了<span class="number">0.126</span>秒.</span><br><span class="line"></span><br><span class="line"><span class="number">41.020</span>: [CMS-concurrent-reset-start]</span><br><span class="line">开始重置</span><br><span class="line"></span><br><span class="line"><span class="number">41.147</span>: [CMS-concurrent-reset: <span class="number">0.127</span>/<span class="number">0.127</span> secs]</span><br><span class="line">CMS数据结构开始重新初始化，下次则会重新开始一个新的周期。这个阶段耗费了 <span class="number">0.127</span>秒</span><br></pre></td></tr></table></figure>
<h2 id="G1_GC">G1 GC</h2><p>Java Performance The Definitive Guide P150: <a href="http://ifeve.com/深入理解g1垃圾收集器/" target="_blank" rel="external">http://ifeve.com/深入理解g1垃圾收集器/</a>  </p>
<p>不同于其他的分代回收算法，G1将堆空间划分成了互相独立的区块。每块区域既有可能属于Old区、也有可能是Young区，且每类区域空间可以是不连续的（对比CMS的O区和Y区都必须是连续的）。这种将O区划分成多块的理念源于：当<strong>并发后台线程</strong>寻找可回收的对象时、有些区块包含可回收的对象要比其他区块多很多。虽然在清理这些区块时G1仍然需要暂停应用线程、但可以用相对较少的时间优先回收包含垃圾最多区块（并不需要回收整个堆空间，而是一次选择一部分，所以G1可以看做是一种增量式的GC）。这也是为什么G1命名为Garbage First的原因：<strong>第一时间处理垃圾最多的区块</strong>。</p>
<p>G1相对于CMS的区别在：</p>
<ol>
<li>G1在压缩空间方面有优势，CMS存在内存碎片不会主动压缩</li>
<li>G1通过将内存空间分成区域（Region）的方式避免内存碎片问题</li>
<li>Eden, Survivor, Old区不再固定、在内存使用效率上来说更灵活</li>
<li>G1可以通过设置预期停顿时间（Pause Time）来控制垃圾收集时间避免应用雪崩现象</li>
<li>G1在回收内存后会马上同时做合并空闲内存的工作、而CMS默认是在STW的时候做</li>
<li>G1会在Young GC中使用、而CMS只能在O区使用</li>
</ol>
<p>下图是各种GC算法针对老年代的比较（当然G1也可以针对YGC）：</p>
<table>
<thead>
<tr>
<th>垃圾收集算法</th>
<th>是否压缩内存</th>
<th>是否有较长Stop the world</th>
</tr>
</thead>
<tbody>
<tr>
<td>Parallel Old</td>
<td>对整个堆压缩</td>
<td>Y</td>
</tr>
<tr>
<td>CMS</td>
<td>不压缩</td>
<td>N</td>
</tr>
<tr>
<td>G1</td>
<td>只对回收的部分Region压缩</td>
<td>N</td>
</tr>
</tbody>
</table>
<p>G1在运行过程中主要包含如下4种操作方式：</p>
<ul>
<li>YGC（young GC，不同于CMS）</li>
<li>并发阶段（Concurrent Mark Cycle）</li>
<li>混合模式（mixed GC）</li>
<li>Full GC （一般是G1出现问题时发生）</li>
</ul>
<h3 id="YGC">YGC</h3><p>YGC的时机：分配的所有Eden Region都被填充满了</p>
<h3 id="并发标记">并发标记</h3><p><strong>并发标记的时机</strong>：</p>
<p>Initiating Heap Occupancy Percent</p>
<ul>
<li>Threshold to start the concurrent marking cycle to identify candidate old regions.</li>
<li>When old generation occupancy crosses this adpative threshold.</li>
<li>Based on the total heap size.</li>
</ul>
<p><strong>并发标记过程</strong>：</p>
<p>Concurrent Marking并发标记包括多个阶段：</p>
<p>1.<code>Initial Mark(STW)</code>:young gc时执行，标记survivor 区块(root regions)中准备移动到old区块的对象</p>
<ul>
<li>stop the world, piggy-backed on a young pause</li>
<li>marks all root objects</li>
</ul>
<p>初始标记阶段：会暂停所有应用线程（部分原因是这个过程会执行一次YGC），出现initial-mark表明后台的并发GC阶段开始了，因为初始标记阶段本身也是要暂停应用线程的，G1正好在YGC的过程中把这个事情也一起干了。</p>
<p>2.<code>Root Region Scanning</code>: Scan survivor regions for references into the old generation.<br>This happens while the application continues to run.<br>The phase must be completed before a young GC can occur.  </p>
<ul>
<li>works concurrently with the mutators</li>
<li>survivor regions are root regions</li>
<li>must complete before the next GC pause</li>
</ul>
<p>扫描根区域：这个阶段不能被YGC所打断、因此后台线程有足够的CPU时间很关键。正常的扫描根区域日志：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">50<span class="class">.819</span>: <span class="attr_selector">[GC concurrent-root-region-scan-start]</span></span><br><span class="line">51<span class="class">.408</span>: <span class="attr_selector">[GC concurrent-root-region-scan-end, 0.5890230]</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>不可能在GC concurrent-root-region-scan中出现YGC，如果有YGC，也只能等到GC concurrent-root-region-scan-end后才开始YGC</p>
</blockquote>
<p>如果Young区空间恰好在Root扫描的时候满了、YGC必须等待root扫描之后才能进行，带来的影响是YGC暂停时间会相应的增加，这时的GC日志是这样的：</p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">350.994</span>: <span class="string">[GC pause (young)</span></span><br><span class="line"><span class="attribute">351.093</span>: <span class="string">[GC concurrent-root-region-scan-end, 0.6100090]</span></span><br><span class="line"><span class="attribute">351.093</span>: <span class="string">[GC concurrent-mark-start],0.37559600 secs]</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注：这种情况非常少见。通常我们看到的GC pause原因是：[GC pause (G1 Evacuation Pause) (young)和[GC pause (G1 Evacuation Pause) (young)、[GC pause (G1 Evacuation Pause) (young) (initial-mark)，很少有[GC pause (young)。</p>
</blockquote>
<p>3.<code>Concurrent Marking</code>:Find live objects over the entire heap.<br>This happens while the application is running.<br>This phase can be interrupted by young generation garbage collections.</p>
<ul>
<li>works concurrently with the mutators（-XX:ConcGCThreads）</li>
<li>pre-write barrier needed</li>
<li>live data accounting</li>
</ul>
<p>并发标记是可以被中断的，比如这个过程中发生了YGC就会被中断（中断的意思是start和end相隔在很远，这中间可以插入其他阶段，不能中断表示的是不允许其他阶段在此过程中运行），日志如下：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">111<span class="class">.382</span>: <span class="attr_selector">[GC concurrent-mark-start]</span></span><br><span class="line">....</span><br><span class="line">120<span class="class">.905</span>: <span class="attr_selector">[GC concurrent-mark-end, 9.5225160 sec]</span></span><br></pre></td></tr></table></figure>
<p>这个阶段之后会有一个二次标记阶段（Remark，STW）、清理阶段（Cleanup，STW），还有额外的一次并发清理阶段（Concurrent Cleanup）：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">120.910</span>: [GC remark <span class="number">120.959</span>: [GC ref-PRC, <span class="number">0.0000890</span> secs], <span class="number">0.0718990</span> secs]</span><br><span class="line"> [Times: user=<span class="number">0.23</span> sys=<span class="number">0.01</span>, real=<span class="number">0.08</span> secs]</span><br><span class="line"><span class="number">120.985</span>: [GC cleanup <span class="number">3510</span>M-&gt;<span class="number">3434</span>M(<span class="number">4096</span>M), <span class="number">0.0111040</span> secs]</span><br><span class="line"> [Times: user=<span class="number">0.04</span> sys=<span class="number">0.00</span>, real=<span class="number">0.01</span> secs]</span><br><span class="line"></span><br><span class="line"><span class="number">120.996</span>: [GC concurrent-cleanup-start]</span><br><span class="line"><span class="number">120.996</span>: [GC concurrent-cleanup-end, <span class="number">0.0004520</span>]</span><br></pre></td></tr></table></figure>
<p>到此为止，正常的一个G1周期（并发标记周期）已完成–这个周期主要做的是发现哪些区域包含可回收的垃圾最多（标记为X），实际空间释放较少。</p>
<blockquote>
<p>cleanup清理的是哪部分数据？</p>
</blockquote>
<h3 id="混合GC（Incremental_Compaction_aka_Mixed_Collection）">混合GC（Incremental Compaction aka Mixed Collection）</h3><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">79.826</span>: [GC pause (mixed), <span class="number">0.26161600</span> secs]</span><br><span class="line">....</span><br><span class="line">[<span class="attribute">Eden</span>: <span class="number">1222</span>M<span class="function"><span class="params">(<span class="number">1222</span>M)</span>-&gt;</span><span class="number">0</span>B<span class="function"><span class="params">(<span class="number">1220</span>M)</span> <span class="title">Survivors</span>: 142<span class="title">M</span>-&gt;</span><span class="number">144</span>M <span class="attribute">Heap</span>: <span class="number">3200</span>M<span class="function"><span class="params">(<span class="number">4096</span>M)</span>-&gt;</span><span class="number">1964</span>M(<span class="number">4096</span>M)]</span><br><span class="line">[<span class="attribute">Times</span>: user=<span class="number">1.01</span> sys=<span class="number">0.00</span>, real=<span class="number">0.26</span> secs]</span><br></pre></td></tr></table></figure>
<p>上面的日志可以注意到Eden释放了1222MB、但整个堆的空间释放内存要大于这个数目=1236M。数量相差看起来比较少、只有16MB，但是要考虑同时有survivor区的对象晋升到O区；另外，<strong>每次混合GC只是清理一部分的O区内存</strong>，整个GC会一直持续到几乎所有的标记区域垃圾对象都被回收，这个阶段完了之后G1会重新回到正常的YGC阶段。周期性的，<strong>当O区内存占用达到一定数量之后G1又会开启一次新的并行GC阶段</strong>。并发标记决定因素：Initiating Heap Occupancy Percent。</p>
<h3 id="提升失败/转移失败">提升失败/转移失败</h3><p>Evacuation Failures</p>
<ul>
<li>When there are no more regions available for survivors or tenured objects, G1 GC encounters an evacuation failure.</li>
<li>An evacuation failure is expensive and the usual pattern is that if you see a couple of evacuation failures; full GC could* soon follow.</li>
</ul>
<h3 id="理解G1">理解G1</h3><p><a href="http://hllvm.group.iteye.com/group/topic/44381" target="_blank" rel="external">http://hllvm.group.iteye.com/group/topic/44381</a>  </p>
<p>分代式G1模式下有两种选定CSet的子模式，分别对应young GC与mixed GC： </p>
<ul>
<li>Young GC：选定所有young gen里的region。通过控制young gen的region个数来控制young GC的开销。 </li>
<li>Mixed GC：选定所有young gen里的region，外加根据global concurrent marking统计得出收集收益高的若干old gen region。在用户指定的开销目标范围内尽可能选择收益高的old gen region。 </li>
</ul>
<p>可以看到young gen region总是在CSet内。因此分代式G1不维护从young gen region出发的引用涉及的RSet更新。 </p>
<p>分代式G1的正常工作流程就是在young GC与mixed GC之间视情况切换，背后定期做做全局并发标记。Initial marking默认搭在young GC上执行；当全局并发标记正在工作时，G1不会选择做mixed GC，反之如果有mixed GC正在进行中G1也不会启动initial marking。<br>在正常工作流程中没有full GC的概念，old gen的收集全靠mixed GC来完成。 </p>
<p>如果mixed GC实在无法跟上程序分配内存的速度，导致old gen填满无法继续进行mixed GC，就会切换到G1之外的serial old GC来收集整个GC heap（注意，包括young、old、perm）。这才是真正的full GC。Full GC之所以叫full就是要收集整个堆，只选择old gen的部分region算不上full GC。进入这种状态的G1就跟-XX:+UseSerialGC的full GC一样</p>
<h2 id="JVM_Tools">JVM Tools</h2><h3 id="JMC">JMC</h3><p><a href="http://coderbee.net/index.php/jvm/20150406/1188" target="_blank" rel="external">http://coderbee.net/index.php/jvm/20150406/1188</a>  <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/tooldescr004.html" target="_blank" rel="external">https://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/tooldescr004.html</a>  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">java -<span class="string">XX:</span>+UnlockCommercialFeatures -<span class="string">XX:</span>+FlightRecorder -<span class="string">XX:</span>StartFlightRecording=delay=<span class="number">60</span>s,duration=<span class="number">300</span>s,name=myrecording,filename=<span class="regexp">/tmp/</span>myrecording.jfr,settings=profile MyApp</span><br><span class="line">java -<span class="string">XX:</span>+UnlockCommercialFeatures -<span class="string">XX:</span>+FlightRecorder -<span class="string">XX:</span>FlightRecorderOptions=defaultrecording=<span class="literal">true</span>,disk=<span class="literal">true</span>,repository=/tmp,maxage=<span class="number">6</span>h,settings=<span class="keyword">default</span> MyApp</span><br><span class="line"></span><br><span class="line">JVM_OPTS=<span class="string">"$JVM_OPTS -XX:+UnlockCommercialFeatures -XX:+FlightRecorder -XX:StartFlightRecording=delay=60s,duration=40m,name=myrecording,filename=/tmp/52_cassandra.jfr,settings=profile"</span></span><br></pre></td></tr></table></figure>
<h3 id="MAT">MAT</h3><table>
<thead>
<tr>
<th>Name</th>
<th>Parameters</th>
<th>function</th>
</tr>
</thead>
<tbody>
<tr>
<td>ParallelGC Thread</td>
<td>-XX:ParallelGCThreads</td>
<td>parallel work during gc</td>
</tr>
<tr>
<td>Parallel Marking Threads</td>
<td>-XX:ConcGCThreads</td>
<td>parallel threads for concurrent marking</td>
</tr>
<tr>
<td>G1 concurrent refinement Threads</td>
<td>-XX:G1ConcRefinementThreads</td>
<td>update RSet concurrently with the application</td>
</tr>
</tbody>
</table>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">#远程API机器建立到Cassandra的连接，并读取文件</span><br><span class="line"><span class="string">"RMI TCP Connection(244)-192.168.47.26"</span> <span class="hexcolor">#499</span> daemon prio=<span class="number">5</span> os_prio=<span class="number">0</span> tid=<span class="number">0</span>x00007f2f36eef700 nid=<span class="number">0</span>x8d93 runnable [<span class="number">0</span>x00007f294e832000]</span><br><span class="line">   java<span class="class">.lang</span><span class="class">.Thread</span><span class="class">.State</span>: RUNNABLE</span><br><span class="line">    at java<span class="class">.net</span><span class="class">.SocketInputStream</span><span class="class">.socketRead0</span>(Native Method)</span><br><span class="line">    at java<span class="class">.net</span><span class="class">.SocketInputStream</span><span class="class">.socketRead</span>(SocketInputStream<span class="class">.java</span>:<span class="number">116</span>)</span><br><span class="line">    at java<span class="class">.net</span><span class="class">.SocketInputStream</span><span class="class">.read</span>(SocketInputStream<span class="class">.java</span>:<span class="number">170</span>)</span><br><span class="line">    at java<span class="class">.net</span><span class="class">.SocketInputStream</span><span class="class">.read</span>(SocketInputStream<span class="class">.java</span>:<span class="number">141</span>)</span><br><span class="line">    at java<span class="class">.io</span><span class="class">.BufferedInputStream</span><span class="class">.fill</span>(BufferedInputStream<span class="class">.java</span>:<span class="number">246</span>)</span><br><span class="line">    at java<span class="class">.io</span><span class="class">.BufferedInputStream</span><span class="class">.read</span>(BufferedInputStream<span class="class">.java</span>:<span class="number">265</span>)</span><br><span class="line">    - locked &lt;<span class="number">0</span>x0000000562402aa8&gt; (<span class="tag">a</span> java<span class="class">.io</span><span class="class">.BufferedInputStream</span>)</span><br><span class="line">    at java<span class="class">.io</span><span class="class">.FilterInputStream</span><span class="class">.read</span>(FilterInputStream<span class="class">.java</span>:<span class="number">83</span>)</span><br><span class="line">    at sun<span class="class">.rmi</span><span class="class">.transport</span><span class="class">.tcp</span><span class="class">.TCPTransport</span><span class="class">.handleMessages</span>(TCPTransport<span class="class">.java</span>:<span class="number">550</span>)</span><br><span class="line">    at sun<span class="class">.rmi</span><span class="class">.transport</span><span class="class">.tcp</span><span class="class">.TCPTransport</span><span class="variable">$ConnectionHandler</span>.<span class="function"><span class="title">run0</span><span class="params">(TCPTransport.java:<span class="number">826</span>)</span></span></span><br><span class="line">    at sun<span class="class">.rmi</span><span class="class">.transport</span><span class="class">.tcp</span><span class="class">.TCPTransport</span><span class="variable">$ConnectionHandler</span>.lambda<span class="variable">$run</span>$<span class="number">79</span>(TCPTransport<span class="class">.java</span>:<span class="number">683</span>)</span><br><span class="line">    at sun<span class="class">.rmi</span><span class="class">.transport</span><span class="class">.tcp</span><span class="class">.TCPTransport</span><span class="variable">$ConnectionHandler</span>$<span class="variable">$Lambda</span>$<span class="number">1</span>/<span class="number">1100329915</span>.<span class="function"><span class="title">run</span><span class="params">(Unknown Source)</span></span></span><br><span class="line">    at java<span class="class">.security</span><span class="class">.AccessController</span><span class="class">.doPrivileged</span>(Native Method)</span><br><span class="line">    at sun<span class="class">.rmi</span><span class="class">.transport</span><span class="class">.tcp</span><span class="class">.TCPTransport</span><span class="variable">$ConnectionHandler</span>.<span class="function"><span class="title">run</span><span class="params">(TCPTransport.java:<span class="number">682</span>)</span></span></span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.ThreadPoolExecutor</span><span class="class">.runWorker</span>(ThreadPoolExecutor<span class="class">.java</span>:<span class="number">1142</span>)</span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.ThreadPoolExecutor</span><span class="variable">$Worker</span>.<span class="function"><span class="title">run</span><span class="params">(ThreadPoolExecutor.java:<span class="number">617</span>)</span></span></span><br><span class="line">    at java<span class="class">.lang</span><span class="class">.Thread</span><span class="class">.run</span>(Thread<span class="class">.java</span>:<span class="number">745</span>)</span><br><span class="line"></span><br><span class="line">#Compaction最后会将几个SSTable文件合并为一个大文件，使用SSTableWriter写文件，Compaction策略为Leveled</span><br><span class="line"><span class="string">"CompactionExecutor:11"</span> <span class="hexcolor">#442</span> daemon prio=<span class="number">1</span> os_prio=<span class="number">4</span> tid=<span class="number">0</span>x00007f2974d591f0 nid=<span class="number">0</span>x8781 runnable [<span class="number">0</span>x00007f294c584000]</span><br><span class="line">   java<span class="class">.lang</span><span class="class">.Thread</span><span class="class">.State</span>: RUNNABLE</span><br><span class="line">    at java<span class="class">.io</span><span class="class">.RandomAccessFile</span><span class="class">.getFilePointer</span>(Native Method)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.io</span><span class="class">.compress</span><span class="class">.CompressedSequentialWriter</span><span class="class">.getOnDiskFilePointer</span>(CompressedSequentialWriter<span class="class">.java</span>:<span class="number">85</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.io</span><span class="class">.sstable</span><span class="class">.SSTableWriter</span><span class="class">.getOnDiskFilePointer</span>(SSTableWriter<span class="class">.java</span>:<span class="number">573</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.LeveledCompactionTask</span><span class="class">.newSSTableSegmentThresholdReached</span>(LeveledCompactionTask<span class="class">.java</span>:<span class="number">41</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.CompactionTask</span><span class="class">.runMayThrow</span>(CompactionTask<span class="class">.java</span>:<span class="number">200</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.utils</span><span class="class">.WrappedRunnable</span><span class="class">.run</span>(WrappedRunnable<span class="class">.java</span>:<span class="number">28</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.CompactionTask</span><span class="class">.executeInternal</span>(CompactionTask<span class="class">.java</span>:<span class="number">73</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.AbstractCompactionTask</span><span class="class">.execute</span>(AbstractCompactionTask<span class="class">.java</span>:<span class="number">59</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.CompactionManager</span><span class="variable">$BackgroundCompactionCandidate</span>.<span class="function"><span class="title">run</span><span class="params">(CompactionManager.java:<span class="number">263</span>)</span></span></span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.Executors</span><span class="variable">$RunnableAdapter</span>.<span class="function"><span class="title">call</span><span class="params">(Executors.java:<span class="number">511</span>)</span></span></span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.FutureTask</span><span class="class">.run</span>(FutureTask<span class="class">.java</span>:<span class="number">266</span>)</span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.ThreadPoolExecutor</span><span class="class">.runWorker</span>(ThreadPoolExecutor<span class="class">.java</span>:<span class="number">1142</span>)</span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.ThreadPoolExecutor</span><span class="variable">$Worker</span>.<span class="function"><span class="title">run</span><span class="params">(ThreadPoolExecutor.java:<span class="number">617</span>)</span></span></span><br><span class="line">    at java<span class="class">.lang</span><span class="class">.Thread</span><span class="class">.run</span>(Thread<span class="class">.java</span>:<span class="number">745</span>)</span><br><span class="line"></span><br><span class="line">#Compaction操作会删除一些数据，比如tombstone，对相同key写在一起，由于文件内容变化，文件索引需要重建</span><br><span class="line"><span class="string">"CompactionExecutor:10"</span> <span class="hexcolor">#439</span> daemon prio=<span class="number">1</span> os_prio=<span class="number">4</span> tid=<span class="number">0</span>x00007f54501de560 nid=<span class="number">0</span>x877e runnable [<span class="number">0</span>x00007f294e7b0000]</span><br><span class="line">   java<span class="class">.lang</span><span class="class">.Thread</span><span class="class">.State</span>: RUNNABLE</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.ArrayBackedSortedColumns</span>$<span class="number">2</span>.&lt;init&gt;(ArrayBackedSortedColumns<span class="class">.java</span>:<span class="number">119</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.ArrayBackedSortedColumns</span><span class="class">.batchRemoveIterator</span>(ArrayBackedSortedColumns<span class="class">.java</span>:<span class="number">117</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.ColumnFamilyStore</span><span class="class">.removeDeletedColumnsOnly</span>(ColumnFamilyStore<span class="class">.java</span>:<span class="number">1284</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.LazilyCompactedRow</span><span class="class">.removeDeleted</span>(LazilyCompactedRow<span class="class">.java</span>:<span class="number">110</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.LazilyCompactedRow</span><span class="class">.access</span>$<span class="number">600</span>(LazilyCompactedRow<span class="class">.java</span>:<span class="number">49</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.LazilyCompactedRow</span><span class="variable">$Reducer</span>.<span class="function"><span class="title">getReduced</span><span class="params">(LazilyCompactedRow.java:<span class="number">297</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.LazilyCompactedRow</span><span class="variable">$Reducer</span>.<span class="function"><span class="title">getReduced</span><span class="params">(LazilyCompactedRow.java:<span class="number">206</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.utils</span><span class="class">.MergeIterator</span><span class="variable">$OneToOne</span>.<span class="function"><span class="title">computeNext</span><span class="params">(MergeIterator.java:<span class="number">206</span>)</span></span></span><br><span class="line">    at com<span class="class">.google</span><span class="class">.common</span><span class="class">.collect</span><span class="class">.AbstractIterator</span><span class="class">.tryToComputeNext</span>(AbstractIterator<span class="class">.java</span>:<span class="number">143</span>)</span><br><span class="line">    at com<span class="class">.google</span><span class="class">.common</span><span class="class">.collect</span><span class="class">.AbstractIterator</span><span class="class">.hasNext</span>(AbstractIterator<span class="class">.java</span>:<span class="number">138</span>)</span><br><span class="line">    at com<span class="class">.google</span><span class="class">.common</span><span class="class">.collect</span><span class="class">.Iterators</span>$<span class="number">7</span>.<span class="function"><span class="title">computeNext</span><span class="params">(Iterators.java:<span class="number">645</span>)</span></span></span><br><span class="line">    at com<span class="class">.google</span><span class="class">.common</span><span class="class">.collect</span><span class="class">.AbstractIterator</span><span class="class">.tryToComputeNext</span>(AbstractIterator<span class="class">.java</span>:<span class="number">143</span>)</span><br><span class="line">    at com<span class="class">.google</span><span class="class">.common</span><span class="class">.collect</span><span class="class">.AbstractIterator</span><span class="class">.hasNext</span>(AbstractIterator<span class="class">.java</span>:<span class="number">138</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.ColumnIndex</span><span class="variable">$Builder</span>.<span class="function"><span class="title">buildForCompaction</span><span class="params">(ColumnIndex.java:<span class="number">166</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.LazilyCompactedRow</span><span class="class">.write</span>(LazilyCompactedRow<span class="class">.java</span>:<span class="number">121</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.io</span><span class="class">.sstable</span><span class="class">.SSTableWriter</span><span class="class">.append</span>(SSTableWriter<span class="class">.java</span>:<span class="number">193</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.io</span><span class="class">.sstable</span><span class="class">.SSTableRewriter</span><span class="class">.append</span>(SSTableRewriter<span class="class">.java</span>:<span class="number">126</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.CompactionTask</span><span class="class">.runMayThrow</span>(CompactionTask<span class="class">.java</span>:<span class="number">197</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.utils</span><span class="class">.WrappedRunnable</span><span class="class">.run</span>(WrappedRunnable<span class="class">.java</span>:<span class="number">28</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.CompactionTask</span><span class="class">.executeInternal</span>(CompactionTask<span class="class">.java</span>:<span class="number">73</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.AbstractCompactionTask</span><span class="class">.execute</span>(AbstractCompactionTask<span class="class">.java</span>:<span class="number">59</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.db</span><span class="class">.compaction</span><span class="class">.CompactionManager</span><span class="variable">$BackgroundCompactionCandidate</span>.<span class="function"><span class="title">run</span><span class="params">(CompactionManager.java:<span class="number">263</span>)</span></span></span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.Executors</span><span class="variable">$RunnableAdapter</span>.<span class="function"><span class="title">call</span><span class="params">(Executors.java:<span class="number">511</span>)</span></span></span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.FutureTask</span><span class="class">.run</span>(FutureTask<span class="class">.java</span>:<span class="number">266</span>)</span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.ThreadPoolExecutor</span><span class="class">.runWorker</span>(ThreadPoolExecutor<span class="class">.java</span>:<span class="number">1142</span>)</span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.ThreadPoolExecutor</span><span class="variable">$Worker</span>.<span class="function"><span class="title">run</span><span class="params">(ThreadPoolExecutor.java:<span class="number">617</span>)</span></span></span><br><span class="line">    at java<span class="class">.lang</span><span class="class">.Thread</span><span class="class">.run</span>(Thread<span class="class">.java</span>:<span class="number">745</span>)</span><br><span class="line"></span><br><span class="line">#每个Cassandra节点要和集群中的其他节点通信</span><br><span class="line"><span class="string">"MessagingService-Incoming-/192.168.48.171"</span> <span class="hexcolor">#400</span> prio=<span class="number">5</span> os_prio=<span class="number">0</span> tid=<span class="number">0</span>x00007f29f32f0e50 nid=<span class="number">0</span>x8377 runnable [<span class="number">0</span>x00007f293c9a2000]</span><br><span class="line">   java<span class="class">.lang</span><span class="class">.Thread</span><span class="class">.State</span>: RUNNABLE</span><br><span class="line">    at sun<span class="class">.nio</span><span class="class">.ch</span><span class="class">.FileDispatcherImpl</span><span class="class">.read0</span>(Native Method)</span><br><span class="line">    at sun<span class="class">.nio</span><span class="class">.ch</span><span class="class">.SocketDispatcher</span><span class="class">.read</span>(SocketDispatcher<span class="class">.java</span>:<span class="number">39</span>)</span><br><span class="line">    at sun<span class="class">.nio</span><span class="class">.ch</span><span class="class">.IOUtil</span><span class="class">.readIntoNativeBuffer</span>(IOUtil<span class="class">.java</span>:<span class="number">223</span>)</span><br><span class="line">    at sun<span class="class">.nio</span><span class="class">.ch</span><span class="class">.IOUtil</span><span class="class">.read</span>(IOUtil<span class="class">.java</span>:<span class="number">197</span>)</span><br><span class="line">    at sun<span class="class">.nio</span><span class="class">.ch</span><span class="class">.SocketChannelImpl</span><span class="class">.read</span>(SocketChannelImpl<span class="class">.java</span>:<span class="number">380</span>)</span><br><span class="line">    - locked &lt;<span class="number">0</span>x0000000330000388&gt; (<span class="tag">a</span> java<span class="class">.lang</span><span class="class">.Object</span>)</span><br><span class="line">    at sun<span class="class">.nio</span><span class="class">.ch</span><span class="class">.SocketAdaptor</span><span class="variable">$SocketInputStream</span>.<span class="function"><span class="title">read</span><span class="params">(SocketAdaptor.java:<span class="number">192</span>)</span></span></span><br><span class="line">    - locked &lt;<span class="number">0</span>x0000000330000398&gt; (<span class="tag">a</span> java<span class="class">.lang</span><span class="class">.Object</span>)</span><br><span class="line">    at sun<span class="class">.nio</span><span class="class">.ch</span><span class="class">.ChannelInputStream</span><span class="class">.read</span>(ChannelInputStream<span class="class">.java</span>:<span class="number">103</span>)</span><br><span class="line">    - locked &lt;<span class="number">0</span>x00000003300003d8&gt; (<span class="tag">a</span> sun<span class="class">.nio</span><span class="class">.ch</span><span class="class">.SocketAdaptor</span><span class="variable">$SocketInputStream</span>)</span><br><span class="line">    at net<span class="class">.jpountz</span><span class="class">.lz4</span><span class="class">.LZ4BlockInputStream</span><span class="class">.readFully</span>(LZ4BlockInputStream<span class="class">.java</span>:<span class="number">215</span>)</span><br><span class="line">    at net<span class="class">.jpountz</span><span class="class">.lz4</span><span class="class">.LZ4BlockInputStream</span><span class="class">.refill</span>(LZ4BlockInputStream<span class="class">.java</span>:<span class="number">149</span>)</span><br><span class="line">    at net<span class="class">.jpountz</span><span class="class">.lz4</span><span class="class">.LZ4BlockInputStream</span><span class="class">.read</span>(LZ4BlockInputStream<span class="class">.java</span>:<span class="number">101</span>)</span><br><span class="line">    at java<span class="class">.io</span><span class="class">.DataInputStream</span><span class="class">.readInt</span>(DataInputStream<span class="class">.java</span>:<span class="number">387</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.net</span><span class="class">.IncomingTcpConnection</span><span class="class">.receiveMessages</span>(IncomingTcpConnection<span class="class">.java</span>:<span class="number">171</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.net</span><span class="class">.IncomingTcpConnection</span><span class="class">.run</span>(IncomingTcpConnection<span class="class">.java</span>:<span class="number">88</span>)</span><br><span class="line"></span><br><span class="line">#除了Incoming也有Outgoing</span><br><span class="line"><span class="string">"MessagingService-Outgoing-/192.168.48.171"</span> <span class="hexcolor">#397</span> daemon prio=<span class="number">5</span> os_prio=<span class="number">0</span> tid=<span class="number">0</span>x00007f2f374f6ff0 nid=<span class="number">0</span>x8374 waiting on condition [<span class="number">0</span>x00007f293c9e3000]</span><br><span class="line">   java<span class="class">.lang</span><span class="class">.Thread</span><span class="class">.State</span>: WAITING (parking)</span><br><span class="line">    at sun<span class="class">.misc</span><span class="class">.Unsafe</span><span class="class">.park</span>(Native Method)</span><br><span class="line">    - parking to wait <span class="keyword">for</span>  &lt;<span class="number">0</span>x0000000330444b38&gt; (<span class="tag">a</span> java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.locks</span><span class="class">.AbstractQueuedSynchronizer</span><span class="variable">$ConditionObject</span>)</span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.locks</span><span class="class">.LockSupport</span><span class="class">.park</span>(LockSupport<span class="class">.java</span>:<span class="number">175</span>)</span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.locks</span><span class="class">.AbstractQueuedSynchronizer</span><span class="variable">$ConditionObject</span>.<span class="function"><span class="title">await</span><span class="params">(AbstractQueuedSynchronizer.java:<span class="number">2039</span>)</span></span></span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.LinkedBlockingQueue</span><span class="class">.take</span>(LinkedBlockingQueue<span class="class">.java</span>:<span class="number">442</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.utils</span><span class="class">.CoalescingStrategies</span><span class="variable">$DisabledCoalescingStrategy</span>.<span class="function"><span class="title">coalesceInternal</span><span class="params">(CoalescingStrategies.java:<span class="number">482</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.utils</span><span class="class">.CoalescingStrategies</span><span class="variable">$CoalescingStrategy</span>.<span class="function"><span class="title">coalesce</span><span class="params">(CoalescingStrategies.java:<span class="number">213</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.net</span><span class="class">.OutboundTcpConnection</span><span class="class">.run</span>(OutboundTcpConnection<span class="class">.java</span>:<span class="number">190</span>)</span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;JVM GC&lt;br&gt;
    
    </summary>
    
      <category term="jvm" scheme="http://github.com/zqhxuyuan/categories/jvm/"/>
    
    
      <category term="jvm" scheme="http://github.com/zqhxuyuan/tags/jvm/"/>
    
  </entry>
  
  <entry>
    <title>Storm Picture</title>
    <link href="http://github.com/zqhxuyuan/2016/07/15/Storm-Picture/"/>
    <id>http://github.com/zqhxuyuan/2016/07/15/Storm-Picture/</id>
    <published>2016-07-14T16:00:00.000Z</published>
    <updated>2016-07-15T10:17:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>图解Storm<br><a id="more"></a></p>
<h2 id="Storm_Component">Storm Component</h2><h2 id="消息传递">消息传递</h2><h2 id="Acker">Acker</h2><h2 id="Trident">Trident</h2><p>TODO</p>
<h2 id="Some_Code">Some Code</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ISpout</span> <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">open</span><span class="params">(Map conf, TopologyContext context, </span><br><span class="line">              SpoutOutputCollector collector)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">nextTuple</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">ack</span><span class="params">(Object msgId)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">fail</span><span class="params">(Object msgId)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitSentenceBolt</span> <span class="keyword">extends</span> <span class="title">BaseRichBolt</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> OutputCollector collector;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">(Map config, TopologyContext context,</span><br><span class="line">                        OutputCollector collector)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.collector = collector;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple)</span> </span>&#123;</span><br><span class="line">        String sentence = tuple.getStringByField(<span class="string">"sentence"</span>);</span><br><span class="line">        String[] words = sentence.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            <span class="keyword">this</span>.collector.emit(tuple, <span class="keyword">new</span> Values(word));</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">this</span>.collector.ack(tuple);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line">        declarer.declare(<span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountBolt</span> <span class="keyword">extends</span> <span class="title">BaseRichBolt</span> </span>&#123;</span><br><span class="line">    OutputCollector collector;</span><br><span class="line">    Map&lt;String, Integer&gt; counts = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">(Map stormConf, TopologyContext context,</span><br><span class="line">                        OutputCollector collector)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.collector = collector;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple)</span> </span>&#123;</span><br><span class="line">        String word = tuple.getString(<span class="number">0</span>);</span><br><span class="line">        Integer count = counts.get(word);</span><br><span class="line">        <span class="keyword">if</span> (count == <span class="keyword">null</span>) count = <span class="number">0</span>;</span><br><span class="line">        count++;</span><br><span class="line">        counts.put(word, count);</span><br><span class="line">        collector.emit(tuple, <span class="keyword">new</span> Values(word, count));</span><br><span class="line">        collector.ack(tuple);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line">        declarer.declare(<span class="keyword">new</span> Fields(<span class="string">"word"</span>, <span class="string">"count"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitSentenceBolt</span> <span class="keyword">extends</span> <span class="title">BaseBasicBolt</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple, BasicOutputCollector collector)</span></span>&#123;</span><br><span class="line">        String sentence = tuple.getString(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">for</span>(String word: sentence.split(<span class="string">" "</span>)) &#123;</span><br><span class="line">            collector.emit(<span class="keyword">new</span> Values(word));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line">        declarer.declare(<span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line">    &#125;      </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RandomSentenceSpout</span> <span class="keyword">implements</span> <span class="title">IRichSpout</span> </span>&#123;</span><br><span class="line">    SpoutOutputCollector collector;</span><br><span class="line">    Random rand;</span><br><span class="line">    String[] sentences = <span class="keyword">null</span>;</span><br><span class="line">    AtomicInteger counter = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Map conf, TopologyContext context, SpoutOutputCollector collector)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.collector = collector;</span><br><span class="line">        rand = <span class="keyword">new</span> Random();</span><br><span class="line">        counter = <span class="keyword">new</span> AtomicInteger(<span class="number">1</span>);</span><br><span class="line">        sentences = <span class="keyword">new</span> String[]&#123; <span class="string">"the cow jumped over the moon"</span>, <span class="string">"an apple a day keeps the doctor away"</span>, <span class="string">"four score and seven years ago"</span>, <span class="string">"snow white and the seven dwarfs"</span>, <span class="string">"i am at two with nature"</span> &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">nextTuple</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Utils.sleep(<span class="number">5000</span>);</span><br><span class="line">        <span class="keyword">int</span> msgId = counter.getAndIncrement();</span><br><span class="line">        String sentence = sentences[rand.nextInt(sentences.length)];</span><br><span class="line">        System.out.println(<span class="string">"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;emit sentence["</span>+msgId+<span class="string">"]:"</span> + sentence);</span><br><span class="line">        <span class="keyword">this</span>.collector.emit(<span class="keyword">new</span> Values(sentence), msgId);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">ack</span><span class="params">(Object msgId)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;ack:"</span>+msgId);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">fail</span><span class="params">(Object msgId)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;fail:"</span>+msgId);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line">    builder.setSpout(<span class="string">"spout"</span>, <span class="keyword">new</span> RandomSentenceSpout(), <span class="number">1</span>);</span><br><span class="line">    builder.setBolt(<span class="string">"split"</span>, <span class="keyword">new</span> SplitSentenceBolt(), <span class="number">2</span>).shuffleGrouping(<span class="string">"spout"</span>);</span><br><span class="line">    builder.setBolt(<span class="string">"count"</span>, <span class="keyword">new</span> WordCountBolt2(), <span class="number">2</span>).fieldsGrouping(<span class="string">"split"</span>, <span class="keyword">new</span> Fields(<span class="string">"word"</span>));</span><br><span class="line">    builder.setBolt(<span class="string">"print"</span>, <span class="keyword">new</span> PrinterBolt(), <span class="number">1</span>).shuffleGrouping(<span class="string">"count"</span>);</span><br><span class="line"></span><br><span class="line">    Config conf = <span class="keyword">new</span> Config();</span><br><span class="line">    conf.setNumAckers(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (args != <span class="keyword">null</span> &amp;&amp; args.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        conf.setNumWorkers(<span class="number">3</span>);</span><br><span class="line">        StormSubmitter.submitTopologyWithProgressBar(args[<span class="number">0</span>], conf, builder.createTopology());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        conf.setMaxTaskParallelism(<span class="number">3</span>);</span><br><span class="line">        LocalCluster cluster = <span class="keyword">new</span> LocalCluster();</span><br><span class="line">        cluster.submitTopology(<span class="string">"word-count"</span>, conf, builder.createTopology());</span><br><span class="line">        Thread.sleep(<span class="number">32000</span>);</span><br><span class="line">        cluster.shutdown();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;图解Storm&lt;br&gt;
    
    </summary>
    
      <category term="storm" scheme="http://github.com/zqhxuyuan/categories/storm/"/>
    
    
      <category term="storm" scheme="http://github.com/zqhxuyuan/tags/storm/"/>
    
  </entry>
  
  <entry>
    <title>Cassandra G1 GC</title>
    <link href="http://github.com/zqhxuyuan/2016/07/07/Cassandra-GC-G1/"/>
    <id>http://github.com/zqhxuyuan/2016/07/07/Cassandra-GC-G1/</id>
    <published>2016-07-06T16:00:00.000Z</published>
    <updated>2016-09-07T10:16:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>Cassandra G1 GC使用，案例<br>参考文档：<br><a href="&#109;&#97;&#x69;&#108;&#116;&#111;&#x3a;&#104;&#x74;&#x74;&#112;&#x73;&#58;&#x2f;&#x2f;&#109;&#x65;&#x64;&#105;&#x75;&#x6d;&#x2e;&#x63;&#x6f;&#109;&#47;&#x40;&#109;&#108;&#x6f;&#x77;&#x69;&#99;&#107;&#x69;&#47;&#109;&#x6f;&#x76;&#101;&#x2d;&#99;&#x61;&#x73;&#115;&#x61;&#110;&#x64;&#114;&#x61;&#x2d;&#50;&#45;&#x31;&#x2d;&#116;&#x6f;&#x2d;&#x67;&#x31;&#45;&#103;&#x61;&#x72;&#x62;&#97;&#x67;&#x65;&#x2d;&#99;&#x6f;&#108;&#x6c;&#101;&#x63;&#116;&#x6f;&#x72;&#x2d;&#x62;&#57;&#x66;&#x62;&#x32;&#x37;&#51;&#54;&#x35;&#x35;&#48;&#x39;&#x23;&#46;&#55;&#117;&#103;&#103;&#x37;&#x38;&#x6e;&#x71;&#55;">&#104;&#x74;&#x74;&#112;&#x73;&#58;&#x2f;&#x2f;&#109;&#x65;&#x64;&#105;&#x75;&#x6d;&#x2e;&#x63;&#x6f;&#109;&#47;&#x40;&#109;&#108;&#x6f;&#x77;&#x69;&#99;&#107;&#x69;&#47;&#109;&#x6f;&#x76;&#101;&#x2d;&#99;&#x61;&#x73;&#115;&#x61;&#110;&#x64;&#114;&#x61;&#x2d;&#50;&#45;&#x31;&#x2d;&#116;&#x6f;&#x2d;&#x67;&#x31;&#45;&#103;&#x61;&#x72;&#x62;&#97;&#x67;&#x65;&#x2d;&#99;&#x6f;&#108;&#x6c;&#101;&#x63;&#116;&#x6f;&#x72;&#x2d;&#x62;&#57;&#x66;&#x62;&#x32;&#x37;&#51;&#54;&#x35;&#x35;&#48;&#x39;&#x23;&#46;&#55;&#117;&#103;&#103;&#x37;&#x38;&#x6e;&#x71;&#55;</a><br><a href="http://www.oracle.com/technetwork/cn/articles/java/g1gc-1984535-zhs.html" target="_blank" rel="external">http://www.oracle.com/technetwork/cn/articles/java/g1gc-1984535-zhs.html</a><br><a id="more"></a></p>
<h2 id="G1_GC">G1 GC</h2><p>关键词：调整</p>
<p>G1 GC 有一个力求达到的暂停时间目标（软实时）。在年轻代回收期间，G1 GC 会调整其年轻代空间（eden 和存活空间大小）以满足软实时目标。<br>在混合回收期间，G1 GC 会根据混合垃圾回收的目标次数调整所回收的旧区域数量，并调整堆的每个区域中存活对象的百分比，以及总体可接受的堆废物百分比。  </p>
<h2 id="从CMS迁移到G1">从CMS迁移到G1</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#MAX_HEAP_SIZE=<span class="string">"8192M"</span></span></span><br><span class="line"><span class="preprocessor">#HEAP_NEWSIZE=<span class="string">"2048M"</span></span></span><br><span class="line">MAX_HEAP_SIZE=<span class="string">"16384M"</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor">#calculate_heap_sizes函数定义的地方可以不用删除，只要调用的地方去掉就可以</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">if</span> [ <span class="string">"x$MAX_HEAP_SIZE"</span> = <span class="string">"x"</span> ] &amp;&amp; [ <span class="string">"x$HEAP_NEWSIZE"</span> = <span class="string">"x"</span> ]; then</span></span><br><span class="line"><span class="preprocessor">#    calculate_heap_sizes</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">else</span></span></span><br><span class="line"><span class="preprocessor">#    <span class="keyword">if</span> [ <span class="string">"x$MAX_HEAP_SIZE"</span> = <span class="string">"x"</span> ] || [ <span class="string">"x$HEAP_NEWSIZE"</span> = <span class="string">"x"</span> ]; then</span></span><br><span class="line"><span class="preprocessor">#        echo <span class="string">"please set or unset MAX_HEAP_SIZE and HEAP_NEWSIZE in pairs (see cassandra-env.sh)"</span></span></span><br><span class="line"><span class="preprocessor">#        exit <span class="number">1</span></span></span><br><span class="line"><span class="preprocessor">#    fi</span></span><br><span class="line"><span class="preprocessor">#fi</span></span><br><span class="line"></span><br><span class="line">JVM_OPTS=<span class="string">"$JVM_OPTS -Xms$&#123;MAX_HEAP_SIZE&#125;"</span></span><br><span class="line">JVM_OPTS=<span class="string">"$JVM_OPTS -Xmx$&#123;MAX_HEAP_SIZE&#125;"</span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -Xmn$&#123;HEAP_NEWSIZE&#125;"</span>  #G1不需要配置New</span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -Xms16G"</span>   #这些原本是不存在的，都去掉</span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -Xmx16G"</span></span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -Xmn4G"</span></span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -XX:MaxDirectMemorySize=6G"</span></span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -XX:+HeapDumpOnOutOfMemoryError"</span></span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -XX:+UseParNewGC"</span></span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -XX:+UseConcMarkSweepGC"</span></span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -XX:+CMSParallelRemarkEnabled"</span></span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -XX:SurvivorRatio=8"</span></span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -XX:MaxTenuringThreshold=1"</span></span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -XX:CMSInitiatingOccupancyFraction=75"</span></span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -XX:+UseCMSInitiatingOccupancyOnly"</span></span></span><br><span class="line"> JVM_OPTS=<span class="string">"$JVM_OPTS -XX:+UseTLAB"</span></span><br><span class="line"> JVM_OPTS=<span class="string">"$JVM_OPTS -XX:CompileCommandFile=$CASSANDRA_CONF/hotspot_compiler"</span></span><br><span class="line"><span class="preprocessor">#JVM_OPTS=<span class="string">"$JVM_OPTS -XX:CMSWaitDuration=10000"</span></span></span><br><span class="line">JVM_OPTS=<span class="string">"$JVM_OPTS -XX:+UseG1GC"</span></span><br><span class="line">JVM_OPTS=<span class="string">"$JVM_OPTS -XX:MaxGCPauseMillis=500"</span></span><br><span class="line">JVM_OPTS=<span class="string">"$JVM_OPTS -XX:G1RSetUpdatingPauseTimePercent=5"</span></span><br><span class="line">JVM_OPTS=<span class="string">"$JVM_OPTS -XX:+AlwaysPreTouch"</span></span><br><span class="line">JVM_OPTS=<span class="string">"$JVM_OPTS -XX:-UseBiasedLocking"</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor">## note: bash evals ‘<span class="number">1.7</span>.x’ as &gt; ‘<span class="number">1.7</span>’ so this is really a &gt;= <span class="number">1.7</span> jvm check</span></span><br><span class="line"><span class="preprocessor">#<span class="keyword">if</span> &#123; [ <span class="string">"$JVM_VERSION"</span> \&gt; <span class="string">"1.7"</span> ] &amp;&amp; [ <span class="string">"$JVM_VERSION"</span> \&lt; <span class="string">"1.8.0"</span> ] &amp;&amp; [ <span class="string">"$JVM_PATCH_VERSION"</span> -ge <span class="string">"60"</span> ]; &#125; || [ <span class="string">"$JVM_VERSION"</span> \&gt; <span class="string">"1.8"</span> ] ; then</span></span><br><span class="line"><span class="preprocessor">#    JVM_OPTS=<span class="string">"$JVM_OPTS -XX:+CMSParallelInitialMarkEnabled -XX:+CMSEdenChunksRecordAlways -XX:CMSWaitDuration=10000"</span></span></span><br><span class="line"><span class="preprocessor">#fi</span></span><br></pre></td></tr></table></figure>
<p>cassandra.yaml需要修改的配置：</p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">table_allocation_type</span>: <span class="string">offheap_objects</span></span><br><span class="line"><span class="attribute">memtable_offheap_space_in_mb</span>: <span class="string">2048</span></span><br></pre></td></tr></table></figure>
<p>CMS切换为G1脚本：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/<span class="operator"><span class="keyword">install</span>/cassandra/conf</span><br><span class="line">mv cassandra-env.sh /usr/<span class="keyword">install</span>/cassandra/conf/cassandra-env.sh.cms_20160621</span><br><span class="line">host=<span class="string">`ifconfig | grep "192.168.4" | awk '/inet addr/&#123;sub("addr:",""); print $2&#125;'`</span></span><br><span class="line">comp=<span class="string">`cat /usr/install/cassandra/conf/cassandra.yaml | grep compactors`</span></span><br><span class="line">wget <span class="keyword">http</span>://<span class="number">192.168</span><span class="number">.47</span><span class="number">.211</span>:<span class="number">8000</span>/cassandra-env.sh</span><br><span class="line">sed -<span class="keyword">i</span> -<span class="keyword">e</span> <span class="string">"s/localhost/$host/g"</span> cassandra-env.sh</span><br><span class="line">sed -<span class="keyword">i</span> -<span class="keyword">e</span> <span class="string">"s/heap_buffers/offheap_objects/g"</span> cassandra.yaml</span><br><span class="line"></span><br><span class="line">echo <span class="string">"memtable_offheap_space_in_mb: 2048"</span> &gt;&gt; cassandra.yaml</span><br><span class="line">sed -<span class="keyword">i</span> -<span class="keyword">e</span> <span class="string">"s/$comp/concurrent_compactors: 4/g"</span> cassandra.yaml</span><br><span class="line"></span><br><span class="line">/usr/<span class="keyword">install</span>/cassandra/<span class="keyword">bin</span>/nodetool <span class="keyword">flush</span></span><br><span class="line">/usr/<span class="keyword">install</span>/cassandra/<span class="keyword">bin</span>/nodetool stopdaemon</span><br><span class="line">/usr/<span class="keyword">install</span>/cassandra/<span class="keyword">bin</span>/cassandra</span></span><br></pre></td></tr></table></figure>
<h2 id="比较GC时间">比较GC时间</h2><h3 id="使用CMS，FGC问题很严重">使用CMS，FGC问题很严重</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">163</span>仍然使用CMS，<span class="number">22</span>号：</span><br><span class="line">[admin@fp-cass048163 ~]$ jstat -gcutil <span class="number">30764</span> <span class="number">1000</span></span><br><span class="line">  S0     S1     E      O      P     YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">  <span class="number">0.00</span>   <span class="number">3.91</span>  <span class="number">58.22</span>  <span class="number">29.55</span>  <span class="number">59.81</span> <span class="number">1554565</span> <span class="number">45480.263</span>  <span class="number">7069</span> <span class="number">2087.903</span> <span class="number">47568.166</span></span><br><span class="line">  <span class="number">6.93</span>   <span class="number">0.00</span>   <span class="number">5.53</span>  <span class="number">29.62</span>  <span class="number">59.81</span> <span class="number">1554566</span> <span class="number">45480.290</span>  <span class="number">7069</span> <span class="number">2087.903</span> <span class="number">47568.193</span></span><br><span class="line">  <span class="number">6.93</span>   <span class="number">0.00</span>  <span class="number">46.36</span>  <span class="number">29.62</span>  <span class="number">59.81</span> <span class="number">1554566</span> <span class="number">45480.290</span>  <span class="number">7069</span> <span class="number">2087.903</span> <span class="number">47568.193</span></span><br><span class="line"><span class="number">24</span>号：</span><br><span class="line">[admin@fp-cass048163 ~]$ jstat -gcutil <span class="number">30764</span> <span class="number">1000</span></span><br><span class="line">  S0     S1     E      O      P     YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">  <span class="number">0.00</span>   <span class="number">6.79</span>  <span class="number">26.05</span>  <span class="number">68.01</span>  <span class="number">59.81</span> <span class="number">1613189</span> <span class="number">47191.337</span>  <span class="number">7335</span> <span class="number">2159.359</span> <span class="number">49350.696</span></span><br><span class="line">  <span class="number">0.00</span>   <span class="number">6.79</span>  <span class="number">63.72</span>  <span class="number">68.01</span>  <span class="number">59.81</span> <span class="number">1613189</span> <span class="number">47191.337</span>  <span class="number">7335</span> <span class="number">2159.359</span> <span class="number">49350.696</span></span><br><span class="line">  <span class="number">6.86</span>   <span class="number">0.00</span>   <span class="number">2.59</span>  <span class="number">68.13</span>  <span class="number">59.81</span> <span class="number">1613190</span> <span class="number">47191.368</span>  <span class="number">7335</span> <span class="number">2159.359</span> <span class="number">49350.726</span></span><br><span class="line">  <span class="number">6.86</span>   <span class="number">0.00</span>  <span class="number">38.73</span>  <span class="number">68.13</span>  <span class="number">59.81</span> <span class="number">1613190</span> <span class="number">47191.368</span>  <span class="number">7335</span> <span class="number">2159.359</span> <span class="number">49350.726</span></span><br><span class="line">  <span class="number">6.86</span>   <span class="number">0.00</span>  <span class="number">77.89</span>  <span class="number">68.13</span>  <span class="number">59.81</span> <span class="number">1613190</span> <span class="number">47191.368</span>  <span class="number">7335</span> <span class="number">2159.359</span> <span class="number">49350.726</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>统计时间</th>
<th>GC次数（超过200ms）</th>
<th>超过1s</th>
</tr>
</thead>
<tbody>
<tr>
<td>2016-06-01</td>
<td>73</td>
<td>16</td>
</tr>
<tr>
<td>2016-06-02</td>
<td>469</td>
<td>103</td>
</tr>
<tr>
<td>2016-06-03</td>
<td>81</td>
<td>2</td>
</tr>
<tr>
<td>2016-06-04</td>
<td>228</td>
<td>28</td>
</tr>
<tr>
<td>2016-06-05</td>
<td>618</td>
<td>137</td>
</tr>
<tr>
<td>2016-06-06</td>
<td>312</td>
<td>47</td>
</tr>
<tr>
<td>2016-06-07</td>
<td>63</td>
<td>9</td>
</tr>
<tr>
<td>2016-06-08</td>
<td>273</td>
<td>60</td>
</tr>
<tr>
<td>2016-06-09</td>
<td>64</td>
<td>7</td>
</tr>
<tr>
<td>2016-06-10</td>
<td>66</td>
<td>8</td>
</tr>
<tr>
<td>2016-06-11</td>
<td>79</td>
<td>8</td>
</tr>
<tr>
<td>2016-06-12</td>
<td>1037</td>
<td>244</td>
</tr>
<tr>
<td>2016-06-13</td>
<td>1160</td>
<td>277</td>
</tr>
<tr>
<td>2016-06-14</td>
<td>65</td>
<td>9</td>
</tr>
<tr>
<td>2016-06-15</td>
<td>82</td>
<td>10</td>
</tr>
<tr>
<td>2016-06-16</td>
<td>880</td>
<td>221</td>
</tr>
<tr>
<td>2016-06-17</td>
<td>622</td>
<td>137</td>
</tr>
<tr>
<td>2016-06-18</td>
<td>1454</td>
<td>343</td>
</tr>
<tr>
<td>2016-06-19</td>
<td>68</td>
<td>5</td>
</tr>
<tr>
<td>2016-06-20</td>
<td>654</td>
<td>175</td>
</tr>
<tr>
<td>2016-06-21</td>
<td>338</td>
<td>87</td>
</tr>
</tbody>
</table>
<p>最后一条数据6-21号这天数据不是很准，因为这一天从CMS切换到G1</p>
<h3 id="使用G1">使用G1</h3><p>在162上测试，YGC超过200ms的都很少了</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[admin@fp-cass048162 logs]$ tail -f system.<span class="built_in">log</span> | grep GC</span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">21</span> <span class="number">11</span>:<span class="number">14</span>:<span class="number">12</span>,<span class="number">947</span> - G1 Young Generation GC in <span class="number">246</span>ms.  G1 Eden Space: <span class="number">8917090304</span> -&gt; <span class="number">0</span>; G1 Old Gen: <span class="number">6059796976</span> -&gt; <span class="number">6071479536</span>; G1 Survivor Space: <span class="number">167772160</span> -&gt; <span class="number">293601280</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">21</span> <span class="number">11</span>:<span class="number">15</span>:<span class="number">36</span>,<span class="number">990</span> - G1 Young Generation GC in <span class="number">245</span>ms.  G1 Eden Space: <span class="number">8506048512</span> -&gt; <span class="number">0</span>; G1 Old Gen: <span class="number">6200014560</span> -&gt; <span class="number">6216498552</span>;</span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">21</span> <span class="number">13</span>:<span class="number">22</span>:<span class="number">22</span>,<span class="number">631</span> - G1 Young Generation GC in <span class="number">283</span>ms.  G1 Eden Space: <span class="number">8849981440</span> -&gt; <span class="number">0</span>; G1 Old Gen: <span class="number">5812656096</span> -&gt; <span class="number">5823218856</span>; G1 Survivor Space: <span class="number">327155712</span> -&gt; <span class="number">335544320</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">21</span> <span class="number">13</span>:<span class="number">50</span>:<span class="number">59</span>,<span class="number">920</span> - G1 Young Generation GC in <span class="number">241</span>ms.  G1 Eden Space: <span class="number">10032775168</span> -&gt; <span class="number">0</span>; G1 Old Gen: <span class="number">4619885032</span> -&gt; <span class="number">4633499088</span>; G1 Survivor Space: <span class="number">268435456</span> -&gt; <span class="number">276824064</span></span><br></pre></td></tr></table></figure>
<p>✅ 162开启G1，22号到24号为止，没有发生一次FGC，下面是24号统计的信息：YGCT/YGC=2496/28780=0.086=86ms</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[admin@fp-cass048162 ~]$ jstat -gcutil <span class="number">47069</span> <span class="number">1000</span></span><br><span class="line">  S0     S1     E      O      P     YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">  <span class="number">0.00</span> <span class="number">100.00</span>  <span class="number">12.13</span>  <span class="number">78.58</span>  <span class="number">97.10</span>  <span class="number">28780</span> <span class="number">2496.678</span>     <span class="number">0</span>    <span class="number">0.000</span> <span class="number">2496.678</span></span><br><span class="line">  <span class="number">0.00</span> <span class="number">100.00</span>  <span class="number">24.17</span>  <span class="number">78.58</span>  <span class="number">97.10</span>  <span class="number">28780</span> <span class="number">2496.678</span>     <span class="number">0</span>    <span class="number">0.000</span> <span class="number">2496.678</span></span><br><span class="line">  <span class="number">0.00</span> <span class="number">100.00</span>  <span class="number">35.87</span>  <span class="number">78.58</span>  <span class="number">97.10</span>  <span class="number">28780</span> <span class="number">2496.678</span>     <span class="number">0</span>    <span class="number">0.000</span> <span class="number">2496.678</span></span><br><span class="line">  <span class="number">0.00</span> <span class="number">100.00</span>  <span class="number">47.74</span>  <span class="number">78.58</span>  <span class="number">97.10</span>  <span class="number">28780</span> <span class="number">2496.678</span>     <span class="number">0</span>    <span class="number">0.000</span> <span class="number">2496.678</span></span><br><span class="line">  <span class="number">0.00</span> <span class="number">100.00</span>  <span class="number">59.27</span>  <span class="number">78.58</span>  <span class="number">97.10</span>  <span class="number">28780</span> <span class="number">2496.678</span>     <span class="number">0</span>    <span class="number">0.000</span> <span class="number">2496.678</span></span><br></pre></td></tr></table></figure>
<p>✅ 171开启G1，22号（运行一天，没有发生一次FGC)：YGCT/YGC=494/6258=0.078=78ms</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[admin@cass048171 ~]$ jstat -gcutil <span class="number">27229</span> <span class="number">1000</span></span><br><span class="line">  S0     S1     E      O      P     YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">  <span class="number">0.00</span> <span class="number">100.00</span>  <span class="number">60.11</span>  <span class="number">80.74</span>  <span class="number">96.16</span>   <span class="number">6258</span>  <span class="number">494.761</span>     <span class="number">0</span>    <span class="number">0.000</span>  <span class="number">494.761</span></span><br><span class="line">  <span class="number">0.00</span> <span class="number">100.00</span>  <span class="number">74.05</span>  <span class="number">80.74</span>  <span class="number">96.16</span>   <span class="number">6258</span>  <span class="number">494.761</span>     <span class="number">0</span>    <span class="number">0.000</span>  <span class="number">494.761</span></span><br><span class="line">  <span class="number">0.00</span> <span class="number">100.00</span>  <span class="number">88.07</span>  <span class="number">80.74</span>  <span class="number">96.16</span>   <span class="number">6258</span>  <span class="number">494.761</span>     <span class="number">0</span>    <span class="number">0.000</span>  <span class="number">494.761</span></span><br><span class="line">  <span class="number">0.00</span> <span class="number">100.00</span>   <span class="number">7.06</span>  <span class="number">80.81</span>  <span class="number">96.16</span>   <span class="number">6259</span>  <span class="number">494.846</span>     <span class="number">0</span>    <span class="number">0.000</span>  <span class="number">494.846</span></span><br></pre></td></tr></table></figure>
<h3 id="GC统计历史数据">GC统计历史数据</h3><blockquote>
<p>注：由于Cassandra日志文件会进行滚动，所以当前日志文件用grep，以前的文件用zgrep统计<br>统计CMS一般在压缩文件中查找：ConcurrentMarkSweep<br>统计G1一般在当前文件中查找：G1 Old Generation GC（当然也可以在压缩文件中找）</p>
</blockquote>
<h4 id="所有GC统计（当前文件，压缩文件）：">所有GC统计（当前文件，压缩文件）：</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">当前文件，所有GC，按天统计</span><br><span class="line">$ grep 'GC' /usr/<span class="operator"><span class="keyword">install</span>/cassandra/<span class="keyword">logs</span>/<span class="keyword">system</span>.<span class="keyword">log</span> | awk <span class="string">'&#123;print $4&#125;'</span> | <span class="keyword">sort</span> | uniq -<span class="keyword">c</span> | awk <span class="string">'&#123;print $2" "$1&#125;'</span> | tail</span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">25</span> <span class="number">29</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">26</span> <span class="number">55</span></span><br><span class="line">压缩文件，所有GC，按天统计</span><br><span class="line">$ zgrep <span class="string">'GC'</span> /usr/<span class="keyword">install</span>/cassandra/<span class="keyword">logs</span>/<span class="keyword">system</span>.<span class="keyword">log</span>.*.zip | awk <span class="string">'&#123;print $4&#125;'</span> | <span class="keyword">sort</span> | uniq -<span class="keyword">c</span> | awk <span class="string">'&#123;print $2" "$1&#125;'</span> | tail</span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">16</span> <span class="number">880</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">622</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">18</span> <span class="number">1454</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">19</span> <span class="number">68</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">20</span> <span class="number">654</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">21</span> <span class="number">362</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">22</span> <span class="number">22</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">84</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">46</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">25</span> <span class="number">62</span></span></span><br></pre></td></tr></table></figure>
<h4 id="超过10s的GC（当前文件，压缩文件）">超过10s的GC（当前文件，压缩文件）</h4><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ zgrep 'GC in [<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]' /usr/install/cassandra/logs/system.log.*.zip | awk '&#123;print $4&#125;' | sort | uniq -c | awk '&#123;print $2" "$1&#125;' | tail</span><br><span class="line">2016-06-13 280</span><br><span class="line">2016-06-14 10</span><br><span class="line">2016-06-15 11</span><br><span class="line">2016-06-16 224</span><br><span class="line">2016-06-17 139</span><br><span class="line">2016-06-18 346</span><br><span class="line">2016-06-19 5</span><br><span class="line">2016-06-20 177</span><br><span class="line">2016-06-21 88</span><br><span class="line">2016-06-23 1</span><br><span class="line"></span><br><span class="line">$ grep 'GC in [<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]' /usr/install/cassandra/logs/system.log | awk '&#123;print $4&#125;' | sort | uniq -c | awk '&#123;print $2" "$1&#125;' | tail</span><br></pre></td></tr></table></figure>
<h4 id="当前文件统计（所有GC，超过10s）*">当前文件统计（所有GC，超过10s）<em>*</em></h4><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">当前文件，所有GC，按时段统计</span><br><span class="line">grep 'GC' /usr/install/cassandra/logs/system.<span class="keyword">log</span> | awk '&#123;<span class="keyword">print</span> <span class="label">$4</span><span class="string">":"</span><span class="literal">substr</span>(<span class="label">$5</span>,0,2)&#125;' | uniq -c | awk '&#123;<span class="keyword">print</span> <span class="label">$2</span><span class="string">" "</span><span class="label">$1&#125;</span>' | <span class="keyword">sort</span> </span><br><span class="line">当前文件，超过10s的所有GC，按天统计</span><br><span class="line">grep 'GC <span class="keyword">in</span> [0-9][0-9][0-9][0-9]' /usr/install/cassandra/logs/system.<span class="keyword">log</span> | awk '&#123;<span class="keyword">print</span> <span class="label">$4&#125;</span>' | uniq -c | awk '&#123;<span class="keyword">print</span> <span class="label">$2</span><span class="string">" "</span><span class="label">$1&#125;</span>' | <span class="keyword">sort</span> </span><br><span class="line">当前文件，超过10s的GC，按时段统计</span><br><span class="line">grep 'GC <span class="keyword">in</span> [0-9][0-9][0-9][0-9]' /usr/install/cassandra/logs/system.<span class="keyword">log</span> | awk '&#123;<span class="keyword">print</span> <span class="label">$4</span><span class="string">":"</span><span class="literal">substr</span>(<span class="label">$5</span>,0,2)&#125;' | uniq -c | awk '&#123;<span class="keyword">print</span> <span class="label">$2</span><span class="string">" "</span><span class="label">$1&#125;</span>' | <span class="keyword">sort</span></span><br></pre></td></tr></table></figure>
<h4 id="压缩文件统计（所有GC，超过10s）">压缩文件统计（所有GC，超过10s）</h4><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">压缩文件，所有GC，按时段统计</span><br><span class="line">zgrep 'GC' /usr/install/cassandra/logs/system.<span class="keyword">log</span>.1.<span class="keyword">zip</span> | awk '&#123;<span class="keyword">print</span> <span class="label">$4</span><span class="string">":"</span><span class="literal">substr</span>(<span class="label">$5</span>,0,2)&#125;' | <span class="keyword">sort</span> | uniq -c | awk '&#123;<span class="keyword">print</span> <span class="label">$2</span><span class="string">" "</span><span class="label">$1&#125;</span>' </span><br><span class="line">压缩文件，超过10s的所有GC，按天统计</span><br><span class="line">zgrep 'GC <span class="keyword">in</span> [0-9][0-9][0-9][0-9]ms' /usr/install/cassandra/logs/system.<span class="keyword">log</span>.1.<span class="keyword">zip</span> | awk '&#123;<span class="keyword">print</span> <span class="label">$4&#125;</span>' | <span class="keyword">sort</span> | uniq -c | awk '&#123;<span class="keyword">print</span> <span class="label">$2</span><span class="string">" "</span><span class="label">$1&#125;</span>'</span><br></pre></td></tr></table></figure>
<h4 id="压缩文件的CMS，当前文件的G1">压缩文件的CMS，当前文件的G1</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">压缩文件，CMS GC，按天统计</span><br><span class="line">$ zgrep 'ConcurrentMarkSweep' /usr/<span class="operator"><span class="keyword">install</span>/cassandra/<span class="keyword">logs</span>/<span class="keyword">system</span>.<span class="keyword">log</span>.*.zip | awk <span class="string">'&#123;print $4&#125;'</span> | <span class="keyword">sort</span> | uniq -<span class="keyword">c</span> | awk <span class="string">'&#123;print $2" "$1&#125;'</span> | tail</span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">12</span> <span class="number">1023</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">13</span> <span class="number">1138</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">14</span> <span class="number">60</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">15</span> <span class="number">75</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">16</span> <span class="number">868</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">17</span> <span class="number">612</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">18</span> <span class="number">1438</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">19</span> <span class="number">62</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">20</span> <span class="number">635</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">21</span> <span class="number">326</span></span><br><span class="line">当前文件，G1 GC，按天统计</span><br><span class="line">$ grep <span class="string">'G1 Old Generation GC'</span> /usr/<span class="keyword">install</span>/cassandra/<span class="keyword">logs</span>/<span class="keyword">system</span>.<span class="keyword">log</span> | awk <span class="string">'&#123;print $4&#125;'</span> | <span class="keyword">sort</span> | uniq -<span class="keyword">c</span> | awk <span class="string">'&#123;print $2" "$1&#125;'</span></span></span><br></pre></td></tr></table></figure>
<h2 id="并不是说不会FGC了">并不是说不会FGC了</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[admin@fp-cass048162 ~]$ grep 'GC' /usr/install/cassandra/logs/system.log | wc -l</span><br><span class="line">134</span><br><span class="line">[<span class="link_label">admin@fp-cass048162 ~</span>]$ grep 'GC in [<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]' /usr/install/cassandra/logs/system.log | wc -l</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<p>咦，还是有一个超过10s的GC：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">08</span>:<span class="number">09</span>:<span class="number">25</span>,<span class="number">970</span> - G1 Young Generation GC in <span class="number">206</span>ms.   G1 Eden Space: <span class="number">9646899200</span> -&gt; <span class="number">0</span>; G1 Old Gen: <span class="number">4639205352</span> -&gt; <span class="number">5004258312</span>;   G1 Survivor Space: <span class="number">511705088</span> -&gt; <span class="number">176160768</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">09</span>:<span class="number">47</span>:<span class="number">38</span>,<span class="number">757</span> - G1 Young Generation GC in <span class="number">3770</span>ms.  G1 Eden Space: <span class="number">8027897856</span> -&gt; <span class="number">0</span>; G1 Old Gen: <span class="number">6979980640</span> -&gt; <span class="number">13109433016</span>;  G1 Survivor Space: <span class="number">150994944</span> -&gt; <span class="number">1023410176</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">09</span>:<span class="number">47</span>:<span class="number">39</span>,<span class="number">607</span> - G1 Young Generation GC in <span class="number">780</span>ms.   G1 Eden Space: <span class="number">159383552</span> -&gt; <span class="number">0</span>;  G1 Old Gen: <span class="number">13109433016</span> -&gt; <span class="number">14160738744</span>; G1 Survivor Space: <span class="number">1023410176</span> -&gt; <span class="number">75497472</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">10</span>:<span class="number">18</span>:<span class="number">11</span>,<span class="number">557</span> - G1 Young Generation GC in <span class="number">257</span>ms.   G1 Eden Space: <span class="number">771751936</span> -&gt; <span class="number">0</span>;  G1 Old Gen: <span class="number">6197828584</span> -&gt; <span class="number">5781983928</span>;   G1 Survivor Space: <span class="number">83886080</span> -&gt; <span class="number">92274688</span></span><br></pre></td></tr></table></figure>
<p>同样的情况发生在171上：24号：<br>YGCT/YGC=3051/57162=0.053=53ms<br>FTCT/FGC=63/2=30s！！！</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[admin@cass048171 ~]$ jstat -gcutil <span class="number">27229 1000</span></span><br><span class="line">  S0     S1     E      O      P     YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">  <span class="number">0.00 100.00</span>  25.22  86.72  96.99  <span class="number">57162 3051</span>.206     2   <span class="number">63.878 31</span>15.084</span><br><span class="line">  <span class="number">0.00 100.00</span>  53.48  86.72  96.99  <span class="number">57162 3051</span>.206     2   <span class="number">63.878 31</span>15.084</span><br><span class="line">  <span class="number">0.00 100.00</span>  78.70  86.65  96.99  <span class="number">57162 3051</span>.206     2   <span class="number">63.878 31</span>15.084</span><br><span class="line"></span><br><span class="line">[admin@cass048171 ~]$ grep 'GC in [0-9][0-9][0-9][0-9]' /usr/install/cassandra/logs/system.log | wc -l</span><br><span class="line">4</span><br><span class="line">[admin@cass048171 ~]$ grep 'GC' /usr/install/cassandra/logs/system.log | wc -l</span><br><span class="line">24</span><br><span class="line"></span><br><span class="line"><span class="number">2016-06-23</span> <span class="number">10:33:10,70</span>6 - G1 Young Generation GC in 353ms.    G1 Eden Space: <span class="number">10049552384</span> -&gt; 0<span class="comment">; G1 Old Gen: 5018700376 -&gt; 5039128024;   G1 Survivor Space: 134217728 -&gt; 914358272</span></span><br><span class="line"><span class="number">2016-06-23</span> <span class="number">10:33:36,36</span>6 - G1 Young Generation GC in 21135ms.  G1 Eden Space: <span class="number">8438939648</span> -&gt; 0<span class="comment">;  G1 Old Gen: 5039128024 -&gt; 13030534304;  G1 Survivor Space: 914358272 -&gt; 1174405120   🙅</span></span><br><span class="line"><span class="number">2016-06-23</span> <span class="number">10:33:37,66</span>7 - G1 Young Generation GC in 1209ms.   G1 Eden Space: <span class="number">8388608</span> -&gt; 0<span class="comment">;     G1 Old Gen: 13030534304 -&gt; 14225910944; G1 Survivor Space: 1174405120 -&gt; 58720256    🙅</span></span><br><span class="line"><span class="number">2016-06-23</span> <span class="number">10:33:38,63</span>8 - G1 Young Generation GC in 259ms.    G1 Eden Space: <span class="number">796917760</span> -&gt; 0<span class="comment">;   G1 Old Gen: 14225910944 -&gt; 14276242592; G1 Survivor Space: 58720256 -&gt; 75497472</span></span><br><span class="line"><span class="number">2016-06-23</span> <span class="number">10:33:48,06</span>6 - G1 Young Generation GC in 1096ms.   G1 Eden Space: <span class="number">771751936</span> -&gt; 0<span class="comment">;   G1 Old Gen: 16117542048 -&gt; 15689723040; G1 Survivor Space: 83886080 -&gt; 33554432      🙅</span></span><br><span class="line"><span class="number">2016-06-23</span> <span class="number">17:58:26,83</span>3 - G1 Young Generation GC in 261ms.    G1 Eden Space: <span class="number">796917760</span> -&gt; 0<span class="comment">;   G1 Old Gen: 13005169216 -&gt; 12363114936;</span></span><br><span class="line"><span class="number">2016-06-23</span> <span class="number">21:01:43,202</span> - G1 Young Generation GC in 268ms.    G1 Eden Space: <span class="number">796917760</span> -&gt; 0<span class="comment">;   G1 Old Gen: 14009140736 -&gt; 13586786088; G1 Survivor Space: 58720256 -&gt; 67108864</span></span><br><span class="line"><span class="number">2016-06-23</span> <span class="number">22:54:34,50</span>8 - G1 Old Generation GC in 35275ms.                                     G1 Old Gen: <span class="number">14394600712</span> -&gt; <span class="number">10991951608</span><span class="comment">; G1 Survivor Space: 25165824 -&gt; 0             🙅(FGC)</span></span><br><span class="line"><span class="number">2016-06-23</span> <span class="number">22:56:53,104</span> - G1 Young Generation GC in 229ms.    G1 Eden Space: <span class="number">796917760</span> -&gt; 0<span class="comment">;   G1 Old Gen: 6362278144 -&gt; 4860703680;   G1 Survivor Space: 58720256 -&gt; 33554432</span></span><br><span class="line"><span class="number">2016-06-23</span> <span class="number">22:56:54,109</span> - G1 Young Generation GC in 263ms.    G1 Eden Space: <span class="number">822083584</span> -&gt; 0<span class="comment">;   G1 Old Gen: 4860703680 -&gt; 3774189968;   G1 Survivor Space: 33554432 -&gt; 50331648</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">01:15:31,75</span>3 - G1 Young Generation GC in 251ms.    G1 Eden Space: <span class="number">10141827072</span> -&gt; 0<span class="comment">; G1 Old Gen: 4901503760 -&gt; 4907845240;   G1 Survivor Space: 134217728 -&gt; 687865856</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">01:15:38,93</span>0 - G1 Young Generation GC in 252ms.    G1 Eden Space: <span class="number">9026142208</span> -&gt; 0<span class="comment">;  G1 Old Gen: 4907845240 -&gt; 4959650376;   G1 Survivor Space: 687865856 -&gt; 645922816</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">01:15:46,31</span>0 - G1 Young Generation GC in 263ms.    G1 Eden Space: <span class="number">9059696640</span> -&gt; 0<span class="comment">;  G1 Old Gen: 4959650376 -&gt; 5025814888;   G1 Survivor Space: 645922816 -&gt; 637534208</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">01:15:53,69</span>2 - G1 Young Generation GC in 232ms.    G1 Eden Space: <span class="number">9009364992</span> -&gt; 0<span class="comment">;  G1 Old Gen: 5025814888 -&gt; 5035267128;   G1 Survivor Space: 637534208 -&gt; 645922816</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">01:16:01,09</span>2 - G1 Young Generation GC in 234ms.    G1 Eden Space: <span class="number">8984199168</span> -&gt; 0<span class="comment">;  G1 Old Gen: 5035267128 -&gt; 5635307288;   G1 Survivor Space: 645922816 -&gt; 100663296</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">02:40:23,73</span>6 - G1 Young Generation GC in 205ms.    G1 Eden Space: <span class="number">771751936</span> -&gt; 0<span class="comment">;   G1 Old Gen: 5298934024 -&gt; 4154593728;</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">04:47:09,84</span>3 - G1 Young Generation GC in 429ms.    G1 Eden Space: <span class="number">8833204224</span> -&gt; 0<span class="comment">;  G1 Old Gen: 6350837368 -&gt; 6645919712;   G1 Survivor Space: 75497472 -&gt; 1115684864</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">04:47:10,62</span>4 - G1 Young Generation GC in 476ms.    G1 Eden Space: <span class="number">545259520</span> -&gt; 0<span class="comment">;   G1 Old Gen: 6645919712 -&gt; 7641490784;   G1 Survivor Space: 1115684864 -&gt; 142606336</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">04:47:16,58</span>1 - G1 Young Generation GC in 796ms.    G1 Eden Space: <span class="number">6660554752</span> -&gt; 0<span class="comment">;  G1 Old Gen: 7779693536 -&gt; 8886989792;   G1 Survivor Space: 142606336 -&gt; 855638016</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">04:47:17,52</span>6 - G1 Young Generation GC in 875ms.    G1 Eden Space: <span class="number">8388608</span> -&gt; 0<span class="comment">;     G1 Old Gen: 8886989792 -&gt; 8136209376;   G1 Survivor Space: 855638016 -&gt; 58720256</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">04:47:18,44</span>7 - G1 Young Generation GC in 286ms.    G1 Eden Space: <span class="number">796917760</span> -&gt; 0<span class="comment">;   G1 Old Gen: 8136209376 -&gt; 7452538096;   G1 Survivor Space: 58720256 -&gt; 109051904</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">04:47:19,135</span> - G1 Young Generation GC in 217ms.    G1 Eden Space: <span class="number">746586112</span> -&gt; 0<span class="comment">;   G1 Old Gen: 7452538096 -&gt; 7683224816;</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">04:47:24,02</span>2 - G1 Young Generation GC in 202ms.    G1 Eden Space: <span class="number">2147483648</span> -&gt; 0<span class="comment">;  G1 Old Gen: 8652109040 -&gt; 8836658416;   G1 Survivor Space: 109051904 -&gt; 285212672</span></span><br><span class="line"><span class="number">2016-06-24</span> <span class="number">04:47:29,255</span> - G1 Young Generation GC in 263ms.    G1 Eden Space: <span class="number">536870912</span> -&gt; 0<span class="comment">;   G1 Old Gen: 8853435632 -&gt; 8970333808;   G1 Survivor Space: 318767104 -&gt; 58720256</span></span><br></pre></td></tr></table></figure>
<p>如果将GC时间和Compaction对照，会发现时间是对应的。但是这能说明什么问题，是因为Compaction导致GC的吗？</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[admin@cass048171 ~]$ cat /usr/install/cassandra/logs/system.<span class="built_in">log</span> | grep CompactionManager</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">10</span>:<span class="number">33</span>:<span class="number">10</span>,<span class="number">755</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">4</span>        <span class="number">17</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">10</span>:<span class="number">33</span>:<span class="number">37</span>,<span class="number">642</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">18</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">10</span>:<span class="number">33</span>:<span class="number">37</span>,<span class="number">726</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">18</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">10</span>:<span class="number">33</span>:<span class="number">38</span>,<span class="number">680</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">18</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">10</span>:<span class="number">33</span>:<span class="number">48</span>,<span class="number">084</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">18</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">17</span>:<span class="number">58</span>:<span class="number">26</span>,<span class="number">881</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">6</span>        <span class="number">61</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">21</span>:<span class="number">01</span>:<span class="number">43</span>,<span class="number">258</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">6</span>        <span class="number">73</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">22</span>:<span class="number">54</span>:<span class="number">34</span>,<span class="number">576</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">81</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">22</span>:<span class="number">56</span>:<span class="number">53</span>,<span class="number">115</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">71</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">23</span> <span class="number">22</span>:<span class="number">56</span>:<span class="number">54</span>,<span class="number">124</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">71</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">01</span>:<span class="number">15</span>:<span class="number">31</span>,<span class="number">762</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">66</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">01</span>:<span class="number">15</span>:<span class="number">38</span>,<span class="number">940</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">66</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">01</span>:<span class="number">15</span>:<span class="number">46</span>,<span class="number">321</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">66</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">01</span>:<span class="number">15</span>:<span class="number">53</span>,<span class="number">717</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">66</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">01</span>:<span class="number">16</span>:<span class="number">01</span>,<span class="number">106</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">66</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">02</span>:<span class="number">40</span>:<span class="number">23</span>,<span class="number">752</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">5</span>        <span class="number">44</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">04</span>:<span class="number">47</span>:<span class="number">09</span>,<span class="number">879</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">6</span>        <span class="number">35</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">04</span>:<span class="number">47</span>:<span class="number">10</span>,<span class="number">636</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">6</span>        <span class="number">35</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">04</span>:<span class="number">47</span>:<span class="number">17</span>,<span class="number">491</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">6</span>        <span class="number">35</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">04</span>:<span class="number">47</span>:<span class="number">17</span>,<span class="number">555</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">6</span>        <span class="number">35</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">04</span>:<span class="number">47</span>:<span class="number">18</span>,<span class="number">458</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">6</span>        <span class="number">35</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">04</span>:<span class="number">47</span>:<span class="number">19</span>,<span class="number">150</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">6</span>        <span class="number">35</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">04</span>:<span class="number">47</span>:<span class="number">24</span>,<span class="number">031</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">6</span>        <span class="number">35</span></span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">06</span>-<span class="number">24</span> <span class="number">04</span>:<span class="number">47</span>:<span class="number">29</span>,<span class="number">287</span> StatusLogger.java:<span class="number">75</span> - CompactionManager                 <span class="number">6</span>        <span class="number">35</span></span><br></pre></td></tr></table></figure>
<h2 id="长时间的YGC案例">长时间的YGC案例</h2><p>业务监控系统：</p>
<p><img src="http://img.blog.csdn.net/20160707150022099" alt="c_ygc_monitor"></p>
<p>OpsCenter：</p>
<p><img src="http://img.blog.csdn.net/20160707150142788" alt="c_ygc_ops"></p>
<p>Cassandra日志文件：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[admin<span class="annotation">@cass</span>048171 ~]$ grep <span class="string">'GC in [0-9][0-9][0-9]'</span> <span class="regexp">/usr/</span>install<span class="regexp">/cassandra/</span>logs/system.log</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">08</span>:<span class="number">19</span>:<span class="number">08</span>,<span class="number">106</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">204</span>ms.  G1 Eden <span class="string">Space:</span>  <span class="number">696254464</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span>  <span class="number">7281313536</span> -&gt;  <span class="number">5909453064</span>; G1 Survivor <span class="string">Space:</span>  <span class="number">159383552</span> -&gt;   <span class="number">92274688</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">08</span>:<span class="number">19</span>:<span class="number">09</span>,<span class="number">324</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">313</span>ms.  G1 Eden <span class="string">Space:</span>  <span class="number">763363328</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span>  <span class="number">5909453064</span> -&gt;  <span class="number">5497545648</span>; G1 Survivor <span class="string">Space:</span>   <span class="number">92274688</span> -&gt;  <span class="number">100663296</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">08</span>:<span class="number">59</span>:<span class="number">00</span>,<span class="number">400</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">270</span>ms.  G1 Eden <span class="string">Space:</span>  <span class="number">763363328</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span>  <span class="number">3972003048</span> -&gt;  <span class="number">3531601136</span>; G1 Survivor <span class="string">Space:</span>   <span class="number">92274688</span> -&gt;   <span class="number">67108864</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">09</span>:<span class="number">49</span>:<span class="number">47</span>,<span class="number">102</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">17887</span>ms.G1 Eden <span class="string">Space:</span> <span class="number">8875147264</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span>  <span class="number">6224121000</span> -&gt; <span class="number">11796004824</span>; G1 Survivor <span class="string">Space:</span>  <span class="number">125829120</span> -&gt; <span class="number">1132462080</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">09</span>:<span class="number">49</span>:<span class="number">48</span>,<span class="number">795</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">536</span>ms.  G1 Eden <span class="string">Space:</span> <span class="number">1241513984</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span> <span class="number">11796004824</span> -&gt; <span class="number">13020741592</span>; G1 Survivor <span class="string">Space:</span> <span class="number">1132462080</span> -&gt;  <span class="number">301989888</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">09</span>:<span class="number">49</span>:<span class="number">51</span>,<span class="number">681</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">464</span>ms.  G1 Eden <span class="string">Space:</span> <span class="number">1619001344</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span> <span class="number">13020741592</span> -&gt; <span class="number">13700218840</span>; G1 Survivor <span class="string">Space:</span>  <span class="number">301989888</span> -&gt;  <span class="number">243269632</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">09</span>:<span class="number">49</span>:<span class="number">53</span>,<span class="number">068</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">311</span>ms.  G1 Eden <span class="string">Space:</span> <span class="number">1006632960</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span> <span class="number">13700218840</span> -&gt; <span class="number">14111260632</span>; G1 Survivor <span class="string">Space:</span>  <span class="number">243269632</span> -&gt;  <span class="number">159383552</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">09</span>:<span class="number">49</span>:<span class="number">54</span>,<span class="number">076</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">263</span>ms.  G1 Eden <span class="string">Space:</span>  <span class="number">746586112</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span> <span class="number">14111260632</span> -&gt; <span class="number">14463582168</span>; G1 Survivor <span class="string">Space:</span>  <span class="number">159383552</span> -&gt;  <span class="number">117440512</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">09</span>:<span class="number">49</span>:<span class="number">55</span>,<span class="number">003</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">236</span>ms.  G1 Eden <span class="string">Space:</span>  <span class="number">738197504</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span> <span class="number">14463582168</span> -&gt; <span class="number">14803320800</span>; G1 Survivor <span class="string">Space:</span>  <span class="number">117440512</span> -&gt;  <span class="number">109051904</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">09</span>:<span class="number">49</span>:<span class="number">55</span>,<span class="number">916</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">225</span>ms.  G1 Eden <span class="string">Space:</span>  <span class="number">746586112</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span> <span class="number">14803320800</span> -&gt; <span class="number">15096922080</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">09</span>:<span class="number">49</span>:<span class="number">57</span>,<span class="number">943</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">202</span>ms.  G1 Eden <span class="string">Space:</span>  <span class="number">746586112</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span> <span class="number">15390523360</span> -&gt; <span class="number">15717679072</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">09</span>:<span class="number">50</span>:<span class="number">02</span>,<span class="number">743</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">251</span>ms.  G1 Eden <span class="string">Space:</span>  <span class="number">830472192</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span> <span class="number">10585068072</span> -&gt;  <span class="number">9267343240</span>; G1 Survivor <span class="string">Space:</span> <span class="number">25165824</span> -&gt; <span class="number">33554432</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">09</span>:<span class="number">50</span>:<span class="number">04</span>,<span class="number">138</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">326</span>ms.  G1 Eden <span class="string">Space:</span>  <span class="number">822083584</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span>  <span class="number">9267343240</span> -&gt;  <span class="number">8527731712</span>;</span><br><span class="line">INFO  [Service Thread] <span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span> <span class="number">09</span>:<span class="number">50</span>:<span class="number">28</span>,<span class="number">182</span> GCInspector.<span class="string">java:</span><span class="number">258</span> - G1 Young Generation GC <span class="keyword">in</span> <span class="number">204</span>ms.  G1 Eden <span class="string">Space:</span>  <span class="number">796917760</span> -&gt; <span class="number">0</span>;  G1 Old <span class="string">Gen:</span>  <span class="number">8527855792</span> -&gt;  <span class="number">8195194872</span>; G1 Survivor <span class="string">Space:</span> <span class="number">58720256</span> -&gt; <span class="number">100663296</span>;</span><br></pre></td></tr></table></figure>
<p><code>2016-07-07 09:49:47</code>发生17s的YGC，堆中各个代的大小： </p>
<blockquote>
<p>在/var/log/cassandra/gc.log中是：2016-07-07T09:49:47</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">G1 Young Generation GC in <span class="number">17887</span>ms.                  YGC花费了<span class="number">17</span>s</span><br><span class="line">G1 Eden Space: <span class="number">8</span>,<span class="number">875</span>,<span class="number">147</span>,<span class="number">264</span> -&gt; <span class="number">0</span>;                  <span class="number">8</span>G -&gt; <span class="number">0</span>      新生代</span><br><span class="line">G1 Old Gen: <span class="number">6</span>,<span class="number">224</span>,<span class="number">121</span>,<span class="number">000</span> -&gt; <span class="number">11</span>,<span class="number">796</span>,<span class="number">004</span>,<span class="number">824</span>;        <span class="number">6</span>G -&gt; <span class="number">11</span>G  </span><br><span class="line">G1 Survivor Space: <span class="number">125</span>,<span class="number">829</span>,<span class="number">120</span> -&gt; <span class="number">1</span>,<span class="number">132</span>,<span class="number">462</span>,<span class="number">080</span>;    <span class="number">125</span>M -&gt; <span class="number">1</span>G</span><br></pre></td></tr></table></figure>
<p>Eden从8G减少到0，Survivor从125M增加到1G，其实很好理解：<br>对象从Eden转移到了Survivor，所以Eden减少，Survivor增加。<br>Old也会增加的原因是：在GC的时候，Survivor的对象到达年龄计数器后，也会转移到Old。<br>所以Survivor增加的空间并不一定都是从Eden转移过来的，因为Survivor有一部分会转移到Old。<br>同时并不是Eden的所有对象都会转移到Survivor，只有需要的对象才会转移到Survivor。  </p>
<p>按照正常转移方式，Old增加的大小，最多不会超过Survivor的大小。但是上面Old从6G增加到11G，<br>而Survivor在GC前只有125M，所以正常来说，就算Survivor的所有对象都转移到Old最多也不会超过7G！  </p>
<p>那么唯一的解释是：Eden的对象直接被转移到了Old！<br>Eden总共有8G，转移了1G到Survivor，并且有4G跳过Survivor直接转移到Old！  </p>
<blockquote>
<p>新生代8G看起来有点大，对比其他日志，最大的也才1.6G。</p>
</blockquote>
<p>年轻代的GC：存活对象会被拷贝/移动到一个或多个“Survivor”区域，存活时间够长的直接移到“老年代”区域。<br>这里会stop the world，但young GC的过程是多线程执行的。  </p>
<p>结论：YGC会STW，如果YGC时间很长，应用程序暂停时间就很长！</p>
<p><strong>总体GC时间</strong>  </p>
<p>上gc.log：</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">下面这两条只是作为对比：</span><br><span class="line"><span class="number">2016-07-07</span>T<span class="number">09:49:22.34</span>1+0800: <span class="number">942859.60</span>4: Total time for which application threads were stopped: <span class="number">0.0814731</span> seconds, Stopping threads took: <span class="number">0.0002340</span> seconds</span><br><span class="line"><span class="number">2016-07-07</span>T<span class="number">09:49:29.213</span>+0800: <span class="number">942866.47</span>6: Total time for which application threads were stopped: <span class="number">0.0012369</span> seconds, Stopping threads took: <span class="number">0.0002430</span> seconds</span><br><span class="line"></span><br><span class="line">这里才开始比较严重的YGC：</span><br><span class="line">&#123;Heap before GC invocations=159966 (full 2):</span><br><span class="line"> garbage-first heap   total <span class="number">16777216</span>K, used <span class="number">14876451</span>K [<span class="number">0x00000003</span>c<span class="number">0000000</span>, <span class="number">0x00000003</span>c<span class="number">0804000</span>, <span class="number">0x00000007</span>c<span class="number">0000000</span>)</span><br><span class="line">  region size 8192K, 1074 young (<span class="number">8798208</span>K), 15 survivors (122880K)</span><br><span class="line"> Metaspace       used 35401K, capacity 35916K, committed 36224K, reserved <span class="number">1081344</span>K</span><br><span class="line">  class space    used 3719K, capacity 3839K, committed 3968K, reserved <span class="number">1048576</span>K</span><br><span class="line"><span class="number">2016-07-07</span>T<span class="number">09:49:29.213</span>+0800: <span class="number">942866.47</span>7: [GC pause (GCLocker Initiated GC) (young)  →YGC,暂停在GCLocker Initiated GC阶段</span><br><span class="line">Desired survivor size <span class="number">566231040</span> bytes, new threshold 15 (max 15)  中间这一段打印的是存活区的信息</span><br><span class="line">- age   1:   <span class="number">16845608</span> bytes,   <span class="number">16845608</span> total</span><br><span class="line">- age   2:    <span class="number">6895480</span> bytes,   <span class="number">23741088</span> total</span><br><span class="line">- age   3:    <span class="number">7210040</span> bytes,   <span class="number">30951128</span> total</span><br><span class="line">- age   4:    <span class="number">6712376</span> bytes,   <span class="number">37663504</span> total</span><br><span class="line">- age   5:    <span class="number">6050320</span> bytes,   <span class="number">43713824</span> total</span><br><span class="line">- age   6:    <span class="number">5356392</span> bytes,   <span class="number">49070216</span> total</span><br><span class="line">- age   7:    <span class="number">5514552</span> bytes,   <span class="number">54584768</span> total</span><br><span class="line">- age   8:    <span class="number">5607616</span> bytes,   <span class="number">60192384</span> total</span><br><span class="line">- age   9:    <span class="number">5430416</span> bytes,   <span class="number">65622800</span> total</span><br><span class="line">- age  10:    <span class="number">5450352</span> bytes,   <span class="number">71073152</span> total</span><br><span class="line">- age  11:    <span class="number">5550480</span> bytes,   <span class="number">76623632</span> total</span><br><span class="line">- age  12:    <span class="number">5110560</span> bytes,   <span class="number">81734192</span> total</span><br><span class="line">- age  13:    <span class="number">5440256</span> bytes,   <span class="number">87174448</span> total</span><br><span class="line">- age  14:    <span class="number">5565200</span> bytes,   <span class="number">92739648</span> total</span><br><span class="line">- age  15:    <span class="number">5550672</span> bytes,   <span class="number">98290320</span> total</span><br><span class="line"> (to-space exhausted), <span class="number">17.8874292</span> secs]  → YGC花费了17s</span><br></pre></td></tr></table></figure>
<p>关于G1日志文件的分析参考：<a href="https://blogs.oracle.com/g1gc/entry/g1gc_logs_how_to_print" target="_blank" rel="external">https://blogs.oracle.com/g1gc/entry/g1gc_logs_how_to_print</a></p>
<figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2014</span>-<span class="number">07</span>-<span class="number">16</span>T11:<span class="number">14</span>:<span class="number">35</span>.<span class="number">792</span>-<span class="number">0700</span>: <span class="number">11</span>.<span class="number">871</span>: [GC <span class="built_in">pause</span> (G1 Evacuation <span class="built_in">Pause</span>) (young),<span class="number">0</span>.<span class="number">0178446</span> secs]</span><br><span class="line"><span class="built_in">Date</span>-and-<span class="built_in">time</span>: timestamp [GC (gc-cause-is-evacuation-<span class="built_in">pause</span>) (gc-<span class="built_in">type</span>-is-young-gc), total-gc-<span class="built_in">pause</span>-<span class="flow">in</span>-sec]</span><br></pre></td></tr></table></figure>
<p>对应我们这里的日志文件：<br>2016-07-07T09:49:29.213+0800: 942866.477: [GC pause (GCLocker Initiated GC) (young) (to-space exhausted), 17.8874292 secs]</p>
<p>在GC类型（这里是young，具体阶段是：GCLocker Initiated GC）之后的是GC的原因：to空间耗尽了。</p>
<blockquote>
<p>问题：为什么to空间不够？  </p>
</blockquote>
<p><a href="http://bboniao.com/jvm/2014-03/g1garbage-first.html" target="_blank" rel="external">http://bboniao.com/jvm/2014-03/g1garbage-first.html</a><br><a href="https://www.infoq.com/articles/tuning-tips-G1-GC" target="_blank" rel="external">https://www.infoq.com/articles/tuning-tips-G1-GC</a>  </p>
<p>解决方案：</p>
<ul>
<li>-XX:G1ReservePercent 增加预存内存量</li>
<li>-XX:InitiatingHeapOccupancyPercent 减少此值,提前启动标记周期</li>
<li>-XX:ConcGCThreads 增加并行标记线程的数目</li>
</ul>
<p>参考线上API的GC设置：</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby"><span class="constant">XX</span><span class="symbol">:+UseG1GC</span> -<span class="constant">XX</span><span class="symbol">:MaxGCPauseMillis=</span><span class="number">200</span> -<span class="constant">XX</span><span class="symbol">:G1ReservePercent=</span><span class="number">30</span> -<span class="constant">XX</span><span class="symbol">:InitiatingHeapOccupancyPercent=</span><span class="number">30</span> -<span class="constant">XX</span><span class="symbol">:ConcGCThreads=</span><span class="number">4</span></span></span><br></pre></td></tr></table></figure>
<p>JVM_OPTS=”$JVM_OPTS -XX:G1ReservePercent=30”  10<br>JVM_OPTS=”$JVM_OPTS -XX:InitiatingHeapOccupancyPercent=30”  45<br>JVM_OPTS=”$JVM_OPTS -XX:ConcGCThreads=4”</p>
<p><strong>GC各个阶段</strong>  </p>
<p>接下来打印了各个阶段的GC时间：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[Parallel <span class="string">Time:</span> <span class="number">16666.1</span> ms, GC <span class="string">Workers:</span> <span class="number">13</span>]  ==》并行收集花了<span class="number">16</span>s</span><br><span class="line">   [GC Worker Start (ms): <span class="string">Min:</span> <span class="number">942866476.9</span>, <span class="string">Avg:</span> <span class="number">942866477.0</span>, <span class="string">Max:</span> <span class="number">942866477.1</span>, <span class="string">Diff:</span> <span class="number">0.2</span>]</span><br><span class="line">   [Ext Root Scanning (ms): <span class="string">Min:</span> <span class="number">2.9</span>, <span class="string">Avg:</span> <span class="number">3.0</span>, <span class="string">Max:</span> <span class="number">3.1</span>, <span class="string">Diff:</span> <span class="number">0.2</span>, <span class="string">Sum:</span> <span class="number">39.0</span>]</span><br><span class="line">   [Update RS (ms): <span class="string">Min:</span> <span class="number">71.8</span>, <span class="string">Avg:</span> <span class="number">72.2</span>, <span class="string">Max:</span> <span class="number">72.7</span>, <span class="string">Diff:</span> <span class="number">0.9</span>, <span class="string">Sum:</span> <span class="number">938.2</span>]</span><br><span class="line">      [Processed <span class="string">Buffers:</span> <span class="string">Min:</span> <span class="number">52</span>, <span class="string">Avg:</span> <span class="number">75.5</span>, <span class="string">Max:</span> <span class="number">102</span>, <span class="string">Diff:</span> <span class="number">50</span>, <span class="string">Sum:</span> <span class="number">982</span>]</span><br><span class="line">   [Scan RS (ms): <span class="string">Min:</span> <span class="number">0.1</span>, <span class="string">Avg:</span> <span class="number">0.2</span>, <span class="string">Max:</span> <span class="number">0.4</span>, <span class="string">Diff:</span> <span class="number">0.3</span>, <span class="string">Sum:</span> <span class="number">2.1</span>]</span><br><span class="line">   [Code Root Scanning (ms): <span class="string">Min:</span> <span class="number">0.0</span>, <span class="string">Avg:</span> <span class="number">0.0</span>, <span class="string">Max:</span> <span class="number">0.0</span>, <span class="string">Diff:</span> <span class="number">0.0</span>, <span class="string">Sum:</span> <span class="number">0.1</span>]</span><br><span class="line">   [Object Copy (ms): <span class="string">Min:</span> <span class="number">16589.9</span>, <span class="string">Avg:</span> <span class="number">16590.4</span>, <span class="string">Max:</span> <span class="number">16590.6</span>, <span class="string">Diff:</span> <span class="number">0.7</span>, <span class="string">Sum:</span> <span class="number">215675.3</span>]  ==&gt;主要在这里！</span><br><span class="line">   [Termination (ms): <span class="string">Min:</span> <span class="number">0.0</span>, <span class="string">Avg:</span> <span class="number">0.1</span>, <span class="string">Max:</span> <span class="number">0.2</span>, <span class="string">Diff:</span> <span class="number">0.2</span>, <span class="string">Sum:</span> <span class="number">1.3</span>]</span><br><span class="line">      [Termination <span class="string">Attempts:</span> <span class="string">Min:</span> <span class="number">1</span>, <span class="string">Avg:</span> <span class="number">1.0</span>, <span class="string">Max:</span> <span class="number">1</span>, <span class="string">Diff:</span> <span class="number">0</span>, <span class="string">Sum:</span> <span class="number">13</span>]</span><br><span class="line">   [GC Worker Other (ms): <span class="string">Min:</span> <span class="number">0.0</span>, <span class="string">Avg:</span> <span class="number">0.1</span>, <span class="string">Max:</span> <span class="number">0.1</span>, <span class="string">Diff:</span> <span class="number">0.1</span>, <span class="string">Sum:</span> <span class="number">0.7</span>]</span><br><span class="line">   [GC Worker Total (ms): <span class="string">Min:</span> <span class="number">16665.8</span>, <span class="string">Avg:</span> <span class="number">16665.9</span>, <span class="string">Max:</span> <span class="number">16666.0</span>, <span class="string">Diff:</span> <span class="number">0.2</span>, <span class="string">Sum:</span> <span class="number">216656.7</span>]</span><br><span class="line">   [GC Worker End (ms): <span class="string">Min:</span> <span class="number">942883142.9</span>, <span class="string">Avg:</span> <span class="number">942883142.9</span>, <span class="string">Max:</span> <span class="number">942883142.9</span>, <span class="string">Diff:</span> <span class="number">0.1</span>]</span><br><span class="line">[Code Root <span class="string">Fixup:</span> <span class="number">0.1</span> ms]</span><br><span class="line">[Code Root <span class="string">Purge:</span> <span class="number">0.0</span> ms]</span><br><span class="line">[Clear <span class="string">CT:</span> <span class="number">2.0</span> ms]</span><br><span class="line">[<span class="string">Other:</span> <span class="number">1219.2</span> ms]</span><br><span class="line">   [Evacuation <span class="string">Failure:</span> <span class="number">1212.5</span> ms]  ==&gt;疏散失败！</span><br><span class="line">   [Choose <span class="string">CSet:</span> <span class="number">0.0</span> ms]</span><br><span class="line">   [Ref <span class="string">Proc:</span> <span class="number">0.7</span> ms]</span><br><span class="line">   [Ref <span class="string">Enq:</span> <span class="number">0.0</span> ms]</span><br><span class="line">   [Redirty <span class="string">Cards:</span> <span class="number">4.4</span> ms]</span><br><span class="line">   [Humongous <span class="string">Register:</span> <span class="number">0.1</span> ms]</span><br><span class="line">   [Humongous <span class="string">Reclaim:</span> <span class="number">0.1</span> ms]</span><br><span class="line">   [Free <span class="string">CSet:</span> <span class="number">0.7</span> ms]</span><br><span class="line">[<span class="string">Eden:</span> <span class="number">8472.0</span>M(<span class="number">8464.0</span>M)-&gt;<span class="number">0.0</span>B(<span class="number">1184.0</span>M) <span class="string">Survivors:</span> <span class="number">120.0</span>M-&gt;<span class="number">1080.0</span>M <span class="string">Heap:</span> <span class="number">14.2</span>G(<span class="number">16.0</span>G)-&gt;<span class="number">12.0</span>G(<span class="number">16.0</span>G)]</span><br></pre></td></tr></table></figure>
<p><strong>堆内存变化</strong>  </p>
<p>上面最后一行是本次GC的堆信息，区域：GC前大小-&gt;GC后大小，括号里是Capacity（也有可能变化）  </p>
<p>Heap的Capacity等于-Xms和-Xmx的值=16G，这个是不会变的，但是Eden的大小是会变化，比如GC前Capacity=8G，GC后变成了1G。</p>
<p>Young中分成Eden和Survivor，其中Eden和Survivor是成比例的。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Eden: <span class="function"><span class="title">used-enden-before-gc</span><span class="params">(eden-capacity)</span></span>-&gt;<span class="function"><span class="title">used-eden-after-gc</span><span class="params">(eden-capacity-for-next-gc)</span></span> </span><br><span class="line">Survivors: used-survivors-before-gc-&gt;used-survivors-before-gc </span><br><span class="line">Heap: <span class="function"><span class="title">used-heap-before-gc</span><span class="params">(heap-capacity)</span></span>-&gt;<span class="function"><span class="title">used-heap-after-gc</span><span class="params">(heap-capacity-for-next-gc)</span></span>]</span><br></pre></td></tr></table></figure>
<p>这里打印的堆信息其实和Cassandra的system.log是一致的，不过上面的日志并没有打印Old！</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">G1 Young Generation GC in <span class="number">17887</span>ms.                  YGC花费了<span class="number">17</span>s</span><br><span class="line">G1 Eden Space: <span class="number">8</span>,<span class="number">875</span>,<span class="number">147</span>,<span class="number">264</span> -&gt; <span class="number">0</span>;                  <span class="number">8</span>G -&gt; <span class="number">0</span>      新生代GC前<span class="number">8</span>G，GC后清空</span><br><span class="line">G1 Old Gen: <span class="number">6</span>,<span class="number">224</span>,<span class="number">121</span>,<span class="number">000</span> -&gt; <span class="number">11</span>,<span class="number">796</span>,<span class="number">004</span>,<span class="number">824</span>;        <span class="number">6</span>G -&gt; <span class="number">11</span>G    老年代从<span class="number">6</span>G增加到<span class="number">11</span>G</span><br><span class="line">G1 Survivor Space: <span class="number">125</span>,<span class="number">829</span>,<span class="number">120</span> -&gt; <span class="number">1</span>,<span class="number">132</span>,<span class="number">462</span>,<span class="number">080</span>;    <span class="number">125</span>M -&gt; <span class="number">1</span>G   存活区从<span class="number">125</span>M增加到<span class="number">1</span>G</span><br></pre></td></tr></table></figure>
<p>查看其它正常的堆变化情况，会发现基本上Survivor不会增大到1G，而且Heap一般</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[admin@cass048171 ~]$ cat /var/<span class="built_in">log</span>/cassandra/gc.<span class="built_in">log</span><span class="number">.4</span>.current | grep Eden | tail</span><br><span class="line">   [Eden: <span class="number">4952.0</span>M(<span class="number">4952.0</span>M)-&gt;<span class="number">0.0</span>B(<span class="number">696.0</span>M) Survivors: <span class="number">120.0</span>M-&gt;<span class="number">120.0</span>M Heap: <span class="number">14.1</span>G(<span class="number">16.0</span>G)-&gt;<span class="number">9511.6</span>M(<span class="number">16.0</span>G)]</span><br><span class="line">   [Eden: <span class="number">696.0</span>M(<span class="number">696.0</span>M)-&gt;<span class="number">0.0</span>B(<span class="number">5056.0</span>M) Survivors: <span class="number">120.0</span>M-&gt;<span class="number">80.0</span>M Heap: <span class="number">10207.6</span>M(<span class="number">16.0</span>G)-&gt;<span class="number">9455.0</span>M(<span class="number">16.0</span>G)]</span><br><span class="line">   [Eden: <span class="number">5056.0</span>M(<span class="number">5056.0</span>M)-&gt;<span class="number">0.0</span>B(<span class="number">5040.0</span>M) Survivors: <span class="number">80.0</span>M-&gt;<span class="number">88.0</span>M Heap: <span class="number">14.2</span>G(<span class="number">16.0</span>G)-&gt;<span class="number">9463.0</span>M(<span class="number">16.0</span>G)]</span><br><span class="line">   [Eden: <span class="number">5040.0</span>M(<span class="number">5040.0</span>M)-&gt;<span class="number">0.0</span>B(<span class="number">4992.0</span>M) Survivors: <span class="number">88.0</span>M-&gt;<span class="number">112.0</span>M Heap: <span class="number">14.2</span>G(<span class="number">16.0</span>G)-&gt;<span class="number">9483.0</span>M(<span class="number">16.0</span>G)]</span><br><span class="line">   [Eden: <span class="number">4992.0</span>M(<span class="number">4992.0</span>M)-&gt;<span class="number">0.0</span>B(<span class="number">4976.0</span>M) Survivors: <span class="number">112.0</span>M-&gt;<span class="number">120.0</span>M Heap: <span class="number">14.1</span>G(<span class="number">16.0</span>G)-&gt;<span class="number">9491.0</span>M(<span class="number">16.0</span>G)]</span><br><span class="line">   [Eden: <span class="number">4976.0</span>M(<span class="number">4976.0</span>M)-&gt;<span class="number">0.0</span>B(<span class="number">696.0</span>M) Survivors: <span class="number">120.0</span>M-&gt;<span class="number">120.0</span>M Heap: <span class="number">14.1</span>G(<span class="number">16.0</span>G)-&gt;<span class="number">9495.0</span>M(<span class="number">16.0</span>G)]</span><br><span class="line">   [Eden: <span class="number">696.0</span>M(<span class="number">696.0</span>M)-&gt;<span class="number">0.0</span>B(<span class="number">5024.0</span>M) Survivors: <span class="number">120.0</span>M-&gt;<span class="number">80.0</span>M Heap: <span class="number">10191.0</span>M(<span class="number">16.0</span>G)-&gt;<span class="number">9488.6</span>M(<span class="number">16.0</span>G)]</span><br><span class="line">   [Eden: <span class="number">5024.0</span>M(<span class="number">5024.0</span>M)-&gt;<span class="number">0.0</span>B(<span class="number">5008.0</span>M) Survivors: <span class="number">80.0</span>M-&gt;<span class="number">88.0</span>M Heap: <span class="number">14.2</span>G(<span class="number">16.0</span>G)-&gt;<span class="number">9496.6</span>M(<span class="number">16.0</span>G)]</span><br><span class="line">   [Eden: <span class="number">5008.0</span>M(<span class="number">5008.0</span>M)-&gt;<span class="number">0.0</span>B(<span class="number">4960.0</span>M) Survivors: <span class="number">88.0</span>M-&gt;<span class="number">112.0</span>M Heap: <span class="number">14.2</span>G(<span class="number">16.0</span>G)-&gt;<span class="number">9524.6</span>M(<span class="number">16.0</span>G)]</span><br><span class="line">   [Eden: <span class="number">4968.0</span>M(<span class="number">4960.0</span>M)-&gt;<span class="number">0.0</span>B(<span class="number">4944.0</span>M) Survivors: <span class="number">112.0</span>M-&gt;<span class="number">120.0</span>M Heap: <span class="number">14.1</span>G(<span class="number">16.0</span>G)-&gt;<span class="number">9532.6</span>M(<span class="number">16.0</span>G)]</span><br></pre></td></tr></table></figure>
<p><strong>GC前后统计</strong>  </p>
<p>最后是Heap在GC前后的统计，这里把GC前的也放在一起进行比较：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;Heap before GC invocations=<span class="number">159966</span> (full <span class="number">2</span>):</span><br><span class="line"> garbage-first heap   total <span class="number">16777216</span>K, used <span class="number">14876451</span>K [<span class="number">0x00000003c0000000</span>, <span class="number">0x00000003c0804000</span>, <span class="number">0x00000007c0000000</span>)</span><br><span class="line">  region size <span class="number">8192</span>K, <span class="number">1074</span> young (<span class="number">8798208</span>K), <span class="number">15</span> survivors (<span class="number">122880</span>K)</span><br><span class="line"> Metaspace       used <span class="number">35401</span>K, capacity <span class="number">35916</span>K, committed <span class="number">36224</span>K, reserved <span class="number">1081344</span>K</span><br><span class="line">  <span class="keyword">class</span> space    used <span class="number">3719</span>K, capacity <span class="number">3839</span>K, committed <span class="number">3968</span>K, reserved <span class="number">1048576</span>K</span><br><span class="line"></span><br><span class="line">Heap after GC invocations=<span class="number">159967</span> (full <span class="number">2</span>):</span><br><span class="line"> garbage-first heap   total <span class="number">16777216</span>K, used <span class="number">12625455</span>K [<span class="number">0x00000003c0000000</span>, <span class="number">0x00000003c0804000</span>, <span class="number">0x00000007c0000000</span>)</span><br><span class="line">  region size <span class="number">8192</span>K, <span class="number">135</span> young (<span class="number">1105920</span>K), <span class="number">135</span> survivors (<span class="number">1105920</span>K)</span><br><span class="line"> Metaspace       used <span class="number">35401</span>K, capacity <span class="number">35916</span>K, committed <span class="number">36224</span>K, reserved <span class="number">1081344</span>K</span><br><span class="line">  <span class="keyword">class</span> space    used <span class="number">3719</span>K, capacity <span class="number">3839</span>K, committed <span class="number">3968</span>K, reserved <span class="number">1048576</span>K</span><br><span class="line">&#125;</span><br><span class="line"> [Times: user=<span class="number">41.47</span> sys=<span class="number">3.94</span>, real=<span class="number">17.88</span> secs]</span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-<span class="number">07</span>T09:<span class="number">49</span>:<span class="number">47.101</span>+<span class="number">0800</span>: <span class="number">942884.364</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">17.8884392</span> seconds, Stopping threads took: <span class="number">0.0001048</span> seconds</span><br></pre></td></tr></table></figure>
<h2 id="其他调优">其他调优</h2><p>The ratio user/real should be close to the number of parallel gc threads.<br>user和real的比值是并行的GC线程，这里41.47/17.88=2.3</p>
<p>-XX:ConcGCThreads：Max((ParallelGCThreads#+2)/4,1)</p>
<p>RegionSize默认=HeapSize/2048，这里Heap=16G，RegionSize=8M =》 region size 8192K</p>
<h2 id="后面的一些正常GC日志">后面的一些正常GC日志</h2><figure class="highlight irpf90"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2016</span>-<span class="number">07</span>-7T09:<span class="number">49</span>:<span class="number">48.103</span>+<span class="number">0800</span>: <span class="number">942885.366</span>: Total time for which application threads were stopped: <span class="number">0.0013006</span> seconds, Stopping threads took: <span class="number">0.0002328</span> seconds</span><br><span class="line">&#123;Heap before GC invocations=<span class="number">159967</span> (full <span class="number">2</span>):</span><br><span class="line"> garbage-first heap   total 6777216K, used 3837871K [x00000003c0000000, x00000003c0804000, x00000007c0000000)</span><br><span class="line">  region <span class="built_in">size</span> 192K, <span class="number">283</span> young (318336K), <span class="number">135</span> survivors (105920K)</span><br><span class="line"> Metaspace       used 5401K, capacity 5916K, committed 6224K, reserved 081344K</span><br><span class="line">  <span class="keyword">class</span> space    used 719K, capacity 839K, committed 968K, reserved 048576K</span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-7T09:<span class="number">49</span>:<span class="number">48.248</span>+<span class="number">0800</span>: <span class="number">942885.511</span>: [GC <span class="keyword">pause</span> (G1 Evacuation <span class="keyword">Pause</span>) (young) (initial-mark), <span class="number">0.5353097</span> secs]</span><br><span class="line">   [Parallel Time: <span class="number">528.2</span> ms, GC Workers: <span class="number">13</span>]</span><br><span class="line">      [GC Worker Start (ms): <span class="built_in">Min</span>: <span class="number">942885512.0</span>, Avg: <span class="number">942885512.1</span>, <span class="built_in">Max</span>: <span class="number">942885512.2</span>, Diff: <span class="number">0.2</span>]</span><br><span class="line">      [Ext Root Scanning (ms): <span class="built_in">Min</span>: <span class="number">2.9</span>, Avg: <span class="number">3.0</span>, <span class="built_in">Max</span>: <span class="number">3.1</span>, Diff: <span class="number">0.2</span>, <span class="built_in">Sum</span>: <span class="number">38.7</span>]</span><br><span class="line">      [Update RS (ms): <span class="built_in">Min</span>: <span class="number">28.9</span>, Avg: <span class="number">29.2</span>, <span class="built_in">Max</span>: <span class="number">30.6</span>, Diff: <span class="number">1.8</span>, <span class="built_in">Sum</span>: <span class="number">379.2</span>]</span><br><span class="line">         [Processed Buffers: <span class="built_in">Min</span>: <span class="number">54</span>, Avg: <span class="number">84.8</span>, <span class="built_in">Max</span>: <span class="number">133</span>, Diff: <span class="number">79</span>, <span class="built_in">Sum</span>: <span class="number">1103</span>]</span><br><span class="line">      [<span class="built_in">Scan</span> RS (ms): <span class="built_in">Min</span>: <span class="number">14.6</span>, Avg: <span class="number">16.2</span>, <span class="built_in">Max</span>: <span class="number">16.5</span>, Diff: <span class="number">1.9</span>, <span class="built_in">Sum</span>: <span class="number">210.2</span>]</span><br><span class="line">      [Code Root Scanning (ms): <span class="built_in">Min</span>: <span class="number">0.0</span>, Avg: <span class="number">0.0</span>, <span class="built_in">Max</span>: <span class="number">0.0</span>, Diff: <span class="number">0.0</span>, <span class="built_in">Sum</span>: <span class="number">0.1</span>]</span><br><span class="line">      [Object Copy (ms): <span class="built_in">Min</span>: <span class="number">479.2</span>, Avg: <span class="number">479.4</span>, <span class="built_in">Max</span>: <span class="number">479.5</span>, Diff: <span class="number">0.3</span>, <span class="built_in">Sum</span>: <span class="number">6231.7</span>]</span><br><span class="line">      [Termination (ms): <span class="built_in">Min</span>: <span class="number">0.0</span>, Avg: <span class="number">0.2</span>, <span class="built_in">Max</span>: <span class="number">0.3</span>, Diff: <span class="number">0.3</span>, <span class="built_in">Sum</span>: <span class="number">3.1</span>]</span><br><span class="line">         [Termination Attempts: <span class="built_in">Min</span>: <span class="number">1</span>, Avg: <span class="number">395.1</span>, <span class="built_in">Max</span>: <span class="number">486</span>, Diff: <span class="number">485</span>, <span class="built_in">Sum</span>: <span class="number">5136</span>]</span><br><span class="line">      [GC Worker Other (ms): <span class="built_in">Min</span>: <span class="number">0.0</span>, Avg: <span class="number">0.1</span>, <span class="built_in">Max</span>: <span class="number">0.1</span>, Diff: <span class="number">0.1</span>, <span class="built_in">Sum</span>: <span class="number">0.8</span>]</span><br><span class="line">      [GC Worker Total (ms): <span class="built_in">Min</span>: <span class="number">527.9</span>, Avg: <span class="number">528.0</span>, <span class="built_in">Max</span>: <span class="number">528.1</span>, Diff: <span class="number">0.2</span>, <span class="built_in">Sum</span>: <span class="number">6863.7</span>]</span><br><span class="line">      [GC Worker <span class="keyword">End</span> (ms): <span class="built_in">Min</span>: <span class="number">942886040.0</span>, Avg: <span class="number">942886040.0</span>, <span class="built_in">Max</span>: <span class="number">942886040.1</span>, Diff: <span class="number">0.1</span>]</span><br><span class="line">   [Code Root Fixup: <span class="number">0.1</span> ms]</span><br><span class="line">   [Code Root Purge: <span class="number">0.0</span> ms]</span><br><span class="line">   [Clear CT: <span class="number">1.4</span> ms]</span><br><span class="line">   [Other: <span class="number">5.6</span> ms]</span><br><span class="line">      [Choose CSet: <span class="number">0.0</span> ms]</span><br><span class="line">      [Ref Proc: <span class="number">0.4</span> ms]</span><br><span class="line">      [Ref Enq: <span class="number">0.0</span> ms]</span><br><span class="line">      [Redirty Cards: <span class="number">3.1</span> ms]</span><br><span class="line">      [Humongous Register: <span class="number">0.0</span> ms]</span><br><span class="line">      [Humongous Reclaim: <span class="number">0.1</span> ms]</span><br><span class="line">      [<span class="keyword">Free</span> CSet: <span class="number">0.7</span> ms]</span><br><span class="line">   [Eden: <span class="number">1184.</span>M(<span class="number">1184.</span>M)-&gt;<span class="number">0.</span>B(<span class="number">1544.</span>M) Survivors: <span class="number">1080.</span>M-&gt;<span class="number">288.</span>M Heap: <span class="number">13.</span>G(<span class="number">16.</span>G)-&gt;<span class="number">12.</span>G(<span class="number">16.</span>G)]</span><br><span class="line">Heap after GC invocations=<span class="number">159968</span> (full <span class="number">2</span>):</span><br><span class="line"> garbage-first heap   total 6777216K, used 3010479K [x00000003c0000000, x00000003c0804000, x00000007c0000000)</span><br><span class="line">  region <span class="built_in">size</span> 192K, <span class="number">36</span> young (94912K), <span class="number">36</span> survivors (94912K)</span><br><span class="line"> Metaspace       used 5401K, capacity 5916K, committed 6224K, reserved 081344K</span><br><span class="line">  <span class="keyword">class</span> space    used 719K, capacity 839K, committed 968K, reserved 048576K</span><br><span class="line">&#125;</span><br><span class="line"> [Times: user=<span class="number">6.90</span> sys=<span class="number">0.01</span>, <span class="type">real</span>=<span class="number">0.54</span> secs]</span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-7T09:<span class="number">49</span>:<span class="number">48.784</span>+<span class="number">0800</span>: <span class="number">942886.047</span>: [GC concurrent-root-region-<span class="built_in">scan</span>-start]</span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-7T09:<span class="number">49</span>:<span class="number">48.784</span>+<span class="number">0800</span>: <span class="number">942886.047</span>: Total time for which application threads were stopped: <span class="number">0.5368812</span> seconds, Stopping threads took: <span class="number">0.0002119</span> seconds</span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-7T09:<span class="number">49</span>:<span class="number">48.973</span>+<span class="number">0800</span>: <span class="number">942886.236</span>: [GC concurrent-root-region-<span class="built_in">scan</span>-<span class="keyword">end</span>, <span class="number">0.1896420</span> secs]</span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-7T09:<span class="number">49</span>:<span class="number">48.973</span>+<span class="number">0800</span>: <span class="number">942886.236</span>: [GC concurrent-mark-start]</span><br><span class="line">&#123;Heap before GC invocations=<span class="number">159968</span> (full <span class="number">2</span>):</span><br><span class="line"> garbage-first heap   total 6777216K, used 4591535K [x00000003c0000000, x00000003c0804000, x00000007c0000000)</span><br><span class="line">  region <span class="built_in">size</span> 192K, <span class="number">229</span> young (875968K), <span class="number">36</span> survivors (94912K)</span><br><span class="line"> Metaspace       used 5401K, capacity 5916K, committed 6224K, reserved 081344K</span><br><span class="line">  <span class="keyword">class</span> space    used 719K, capacity 839K, committed 968K, reserved 048576K</span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-7T09:<span class="number">49</span>:<span class="number">51.216</span>+<span class="number">0800</span>: <span class="number">942888.479</span>: [GC <span class="keyword">pause</span> (G1 Evacuation <span class="keyword">Pause</span>) (young)</span><br><span class="line">Desired survivor <span class="built_in">size</span> <span class="number">121634816</span> bytes, new threshold <span class="number">1</span> (<span class="built_in">max</span> <span class="number">15</span>)</span><br><span class="line">- age   <span class="number">1</span>:  <span class="number">277632680</span> bytes,  <span class="number">277632680</span> total</span><br><span class="line">, <span class="number">0.4639409</span> secs]</span><br><span class="line">   [Parallel Time: <span class="number">455.9</span> ms, GC Workers: <span class="number">13</span>]</span><br><span class="line">      [GC Worker Start (ms): <span class="built_in">Min</span>: <span class="number">942888479.3</span>, Avg: <span class="number">942888479.4</span>, <span class="built_in">Max</span>: <span class="number">942888479.5</span>, Diff: <span class="number">0.3</span>]</span><br><span class="line">      [Ext Root Scanning (ms): <span class="built_in">Min</span>: <span class="number">2.8</span>, Avg: <span class="number">3.0</span>, <span class="built_in">Max</span>: <span class="number">3.1</span>, Diff: <span class="number">0.2</span>, <span class="built_in">Sum</span>: <span class="number">38.4</span>]</span><br><span class="line">      [Update RS (ms): <span class="built_in">Min</span>: <span class="number">166.4</span>, Avg: <span class="number">167.6</span>, <span class="built_in">Max</span>: <span class="number">168.7</span>, Diff: <span class="number">2.4</span>, <span class="built_in">Sum</span>: <span class="number">2178.7</span>]</span><br><span class="line">         [Processed Buffers: <span class="built_in">Min</span>: <span class="number">59</span>, Avg: <span class="number">81.7</span>, <span class="built_in">Max</span>: <span class="number">109</span>, Diff: <span class="number">50</span>, <span class="built_in">Sum</span>: <span class="number">1062</span>]</span><br><span class="line">      [<span class="built_in">Scan</span> RS (ms): <span class="built_in">Min</span>: <span class="number">6.8</span>, Avg: <span class="number">7.9</span>, <span class="built_in">Max</span>: <span class="number">8.2</span>, Diff: <span class="number">1.4</span>, <span class="built_in">Sum</span>: <span class="number">102.4</span>]</span><br><span class="line">      [Code Root Scanning (ms): <span class="built_in">Min</span>: <span class="number">0.0</span>, Avg: <span class="number">0.0</span>, <span class="built_in">Max</span>: <span class="number">0.0</span>, Diff: <span class="number">0.0</span>, <span class="built_in">Sum</span>: <span class="number">0.0</span>]</span><br><span class="line">      [Object Copy (ms): <span class="built_in">Min</span>: <span class="number">277.0</span>, Avg: <span class="number">277.0</span>, <span class="built_in">Max</span>: <span class="number">277.1</span>, Diff: <span class="number">0.2</span>, <span class="built_in">Sum</span>: <span class="number">3601.5</span>]</span><br><span class="line">      [Termination (ms): <span class="built_in">Min</span>: <span class="number">0.0</span>, Avg: <span class="number">0.0</span>, <span class="built_in">Max</span>: <span class="number">0.0</span>, Diff: <span class="number">0.0</span>, <span class="built_in">Sum</span>: <span class="number">0.0</span>]</span><br><span class="line">         [Termination Attempts: <span class="built_in">Min</span>: <span class="number">1</span>, Avg: <span class="number">1.0</span>, <span class="built_in">Max</span>: <span class="number">1</span>, Diff: <span class="number">0</span>, <span class="built_in">Sum</span>: <span class="number">13</span>]</span><br><span class="line">      [GC Worker Other (ms): <span class="built_in">Min</span>: <span class="number">0.0</span>, Avg: <span class="number">0.1</span>, <span class="built_in">Max</span>: <span class="number">0.1</span>, Diff: <span class="number">0.1</span>, <span class="built_in">Sum</span>: <span class="number">0.8</span>]</span><br><span class="line">      [GC Worker Total (ms): <span class="built_in">Min</span>: <span class="number">455.5</span>, Avg: <span class="number">455.6</span>, <span class="built_in">Max</span>: <span class="number">455.7</span>, Diff: <span class="number">0.2</span>, <span class="built_in">Sum</span>: <span class="number">5923.0</span>]</span><br><span class="line">      [GC Worker <span class="keyword">End</span> (ms): <span class="built_in">Min</span>: <span class="number">942888935.0</span>, Avg: <span class="number">942888935.0</span>, <span class="built_in">Max</span>: <span class="number">942888935.1</span>, Diff: <span class="number">0.1</span>]</span><br><span class="line">   [Code Root Fixup: <span class="number">0.3</span> ms]</span><br><span class="line">   [Code Root Purge: <span class="number">0.0</span> ms]</span><br><span class="line">   [Clear CT: <span class="number">1.2</span> ms]</span><br><span class="line">   [Other: <span class="number">6.6</span> ms]</span><br><span class="line">      [Choose CSet: <span class="number">0.0</span> ms]</span><br><span class="line">      [Ref Proc: <span class="number">3.6</span> ms]</span><br><span class="line">      [Ref Enq: <span class="number">0.0</span> ms]</span><br><span class="line">      [Redirty Cards: <span class="number">1.7</span> ms]</span><br><span class="line">      [Humongous Register: <span class="number">0.1</span> ms]</span><br><span class="line">      [Humongous Reclaim: <span class="number">0.1</span> ms]</span><br><span class="line">      [<span class="keyword">Free</span> CSet: <span class="number">0.5</span> ms]</span><br><span class="line">   [Eden: <span class="number">1544.</span>M(<span class="number">1544.</span>M)-&gt;<span class="number">0.</span>B(<span class="number">960.</span>M) Survivors: <span class="number">288.</span>M-&gt;<span class="number">232.</span>M Heap: <span class="number">13.</span>G(<span class="number">16.</span>G)-&gt;<span class="number">13.</span>G(<span class="number">16.</span>G)]</span><br><span class="line">Heap after GC invocations=<span class="number">159969</span> (full <span class="number">2</span>):</span><br><span class="line"> garbage-first heap   total 6777216K, used 3616687K [x00000003c0000000, x00000003c0804000, x00000007c0000000)</span><br><span class="line">  region <span class="built_in">size</span> 192K, <span class="number">29</span> young (37568K), <span class="number">29</span> survivors (37568K)</span><br><span class="line"> Metaspace       used 5401K, capacity 5916K, committed 6224K, reserved 081344K</span><br><span class="line">  <span class="keyword">class</span> space    used 719K, capacity 839K, committed 968K, reserved 048576K</span><br><span class="line">&#125;</span><br><span class="line"> [Times: user=<span class="number">5.99</span> sys=<span class="number">0.00</span>, <span class="type">real</span>=<span class="number">0.46</span> secs]</span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-7T09:<span class="number">49</span>:<span class="number">51.680</span>+<span class="number">0800</span>: <span class="number">942888.943</span>: Total time for which application threads were stopped: <span class="number">0.4655046</span> seconds, Stopping threads took: <span class="number">0.0001802</span> seconds</span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-7T09:<span class="number">49</span>:<span class="number">51.680</span>+<span class="number">0800</span>: <span class="number">942888.943</span>: [GC concurrent-mark-reset-for-overflow]</span><br><span class="line">&#123;Heap before GC invocations=<span class="number">159969</span> (full <span class="number">2</span>):</span><br><span class="line"> garbage-first heap   total 6777216K, used 4599727K [x00000003c0000000, x00000003c0804000, x00000007c0000000)</span><br><span class="line">  region <span class="built_in">size</span> 192K, <span class="number">149</span> young (220608K), <span class="number">29</span> survivors (37568K)</span><br><span class="line"> Metaspace       used 5401K, capacity 5916K, committed 6224K, reserved 081344K</span><br><span class="line">  <span class="keyword">class</span> space    used 719K, capacity 839K, committed 968K, reserved 048576K</span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-7T09:<span class="number">49</span>:<span class="number">52.756</span>+<span class="number">0800</span>: <span class="number">942890.019</span>: [GC <span class="keyword">pause</span> (G1 Evacuation <span class="keyword">Pause</span>) (young)</span><br><span class="line">Desired survivor <span class="built_in">size</span> <span class="number">79691776</span> bytes, new threshold <span class="number">1</span> (<span class="built_in">max</span> <span class="number">15</span>)</span><br><span class="line">- age   <span class="number">1</span>:  <span class="number">243266912</span> bytes,  <span class="number">243266912</span> total</span><br><span class="line">, <span class="number">0.3106738</span> secs]</span><br><span class="line">   [Parallel Time: <span class="number">303.5</span> ms, GC Workers: <span class="number">13</span>]</span><br><span class="line">      [GC Worker Start (ms): <span class="built_in">Min</span>: <span class="number">942890019.9</span>, Avg: <span class="number">942890020.0</span>, <span class="built_in">Max</span>: <span class="number">942890020.0</span>, Diff: <span class="number">0.1</span>]</span><br><span class="line">      [Ext Root Scanning (ms): <span class="built_in">Min</span>: <span class="number">3.0</span>, Avg: <span class="number">3.0</span>, <span class="built_in">Max</span>: <span class="number">3.1</span>, Diff: <span class="number">0.1</span>, <span class="built_in">Sum</span>: <span class="number">39.2</span>]</span><br><span class="line">      [Update RS (ms): <span class="built_in">Min</span>: <span class="number">112.8</span>, Avg: <span class="number">114.5</span>, <span class="built_in">Max</span>: <span class="number">115.1</span>, Diff: <span class="number">2.3</span>, <span class="built_in">Sum</span>: <span class="number">1488.6</span>]</span><br><span class="line">         [Processed Buffers: <span class="built_in">Min</span>: <span class="number">52</span>, Avg: <span class="number">75.5</span>, <span class="built_in">Max</span>: <span class="number">115</span>, Diff: <span class="number">63</span>, <span class="built_in">Sum</span>: <span class="number">981</span>]</span><br><span class="line">      [<span class="built_in">Scan</span> RS (ms): <span class="built_in">Min</span>: <span class="number">3.4</span>, Avg: <span class="number">3.9</span>, <span class="built_in">Max</span>: <span class="number">4.1</span>, Diff: <span class="number">0.6</span>, <span class="built_in">Sum</span>: <span class="number">50.8</span>]</span><br><span class="line">      [Code Root Scanning (ms): <span class="built_in">Min</span>: <span class="number">0.0</span>, Avg: <span class="number">0.0</span>, <span class="built_in">Max</span>: <span class="number">0.0</span>, Diff: <span class="number">0.0</span>, <span class="built_in">Sum</span>: <span class="number">0.0</span>]</span><br><span class="line">      [Object Copy (ms): <span class="built_in">Min</span>: <span class="number">181.6</span>, Avg: <span class="number">181.7</span>, <span class="built_in">Max</span>: <span class="number">181.8</span>, Diff: <span class="number">0.2</span>, <span class="built_in">Sum</span>: <span class="number">2362.2</span>]</span><br><span class="line">      [Termination (ms): <span class="built_in">Min</span>: <span class="number">0.0</span>, Avg: <span class="number">0.0</span>, <span class="built_in">Max</span>: <span class="number">0.0</span>, Diff: <span class="number">0.0</span>, <span class="built_in">Sum</span>: <span class="number">0.0</span>]</span><br><span class="line">         [Termination Attempts: <span class="built_in">Min</span>: <span class="number">1</span>, Avg: <span class="number">1.0</span>, <span class="built_in">Max</span>: <span class="number">1</span>, Diff: <span class="number">0</span>, <span class="built_in">Sum</span>: <span class="number">13</span>]</span><br><span class="line">      [GC Worker Other (ms): <span class="built_in">Min</span>: <span class="number">0.0</span>, Avg: <span class="number">0.1</span>, <span class="built_in">Max</span>: <span class="number">0.1</span>, Diff: <span class="number">0.1</span>, <span class="built_in">Sum</span>: <span class="number">0.7</span>]</span><br><span class="line">      [GC Worker Total (ms): <span class="built_in">Min</span>: <span class="number">303.3</span>, Avg: <span class="number">303.3</span>, <span class="built_in">Max</span>: <span class="number">303.4</span>, Diff: <span class="number">0.1</span>, <span class="built_in">Sum</span>: <span class="number">3943.4</span>]</span><br><span class="line">      [GC Worker <span class="keyword">End</span> (ms): <span class="built_in">Min</span>: <span class="number">942890323.3</span>, Avg: <span class="number">942890323.3</span>, <span class="built_in">Max</span>: <span class="number">942890323.4</span>, Diff: <span class="number">0.1</span>]</span><br><span class="line">   [Code Root Fixup: <span class="number">0.1</span> ms]</span><br><span class="line">   [Code Root Purge: <span class="number">0.0</span> ms]</span><br><span class="line">   [Clear CT: <span class="number">0.9</span> ms]</span><br><span class="line">   [Other: <span class="number">6.1</span> ms]</span><br><span class="line">      [Choose CSet: <span class="number">0.0</span> ms]</span><br><span class="line">      [Ref Proc: <span class="number">3.8</span> ms]</span><br><span class="line">      [Ref Enq: <span class="number">0.0</span> ms]</span><br><span class="line">      [Redirty Cards: <span class="number">1.1</span> ms]</span><br><span class="line">      [Humongous Register: <span class="number">0.1</span> ms]</span><br><span class="line">      [Humongous Reclaim: <span class="number">0.1</span> ms]</span><br><span class="line">      [<span class="keyword">Free</span> CSet: <span class="number">0.3</span> ms]</span><br><span class="line">   [Eden: <span class="number">960.</span>M(<span class="number">960.</span>M)-&gt;<span class="number">0.</span>B(<span class="number">712.</span>M) Survivors: <span class="number">232.</span>M-&gt;<span class="number">152.</span>M Heap: <span class="number">13.</span>G(<span class="number">16.</span>G)-&gt;<span class="number">13.</span>G(<span class="number">16.</span>G)]</span><br><span class="line">Heap after GC invocations=<span class="number">159970</span> (full <span class="number">2</span>):</span><br><span class="line"> garbage-first heap   total 6777216K, used 3936175K [x00000003c0000000, x00000003c0804000, x00000007c0000000)</span><br><span class="line">  region <span class="built_in">size</span> 192K, <span class="number">19</span> young (55648K), <span class="number">19</span> survivors (55648K)</span><br><span class="line"> Metaspace       used 5401K, capacity 5916K, committed 6224K, reserved 081344K</span><br><span class="line">  <span class="keyword">class</span> space    used 719K, capacity 839K, committed 968K, reserved 048576K</span><br><span class="line">&#125;</span><br><span class="line"> [Times: user=<span class="number">4.00</span> sys=<span class="number">0.01</span>, <span class="type">real</span>=<span class="number">0.31</span> secs]</span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-7T09:<span class="number">49</span>:<span class="number">53.067</span>+<span class="number">0800</span>: <span class="number">942890.330</span>: Total time for which application threads were stopped: <span class="number">0.3122465</span> seconds, Stopping threads took: <span class="number">0.0002199</span> seconds</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">S0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2080768.0</span> <span class="number">1032192.0</span> <span class="number">14606336.0</span> <span class="number">11107990.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3240</span>  <span class="number">230.268</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.268</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2080768.0</span> <span class="number">1556480.0</span> <span class="number">14606336.0</span> <span class="number">11107990.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3240</span>  <span class="number">230.268</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.268</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2080768.0</span> <span class="number">1974272.0</span> <span class="number">14606336.0</span> <span class="number">11107990.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3240</span>  <span class="number">230.268</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.268</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2080768.0</span> <span class="number">1974272.0</span> <span class="number">14606336.0</span> <span class="number">11107990.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3240</span>  <span class="number">230.268</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.268</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2080768.0</span> <span class="number">319488.0</span> <span class="number">14606336.0</span> <span class="number">11111529.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3241</span>  <span class="number">230.308</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.308</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2080768.0</span> <span class="number">606208.0</span> <span class="number">14606336.0</span> <span class="number">11111529.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3241</span>  <span class="number">230.308</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.308</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2080768.0</span> <span class="number">892928.0</span> <span class="number">14606336.0</span> <span class="number">11111529.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3241</span>  <span class="number">230.308</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.308</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2080768.0</span> <span class="number">1228800.0</span> <span class="number">14606336.0</span> <span class="number">11111529.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3241</span>  <span class="number">230.308</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.308</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2080768.0</span> <span class="number">1581056.0</span> <span class="number">14606336.0</span> <span class="number">11111529.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3241</span>  <span class="number">230.308</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.308</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2080768.0</span> <span class="number">1941504.0</span> <span class="number">14606336.0</span> <span class="number">11111529.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3241</span>  <span class="number">230.308</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.308</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">286720.0</span> <span class="number">14598144.0</span> <span class="number">11118135.9</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3242</span>  <span class="number">230.347</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.347</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">606208.0</span> <span class="number">14598144.0</span> <span class="number">11118135.9</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3242</span>  <span class="number">230.347</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.347</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">950272.0</span> <span class="number">14598144.0</span> <span class="number">11118135.9</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3242</span>  <span class="number">230.347</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.347</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">1269760.0</span> <span class="number">14598144.0</span> <span class="number">11118135.9</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3242</span>  <span class="number">230.347</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.347</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">1622016.0</span> <span class="number">14598144.0</span> <span class="number">11118135.9</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3242</span>  <span class="number">230.347</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.347</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">1974272.0</span> <span class="number">14598144.0</span> <span class="number">11118135.9</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3242</span>  <span class="number">230.347</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.347</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">311296.0</span> <span class="number">14598144.0</span> <span class="number">11119663.4</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3243</span>  <span class="number">230.387</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.387</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">688128.0</span> <span class="number">14598144.0</span> <span class="number">11119663.4</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3243</span>  <span class="number">230.387</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.387</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">1097728.0</span> <span class="number">14598144.0</span> <span class="number">11119663.4</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3243</span>  <span class="number">230.387</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.387</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">1425408.0</span> <span class="number">14598144.0</span> <span class="number">11119663.4</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3243</span>  <span class="number">230.387</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.387</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">1769472.0</span> <span class="number">14598144.0</span> <span class="number">11119663.4</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3243</span>  <span class="number">230.387</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.387</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">81920.0</span>  <span class="number">0.0</span>   <span class="number">81920.0</span> <span class="number">2088960.0</span> <span class="number">90112.0</span>  <span class="number">14606336.0</span> <span class="number">11122090.6</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3244</span>  <span class="number">230.429</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.429</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">81920.0</span>  <span class="number">0.0</span>   <span class="number">81920.0</span> <span class="number">2088960.0</span> <span class="number">409600.0</span> <span class="number">14606336.0</span> <span class="number">11113898.6</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3244</span>  <span class="number">230.429</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.429</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">81920.0</span>  <span class="number">0.0</span>   <span class="number">81920.0</span> <span class="number">2088960.0</span> <span class="number">860160.0</span> <span class="number">14606336.0</span> <span class="number">11113898.6</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3244</span>  <span class="number">230.429</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.429</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">81920.0</span>  <span class="number">0.0</span>   <span class="number">81920.0</span> <span class="number">2088960.0</span> <span class="number">1294336.0</span> <span class="number">14606336.0</span> <span class="number">11113898.6</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3244</span>  <span class="number">230.429</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.429</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">81920.0</span>  <span class="number">0.0</span>   <span class="number">81920.0</span> <span class="number">2088960.0</span> <span class="number">1744896.0</span> <span class="number">14606336.0</span> <span class="number">11113898.6</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3244</span>  <span class="number">230.429</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.429</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">81920.0</span>  <span class="number">0.0</span>   <span class="number">81920.0</span> <span class="number">802816.0</span> <span class="number">237568.0</span> <span class="number">15892480.0</span> <span class="number">11121098.8</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3245</span>  <span class="number">230.472</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.472</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2113536.0</span> <span class="number">98304.0</span>  <span class="number">14589952.0</span> <span class="number">11111715.9</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3246</span>  <span class="number">230.523</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.523</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2113536.0</span> <span class="number">704512.0</span> <span class="number">14589952.0</span> <span class="number">11111715.9</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3246</span>  <span class="number">230.523</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.523</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2113536.0</span> <span class="number">1327104.0</span> <span class="number">14589952.0</span> <span class="number">11111715.9</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3246</span>  <span class="number">230.523</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.523</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2113536.0</span> <span class="number">1949696.0</span> <span class="number">14589952.0</span> <span class="number">11111715.9</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3246</span>  <span class="number">230.523</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.523</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">507904.0</span> <span class="number">14598144.0</span> <span class="number">11121792.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3247</span>  <span class="number">230.570</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.570</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">1138688.0</span> <span class="number">14598144.0</span> <span class="number">11121792.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3247</span>  <span class="number">230.570</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.570</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">1769472.0</span> <span class="number">14598144.0</span> <span class="number">11121792.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3247</span>  <span class="number">230.570</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.570</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">204800.0</span> <span class="number">14598144.0</span> <span class="number">11125318.7</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3248</span>  <span class="number">230.613</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.613</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">548864.0</span> <span class="number">14598144.0</span> <span class="number">11125318.7</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3248</span>  <span class="number">230.613</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.613</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">884736.0</span> <span class="number">14598144.0</span> <span class="number">11125318.7</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3248</span>  <span class="number">230.613</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.613</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">1228800.0</span> <span class="number">14598144.0</span> <span class="number">11125318.7</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3248</span>  <span class="number">230.613</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.613</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">1597440.0</span> <span class="number">14598144.0</span> <span class="number">11125318.7</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3248</span>  <span class="number">230.613</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.613</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2105344.0</span> <span class="number">1916928.0</span> <span class="number">14598144.0</span> <span class="number">11125318.7</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3248</span>  <span class="number">230.613</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.613</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2097152.0</span> <span class="number">262144.0</span> <span class="number">14606336.0</span> <span class="number">11127363.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3249</span>  <span class="number">230.658</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.658</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2097152.0</span> <span class="number">598016.0</span> <span class="number">14606336.0</span> <span class="number">11127363.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3249</span>  <span class="number">230.658</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.658</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2097152.0</span> <span class="number">942080.0</span> <span class="number">14606336.0</span> <span class="number">11127363.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3249</span>  <span class="number">230.658</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.658</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2097152.0</span> <span class="number">1277952.0</span> <span class="number">14606336.0</span> <span class="number">11127363.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3249</span>  <span class="number">230.658</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.658</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2097152.0</span> <span class="number">1630208.0</span> <span class="number">14606336.0</span> <span class="number">11127363.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3249</span>  <span class="number">230.658</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.658</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">73728.0</span>  <span class="number">0.0</span>   <span class="number">73728.0</span> <span class="number">2097152.0</span> <span class="number">1990656.0</span> <span class="number">14606336.0</span> <span class="number">11127363.3</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3250</span>  <span class="number">230.658</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.658</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2064384.0</span> <span class="number">311296.0</span> <span class="number">14622720.0</span> <span class="number">11126943.6</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3250</span>  <span class="number">230.702</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.702</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2064384.0</span> <span class="number">655360.0</span> <span class="number">14622720.0</span> <span class="number">11126943.6</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3250</span>  <span class="number">230.702</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.702</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2064384.0</span> <span class="number">999424.0</span> <span class="number">14622720.0</span> <span class="number">11126943.6</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3250</span>  <span class="number">230.702</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.702</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2064384.0</span> <span class="number">1351680.0</span> <span class="number">14622720.0</span> <span class="number">11126943.6</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3250</span>  <span class="number">230.702</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.702</span></span><br><span class="line"> <span class="number">0.0</span>   <span class="number">90112.0</span>  <span class="number">0.0</span>   <span class="number">90112.0</span> <span class="number">2064384.0</span> <span class="number">1720320.0</span> <span class="number">14622720.0</span> <span class="number">11126943.6</span> <span class="number">36220.0</span> <span class="number">35414.9</span> <span class="number">3964.0</span> <span class="number">3758.7</span>   <span class="number">3250</span>  <span class="number">230.702</span>   <span class="number">0</span>      <span class="number">0.000</span>  <span class="number">230.702</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Improving Apache Cassandra Performance</p>
</blockquote>
<p>Cassandra’s internal operations can also be a major driver of performance issues. One example is “tombstones”. A major advantage of Cassandra is its dynamic structure, where developers can create and remove columns on the fly. When rows have dynamic columns and the application deletes those columns, Cassandra creates a “tombstone” that exists until the next compaction runs and cleans up any that have expired (ones that have passed gc_grace_seconds). If the client application then performs a slice query to read a few columns, Cassandra will scan all the tombstones while looking for the non- deleted columns requested. For applications with hundreds of tombstones, Cassandra can fill up heap memory in Java reading them all and is likely to trigger a long pause for garbage collection, stalling the node.</p>
<p>Compaction can also be a problem. Cassandra by default uses a single thread for compaction. During this process, it reads each row, cleans up expired tombstones, then sorts the columns and writes the row to a new SSTable. Compaction only uses memory up to the limit you set (the default is 64 MB). Depending on how the process is throttled, objects can live in the heap too long and get promoted to the old generation causing it to fill up. Once it fills, the JVM will pause to perform a full GC and stall the node.</p>
<p>Similarly, Cassandra-based applications sometimes make use of wide rows, or those with hundreds of columns. If the application reads hundreds of columns from a single wide row, it will quickly fill the heap’s new generation and can cause promotion failures or concurrent mode failures. Once that happens, the JVM will pause for a full GC and stall the node.</p>
<p>A separate issue is the creation of short-lived objects for read requests under heavy load. Each read allocates short-lived objects in the heap for results returned to the client/coordinator and for any objects that actually process the request. As the number of read requests grows, the new gen quickly fills up with objects that aren’t yet garbage when the new gen GC runs (but will be soon). Some are moved into the old gen space to free up space in new gen for more objects. When the old gen fills up with these short-lived objects, a major GC is triggered that stops Cassandra for a long time, often several seconds or more.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Cassandra G1 GC使用，案例&lt;br&gt;参考文档：&lt;br&gt;&lt;a href=&quot;&amp;#109;&amp;#97;&amp;#x69;&amp;#108;&amp;#116;&amp;#111;&amp;#x3a;&amp;#104;&amp;#x74;&amp;#x74;&amp;#112;&amp;#x73;&amp;#58;&amp;#x2f;&amp;#x2f;&amp;#109;&amp;#x65;&amp;#x64;&amp;#105;&amp;#x75;&amp;#x6d;&amp;#x2e;&amp;#x63;&amp;#x6f;&amp;#109;&amp;#47;&amp;#x40;&amp;#109;&amp;#108;&amp;#x6f;&amp;#x77;&amp;#x69;&amp;#99;&amp;#107;&amp;#x69;&amp;#47;&amp;#109;&amp;#x6f;&amp;#x76;&amp;#101;&amp;#x2d;&amp;#99;&amp;#x61;&amp;#x73;&amp;#115;&amp;#x61;&amp;#110;&amp;#x64;&amp;#114;&amp;#x61;&amp;#x2d;&amp;#50;&amp;#45;&amp;#x31;&amp;#x2d;&amp;#116;&amp;#x6f;&amp;#x2d;&amp;#x67;&amp;#x31;&amp;#45;&amp;#103;&amp;#x61;&amp;#x72;&amp;#x62;&amp;#97;&amp;#x67;&amp;#x65;&amp;#x2d;&amp;#99;&amp;#x6f;&amp;#108;&amp;#x6c;&amp;#101;&amp;#x63;&amp;#116;&amp;#x6f;&amp;#x72;&amp;#x2d;&amp;#x62;&amp;#57;&amp;#x66;&amp;#x62;&amp;#x32;&amp;#x37;&amp;#51;&amp;#54;&amp;#x35;&amp;#x35;&amp;#48;&amp;#x39;&amp;#x23;&amp;#46;&amp;#55;&amp;#117;&amp;#103;&amp;#103;&amp;#x37;&amp;#x38;&amp;#x6e;&amp;#x71;&amp;#55;&quot;&gt;&amp;#104;&amp;#x74;&amp;#x74;&amp;#112;&amp;#x73;&amp;#58;&amp;#x2f;&amp;#x2f;&amp;#109;&amp;#x65;&amp;#x64;&amp;#105;&amp;#x75;&amp;#x6d;&amp;#x2e;&amp;#x63;&amp;#x6f;&amp;#109;&amp;#47;&amp;#x40;&amp;#109;&amp;#108;&amp;#x6f;&amp;#x77;&amp;#x69;&amp;#99;&amp;#107;&amp;#x69;&amp;#47;&amp;#109;&amp;#x6f;&amp;#x76;&amp;#101;&amp;#x2d;&amp;#99;&amp;#x61;&amp;#x73;&amp;#115;&amp;#x61;&amp;#110;&amp;#x64;&amp;#114;&amp;#x61;&amp;#x2d;&amp;#50;&amp;#45;&amp;#x31;&amp;#x2d;&amp;#116;&amp;#x6f;&amp;#x2d;&amp;#x67;&amp;#x31;&amp;#45;&amp;#103;&amp;#x61;&amp;#x72;&amp;#x62;&amp;#97;&amp;#x67;&amp;#x65;&amp;#x2d;&amp;#99;&amp;#x6f;&amp;#108;&amp;#x6c;&amp;#101;&amp;#x63;&amp;#116;&amp;#x6f;&amp;#x72;&amp;#x2d;&amp;#x62;&amp;#57;&amp;#x66;&amp;#x62;&amp;#x32;&amp;#x37;&amp;#51;&amp;#54;&amp;#x35;&amp;#x35;&amp;#48;&amp;#x39;&amp;#x23;&amp;#46;&amp;#55;&amp;#117;&amp;#103;&amp;#103;&amp;#x37;&amp;#x38;&amp;#x6e;&amp;#x71;&amp;#55;&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://www.oracle.com/technetwork/cn/articles/java/g1gc-1984535-zhs.html&quot;&gt;http://www.oracle.com/technetwork/cn/articles/java/g1gc-1984535-zhs.html&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="cassandra" scheme="http://github.com/zqhxuyuan/categories/cassandra/"/>
    
    
      <category term="cassandra" scheme="http://github.com/zqhxuyuan/tags/cassandra/"/>
    
  </entry>
  
  <entry>
    <title>Cassandra CMS GC</title>
    <link href="http://github.com/zqhxuyuan/2016/07/07/Cassandra-GC-CMS/"/>
    <id>http://github.com/zqhxuyuan/2016/07/07/Cassandra-GC-CMS/</id>
    <published>2016-07-06T16:00:00.000Z</published>
    <updated>2016-07-28T09:26:13.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://aryanet.com/blog/cassandra-garbage-collector-tuning" target="_blank" rel="external">http://aryanet.com/blog/cassandra-garbage-collector-tuning</a><br><a href="http://tech.shift.com/post/74311817513/cassandra-tuning-the-jvm-for-read-heavy-workloads" target="_blank" rel="external">http://tech.shift.com/post/74311817513/cassandra-tuning-the-jvm-for-read-heavy-workloads</a><br><a href="http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html" target="_blank" rel="external">http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html</a></p>
<a id="more"></a>
<h3 id="GC_Background">GC Background</h3><blockquote>
<p>When tuning your garbage collection configuration, the main things you need to worry about are pause time and throughput. 关注停顿时间和吞吐量<br>Pause time is the length of time the collector stops the application while it frees up memory. 停顿是垃圾收集器释放内存,停止响应应用的时间.<br>Throughput is determined by how often the garbage collection runs, and pauses the application. 吞吐量由垃圾收集器多长时间运行一次并停止应用程序决定的.<br>The more often the collector runs, the lower the throughput. 垃圾收集器运行的越频繁, 就会降低吞吐量.  </p>
<p>the new gen is collected by the Parallel New (ParNew) collector, 新生代的对象由ParNew垃圾收集器收集<br>and the old gen is collected by the Concurrent Mark and Sweep (CMS) collector. 老年代的对象由CMS垃圾收集器收集.  </p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20151022134521140" alt="gc_memory"></p>
<blockquote>
<p>When eden fills up with new objects, a minor gc is triggered. A minor gc stops execution, iterates over the objects in eden, 当伊甸区填满新生对象,YGC被触发:它会停止应用程序的执行<br>copies any objects that are not (yet) garbage to the active survivor space, and clears eden. 迭代伊甸区的对象,拷贝所有没有被垃圾回收的对象到一个存活区中,并清理伊甸区.</p>
<p>为什么YGC发生的时候会StopTheWorld! 因为新创建new出来的对象会首先放到Eden中,现在Eden满了,新创建的对象就没有地方放了.<br>得等到Eden中的全部对象都转移到Survivor,并清空Eden, 新创建的对象才可以继续往Eden中存放.这时才从StopTheWorld中恢复过来.  </p>
<p>If an object has survived a certain number of survivor space collections, (cassandra defaults to 1), 如果一个对象在多次存活区的垃圾回收中仍存活<br>it is promoted to the old generation. Once this is done, the application resumes execution. 它就会被提升到老年代,当这个(YGC)完成时,应用程序才恢复执行.</p>
<p>If you have long ParNew pauses, it means that a lot of the objects in eden are not (yet) garbage, 如果有长时间的ParNew收集,说明很多的对象<br>and they’re being copied around to the survivor space, or into the old gen. 在伊甸区还没有被垃圾回收,就被拷贝到存活区或者老年代.  </p>
<p>什么是垃圾收集,一个很简单的理解方式是: 垃圾被收集. 这样原来被垃圾占满的空间, 被收集之后, <code>空间</code>就被释放了.<br>在收集垃圾的时候, 你是没办法做其他事情的, 因此如果收集垃圾的这个过程很长, 应用程序就会被阻塞住了,就会出现<code>超时</code>的现象.  </p>
</blockquote>
<h3 id="Cassandra_&amp;_GCInspector">Cassandra &amp; GCInspector</h3><blockquote>
<p>ParNew是YGC, 会<code>Stop The World</code>. 所以通常GC for ParNew不会超过一秒. Cassandra会打印出超过200ms的ParNew.<br>Cassandra’s GCInspector class logs information about garbage collection whenever a garbage collection takes longer than 200ms. 超过200ms的GC会被记录<br>Garbage collections that occur frequently and take a moderate length of time to complete (such as ConcurrentMarkSweep taking a few seconds), 垃圾收集频繁且缓慢<br>indicate that there is a lot of garbage collection pressure on the JVM. 说明很多的垃圾回收操作导致JVM有压力.</p>
</blockquote>
<h2 id="Problem_1:_进程假死,_很长时间的GC_for_CMS,_最终OOM_[202@2015-10-21_16:19,_21:10]">Problem 1: 进程假死, 很长时间的GC for CMS, 最终OOM [202@2015-10-21 16:19, 21:10]</h2><p>线上遇到一个问题: Cassandra进程虽然存在,但是使用status查询状态为DN. 这时查看system.log发现这段时间(进程假死)CMS的时间很长.  </p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat /var/log/cassandra/system.log | grep 'GC for .*: [<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>]' | grep '2015-10-21' </span><br><span class="line">cat /var/log/cassandra/system.log | grep "GC for ParNew: [<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]" | grep "2015-10-21"</span><br><span class="line">cat /var/log/cassandra/system.log | grep "GC for ConcurrentMarkSweep: [<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]" | grep "2015-10-21"</span><br></pre></td></tr></table></figure>
<p>超过1s/10s中的YGC(ParNew):<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[<span class="link_label">qihuang.zheng@cass047202 cassandra</span>]$ cat system.log | grep "GC for ParNew: [<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]" | grep "2015-10-21" | wc -l</span><br><span class="line">138</span><br><span class="line">[<span class="link_label">qihuang.zheng@cass047202 cassandra</span>]$ cat system.log | grep "GC for ParNew: [<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>]" | grep "2015-10-21"</span><br><span class="line"> INFO [ScheduledTasks:1] 2015-10-21 16:19:08,479 GCInspector.java (line 116) GC for ParNew: 11158 ms for 2 collections, 14193894536 used; max is 16750411776</span><br></pre></td></tr></table></figure></p>
<p>超过10s的CMS(ConcurrentMarkSweep)<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047202 cassandra]$ cat system.<span class="built_in">log</span> | grep <span class="string">"GC for ConcurrentMarkSweep: [0-9][0-9][0-9][0-9][0-9]"</span> | grep <span class="string">"2015-10-21"</span> | wc -l</span><br><span class="line"><span class="number">40</span></span><br><span class="line">[qihuang.zheng@cass047202 cassandra]$ cat system.<span class="built_in">log</span> | grep <span class="string">"GC for ConcurrentMarkSweep: [0-9][0-9][0-9][0-9][0-9]"</span> | grep <span class="string">"2015-10-21"</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">16</span>:<span class="number">19</span>:<span class="number">08</span>,<span class="number">577</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">35447</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">1486</span>,<span class="number">5198320</span> used; max is <span class="number">1675</span>,<span class="number">0411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">16</span>:<span class="number">20</span>:<span class="number">31</span>,<span class="number">955</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">57793</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">1638</span>,<span class="number">9516152</span> used; max is <span class="number">1675</span>,<span class="number">0411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">16</span>:<span class="number">21</span>:<span class="number">56</span>,<span class="number">322</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">53039</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">1668</span>,<span class="number">0865152</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">16</span>:<span class="number">23</span>:<span class="number">21</span>,<span class="number">821</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">58749</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">1674</span>,<span class="number">1060824</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">17</span>:<span class="number">02</span>:<span class="number">46</span>,<span class="number">410</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">1325054</span> ms <span class="keyword">for</span> <span class="number">31</span> collections, <span class="number">1650</span>,<span class="number">1107888</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">10</span>:<span class="number">56</span>,<span class="number">385</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">41123</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">1129</span>,<span class="number">1487752</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">11</span>:<span class="number">52</span>,<span class="number">609</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">34233</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">1363</span>,<span class="number">4908632</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> ...(total <span class="number">17</span> counts)</span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">39</span>:<span class="number">14</span>,<span class="number">721</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">177348</span> ms <span class="keyword">for</span> <span class="number">3</span> collections, <span class="number">1671</span>,<span class="number">3835416</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">22</span>:<span class="number">43</span>:<span class="number">08</span>,<span class="number">348</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">2243741</span> ms <span class="keyword">for</span> <span class="number">51</span> collections, <span class="number">1647</span>,<span class="number">0811144</span> used; max is <span class="number">16750411776</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>注意: 这里我们只是先简单地查看下GC的情况, 具体是什么引起GC, 要结合system.log中具体的日志查看的! 同时我们统计的是比较耗时的(比如超过1s或10秒)  </p>
<p>GC for CMS是老年代的GC. 如果这种情况出现很多, 并且时间超过10s, 就会造成Cassandra进程假死的状态: 进程没有停掉, 但是状态为DN.<br>简单地分析其中的一条日志:<br>GC for ConcurrentMarkSweep: 35447 ms for 1 collections, 1486,5198320 used; max is 1675,0411776<br>收集了一次, 花费了35s, 这时候堆内存为14G, 最大值为16G. 越往后几条数据的堆内存越接近16G(从而导致OOM).   </p>
</blockquote>
<p>查看其他节点这时候的日志, 因为Gossip协议会连接到这台假死的202节点, 就会报告202节点是DOWN的:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047205 cassandra]$ cat  system.<span class="built_in">log</span> | grep <span class="string">"202 is now DOWN"</span></span><br><span class="line"></span><br><span class="line"> INFO [GossipTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">16</span>:<span class="number">18</span>:<span class="number">22</span>,<span class="number">861</span> Gossiper.java (line <span class="number">962</span>) InetAddress /<span class="number">192.168</span><span class="number">.47</span><span class="number">.202</span> is now DOWN</span><br><span class="line"> INFO [GossipStage:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">17</span>:<span class="number">02</span>:<span class="number">45</span>,<span class="number">239</span> Gossiper.java (line <span class="number">962</span>) InetAddress /<span class="number">192.168</span><span class="number">.47</span><span class="number">.202</span> is now DOWN</span><br><span class="line"> INFO [GossipTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">10</span>:<span class="number">13</span>,<span class="number">752</span> Gossiper.java (line <span class="number">962</span>) InetAddress /<span class="number">192.168</span><span class="number">.47</span><span class="number">.202</span> is now DOWN</span><br><span class="line"> INFO [GossipTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">12</span>:<span class="number">18</span>,<span class="number">980</span> Gossiper.java (line <span class="number">962</span>) InetAddress /<span class="number">192.168</span><span class="number">.47</span><span class="number">.202</span> is now DOWN</span><br><span class="line"> ...</span><br><span class="line"> INFO [GossipTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">38</span>:<span class="number">04</span>,<span class="number">605</span> Gossiper.java (line <span class="number">962</span>) InetAddress /<span class="number">192.168</span><span class="number">.47</span><span class="number">.202</span> is now DOWN</span><br><span class="line"> INFO [GossipStage:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">22</span>:<span class="number">43</span>:<span class="number">08</span>,<span class="number">139</span> Gossiper.java (line <span class="number">962</span>) InetAddress /<span class="number">192.168</span><span class="number">.47</span><span class="number">.202</span> is now DOWN</span><br><span class="line"> INFO [GossipTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">22</span>:<span class="number">43</span>:<span class="number">26</span>,<span class="number">225</span> Gossiper.java (line <span class="number">962</span>) InetAddress /<span class="number">192.168</span><span class="number">.47</span><span class="number">.202</span> is now DOWN</span><br></pre></td></tr></table></figure></p>
<p>在这台假死的202节点上可以看到最终因为OOM,Cassandra进程被杀死:<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ERROR [RMI TCP <span class="function"><span class="title">Connection</span><span class="params">(idle)</span></span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">17</span>:<span class="number">02</span>:<span class="number">44</span>,<span class="number">484</span> CassandraDaemon<span class="class">.java</span> (line <span class="number">258</span>) Exception <span class="keyword">in</span> thread Thread[RMI TCP <span class="function"><span class="title">Connection</span><span class="params">(idle)</span></span>,<span class="number">5</span>,RMI Runtime]</span><br><span class="line">java<span class="class">.lang</span><span class="class">.OutOfMemoryError</span>: Java heap space</span><br><span class="line">        at java<span class="class">.io</span><span class="class">.ObjectStreamClass</span><span class="class">.lookup</span>(ObjectStreamClass<span class="class">.java</span>:<span class="number">322</span>)</span><br><span class="line"></span><br><span class="line">ERROR [RMI TCP <span class="function"><span class="title">Connection</span><span class="params">(idle)</span></span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">22</span>:<span class="number">39</span>:<span class="number">03</span>,<span class="number">802</span> CassandraDaemon<span class="class">.java</span> (line <span class="number">258</span>) Exception <span class="keyword">in</span> thread Thread[RMI TCP <span class="function"><span class="title">Connection</span><span class="params">(idle)</span></span>,<span class="number">5</span>,RMI Runtime]</span><br><span class="line">java<span class="class">.lang</span><span class="class">.OutOfMemoryError</span>: Java heap space</span><br><span class="line">        at java<span class="class">.util</span><span class="class">.Arrays</span><span class="class">.copyOfRange</span>(Arrays<span class="class">.java</span>:<span class="number">2694</span>)</span><br><span class="line"></span><br><span class="line">ERROR [ReadStage:<span class="number">41</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">22</span>:<span class="number">43</span>:<span class="number">07</span>,<span class="number">512</span> CassandraDaemon<span class="class">.java</span> (line <span class="number">258</span>) Exception <span class="keyword">in</span> thread Thread[ReadStage:<span class="number">41</span>,<span class="number">5</span>,main]</span><br><span class="line">java<span class="class">.lang</span><span class="class">.OutOfMemoryError</span>: Java heap space</span><br><span class="line">        at java<span class="class">.nio</span><span class="class">.ByteBuffer</span><span class="class">.allocate</span>(ByteBuffer<span class="class">.java</span>:<span class="number">331</span>)</span><br><span class="line">        at org<span class="class">.apache</span><span class="class">.cassandra</span><span class="class">.io</span><span class="class">.util</span><span class="class">.MappedFileDataInput</span><span class="class">.readBytes</span>(MappedFileDataInput<span class="class">.java</span>:<span class="number">146</span>)</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>旧生代(Old Gen)空间只有在<code>新生代对象转入</code>及<code>创建大对象</code>(大对象不会在新生代中,会直接转到旧生代)才会出现不足的现象，<br>当执行<code>Full GC后老年代空间仍然不足</code>，则抛出如下错误：java.lang.OutOfMemoryError: Java heap space  </p>
<p>新生代对象要转入到老年代中, 但是老年代的空间不足以存放新生代的对象, 因此需要首先对老年代进行一次FGC(还是CMS GC?)<br>当FGC完成后, 就可以把新生代对象转入了. 但是但是如果FGC后, 老年代的空间还是不够新生代的转入,就报堆内存不足OOM了!  </p>
</blockquote>
<p>分析问题的入口点是: 什么引起很长时间的GC, 因为GC, 最终导致OOM.  </p>
<h3 id="Zabbix监控">Zabbix监控</h3><p>上面的日志显示在16:19 - 17:02这段时间发生了频繁的FGC,并且时间都很长. 同样的情况发生在21:10 - 22:43.<br>下图分别是堆内存的使用情况以及CMS老年代占用的大小. 可以看到这两个时间段都没有数据显示出来.  </p>
<p><img src="http://img.blog.csdn.net/20151022101159811" alt="c_heap">  </p>
<blockquote>
<p>从Heap的趋势可以看到一旦到达12G,就有个下降的趋势,然后继续上升. 回想垃圾收集的原理:垃圾被收集后,空间被释放!<br>这是因为最大Heap内存为16G,在达到0.75*16G=12G时发生CMS GC(下图), GC完成后, 堆内存就被释放.<br><code>-Xms16G -Xmx16G</code> -Xmn4G -XX:MaxDirectMemorySize=6G -XX:+HeapDumpOnOutOfMemoryError -Xss256k<br>-XX:StringTableSize=1000003 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled<br>-XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=<code>75</code><br>-XX:+UseCMSInitiatingOccupancyOnly -XX:+UseTLAB -XX:+UseCondCardMark<br>最大堆内存=16G, 新生代内存=4G, 新生代中Edeng占比80%=3276MB.   </p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20151022101214392" alt="c_cms"></p>
<blockquote>
<p>当达到老年代的75%, CMS收集器就开始运行. 老年代大小=12G, *0.75=9G. 所以上图在9G时有个下降的趋势.<br>When a pre-determined percentage of the old generation is full (75% by default in cassandra), the CMS collector is run.  </p>
</blockquote>
<p>Heap堆内存的使用和Old Gen的大小是同向的. 因为老年代内存增加,会占用更多的堆内存. 老年代收集器运行完成后, 释放老年代空间, 就减少了堆内存.  </p>
<p><img src="http://img.blog.csdn.net/20151023185937620" alt="202-jvm"></p>
<blockquote>
<p>上图是更完整的GC信息. 淡绿色的是CMS Perm Gen. 这个区域是永久区,不会有很大的变化, 所以可以看到最大值和平均值都只是在27MB.<br>蓝色的Par Eden区是新创建的对象, 最小值32M, 最大值3.2G. 这是因为Eden为空时,刚创建的对象没几个,所以很小. 当Eden填满时, 达到最大值.<br>那么Eden的容量是多大, 可以通过jstat -gc pid查看.  Eden和Survivor的比例是8:1, 所以Survivor的大小是400M. ParNew的总大小=3.2+800M=4G.<br>淡蓝色的Par Survivor的最大值是409Mb. 可见上面我们的推论也是正确的. 因为Survivor和Eden一样都有一个清空的过程, 所以最小值都是很小的.<br>最后一个橘黄色的是CMS老年代. 最大值为10.4G(和上面我们说的9G有差别啊,不过这只是个别情况).  老年代的10G+新生代的4G接近堆内存最大值的16G.  </p>
<p>上图基本上是很多个山峰的形状. 说明了:<br>1.随着对象的不断创建, 对象会先后在Eden, Survivor, Old中存活.<br>2.对象创建,就会消耗一定的内存(堆内存会不断增加), 对象首先在新生代中存活.<br>3.在Survivor中存活的对象被提升到老年代(因此老年代的内存增加也会导致堆内存不断增加).<br>4.老年代垃圾收集完成后, 堆内存会被释放(堆内存减少的比较明显,因为老年代的大小比较大,收集之后释放的空间也比较多).   </p>
</blockquote>
<h3 id="system-log_&amp;_gc-log_关于GC的吻合">system.log &amp; gc.log 关于GC的吻合</h3><p>在21号202节点一共当过(其实都是假死)三次,并重启. 在<code>重启之前(并不是进程当掉的时候)</code>会产生一个新的gc日志文件用于<code>下一个时间段</code>.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047202 cassandra]$ ll -h</span><br><span class="line">-rw-r--r--. <span class="number">1</span> admin admin <span class="number">2.5</span>G <span class="number">10</span>月 <span class="number">21</span> <span class="number">10</span>:<span class="number">57</span> gc-<span class="number">1442762094.l</span>og   在<span class="number">10</span>:<span class="number">57</span>分重启下, 这里包含了<span class="number">21</span>好之前的所有gc日志,所以日志文件很大</span><br><span class="line">-rw-r--r--. <span class="number">1</span> admin admin  <span class="number">22</span>M <span class="number">10</span>月 <span class="number">21</span> <span class="number">17</span>:<span class="number">02</span> gc-<span class="number">1445396282.l</span>og   <span class="number">16</span>:<span class="number">19</span>开始FGC,这时C进程还在,直到<span class="number">17</span>:<span class="number">02</span>分进程当掉,开始重启.这里日志表示<span class="number">10</span>:<span class="number">57</span>到<span class="number">17</span>:<span class="number">02</span>之间的gc日志</span><br><span class="line">-rw-r--r--. <span class="number">1</span> admin admin  <span class="number">33</span>M <span class="number">10</span>月 <span class="number">21</span> <span class="number">22</span>:<span class="number">43</span> gc-<span class="number">1445418381.l</span>og   <span class="number">21</span>:<span class="number">10</span>开始FGC,在<span class="number">22</span>:<span class="number">43</span>才重启. 这里日志表示<span class="number">17</span>:<span class="number">02</span>到<span class="number">22</span>:<span class="number">43</span>之间的gc日志</span><br><span class="line">-rw-r--r--. <span class="number">1</span> admin admin  <span class="number">28</span>M <span class="number">10</span>月 <span class="number">22</span> <span class="number">10</span>:<span class="number">24</span> gc-<span class="number">1445439377.l</span>og   这里表示<span class="number">22</span>:<span class="number">43</span>一直到现在的gc日志.</span><br></pre></td></tr></table></figure></p>
<p>分别查看不同的gc日志文件. GC时间超过10s的和system.log中超过10s的FGC的时间段是对应的(total time的秒数有点区别).<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047202 cassandra]$ cat gc-<span class="number">1445396282.l</span>og | grep <span class="string">"2015-10-2"</span> | grep <span class="string">"Total time for which application threads were stopped: [^0][0-9].*$"</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T16:<span class="number">18</span>:<span class="number">56.346</span>+<span class="number">0800</span>: <span class="number">19252.706</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">52.9533050</span> seconds</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T16:<span class="number">19</span>:<span class="number">08.005</span>+<span class="number">0800</span>: <span class="number">19264.365</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">11.1686950</span> seconds</span><br><span class="line">....</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T17:<span class="number">02</span>:<span class="number">41.868</span>+<span class="number">0800</span>: <span class="number">21878.228</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">81.2595120</span> seconds</span><br><span class="line"></span><br><span class="line">[qihuang.zheng@cass047202 cassandra]$ cat gc-<span class="number">1445418381.l</span>og | grep <span class="string">"2015-10-2"</span> | grep <span class="string">"Total time for which application threads were stopped: [^0][0-9].*$"</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">10</span>:<span class="number">56.372</span>+<span class="number">0800</span>: <span class="number">14674.292</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">61.2555790</span> seconds</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">11</span>:<span class="number">52.177</span>+<span class="number">0800</span>: <span class="number">14730.097</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">55.4579170</span> seconds</span><br><span class="line">....</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T22:<span class="number">39</span>:<span class="number">03.800</span>+<span class="number">0800</span>: <span class="number">19961.719</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">82.8747310</span> seconds</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T22:<span class="number">43</span>:<span class="number">04.221</span>+<span class="number">0800</span>: <span class="number">20202.140</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">240.4117940</span> seconds</span><br></pre></td></tr></table></figure></p>
<p>观察正常的203节点发生GC for CMS情况: 会存在10几秒的, 但是不会夸张到一分钟多的.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047203 cassandra]$ cat system.<span class="built_in">log</span> | grep <span class="string">"GC for ConcurrentMarkSweep: [0-9][0-9][0-9][0-9][0-9]"</span> | grep <span class="string">"2015-10-2"</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">20</span> <span class="number">04</span>:<span class="number">54</span>:<span class="number">07</span>,<span class="number">318</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">13873</span> ms <span class="keyword">for</span> <span class="number">2</span> collections, <span class="number">2258327168</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">05</span>:<span class="number">10</span>:<span class="number">31</span>,<span class="number">167</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">15803</span> ms <span class="keyword">for</span> <span class="number">2</span> collections, <span class="number">3578782544</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span> <span class="number">04</span>:<span class="number">58</span>:<span class="number">11</span>,<span class="number">610</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">13137</span> ms <span class="keyword">for</span> <span class="number">2</span> collections, <span class="number">2438639584</span> used; max is <span class="number">16750411776</span></span><br></pre></td></tr></table></figure></p>
<p>对应的GC日志(大于10s的):  可以看到时间都能对应上.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047203 cassandra]$ cat gc-<span class="number">1441001670.l</span>og | grep <span class="string">"2015-10-2"</span> | grep <span class="string">"Total time for which application threads were stopped: [^0][0-9].*$"</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">20</span>T04:<span class="number">54</span>:<span class="number">07.168</span>+<span class="number">0800</span>: <span class="number">4286376.327</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">27.1404730</span> seconds</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T05:<span class="number">10</span>:<span class="number">30.907</span>+<span class="number">0800</span>: <span class="number">4373760.065</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">30.5866390</span> seconds</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span>T04:<span class="number">58</span>:<span class="number">11.372</span>+<span class="number">0800</span>: <span class="number">4459420.530</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">23.8347940</span> seconds</span><br></pre></td></tr></table></figure></p>
<h3 id="GC_Log分析">GC Log分析</h3><h3 id="YGC">YGC</h3><p><strong>1.一次比较短暂的GC(YGC)示例</strong>  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">05</span>:<span class="number">22.465</span>+<span class="number">0800</span>: <span class="number">14340.384</span>: [GCBefore GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">05</span>:<span class="number">22.467</span>+<span class="number">0800</span>: <span class="number">14340.386</span>: [ParNew: <span class="number">3363</span>,<span class="number">261</span>K-&gt;<span class="number">11</span>,<span class="number">273</span>K(<span class="number">3774</span>,<span class="number">912</span>K), <span class="number">0.0429780</span> secs] <span class="number">6720</span>,<span class="number">828</span>K-&gt;<span class="number">3371</span>,<span class="number">977</span>K(<span class="number">16357824</span>K)After GC:</span><br><span class="line">, <span class="number">0.0464130</span> secs] [Times: user=<span class="number">0.28</span> sys=<span class="number">0.00</span>, real=<span class="number">0.04</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">05</span>:<span class="number">22.512</span>+<span class="number">0800</span>: <span class="number">14340.431</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.0502940</span> seconds</span><br></pre></td></tr></table></figure>
<p>Total time stopped=0.05的计算方式是 当前打印的时间.512 - BeforeGC=.465 = 0.047<br>可以看到在After GC之前, ParNew由3.2G减少到11M(新生代总的容量为3.7G). 并且GC时间很短, 只有0.04s. 因为是ParNew,说明这是一次YGC!  </p>
<blockquote>
<p>像这种在gc.log中的ParNew收集, 对应在system.log中就会出现GC for ParNew的日志.<br>当然gc.log对于所有的ParNew都会打印, 而system.log中只有超过200ms的才打印.<br>比如上面gc.log的时间为0.05秒=50ms, 就不会在system.log中出现.  </p>
</blockquote>
<p><strong>2.统计GC for ParNew和for CMS</strong>  </p>
<p>我们可以选择system.log中出现GC的记录, 再来找gc.log中对应的记录, 对比着分析. 这是system.log中21点所有的GC日志:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047202 cassandra]$ cat system.<span class="built_in">log</span> | grep <span class="string">"GC"</span> | grep <span class="string">"2015-10-21 21"</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">02</span>:<span class="number">31</span>,<span class="number">882</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">672</span> ms <span class="keyword">for</span> <span class="number">2</span> collections, <span class="number">9135816744</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">02</span>:<span class="number">39</span>,<span class="number">945</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">372</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">9325350800</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">02</span>:<span class="number">42</span>,<span class="number">106</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">247</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">9448812016</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">03</span>:<span class="number">19</span>,<span class="number">142</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">267</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">9969463696</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">03</span>:<span class="number">56</span>,<span class="number">802</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">1549</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">10679738240</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">03</span>:<span class="number">57</span>,<span class="number">783</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">750</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">11082238408</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">03</span>:<span class="number">59</span>,<span class="number">784</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">351</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">11876803840</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">09</span>:<span class="number">42</span>,<span class="number">724</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">1420</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">5274947408</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">09</span>:<span class="number">45</span>,<span class="number">891</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">2342</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">7007901632</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">09</span>:<span class="number">49</span>,<span class="number">573</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">3161</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">9012309600</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">09</span>:<span class="number">53</span>,<span class="number">443</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">3277</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">11212220496</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">09</span>:<span class="number">54</span>,<span class="number">790</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">1142</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">12623059520</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">10</span>:<span class="number">56</span>,<span class="number">385</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">41123</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">11291487752</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">11</span>:<span class="number">52</span>,<span class="number">609</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">34233</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">13634908632</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">13</span>:<span class="number">14</span>,<span class="number">546</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">50091</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16324832400</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">14</span>:<span class="number">39</span>,<span class="number">080</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">53386</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16329236976</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">16</span>:<span class="number">02</span>,<span class="number">382</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">51146</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16327015232</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">17</span>:<span class="number">26</span>,<span class="number">713</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">50636</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16331153704</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">18</span>:<span class="number">55</span>,<span class="number">963</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">54231</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16330768976</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">20</span>:<span class="number">20</span>,<span class="number">116</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">50762</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16393698912</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">21</span>:<span class="number">49</span>,<span class="number">551</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">56023</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16542754632</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">23</span>:<span class="number">16</span>,<span class="number">369</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">56855</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16668440992</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">29</span>:<span class="number">02</span>,<span class="number">054</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">232369</span> ms <span class="keyword">for</span> <span class="number">4</span> collections, <span class="number">16133717848</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">30</span>:<span class="number">31</span>,<span class="number">795</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">53874</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16326381000</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">31</span>:<span class="number">56</span>,<span class="number">763</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">52632</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16438384848</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">33</span>:<span class="number">25</span>,<span class="number">591</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">55474</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16543840528</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">34</span>:<span class="number">51</span>,<span class="number">572</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">54773</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16613439072</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">39</span>:<span class="number">14</span>,<span class="number">721</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">177348</span> ms <span class="keyword">for</span> <span class="number">3</span> collections, <span class="number">16713835416</span> used; max is <span class="number">16750411776</span></span><br></pre></td></tr></table></figure></p>
<p>观察下面几个时间点的used. 在03:57的CMS和03:59的ParNew时, used是增加的, 但是接下来的09:42,则一下子降到了5G. 具体原因是CMS收集释放了老年代内存.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">21</span>:<span class="number">03</span>:<span class="number">57</span>,<span class="number">783</span> GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">750</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">11082238408</span> used; max is <span class="number">16750411776</span></span><br><span class="line"><span class="number">21</span>:<span class="number">03</span>:<span class="number">59</span>,<span class="number">784</span> GC <span class="keyword">for</span> ParNew: <span class="number">351</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">11876803840</span> used; max is <span class="number">16750411776</span></span><br><span class="line">... 这中间虽然没有GC <span class="keyword">for</span> ..., 但是有CMS的日志. 我们上面这么过滤只查询GC,所以没有CMS的日志出来.  </span><br><span class="line"><span class="number">21</span>:<span class="number">09</span>:<span class="number">42</span>,<span class="number">724</span> GC <span class="keyword">for</span> ParNew: <span class="number">1420</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">5274947408</span> used; max is <span class="number">16750411776</span></span><br></pre></td></tr></table></figure></p>
<p><strong>3.对比gc.log</strong>   </p>
<blockquote>
<p>通常来说gc.log中打印的Total time stopped会比GC for 的时间要长. 这是因为stopped的时间不仅仅包括GC. GC for ParNew跟[ParNew: ** secs]比较接近.   </p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">02</span>:<span class="number">31.202</span>+<span class="number">0800</span>: <span class="number">14169.122</span>: [GCBefore GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">02</span>:<span class="number">31.203</span>+<span class="number">0800</span>: <span class="number">14169.122</span>: [ParNew: <span class="number">3365</span>,<span class="number">222</span>K-&gt;<span class="number">12</span>,<span class="number">415</span>K(<span class="number">3774</span>,<span class="number">912</span>K), <span class="number">0.4782790</span> secs] <span class="number">12265</span>,<span class="number">316</span>K-&gt;<span class="number">8916</span>,<span class="number">561</span>K(<span class="number">16357824</span>K)After GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">02</span>:<span class="number">31.682</span>+<span class="number">0800</span>: <span class="number">14169.601</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.4870030</span> seconds</span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">02</span>:<span class="number">39.570</span>+<span class="number">0800</span>: <span class="number">14177.489</span>: [GCBefore GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">02</span>:<span class="number">39.571</span>+<span class="number">0800</span>: <span class="number">14177.490</span>: [ParNew: <span class="number">3368051</span>K-&gt;<span class="number">178376</span>K(<span class="number">3774912</span>K), <span class="number">0.3727510</span> secs] <span class="number">12288868</span>K-&gt;<span class="number">9106223</span>K(<span class="number">16357824</span>K)After GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">02</span>:<span class="number">39.944</span>+<span class="number">0800</span>: <span class="number">14177.863</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.3774920</span> seconds</span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">02</span>:<span class="number">41.857</span>+<span class="number">0800</span>: <span class="number">14179.776</span>: [GCBefore GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">02</span>:<span class="number">41.857</span>+<span class="number">0800</span>: <span class="number">14179.777</span>: [ParNew: <span class="number">3533896</span>K-&gt;<span class="number">126378</span>K(<span class="number">3774912</span>K), <span class="number">0.2469170</span> secs] <span class="number">12461743</span>K-&gt;<span class="number">9226735</span>K(<span class="number">16357824</span>K)After GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">02</span>:<span class="number">42.105</span>+<span class="number">0800</span>: <span class="number">14180.024</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.2506860</span> seconds</span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">55.249</span>+<span class="number">0800</span>: <span class="number">14253.168</span>: [GCBefore GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">55.249</span>+<span class="number">0800</span>: <span class="number">14253.169</span>: [ParNew: <span class="number">3416062</span>K-&gt;<span class="number">419392</span>K(<span class="number">3774912</span>K), <span class="number">1.5487250</span> secs] <span class="number">12664844</span>K-&gt;<span class="number">10423713</span>K(<span class="number">16357824</span>K)After GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">56.799</span>+<span class="number">0800</span>: <span class="number">14254.718</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">1.5529860</span> seconds</span><br></pre></td></tr></table></figure>
<p><strong>4.jstat -gc查看YGC引起的堆内存变化</strong>  </p>
<p><a href="http://www.importnew.com/1993.html" target="_blank" rel="external">http://www.importnew.com/1993.html</a><br><code>新生代（Young generation）</code>: 绝大多数最新被创建的对象会被分配到这里，由于大部分对象在创建后会很快变得不可到达，<br>所以很多对象被创建在新生代，然后消失。对象从这个区域消失的过程我们称之为”minor GC“。</p>
<p><code>老年代（Old generation）</code>: 对象没有变得不可达，并且从新生代中存活下来，会被拷贝到这里。其所占用的空间要比新生代多。<br>也正由于其相对较大的空间，发生在老年代上的GC要比新生代少得多。对象从老年代中消失的过程，我们称之为”major GC”或full GC  </p>
<p>新生代其中一个存活区一直是空的(<code>下图第一次YGC前,S1=0</code>)，是eden区和另一个存活区(<code>S0,不为0</code>)在<code>下一次copy collection后活着的对象</code>的目的地.<br>对象<code>在存活区被复制(从from survivor复制到to survivor)</code>直到转移到老年代, 晋升的依据是对象的年龄计数器, 达到计数器阈值, 即可晋升到老年代.  </p>
<p>Minor GC(YGC)会把Eden中的所有活的对象都移到Survivor区中，如果Survivor区中放不下，那么剩下的活的对象就被移到Old Gen中.  </p>
<blockquote>
<p>在垃圾回收时，eden空间中的存活对象会被复制到未使用的survivor空间中(假设是to)，<br>正在使用的survivor空间(假设是from)中的年轻对象(年龄计数器小于阈值)也会被复制到to空间中.<br>大对象，或者老年对象(年龄计数器超过阈值)会直接进入老年代，<code>如果to空间已满，则对象也会直接进入老年代</code>。<br>此时，eden空间和from空间中的剩余对象就是垃圾对象，可以直接清空，to空间则存放此次回收后的存活对象。</p>
</blockquote>
<p>如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并将对象年龄设为1.<br>对象在Survivor区中每熬过一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁）时，就会被晋升到老年代中.  </p>
<blockquote>
<p>当新生代空间不足时发生minor gc(YGC),根据复制算法, jvm会<br>1）将eden和from survivor(<code>下图中是S0</code>)中存活的对象拷贝到to survior(<code>S1</code>)中，<br>2）释放eden和from中的所有需要回收对象(<code>S0U和EU都被清空=0</code>)，<br>3）调换from/to survior，jvm将eden和新的from survior(<code>现在from是S1了,下一次拷贝E和S1到to survivor:S0</code>)作为新生代  </p>
</blockquote>
<p>查看新生代内存以及老年代内存之间的变化, 可以使用下面的命令每隔100毫秒打印一次GC信息:<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sudo -u admin jstat -gc `sudo -u admin jps | grep CassandraDaemon |awk '&#123;print $1&#125;'` 100</span><br><span class="line"></span><br><span class="line"> S0C     S1C      S0U      S1U   EC        EU        OC         OU         PC      PU      YGC    YGCT      FGC    FGCT   GCT</span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span> <span class="number">16805.3</span>  0.0   <span class="number">3355520.0</span> <span class="number">2174554.4</span> <span class="number">12582912.0</span> <span class="number">7490006.5</span>  <span class="number">46492.0 27</span><span class="number">829.5 138</span><span class="number">874 5399</span>.166  310    <span class="number">91.654 54</span>90.820</span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span> <span class="number">16805.3</span>  0.0   <span class="number">3355520.0</span> <span class="number">2457122.2 125</span><span class="number">82912.0 74</span><span class="number">90006.5</span>  <span class="number">46492.0 27</span><span class="number">829.5 138</span><span class="number">874 5399</span>.166  310    <span class="number">91.654 54</span>90.820</span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span> <span class="number">16805.3</span>  0.0   <span class="number">3355520.0</span> <span class="number">2794739.1</span> <span class="number">12582912.0</span> <span class="number">7490006.5</span>  <span class="number">46492.0 27</span><span class="number">829.5 138</span><span class="number">874 5399</span>.166  310    <span class="number">91.654 54</span>90.820</span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span> <span class="number">16805.3</span>  0.0   <span class="number">3355520.0</span> <span class="number">3066774.6</span> <span class="number">12582912.0</span> <span class="number">7490006.5</span>  <span class="number">46492.0 27</span><span class="number">829.5 138</span><span class="number">874 5399</span>.166  310    <span class="number">91.654 54</span>90.820</span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span> <span class="number">16805.3</span>  0.0   <span class="number">3355520.0</span> <span class="number">3207202.1</span> <span class="number">12582912.0</span> <span class="number">7490006.5</span>  <span class="number">46492.0 27</span><span class="number">829.5 138</span><span class="number">874 5399</span>.166  310    <span class="number">91.654 54</span>90.820  ①</span><br><span class="line"></span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  0.0   <span class="number">7578.0 33</span><span class="number">55520.0</span>   0.0    <span class="number">12582912.0</span> <span class="number">7494329.9</span>  <span class="number">46492.0 27</span><span class="number">829.5 138</span><span class="number">875 5399</span>.197  310    <span class="number">91.654 54</span>90.851  ②</span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  0.0   <span class="number">7578.0 33</span><span class="number">55520.0 141</span><span class="number">496.6 125</span><span class="number">82912.0 74</span><span class="number">94329.9</span>  <span class="number">46492.0 27</span><span class="number">829.5 138</span><span class="number">875 5399</span>.197  310    <span class="number">91.654 54</span>90.851</span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  0.0   <span class="number">7578.0 33</span><span class="number">55520.0 49</span><span class="number">2729.4 125</span><span class="number">82912.0 74</span><span class="number">94329.9</span>  <span class="number">46492.0 27</span><span class="number">829.5 138</span><span class="number">875 5399</span>.197  310    <span class="number">91.654 54</span>90.851</span><br><span class="line">...</span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  0.0   <span class="number">7578.0 33</span><span class="number">55520.0 29</span><span class="number">93116.6 125</span><span class="number">82912.0 74</span><span class="number">94329.9</span>  <span class="number">46492.0 27</span><span class="number">829.5 138</span><span class="number">875 5399</span>.197  310    <span class="number">91.654 54</span>90.851</span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  0.0   <span class="number">7578.0 33</span><span class="number">55520.0 32</span><span class="number">15684.2 125</span><span class="number">82912.0 74</span><span class="number">94329.9</span>  <span class="number">46492.0 27</span><span class="number">829.5 138</span><span class="number">875 5399</span>.197  310    <span class="number">91.654 54</span>90.851</span><br><span class="line"></span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span> <span class="number">16224.9</span>  0.0   <span class="number">3355520.0</span>   0.0    <span class="number">12582912.0</span> <span class="number">7498158.7</span>  <span class="number">46492.0 27</span><span class="number">829.5 138</span><span class="number">876 5399</span>.248  310    <span class="number">91.654 54</span>90.902</span><br></pre></td></tr></table></figure></p>
<p>① 第一次YGC之前, EU不断增大(伊甸区不断创建新对象), 因为还没有发生YGC, 所以OU,S0U都没有变化. S0U是上一次YGC的to survivor.<br>② 发生YGC时, <code>Eden中存活的对象转入S1, S0中存活的对象转入Old</code>. 因为Cassandra的对象年龄计数器=1, 只要在Survivor中存活过一次,就<code>有机会</code>晋升到老年代.  </p>
<p>Cassandra的对象年龄计数器默认是1: <code>-XX:MaxTenuringThreshold=1</code>. 即对象在第一次YGC时,进入到Survivor,这时计数器=1.<br><strong>1) 当下一次YGC时,对象如果还存活,则计数器=2,超过MaxTenuringThreshold=1的阈值, 于是被晋升到了老年代中.</strong><br><strong>2) 还是说: 当第一次YGC时, 因为年龄计数器=1, 满足MaxTenuringThreshold=1的阈值条件了, 就马上被转移到老年代?</strong>  </p>
<p>结论: <strong>XX:MaxTenuringThreshold=1 指的是<code>GC的次数</code>, 对象存活的年龄计数器<code>&gt;</code>GC次数时,才会被转移到老年代, 等于时并没有转移.</strong>  </p>
<blockquote>
<p>对象年龄计数器引起从Survivor到Old:<br>1) 第二次YGC时转移:<br>以第一次YGC为例, ①在YGC之前中S0U的对象计数器=1, ②当发生YGC时,S0U中存活的对象会被拷贝到S1U中,并且这些对象的年龄计数器=2.<br>同时S1U中还包括了从Eden中存活的对象,这些对象的年龄计数器=1. 即S1包括了从Eden和经过一次YGC后在S0中仍存活的对象.<br>而S0中存活的对象年龄计数器=2超过阈值=1, 因此会被转移到老年代中. 所以S1中剩下的只是从Eden中存活的对象(计数器=1).<br>因此在阈值为1的情况, 可以认为: <code>在YGC时, S0中可存活的对象直接转移到Old, Eden中可存活的对象转移到S1</code>.<br>实际上不能说S0直接转移到了Old, S0中的对象需要经过YGC之后, 才能判断是否存活, 并拷贝到S1.<br>如果存活,对象计数器才增加, 这时再判断是否超过GC的阈值, 如果超过阈值,才将S1中存活的对象再转移到老年代.   </p>
<p>在这次YGC时, 老年代内存增加了: 7494329.9-7490006.5=4323.4. 说明有4M的对象被晋升到老年代.<br>按照上面的结论, 这4M的数据都是从S0的16M中过来的, 说明S0中有12M的对象被垃圾回收了. 同时S1的7M对象则全部来自于Eden中存活的对象.<br>在第二次的YGC时, 老年代内存增加了7498158-7494329=3828K=3M, 这些增加的对象是来源于上一次YGC的S1:7528.<br>说明第二次的YGC, 有7528-3828=3700被垃圾回收了. 同时S0的16M对象也是全部来自于第二次的Eden中存活的对象.<br>那么我们还能得出一个结论: <code>每次YGC后Old增加的内存总是来自于上一次的Survivor中存活的对象,它不会超过这个Survivor的大小</code>.  </p>
<p>2) 第一次YGC时就转移:<br>因为MaxTenuringThreshold=1, 所以在第一次YGC时, 从Eden中存活的对象能够到Survivor, 这些对象的年龄计数器=1, 满足条件, 就立马被转移到老年代了.<br>那么相当于Eden中的存活对象,首先被复制到Survivor, 然后因为达到年龄计数器阈值, Survivor中的这些对象又被转移到Old.<br>也就是说Survivor最终不会保留从Eden过来的存活对象. 但是从上面的gc日志查看Survivor是有空间的. 所以这种说法是错误的.   </p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047202 ~]$ sudo -u admin jstat -gc `sudo -u admin jps | grep CassandraDaemon |awk '&#123;print $1&#125;'` 300 | awk '&#123;printf("%10s\t%10s\t%10s\t%10s\t%10s\t\n",$3,$4,$6,$8,$11)&#125;'</span><br><span class="line">       S0U         S1U          EU          OU         YGC    Old Increment</span><br><span class="line">       0.0      7596.6         0.0   <span class="number">8280664.8</span>      210091    </span><br><span class="line">       0.0      7596.6   <span class="number">3330543.3</span>   <span class="number">8280664.8</span>      210092</span><br><span class="line">   <span class="number">13250.2</span>         0.0    <span class="number">735411.8</span>   <span class="number">8283003.0</span>      210092    <span class="number">3000&lt;7596</span></span><br><span class="line">   <span class="number">13250.2</span>         0.0   <span class="number">2846976.6</span>   <span class="number">8283003.0</span>      210092</span><br><span class="line">       0.0      6458.5    <span class="number">162674.7</span>   <span class="number">8288867.9</span>      210093    <span class="number">5800&lt;13250</span>    </span><br><span class="line">       0.0      6458.5   <span class="number">3162963.9</span>   <span class="number">8288867.9</span>      210093</span><br><span class="line">    6035.6         0.0    <span class="number">446169.8</span>   <span class="number">8291703.2</span>      210094    <span class="number">2836&lt;6458</span></span><br><span class="line">    6035.6         0.0   <span class="number">3171635.2</span>   <span class="number">8291703.2</span>      210094   </span><br><span class="line">       0.0      9745.3    <span class="number">595891.9</span>   <span class="number">8294672.9</span>      210095    <span class="number">3000&lt;6035</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>是否需要修改MaxTenuringThreshold的值</strong><br><a href="http://stackoverflow.com/questions/21992943/persistant-gc-issues-with-cassandra-long-app-pauses" target="_blank" rel="external">http://stackoverflow.com/questions/21992943/persistant-gc-issues-with-cassandra-long-app-pauses</a>  </p>
<blockquote>
<p>For read/write heavy loads chances are the default tenuring threshold flag in GC setting is very low causing premature promotions.<br>Try increasing the tenuringthreshold to a large value like 32 to prevent premature promotions so that ParNew collects most garbage in young generation.</p>
<p>You can observer this tenuring problem and promotion failures by running jstat command and see how the survivor spaces in heap are utilized.<br>I bet you they are not utilized much and objects go straight from eden to old generation.</p>
</blockquote>
<p><strong>5.分析gc.log中的ParNew</strong>  </p>
<p>分析ParNew的内存变化: <code>[ParNew: 3365,222K-&gt;12,415K(3774,912K), 0.4782790 secs] 12265,316K-&gt;8916,561K(16357,824K)</code><br>括号内的分别是ParNew的总内存3774912KB(约3.6G),这个值在多次ParNew中都没有变化, 最后一个括号是堆的总内存约16G,同样多次ParNew也没有变化. 主要分析-&gt;的变化.   </p>
<blockquote>
<p>注意: 新生代的内存3774,912K的计算方式是Eden的Capacity+一个Survivor的Capacity:419392. 即3355520+419392=3774912.<br>本来以为新生代内存是Eden+2*SurvivorCapacity, 发现加起来并不等于3774912K. 所以在说新生代时,指的是<code>Eden和其中一个非空的Survivor</code>.<br><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047202 ~]$ sudo -u <span class="literal">admin</span> jstat -gc `sudo -u <span class="literal">admin</span> jps | grep CassandraDaemon |awk '&#123;<span class="literal">print</span> <span class="variable">$1</span>&#125;'` <span class="number">100</span> <span class="number">1</span> | awk '&#123;printf(<span class="string">"%10s\t%10s\t%10s\t%10s\t\n"</span>,<span class="variable">$1</span>,<span class="variable">$2</span>,<span class="variable">$5</span>,<span class="variable">$7</span>)&#125;'</span><br><span class="line">       S0C         S1C          EC          OC</span><br><span class="line">  <span class="number">419392.0</span>    <span class="number">419392.0</span>   <span class="number">3355520.0</span>  <span class="number">12582912.0</span></span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>年轻代内存减少了3365222-12415=3352807K, 总的内存减少=12265316-8916561=3348755K. 即有3352807-3348755=4052K=4M对象晋升到老年代.<br>(这些对象有部分可能是已经到了晋升的年龄，然而由于Minor GC后Survivor几乎满了，因而基本可以确定有部分对象并没有到达晋升年龄而直接晋升了)  </p>
<p>假设内存的变化是: [ParNew: A-&gt;B] C-&gt;D. 则新生代晋升到老年代的大小=D-B-(C-A) = D-C-(B-A)=A-B-(C-D)<br><img src="http://img.blog.csdn.net/20151025102209440" alt="gc-changes"></p>
<p>YGC回收后的老年代内存=回收后的堆内存-回收后的新生代内存, 因为回收后新生代只有S0=B, 所以回收后老年代内存=D-B. </p>
<table>
<thead>
<tr>
<th>Time</th>
<th>GC for ParNew</th>
<th>gc.log: ParNew</th>
<th>Total stopped</th>
<th>After YGC Old Gen Size</th>
<th>New 2 Old</th>
</tr>
</thead>
<tbody>
<tr>
<td>21:02:31</td>
<td>672ms[2]</td>
<td>0.4782s</td>
<td>0.4870s</td>
<td>8904146</td>
<td>4052</td>
</tr>
<tr>
<td>21:02:39</td>
<td>372ms</td>
<td>0.3727s</td>
<td>0.3774s</td>
<td>8927847</td>
<td>7030</td>
</tr>
<tr>
<td>21:02:42</td>
<td>247ms</td>
<td>0.2469s</td>
<td>0.2506s</td>
<td>9100357</td>
<td>172510</td>
</tr>
<tr>
<td>21:03:56</td>
<td>1549ms</td>
<td>1.5487s</td>
<td>1.5529s</td>
<td>10004321</td>
<td>755539</td>
</tr>
</tbody>
</table>
<p>YGC之后,老年代内存不断增加,因为不断会有Survivor区的对象被晋升到老年代中.<br>当然堆内存也是一直增加的,比较直观地观察老年代内存增加是使用gcutil其中O的占比不断增加<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047202 cassandra]$ sudo -u admin jstat -gcutil `sudo -u admin jps | grep CassandraDaemon |awk '&#123;print $1&#125;'` 1000 10</span><br><span class="line">  S0     S1     E      O      P     YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line">  1.60   0.00   0.00  62.97  <span class="number">59.86 152418</span> <span class="number">5924.029</span>   338   <span class="number">96.775 60</span>20.804</span><br><span class="line">  1.60   0.00  81.17  62.97  <span class="number">59.86 152418</span> <span class="number">5924.029</span>   338   <span class="number">96.775 60</span>20.804</span><br><span class="line">  0.00   2.24  63.60  62.99  <span class="number">59.86 152419</span> <span class="number">5924.061</span>   338   <span class="number">96.775 60</span>20.836</span><br><span class="line">  1.10   0.00  49.40  63.02  <span class="number">59.86 152420</span> <span class="number">5924.093</span>   338   <span class="number">96.775 60</span>20.868</span><br><span class="line">  0.00   1.56  32.86  63.04  <span class="number">59.86 152421</span> <span class="number">5924.126</span>   338   <span class="number">96.775 60</span>20.901</span><br><span class="line">  0.00   0.68  12.10  63.11  <span class="number">59.86 152423</span> <span class="number">5924.189</span>   338   <span class="number">96.775 60</span>20.964</span><br><span class="line">  0.00   0.68  95.35  63.11  <span class="number">59.86 152423</span> <span class="number">5924.189</span>   338   <span class="number">96.775 60</span>20.964</span><br><span class="line">  1.25   0.00  78.39  63.11  <span class="number">59.86 152424</span> <span class="number">5924.218</span>   338   <span class="number">96.775 60</span>20.993</span><br><span class="line">  0.00   1.75  62.09  63.13  <span class="number">59.86 152425</span> <span class="number">5924.250</span>   338   <span class="number">96.775 60</span>21.025</span><br></pre></td></tr></table></figure></p>
<p><strong>6.查看gc时system.log发生了什么</strong>  </p>
<p>1).选取21:02:31发生的<code>GC for ParNew: 672 ms</code>: 在GC for ParNew之前, 执行了两个SliceQueryFilter, 并且都是带有tombstone的查询.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">INFO [CompactionExecutor:<span class="number">529</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">02</span>:<span class="number">03</span>,<span class="number">193</span> CompactionTask.java (line <span class="number">299</span>) Compacted <span class="number">5</span> sstables to [/home/admin/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-jb-<span class="number">1129535</span>,].  <span class="number">751</span> bytes to <span class="number">351</span> (~<span class="number">46</span>% of original) in <span class="number">33</span>ms = <span class="number">0.010144</span>MB/s.  <span class="number">7</span> total partitions merged to <span class="number">4.</span>  Partition merge counts were &#123;<span class="number">1</span>:<span class="number">5</span>, <span class="number">2</span>:<span class="number">1</span>, &#125;</span><br><span class="line">WARN [ReadStage:<span class="number">60</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">02</span>:<span class="number">18</span>,<span class="number">412</span> SliceQueryFilter.java (line <span class="number">231</span>) Read <span class="number">991</span> live and <span class="number">2109</span> tombstone cells in forseti.velocity (see tombstone_warn_threshold). <span class="number">1000</span> columns was requested, slices=[dafycredit:dafycredit:accountLogin:!-dafycredit:dafycredit:accountLogin]</span><br><span class="line">WARN [ReadStage:<span class="number">58</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">02</span>:<span class="number">18</span>,<span class="number">421</span> SliceQueryFilter.java (line <span class="number">231</span>) Read <span class="number">991</span> live and <span class="number">2109</span> tombstone cells in forseti.velocity (see tombstone_warn_threshold). <span class="number">1000</span> columns was requested, slices=[dafycredit:dafycredit:accountLogin:!-dafycredit:dafycredit:accountLogin]</span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">02</span>:<span class="number">31</span>,<span class="number">882</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">672</span> ms <span class="keyword">for</span> <span class="number">2</span> collections, <span class="number">9135816744</span> used; max is <span class="number">16750411776</span></span><br><span class="line">INFO [CompactionExecutor:<span class="number">530</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">02</span>:<span class="number">36</span>,<span class="number">113</span> ColumnFamilyStore.java (line <span class="number">795</span>) Enqueuing flush of Memtable-compactions_in_progress@<span class="number">410924923</span>(<span class="number">1</span>/<span class="number">10</span> serialized/live bytes, <span class="number">1</span> ops)</span><br></pre></td></tr></table></figure></p>
<p>2).选取21:03:56发生的<code>GC for ParNew: 1549 ms</code>: 在GC前发生了一次Compaction操作, 并在之后打印了StatusLogger.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">INFO [CompactionExecutor:<span class="number">506</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">02</span>:<span class="number">45</span>,<span class="number">116</span> CompactionTask.java (line <span class="number">299</span>) Compacted <span class="number">1</span> sstables to [/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-<span class="number">297051</span>,].  </span><br><span class="line"> <span class="number">163</span>,<span class="number">330</span>,<span class="number">070</span> bytes to <span class="number">161</span>,<span class="number">872</span>,<span class="number">089</span> (~<span class="number">99</span>% of original) in <span class="number">42</span>,<span class="number">087</span>ms = <span class="number">3.667956</span>MB/s.  <span class="number">132</span>,<span class="number">045</span> total partitions merged to <span class="number">130</span>,<span class="number">604.</span>  Partition merge counts were &#123;<span class="number">1</span>:<span class="number">132045</span>, &#125;</span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">03</span>:<span class="number">19</span>,<span class="number">142</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">267</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">9969463696</span> used; max is <span class="number">16750411776</span></span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">03</span>:<span class="number">56</span>,<span class="number">802</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">1549</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">10679738240</span> used; max is <span class="number">16750411776</span></span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">03</span>:<span class="number">56</span>,<span class="number">803</span> StatusLogger.java (line <span class="number">55</span>) Pool Name                    Active   Pending      Completed   Blocked  All Time Blocked</span><br><span class="line">...</span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">03</span>:<span class="number">57</span>,<span class="number">783</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">750</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">11082238408</span> used; max is <span class="number">16750411776</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>上面两个比较耗时的GC for ParNew, 我们能得出的结论是跟查询带有tombstone以及Compaction操作有关. 但是具体是不是他们一起的, 还需要进一步收集更多的信息.<br>但其实system.log中发生Compaction以及查询tombstone的操作是很频繁的. 不能根据这个就认为是他们引起的GC.  </p>
</blockquote>
<p>3).紧接着在21:03:57和21:03:59执行了一次750ms的<code>GC for ConcurrentMarkSweep</code>和351ms的<code>GC for ParNew</code>:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">03</span>:<span class="number">57</span>,<span class="number">783</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">750</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">11082</span>,<span class="number">238408</span> used; max is <span class="number">16750</span>,<span class="number">411776</span></span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span> <span class="number">21</span>:<span class="number">03</span>:<span class="number">59</span>,<span class="number">784</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">351</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">11876803840</span> used; max is <span class="number">16750411776</span></span><br></pre></td></tr></table></figure></p>
<p>看看gc.log中对应的这次CMS, 很快就完成了: 初始标记花了0.749s, 总共花了0.842s. 这次CMS并没有引起FGC. 接下来是另外一次的YGC.<br>CMS和Full GC不是一个概念! 所以下面的日志第一段中你看到了CMS…, 但是并没有Full GC.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">56.997</span>+<span class="number">0800</span>: <span class="number">14254.917</span>: [GC [<span class="number">1</span> CMS-initial-mark: <span class="number">10004321</span>K(<span class="number">12582912</span>K)] <span class="number">10725707</span>K(<span class="number">16357824</span>K), <span class="number">0.7497450</span> secs] [Times: user=<span class="number">0.74</span> sys=<span class="number">0.00</span>, real=<span class="number">0.75</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">57.747</span>+<span class="number">0800</span>: <span class="number">14255.667</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.8420690</span> seconds</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">57.748</span>+<span class="number">0800</span>: <span class="number">14255.667</span>: [CMS-concurrent-mark-start]</span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">58.943</span>+<span class="number">0800</span>: <span class="number">14256.862</span>: [GCBefore GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">58.943</span>+<span class="number">0800</span>: <span class="number">14256.862</span>: [ParNew: <span class="number">3774</span>,<span class="number">912</span>K-&gt;<span class="number">310</span>,<span class="number">526</span>K(<span class="number">3774912</span>K), <span class="number">0.3513860</span> secs] <span class="number">13779233</span>K-&gt;<span class="number">10596099</span>K(<span class="number">16357824</span>K)After GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">59.295</span>+<span class="number">0800</span>: <span class="number">14257.214</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.3553810</span> seconds</span><br></pre></td></tr></table></figure></p>
<p>将CMS出现的上下几条关于ParNew进行对比, 会看到21:09:41总的堆总内存由上一次的10596099K减少到5092663K.<br>可见21:03:56的CMS肯定是发挥了作用: 因为CMS垃圾收集会释放老年代的空间, 从而减少堆内存的占用.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">56.997</span>+<span class="number">0800</span>: <span class="number">14254.917</span>: [GC [<span class="number">1</span> CMS-initial-mark: <span class="number">10004</span>,<span class="number">321</span>K(<span class="number">12582912</span>K)] <span class="number">10725707</span>K(<span class="number">16357824</span>K), <span class="number">0.7497450</span> secs] [Times: user=<span class="number">0.74</span> sys=<span class="number">0.00</span>, real=<span class="number">0.75</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">58.943</span>+<span class="number">0800</span>: <span class="number">14256.862</span>: [ParNew: <span class="number">3774</span>,<span class="number">912</span>K-&gt;<span class="number">310</span>,<span class="number">526</span>K(<span class="number">3774912</span>K), <span class="number">0.3513860</span> secs] <span class="number">13779233</span>K-&gt;<span class="number">10596099</span>K(<span class="number">16357824</span>K)After GC:</span><br><span class="line">... 这中间是有CMS日志的, 见下文 </span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">09</span>:<span class="number">41.291</span>+<span class="number">0800</span>: <span class="number">14599.210</span>: [ParNew: <span class="number">3374</span>,<span class="number">088</span>K-&gt;<span class="number">419</span>,<span class="number">392</span>K(<span class="number">3774912</span>K), <span class="number">1.4201840</span> secs] <span class="number">7553474</span>K-&gt;<span class="number">5092663</span>K(<span class="number">16357824</span>K)After GC:</span><br></pre></td></tr></table></figure></p>
<p>4).CMS GC, not trigger FGC</p>
<p>理解CMS GC日志: <a href="https://blogs.oracle.com/poonam/entry/understanding_cms_gc_logs" target="_blank" rel="external">https://blogs.oracle.com/poonam/entry/understanding_cms_gc_logs</a><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">56.997</span>+<span class="number">0800</span>: <span class="number">14254.917</span>: [GC [<span class="number">1</span> 👉CMS-initial-mark: <span class="number">10004321</span>K(<span class="number">12582912</span>K)] <span class="number">10725707</span>K(<span class="number">16357824</span>K), <span class="number">0.7497450</span> secs] [Times: user=<span class="number">0.74</span> sys=<span class="number">0.00</span>, real=<span class="number">0.75</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">57.747</span>+<span class="number">0800</span>: <span class="number">14255.667</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.8420690</span> seconds</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">03</span>:<span class="number">57.748</span>+<span class="number">0800</span>: <span class="number">14255.667</span>: [CMS-concurrent-mark-start]</span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">01.922</span>+<span class="number">0800</span>: <span class="number">14259.842</span>: [👉CMS-concurrent-mark: <span class="number">3.774</span>/<span class="number">4.175</span> secs] [Times: user=<span class="number">21.69</span> sys=<span class="number">1.42</span>, real=<span class="number">4.18</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">01.923</span>+<span class="number">0800</span>: <span class="number">14259.842</span>: [CMS-concurrent-preclean-start]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">02.472</span>+<span class="number">0800</span>: <span class="number">14260.391</span>: [CMS-concurrent-preclean: <span class="number">0.534</span>/<span class="number">0.549</span> secs] [Times: user=<span class="number">1.89</span> sys=<span class="number">0.20</span>, real=<span class="number">0.54</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">02.472</span>+<span class="number">0800</span>: <span class="number">14260.391</span>: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">02.855</span>+<span class="number">0800</span>: <span class="number">14260.774</span>: [👉ParNew: <span class="number">3436653</span>K-&gt;<span class="number">21828</span>K(<span class="number">3774912</span>K), <span class="number">0.0333360</span> secs] <span class="number">13732995</span>K-&gt;<span class="number">10322861</span>K(<span class="number">16357824</span>K)After GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">04.134</span>+<span class="number">0800</span>: <span class="number">14262.054</span>: [CMS-concurrent-abortable-preclean: <span class="number">1.619</span>/<span class="number">1.663</span> secs] [Times: user=<span class="number">5.98</span> sys=<span class="number">0.57</span>, real=<span class="number">1.67</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">04.138</span>+<span class="number">0800</span>: <span class="number">14262.057</span>: [GC[👉YG occupancy: <span class="number">1747139</span> K (<span class="number">3774912</span> K)]</span><br><span class="line">  <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">04.138</span>+<span class="number">0800</span>: <span class="number">14262.057</span>: [Rescan (parallel) , <span class="number">0.2811700</span> secs]</span><br><span class="line">  <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">04.420</span>+<span class="number">0800</span>: <span class="number">14262.339</span>: [weak refs processing, <span class="number">0.0024920</span> secs]</span><br><span class="line">  <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">04.422</span>+<span class="number">0800</span>: <span class="number">14262.341</span>: [<span class="keyword">class</span> unloading, <span class="number">0.0154740</span> secs]</span><br><span class="line">  <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">04.438</span>+<span class="number">0800</span>: <span class="number">14262.357</span>: [scrub symbol table, <span class="number">0.0037670</span> secs]</span><br><span class="line">  <span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">04.441</span>+<span class="number">0800</span>: <span class="number">14262.361</span>: [scrub <span class="built_in">string</span> table, <span class="number">0.0047420</span> secs] </span><br><span class="line">  [<span class="number">1</span> 👉CMS-remark: <span class="number">10301033</span>K(<span class="number">12582912</span>K)] <span class="number">12048173</span>K(<span class="number">16357824</span>K), <span class="number">0.3147390</span> secs] [Times: user=<span class="number">2.15</span> sys=<span class="number">0.00</span>, real=<span class="number">0.32</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">04.453</span>+<span class="number">0800</span>: <span class="number">14262.372</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.3184590</span> seconds</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">04.453</span>+<span class="number">0800</span>: <span class="number">14262.373</span>: [CMS-concurrent-sweep-start]</span><br><span class="line">CMS: Large Block: <span class="number">0x00000007c1173180</span>; Proximity: <span class="number">0x00000007ae04a648</span> -&gt; <span class="number">0x00000007b9fd0a80</span></span><br><span class="line"></span><br><span class="line">CMS: Large block <span class="number">0x00000007c1173180</span></span><br><span class="line">CMS: Large Block: <span class="number">0x00000007fc840000</span>; Proximity: <span class="number">0x00000007fc7fccb0</span> -&gt; <span class="number">0x00000007fc7fccb0</span></span><br><span class="line">CMS: Large block <span class="number">0x00000007fc840000</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">14.119</span>+<span class="number">0800</span>: <span class="number">14272.038</span>: [👉CMS-concurrent-sweep: <span class="number">9.423</span>/<span class="number">9.665</span> secs] [Times: user=<span class="number">36.55</span> sys=<span class="number">4.19</span>, real=<span class="number">9.66</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">14.119</span>+<span class="number">0800</span>: <span class="number">14272.038</span>: [CMS-concurrent-reset-start]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">04</span>:<span class="number">14.163</span>+<span class="number">0800</span>: <span class="number">14272.083</span>: [👉CMS-concurrent-reset: <span class="number">0.045</span>/<span class="number">0.045</span> secs] [Times: user=<span class="number">0.17</span> sys=<span class="number">0.02</span>, real=<span class="number">0.04</span> secs]</span><br></pre></td></tr></table></figure></p>
<p>注意: 在CMS-concurrent-abortable-preclean-start后有一次ParNew的新生代垃圾收集.   </p>
<blockquote>
<p>after a preclean, ‘abortable preclean’ starts. After the young generation collection,<br>the young gen occupancy drops down from 3436653K to 21828K.<br>When young gen occupancy reaches 1747139 which is 50% of the total capacity(3774912),<br>precleaning is aborted and ‘remark’ phase is started.</p>
</blockquote>
<p>5).观察system.log统计的GC信息, 发现在这之后的<code>21:09:42</code>开始<code>GC for ParNew</code>时间越来越长, 并且出现了多次的<code>GC for CMS</code>.<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[CompactionExecutor:546]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:10</span>,664 <span class="tag">CompactionTask</span><span class="class">.java</span> (<span class="tag">line</span> 120) <span class="tag">Compacting</span> <span class="attr_selector">[SSTableReader(path='/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-297018-Data.db'), SSTableReader(path='/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-297014-Data.db'), SSTableReader(path='/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-297007-Data.db'), SSTableReader(path='/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-297012-Data.db'), SSTableReader(path='/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-297006-Data.db'), SSTableReader(path='/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-297008-Data.db'), SSTableReader(path='/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-297057-Data.db'), SSTableReader(path='/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-297009-Data.db'), SSTableReader(path='/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-297013-Data.db'), SSTableReader(path='/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-297017-Data.db'), SSTableReader(path='/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-297056-Data.db'), SSTableReader(path='/home/admin/cassandra/data/forseti/velocity/forseti-velocity-jb-297011-Data.db')]</span></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:42</span>,724 <span class="tag">GCInspector</span><span class="class">.java</span> (<span class="tag">line</span> 116) <span class="tag">GC</span> <span class="tag">for</span> <span class="tag">ParNew</span>: 1420 <span class="tag">ms</span> <span class="tag">for</span> 1 <span class="tag">collections</span>, 5274947408 <span class="tag">used</span>; <span class="tag">max</span> <span class="tag">is</span> 16750411776</span><br><span class="line"></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:45</span>,891 <span class="tag">GCInspector</span><span class="class">.java</span> (<span class="tag">line</span> 116) <span class="tag">GC</span> <span class="tag">for</span> <span class="tag">ParNew</span>: 2342 <span class="tag">ms</span> <span class="tag">for</span> 1 <span class="tag">collections</span>, 7007901632 <span class="tag">used</span>; <span class="tag">max</span> <span class="tag">is</span> 16750411776</span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:45</span>,892 <span class="tag">StatusLogger</span><span class="class">.java</span> (<span class="tag">line</span> 55) <span class="tag">Pool</span> <span class="tag">Name</span>                    <span class="tag">Active</span>   <span class="tag">Pending</span>      <span class="tag">Completed</span>   <span class="tag">Blocked</span>  <span class="tag">All</span> <span class="tag">Time</span> <span class="tag">Blocked</span></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:45</span>,892 <span class="tag">StatusLogger</span><span class="class">.java</span> (<span class="tag">line</span> 70) <span class="tag">ReadStage</span>                        23       129         645249         0                 0</span><br><span class="line"></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:49</span>,573 <span class="tag">GCInspector</span><span class="class">.java</span> (<span class="tag">line</span> 116) <span class="tag">GC</span> <span class="tag">for</span> <span class="tag">ParNew</span>: 3161 <span class="tag">ms</span> <span class="tag">for</span> 1 <span class="tag">collections</span>, 9012309600 <span class="tag">used</span>; <span class="tag">max</span> <span class="tag">is</span> 16750411776</span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:49</span>,574 <span class="tag">StatusLogger</span><span class="class">.java</span> (<span class="tag">line</span> 55) <span class="tag">Pool</span> <span class="tag">Name</span>                    <span class="tag">Active</span>   <span class="tag">Pending</span>      <span class="tag">Completed</span>   <span class="tag">Blocked</span>  <span class="tag">All</span> <span class="tag">Time</span> <span class="tag">Blocked</span></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:49</span>,575 <span class="tag">StatusLogger</span><span class="class">.java</span> (<span class="tag">line</span> 70) <span class="tag">ReadStage</span>                        32        94         645515         0                 0</span><br><span class="line"></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:53</span>,443 <span class="tag">GCInspector</span><span class="class">.java</span> (<span class="tag">line</span> 116) <span class="tag">GC</span> <span class="tag">for</span> <span class="tag">ParNew</span>: 3277 <span class="tag">ms</span> <span class="tag">for</span> 1 <span class="tag">collections</span>, 11212220496 <span class="tag">used</span>; <span class="tag">max</span> <span class="tag">is</span> 16750411776</span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:53</span>,443 <span class="tag">StatusLogger</span><span class="class">.java</span> (<span class="tag">line</span> 55) <span class="tag">Pool</span> <span class="tag">Name</span>                    <span class="tag">Active</span>   <span class="tag">Pending</span>      <span class="tag">Completed</span>   <span class="tag">Blocked</span>  <span class="tag">All</span> <span class="tag">Time</span> <span class="tag">Blocked</span></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:53</span>,444 <span class="tag">StatusLogger</span><span class="class">.java</span> (<span class="tag">line</span> 70) <span class="tag">ReadStage</span>                        32       254         645647         0                 0</span><br><span class="line"></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:54</span>,790 <span class="tag">GCInspector</span><span class="class">.java</span> (<span class="tag">line</span> 116) <span class="tag">GC</span> <span class="tag">for</span> <span class="tag">ConcurrentMarkSweep</span>: 1142 <span class="tag">ms</span> <span class="tag">for</span> 1 <span class="tag">collections</span>, 12623059520 <span class="tag">used</span>; <span class="tag">max</span> <span class="tag">is</span> 16750411776</span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:54</span>,790 <span class="tag">StatusLogger</span><span class="class">.java</span> (<span class="tag">line</span> 55) <span class="tag">Pool</span> <span class="tag">Name</span>                    <span class="tag">Active</span>   <span class="tag">Pending</span>      <span class="tag">Completed</span>   <span class="tag">Blocked</span>  <span class="tag">All</span> <span class="tag">Time</span> <span class="tag">Blocked</span></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:54</span>,791 <span class="tag">StatusLogger</span><span class="class">.java</span> (<span class="tag">line</span> 70) <span class="tag">ReadStage</span>                        32       475         645647         0                 0</span><br><span class="line"></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:55</span>,022 <span class="tag">MessagingService</span><span class="class">.java</span> (<span class="tag">line</span> 876) 52 <span class="tag">READ</span> <span class="tag">messages</span> <span class="tag">dropped</span> <span class="tag">in</span> <span class="tag">last</span> 5000<span class="tag">ms</span></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:55</span>,025 <span class="tag">StatusLogger</span><span class="class">.java</span> (<span class="tag">line</span> 55) <span class="tag">Pool</span> <span class="tag">Name</span>                    <span class="tag">Active</span>   <span class="tag">Pending</span>      <span class="tag">Completed</span>   <span class="tag">Blocked</span>  <span class="tag">All</span> <span class="tag">Time</span> <span class="tag">Blocked</span></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:09</span><span class="pseudo">:55</span>,026 <span class="tag">StatusLogger</span><span class="class">.java</span> (<span class="tag">line</span> 70) <span class="tag">ReadStage</span>                        32       512         645647         0                 0</span><br><span class="line"></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:10</span><span class="pseudo">:56</span>,385 <span class="tag">GCInspector</span><span class="class">.java</span> (<span class="tag">line</span> 116) <span class="tag">GC</span> <span class="tag">for</span> <span class="tag">ConcurrentMarkSweep</span>: 41123 <span class="tag">ms</span> <span class="tag">for</span> 1 <span class="tag">collections</span>, 11291487752 <span class="tag">used</span>; <span class="tag">max</span> <span class="tag">is</span> 16750411776</span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:10</span><span class="pseudo">:56</span>,385 <span class="tag">StatusLogger</span><span class="class">.java</span> (<span class="tag">line</span> 55) <span class="tag">Pool</span> <span class="tag">Name</span>                    <span class="tag">Active</span>   <span class="tag">Pending</span>      <span class="tag">Completed</span>   <span class="tag">Blocked</span>  <span class="tag">All</span> <span class="tag">Time</span> <span class="tag">Blocked</span></span><br><span class="line"><span class="tag">INFO</span> <span class="attr_selector">[ScheduledTasks:1]</span> 2015<span class="tag">-10-21</span> 21<span class="pseudo">:10</span><span class="pseudo">:56</span>,386 <span class="tag">StatusLogger</span><span class="class">.java</span> (<span class="tag">line</span> 70) <span class="tag">ReadStage</span>                        32       515         645647         0                 0</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>system.log中在这几次比较长的GC for ParNew以及GC for CMS都没有什么异常信息, 不过打印了很多SatusLogger的信息.  </p>
<blockquote>
<p>StatusLogger会在GCInspector计算<code>GC duration超过1s</code>时打印. 具体可以查看GCInspector.java中的StatsLogger的输出条件.   </p>
</blockquote>
<p>看看gc.log对应这段时间的日志, 也都只是简单地打印了before, after gc的, 并没有打印heap中对象的信息.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">09</span>:<span class="number">41.291</span>+<span class="number">0800</span>: <span class="number">14599.210</span>: [ParNew: <span class="number">3374088</span>K-&gt;<span class="number">419392</span>K(<span class="number">3774912</span>K), <span class="number">1.4201840</span> secs] <span class="number">7553474</span>K-&gt;<span class="number">5092663</span>K(<span class="number">16357824</span>K)After GC: ①</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">09</span>:<span class="number">42.712</span>+<span class="number">0800</span>: <span class="number">14600.632</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">1.4269430</span> seconds</span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">09</span>:<span class="number">43.530</span>+<span class="number">0800</span>: <span class="number">14601.449</span>: [ParNew: <span class="number">3774641</span>K-&gt;<span class="number">419392</span>K(<span class="number">3774912</span>K), <span class="number">2.3420270</span> secs] <span class="number">8447913</span>K-&gt;<span class="number">6726642</span>K(<span class="number">16357824</span>K)After GC: ②</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">09</span>:<span class="number">45.873</span>+<span class="number">0800</span>: <span class="number">14603.792</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">2.3507670</span> seconds</span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">09</span>:<span class="number">46.400</span>+<span class="number">0800</span>: <span class="number">14604.320</span>: [ParNew: <span class="number">3774912</span>K-&gt;<span class="number">419392</span>K(<span class="number">3774912</span>K), <span class="number">3.1607140</span> secs] <span class="number">10082162</span>K-&gt;<span class="number">8787059</span>K(<span class="number">16357824</span>K)After GC: ③</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">09</span>:<span class="number">49.562</span>+<span class="number">0800</span>: <span class="number">14607.481</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">3.1836570</span> seconds</span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">09</span>:<span class="number">50.160</span>+<span class="number">0800</span>: <span class="number">14608.079</span>: [ParNew: <span class="number">3774912</span>K-&gt;<span class="number">419392</span>K(<span class="number">3774912</span>K), <span class="number">3.2769040</span> secs] <span class="number">12142579</span>K-&gt;<span class="number">10947142</span>K(<span class="number">16357824</span>K)After GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">21</span>T21:<span class="number">09</span>:<span class="number">53.437</span>+<span class="number">0800</span>: <span class="number">14611.356</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">3.2819760</span> seconds</span><br></pre></td></tr></table></figure></p>
<p>但是等等, 仔细看下ParNew中回收后的新生代内存大小, 有没有发现什么??  </p>
<p>7.<strong>问题初/出现: ParNew后Survivor满了</strong>  </p>
<p>和之前我们分析的ParNew不同的是这里回收之前新生代的内存就和总内存一样都是3774912K(前面两个只差一点, 后面两个完全一样).<br>回收后新生代内存都是419392K. 这种一模一样的数据说明了什么问题? 一般而言, 随机的数据是没有问题的, 固定的数据就是有问题了.   </p>
<p><strong>可以看到ParNew之前的新生代内存已经达到了新生代内存的容量3774912K了(前面使用-gc命令,EC+一个SurvivorCapacity的值=3774912K).</strong><br><strong>说明在回收前, Eden和一个Survivor都完全填满对象了. 那么在YGC时, 这个满了的Survivor因为对象计数器&gt;1晋升到老年代,</strong><br><strong>而我们看到回收后的新生代内存仍然是满了的SurvivorCapacity:419392K. 说明Eden中经过这次YGC仍然存活的对象不止400M,</strong><br><strong>由于Eden存活的对象不能够放入Survivor,所以剩余的这些对象会直接进入老年代.  观察老年代的内存②比①增加了1.6G多:</strong><br><strong>包括了Survivor晋升过来的最多400M,以及因为Survivor满了无法存放而直接转移的Eden中存活的对象有近至少1.2G!</strong><br><strong>Eden中存活的对象因为Survivor满了而直接转移到老年代, 这种情况发生的时候是很不妙的. 因为对象没有经过Survivor.</strong><br><strong>不过这个问题的根源应该是: 为什么Eden的存活对象之前是只有最多4M, 突然一下子增大到有1.2G的存活对象?</strong>   </p>
<blockquote>
<p><code>YGC会把Eden中的所有活的对象都移到Survivor区中，如果Survivor区中放不下，那么剩下的活的对象就被移到Old Gen中.</code><br>因为YGC会把Eden中存活的对象以及from survivor拷贝到to survivor, 但是由于现在from survivor已经满了,所以Eden中存活的对象会直接拷贝到老年代.  </p>
</blockquote>
<h4 id="很长的FGC">很长的FGC</h4><p>1.<strong>CMS GC</strong>  </p>
<p>聊聊JVM（四）深入理解Major GC, Full GC, CMS: <a href="http://blog.csdn.net/iter_zc/article/details/41825395" target="_blank" rel="external">http://blog.csdn.net/iter_zc/article/details/41825395</a><br>[HotSpot VM] JVM调优的”标准参数”的各种陷阱: <a href="http://hllvm.group.iteye.com/group/topic/27945" target="_blank" rel="external">http://hllvm.group.iteye.com/group/topic/27945</a>  </p>
<blockquote>
<p>CMS垃圾收集器包括了四个阶段: 初始标记<code>initial-mark</code>, 并发标记<code>concurrent-mark</code>, 重新标记<code>remark</code>, 并发整理<code>concurrent sweep</code>.<br>其中initital mark和remark两个没有带concurrent的会StopTheWorld.  </p>
<p>CMS至少会给Full GC的次数 + 2，因为Full GC的次数是按照老年代GC时stop the world的次数而定的.<br>Full GC的Time的定义，可以理解它指的是老年代GC时stop the world的时间  </p>
<p>CMS 不等于Full GC，我们可以看到CMS分为多个阶段，只有stop the world的阶段被计算到了Full GC的次数和时间，而和业务线程并发的GC的次数和时间则不被认为是Full GC</p>
<p>正常的CMS的stop the world的时间很短，都是在几十到几百ms的级别，对Full GC的时间影响很小</p>
</blockquote>
<p>2.<strong>Class Histogram FGC</strong>  </p>
<p>我们分析21:10分左右开始到22:43分之间Cassandra进程假死这个过程到底发生了什么: <code>33M 10月 21 22:43 gc-1445418381.log</code>. 找到21:10分之前最近的GC日志信息:<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015-10-21</span>T<span class="number">21:09:53.57</span>7+0800: <span class="number">14611.496</span>: [GC [1 CMS-initial-mark: 👉<span class="number">10,527,75</span>0K(<span class="number">12,582,91</span>2K)👈] <span class="number">11877,789</span>K(<span class="number">16357,824</span>K), <span class="number">1.1421340</span> secs] [Times: user=1.15 sys=0.00, real=1.14 secs]</span><br><span class="line"><span class="number">2015-10-21</span>T<span class="number">21:09:54.71</span>9+0800: <span class="number">14612.639</span>: Total time for which application threads were stopped: <span class="number">1.1519090</span> seconds  ①</span><br><span class="line"><span class="number">2015-10-21</span>T<span class="number">21:09:54.72</span>0+0800: <span class="number">14612.639</span>: [CMS-concurrent-mark-start]</span><br><span class="line"><span class="number">2015-10-21</span>T<span class="number">21:09:54.94</span>6+0800: <span class="number">14612.865</span>: Total time for which application threads were stopped: <span class="number">0.0044060</span> seconds</span><br><span class="line"><span class="number">2015-10-21</span>T<span class="number">21:09:54.94</span>9+0800: <span class="number">14612.868</span>: Total time for which application threads were stopped: <span class="number">0.0028860</span> seconds</span><br><span class="line"><span class="number">2015-10-21</span>T<span class="number">21:09:55.120</span>+0800: <span class="number">14613.039</span>: [GCBefore GC:</span><br><span class="line">Statistics for BinaryTreeDictionary:</span><br><span class="line">------------------------------------</span><br><span class="line">Total Free Space: <span class="number">132,062,67</span>0  剩余容量为132M. </span><br><span class="line">Max   Chunk Size: <span class="number">108,169,992</span>  最大Chunk的容量为108M.</span><br><span class="line">Number of Blocks: 198          块的数量(多少Chunks)</span><br><span class="line">Av.  Block  Size: <span class="number">666,983</span>.     NumOfBlocks*AvgBlockSize=TotalFreeSpace</span><br><span class="line">Tree      Height: 11</span><br><span class="line">Before GC:</span><br><span class="line">Statistics for BinaryTreeDictionary:   这里为什么有两段信息?</span><br><span class="line">------------------------------------</span><br><span class="line">Total Free Space: <span class="number">2,285,36</span>1</span><br><span class="line">Max   Chunk Size: <span class="number">2,281,47</span>2</span><br><span class="line">Number of Blocks: 10</span><br><span class="line">Av.  Block  Size: <span class="number">228,536</span></span><br><span class="line">Tree      Height: 6</span><br><span class="line"><span class="number">2015-10-21</span>T<span class="number">21:09:55.120</span>+0800: <span class="number">14613.039</span>: [👉ParNew: <span class="number">3774,912</span>K-&gt;<span class="number">3774,912</span>K👈(<span class="number">3774912</span>K), <span class="number">0.0000490</span> secs]<span class="number">2015-10-21</span>T<span class="number">21:09:55.120</span>+0800: <span class="number">14613.039</span>: [Class Histogram (before full gc):</span><br><span class="line"> num     #instances         #bytes  class name</span><br><span class="line">----------------------------------------------</span><br><span class="line">   1:      <span class="number">80698905</span>     <span class="number">6645469872</span>  [B</span><br><span class="line">   2:      <span class="number">88784779</span>     <span class="number">4261669392</span>  java.nio.HeapByteBuffer</span><br><span class="line">   3:      <span class="number">39890367</span>     <span class="number">1595614680</span>  org.apache.cassandra.io.sstable.IndexHelper$IndexInfo</span><br><span class="line">   4:      <span class="number">15687882</span>     <span class="number">1004024448</span>  java.nio.DirectByteBufferR</span><br><span class="line">   5:        104774      <span class="number">385911192</span>  [Ljava.lang.Object<span class="comment">;</span></span><br><span class="line">   6:       <span class="number">3206670</span>      <span class="number">153920160</span>  edu.stanford.ppl.concurrent.SnapTreeMap$Node</span><br><span class="line">   7:       <span class="number">2783478</span>      <span class="number">111339120</span>  org.apache.cassandra.db.ExpiringColumn</span><br><span class="line">   8:        531668       <span class="number">51040128</span>  edu.stanford.ppl.concurrent.CopyOnWriteManager$COWEpoch</span><br><span class="line">   9:        536228       <span class="number">25738944</span>  edu.stanford.ppl.concurrent.SnapTreeMap$RootHolder</span><br><span class="line">  10:        823822       <span class="number">19771728</span>  java.util.concurrent.ConcurrentSkipListMap$Node</span><br><span class="line">  11:        531668       <span class="number">17013376</span>  edu.stanford.ppl.concurrent.CopyOnWriteManager$Latch</span><br><span class="line">  ...</span><br><span class="line">Total     <span class="number">239496755</span>    <span class="number">14467348864</span>  👉6G+4G+1G+500M=12.6G, 再加上其他,一共有近14G.👈 </span><br><span class="line">, <span class="number">11.1813410</span> secs]</span><br><span class="line"><span class="number">2015-10-21</span>T<span class="number">21:10:06.30</span>2+0800: <span class="number">14624.221</span>: [CMS<span class="number">2015-10-21</span>T<span class="number">21:10:18.42</span>0+0800: <span class="number">14636.339</span>: [CMS-concurrent-mark: <span class="number">12.488/23</span>.700 secs] [Times: user=37.26 sys=0.72, real=23.70 secs]</span><br><span class="line">✨(concurrent mode failure)✨CMS: Large block <span class="number">0x00000007</span><span class="number">96a01988</span> ④</span><br><span class="line">: <span class="number">10527750</span>K-&gt;<span class="number">10940294</span>K(<span class="number">12582912</span>K), <span class="number">41.1230770</span> secs]  </span><br><span class="line"><span class="number">2015-10-21</span>T<span class="number">21:10:47.42</span>5+0800: <span class="number">14665.344</span>: [Class Histogram (after full gc):</span><br><span class="line"> num     #instances         #bytes  class name</span><br><span class="line">----------------------------------------------</span><br><span class="line">   1:      <span class="number">72165660</span>     <span class="number">5523991976</span>  [B</span><br><span class="line">   2:      <span class="number">75437882</span>     <span class="number">3621018336</span>  java.nio.HeapByteBuffer</span><br><span class="line">   3:      <span class="number">36048357</span>     <span class="number">1441934280</span>  org.apache.cassandra.io.sstable.IndexHelper$IndexInfo</span><br><span class="line">   4:         25412      <span class="number">365368784</span>  [Ljava.lang.Object<span class="comment">;</span></span><br><span class="line">   5:       <span class="number">1368526</span>       <span class="number">65689248</span>  edu.stanford.ppl.concurrent.SnapTreeMap$Node</span><br><span class="line">   6:       <span class="number">1365018</span>       <span class="number">54600720</span>  org.apache.cassandra.db.ExpiringColumn  ⑤</span><br><span class="line">   7:        150348       <span class="number">14433408</span>  edu.stanford.ppl.concurrent.CopyOnWriteManager$COWEpoch</span><br><span class="line">   8:         15940       <span class="number">12236136</span>  [J</span><br><span class="line">   9:        222563        <span class="number">8902520</span>  java.util.TreeMap$Entry</span><br><span class="line">  10:        357980        <span class="number">8591520</span>  java.lang.Long</span><br><span class="line">  11:         62812        <span class="number">8470512</span>  [C</span><br><span class="line">  12:        304483        <span class="number">7307592</span>  java.lang.Double</span><br><span class="line">  13:        150424        <span class="number">7220352</span>  edu.stanford.ppl.concurrent.SnapTreeMap$RootHolder  </span><br><span class="line"> Total     <span class="number">190044152</span>    <span class="number">11225332704</span></span><br><span class="line">, <span class="number">8.9471970</span> secs]</span><br><span class="line"> <span class="number">14302662</span>K-&gt;<span class="number">10940294</span>K(<span class="number">16357824</span>K), [CMS Perm : 26777K-&gt;26774K(44704K)]After GC:   ②</span><br><span class="line">Statistics for BinaryTreeDictionary:</span><br><span class="line">------------------------------------</span><br><span class="line">Total Free Space: <span class="number">210,238,671</span>.   原先剩余132M,现在是210M.</span><br><span class="line">Max   Chunk Size: <span class="number">210,238,671</span>    只有一个块了</span><br><span class="line">Number of Blocks: 1</span><br><span class="line">Av.  Block  Size: <span class="number">210238671</span>      所以最大值=平均值=Total  </span><br><span class="line">Tree      Height: 1</span><br><span class="line">After GC:</span><br><span class="line">Statistics for BinaryTreeDictionary:</span><br><span class="line">------------------------------------</span><br><span class="line">Total Free Space: 0</span><br><span class="line">Max   Chunk Size: 0</span><br><span class="line">Number of Blocks: 0</span><br><span class="line">Tree      Height: 0</span><br><span class="line">, <span class="number">61.2523050</span> secs] [Times: user=72.60 sys=0.62, real=61.24 secs]</span><br><span class="line"><span class="number">2015-10-21</span>T<span class="number">21:10:56.37</span>2+0800: <span class="number">14674.292</span>: Total time for which application threads were stopped: <span class="number">61.2555790</span> seconds  ③</span><br></pre></td></tr></table></figure></p>
<p>① initial-mark时间很短,只有1.14秒. 虽然只有1.14秒, 但日志接着打印出Total time … stopped:1.15s. 因为初始标记会STW.   </p>
<blockquote>
<p>Beginning of tenured generation collection with CMS collector. This is initial Marking phase of CMS where<br>all the objects directly reachable from roots are marked and this is done with all the mutator threads stopped.<br>老年代的容量OC为12G,当它被使用(occupancy)了10G时, CMS收集器被触发(trigger).  </p>
</blockquote>
<p>② 回收前的13.96G减少到10.68G. Perm永久代在垃圾收集过程中没有什么变化.<br>③ GC的时间从<code>21:09:55</code>-<code>21:10:56</code>刚好是<code>61s</code>. FGC的这段时间就是应用程序线程被中断Stopped的总时间.<br>④ 有个23.7s的<code>concurrent mark</code>, 并且<code>(concurrent mode failure)CMS: Large block</code>.    </p>
<blockquote>
<p>CMS GC时出现promotion failed和concurrent mode failure:<br>promotion failed是在进行Minor GC时，survivor space放不下、对象只能放入旧生代，而此时旧生代也放不下造成的；<br>concurrent mode failure是在执行CMS GC的过程中同时有对象要放入旧生代，而此时旧生代空间不足造成的。<br>应对措施为：增大survivor space、旧生代空间或调低触发并发GC的比率  </p>
</blockquote>
<p>⑤ ExpiringColumn在GC后被回收, 这些对象来自于查询时把tombstone都加载到内存,而没有及时回收. 不过只有50M左右.  </p>
<p>除了表面的这些日志信息外, 我们还能挖掘出什么信息:<br>1) 观察上面一次FGC的过程, DirectByteBufferR对象在GC后没有存在(将近1G), 其他对象的instances和bytes都有所减少.   </p>
<p>2) SSTable的IndexInfo是数据的索引信息. 占了约1.5G.  </p>
<p>3) BinaryTreeDictionary是Perm的statics吗</p>
<p>3.<strong>concurrent mode failure</strong>  </p>
<p><a href="https://blogs.oracle.com/poonam/entry/troubleshooting_long_gc_pauses" target="_blank" rel="external">https://blogs.oracle.com/poonam/entry/troubleshooting_long_gc_pauses</a>  </p>
<blockquote>
<p>Fragmentation in the Java Heap can cause GCs to occur more frequently and also sometimes causing long pauses in the GCs.<br>This is more probable in the case of Concurrent Mark Sweep collector, also known as CMS, 在CMS阶段由于碎片更容易引起GC.<br>where the tenured generation space is not compacted with the concurrent collections. 因为老年代的空间在并发收集时并没有做压缩操作.    </p>
<p>In case of the CMS, due to fragmentation in the tenured generation space, the young generation collections can face 新生代的收集会面临晋升失败<br>promotion failures and thus triggering ‘Concurrent Mode Failure’ collections that are stop-the-world Full GCs, 从而触发<code>并发模式失败</code>引起FGC<br>and Full GCs take a long time to finish as compared to the concurrent collection pauses. FGC会花费更多的时间  </p>
<p>Due to the fragmentation, the direct allocations in the tenured generation may fail 因为碎片存在,即使老年代有足够的空间. 但是因为这些碎片不是连续的<br>even when there is lot of free space available and thus causing Full GCs. 如果新生代要求一个连续的空间,老年代没有这样的,就会导致FGC.<br>Fragmentation can also cause frequent allocation failures and thus triggering frequent Full GCs that increase the overall time the application is paused for.</p>
</blockquote>
<h4 id="最终OOM时">最终OOM时</h4><p>最后导致Cassandra进程挂掉时打印的堆信息: </p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015-10-21</span>T<span class="number">22:37:40.92</span>8+0800: <span class="number">19878.847</span>: [Class Histogram (before full gc):</span><br><span class="line"> num     #instances         #bytes  class name</span><br><span class="line">----------------------------------------------</span><br><span class="line">   1:     <span class="number">109883385</span>     <span class="number">8494073440</span>  [B  ②</span><br><span class="line">   2:     <span class="number">113572533</span>     <span class="number">5451481584</span>  java.nio.HeapByteBuffer</span><br><span class="line">   3:      <span class="number">54818129</span>     <span class="number">2192725160</span>  org.apache.cassandra.io.sstable.IndexHelper$IndexInfo</span><br><span class="line">   4:         38540      <span class="number">329177992</span>  [Ljava.lang.Object<span class="comment">;</span></span><br><span class="line">   5:       <span class="number">1558136</span>       <span class="number">74790528</span>  edu.stanford.ppl.concurrent.SnapTreeMap$Node</span><br><span class="line">   6:       <span class="number">1661366</span>       <span class="number">66454640</span>  org.apache.cassandra.db.ExpiringColumn</span><br><span class="line">   7:        175824       <span class="number">16879104</span>  edu.stanford.ppl.concurrent.CopyOnWriteManager$COWEpoch</span><br><span class="line"></span><br><span class="line"><span class="number">2015-10-21</span>T<span class="number">22:37:53.86</span>7+0800: <span class="number">19891.786</span>: [CMS<span class="number">2015-10-21</span>T<span class="number">22:38:10.33</span>3+0800: <span class="number">19908.252</span>: [CMS-concurrent-mark: <span class="number">16.465/29</span>.409 secs] [Times: user=45.59 sys=0.10, real=29.41 secs]</span><br><span class="line"> (concurrent mode failure)CMS: Large block <span class="number">0x00000000</span><span class="number">00000000</span></span><br><span class="line">: <span class="number">12582912</span>K-&gt;<span class="number">12582911</span>K(<span class="number">12582912</span>K), <span class="number">57.4368210</span> secs]</span><br><span class="line"><span class="number">2015-10-21</span>T<span class="number">22:38:51.30</span>4+0800: <span class="number">19949.223</span>: [Class Histogram (after full gc):</span><br><span class="line"> num     #instances         #bytes  class name</span><br><span class="line">----------------------------------------------</span><br><span class="line">   1:     <span class="number">107032770</span>     <span class="number">8287003856</span>  [B</span><br><span class="line">   2:     <span class="number">110721923</span>     <span class="number">5314652304</span>  java.nio.HeapByteBuffer</span><br><span class="line">   3:      <span class="number">53392823</span>     <span class="number">2135712920</span>  org.apache.cassandra.io.sstable.IndexHelper$IndexInfo</span><br><span class="line">   4:         38529      <span class="number">318767288</span>  [Ljava.lang.Object<span class="comment">;</span></span><br><span class="line">   5:       <span class="number">1558136</span>       <span class="number">74790528</span>  edu.stanford.ppl.concurrent.SnapTreeMap$Node</span><br><span class="line">   6:       <span class="number">1661366</span>       <span class="number">66454640</span>  org.apache.cassandra.db.ExpiringColumn</span><br><span class="line">   7:        175824       <span class="number">16879104</span>  edu.stanford.ppl.concurrent.CopyOnWriteManager$COWEpoch</span><br><span class="line">   8:         19299       <span class="number">12399344</span>  [J</span><br><span class="line">   9:        374876        <span class="number">8997024</span>  java.lang.Long</span><br><span class="line"></span><br><span class="line">① <span class="number">16,357,82</span>3K-&gt;<span class="number">15,956,145</span>K(<span class="number">16357824</span>K), [CMS Perm : 26698K-&gt;26684K(44704K)]After GC:</span><br><span class="line">Statistics for BinaryTreeDictionary:</span><br><span class="line">------------------------------------</span><br><span class="line">Total Free Space: 0</span><br><span class="line">Max   Chunk Size: 0</span><br><span class="line">Number of Blocks: 0</span><br><span class="line">Tree      Height: 0</span><br><span class="line">After GC:</span><br><span class="line">Statistics for BinaryTreeDictionary:</span><br><span class="line">------------------------------------</span><br><span class="line">Total Free Space: 0</span><br><span class="line">Max   Chunk Size: 0</span><br><span class="line">Number of Blocks: 0</span><br><span class="line">Tree      Height: 0</span><br><span class="line"></span><br><span class="line"><span class="number">2015-10-21</span>T<span class="number">22:43:09.60</span>6+0800: <span class="number">20207.525</span>: Total time for which application threads were stopped: <span class="number">0.0032060</span> seconds</span><br><span class="line">Heap  ③</span><br><span class="line"> par new generation   total <span class="number">3,774,91</span>2K, used <span class="number">3,677,02</span>1K [<span class="number">0x00000003</span>fae00000, <span class="number">0x00000004</span>fae00000, <span class="number">0x00000004</span>fae00000)</span><br><span class="line">  eden space <span class="number">3,355,52</span>0K, 100% used [<span class="number">0x00000003</span>fae00000, <span class="number">0x00000004</span>c7ae0000, <span class="number">0x00000004</span>c7ae0000)</span><br><span class="line">  from space <span class="number">419,392</span>K,  76% used [<span class="number">0x00000004</span>c7ae0000, <span class="number">0x00000004</span>db4d79d0, <span class="number">0x00000004</span>e<span class="number">1470000</span>)</span><br><span class="line">  to   space <span class="number">419,392</span>K,   0% used [<span class="number">0x00000004</span>e<span class="number">1470000</span>, <span class="number">0x00000004</span>e<span class="number">1470000</span>, <span class="number">0x00000004</span>fae00000)</span><br><span class="line"> concurrent mark-sweep generation total <span class="number">12,582,91</span>2K, used <span class="number">12,582,91</span>1K [<span class="number">0x00000004</span>fae00000, <span class="number">0x00000007</span>fae00000, <span class="number">0x00000007</span>fae00000)</span><br><span class="line"> concurrent-mark-sweep perm gen total 44,704K, used 26,855K [<span class="number">0x00000007</span>fae00000, <span class="number">0x00000007</span>fd9a8000, <span class="number">0x00000008</span><span class="number">00000000</span>)</span><br></pre></td></tr></table></figure>
<p>① GC前和GC后几乎没有回收多少内存<br>② 查看各个对象占用的大小: 8+5+2=15已经接近堆内存的最大值16G了<br>③ 从最后OOM时打印的Heap信息查看堆内存中各个部分的大小如下:  </p>
<p>新生代:3.7G, 其中Eden:3.3G, Survivor0:400MB.<br>老年代:12G, 使用了100%. 永久代44M.<br>所以新生代+老年代=12+3.7, 接近最大内存16G, OOM就是因为堆内存被使用光了引起的.<br>通常来说: 老年代(CMS)的对象没有及时被回收释放, 就会造成OOM!<br>也就是说老年代的对象太多了, 造成了<code>内存泄露</code>!  </p>
<p>最后OOM, 因此Cassandra进程最终终于挂掉了(当然OOM异常信息是打印在了system.log中, 这里只是打印出发生OOM时的Heap信息)!<br>因为我们指定了<code>-XX:+HeapDumpOnOutOfMemoryError</code>: 在OOM错误时dump heap.  </p>
<h3 id="GC问题再现:_[203@2015-10-28_19:55]">GC问题再现: [203@2015-10-28 19:55]</h3><p>本来正在下线226节点. 用nodetool status查看时发现203状态为DN, 查看后台很长的GC for CMS.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cat  /var/<span class="built_in">log</span>/cassandra/system.<span class="built_in">log</span>  | grep <span class="string">"GC"</span> | grep <span class="string">"2015-10-28"</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">19</span>:<span class="number">54</span>:<span class="number">38</span>,<span class="number">278</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">235</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">2419573000</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">19</span>:<span class="number">55</span>:<span class="number">25</span>,<span class="number">997</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">1504</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">3733428056</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">19</span>:<span class="number">55</span>:<span class="number">28</span>,<span class="number">946</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">2330</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">5676292056</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">19</span>:<span class="number">55</span>:<span class="number">32</span>,<span class="number">165</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">2739</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">7921578528</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">19</span>:<span class="number">55</span>:<span class="number">35</span>,<span class="number">572</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">2952</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">10161987488</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">20</span>:<span class="number">10</span>:<span class="number">53</span>,<span class="number">290</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">866487</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">11508739304</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">20</span>:<span class="number">10</span>:<span class="number">53</span>,<span class="number">376</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">31607</span> ms <span class="keyword">for</span> <span class="number">2</span> collections, <span class="number">12110583384</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">20</span>:<span class="number">12</span>:<span class="number">06</span>,<span class="number">522</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">51422</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16334886216</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">20</span>:<span class="number">13</span>:<span class="number">24</span>,<span class="number">020</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">51095</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">14423806424</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">20</span>:<span class="number">14</span>:<span class="number">43</span>,<span class="number">251</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">49143</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16170444160</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">20</span>:<span class="number">16</span>:<span class="number">03</span>,<span class="number">026</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">49301</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16327286728</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">20</span>:<span class="number">17</span>:<span class="number">27</span>,<span class="number">808</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">56111</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16416467232</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">20</span>:<span class="number">18</span>:<span class="number">48</span>,<span class="number">870</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">53220</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16494725704</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">20</span>:<span class="number">20</span>:<span class="number">10</span>,<span class="number">354</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">53595</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">16644566464</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">28</span> <span class="number">20</span>:<span class="number">24</span>:<span class="number">10</span>,<span class="number">303</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">163458</span> ms <span class="keyword">for</span> <span class="number">3</span> collections, <span class="number">16649751064</span> used; max is <span class="number">16750411776</span></span><br></pre></td></tr></table></figure></p>
<p>于是用jstat查看了gc, 发现新生代, 老年代空间都满了!<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> S0C     S1C       S0U   S1U      EC        EU        OC         OU       PC        PU      YGC     YGCT       FGC   FGCT     GCT</span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  <span class="number">0.0</span>   <span class="number">419392.0</span> <span class="number">3355520.0</span> <span class="number">3355520.0</span> <span class="number">12582912.0</span> <span class="number">12582912.0</span> <span class="number">48904.0</span> <span class="number">29197.6</span> <span class="number">4066626</span> <span class="number">147156.174</span> <span class="number">10140</span> <span class="number">6473.549</span> <span class="number">153629.722</span></span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  <span class="number">0.0</span>   <span class="number">419392.0</span> <span class="number">3355520.0</span> <span class="number">3355520.0</span> <span class="number">12582912.0</span> <span class="number">12582912.0</span> <span class="number">48904.0</span> <span class="number">29195.8</span> <span class="number">4066626</span> <span class="number">147156.174</span> <span class="number">10140</span> <span class="number">6473.549</span> <span class="number">153629.722</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-rw-r--r--. <span class="number">1</span> admin admin <span class="number">4.5</span>G <span class="number">10</span>月 <span class="number">28</span> <span class="number">20</span>:<span class="number">26</span> gc-<span class="number">1441001670.l</span>og   停掉C进程    </span><br><span class="line">-rw-r--r--. <span class="number">1</span> admin admin <span class="number">124</span>K <span class="number">10</span>月 <span class="number">28</span> <span class="number">20</span>:<span class="number">34</span> gc-<span class="number">1446035196.l</span>og   第一次启动, 启动失败</span><br><span class="line">-rw-r--r--. <span class="number">1</span> admin admin <span class="number">2.8</span>M <span class="number">10</span>月 <span class="number">28</span> <span class="number">21</span>:<span class="number">14</span> gc-<span class="number">1446035781.l</span>og   第二次启动</span><br></pre></td></tr></table></figure>
<h2 id="Problem_2:_GC引起的读超时?_[229@2015-10-22_17:06-08]">Problem 2: GC引起的读超时? [229@2015-10-22 17:06-08]</h2><blockquote>
<p>GC的告白: 不要把一切原因都归结是我的问题啊! 可是总要找个借口吧: 不知道什么引起的读超时,先把GC你当做挡箭牌吧. </p>
</blockquote>
<p>线上报警2015-10-22 17:06-08分超时很严重. 查看202几个forseti节点的system.log,都提示fp的229节点DOWN.<br>但是实际上229并没有当掉, 只是这个节点在执行比较长的GC而已.  </p>
<p>下面是截取的部分日志, 因为GC超过1m,它自己连接别的节点状态都是Down. 还进行了一个26s的CMS FullGC.<br>FullGC一旦完成, Gossiper就提示连接其他节点的状态为Up了. 不过这时候丢弃了一些MUTATION消息.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">08</span>:<span class="number">01</span>,<span class="number">469</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">1653</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">3675</span>,<span class="number">714864</span> used; max is <span class="number">16750</span>,<span class="number">411776</span></span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">08</span>:<span class="number">01</span>,<span class="number">470</span> StatusLogger.java (line <span class="number">55</span>) Pool Name                    Active   Pending      Completed   Blocked  All Time Blocked</span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">08</span>:<span class="number">01</span>,<span class="number">471</span> StatusLogger.java (line <span class="number">70</span>) ReadStage                         <span class="number">0</span>        <span class="number">69</span>       <span class="number">80126766</span>         <span class="number">0</span>                 <span class="number">0</span></span><br><span class="line">INFO [GossipTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">08</span>:<span class="number">01</span>,<span class="number">484</span> Gossiper.java (line <span class="number">962</span>) InetAddress /<span class="number">192.168</span><span class="number">.47</span><span class="number">.226</span> is now DOWN</span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">08</span>:<span class="number">01</span>,<span class="number">502</span> StatusLogger.java (line <span class="number">70</span>) MutationStage                    <span class="number">64</span>      <span class="number">3144</span>     <span class="number">7568222241</span>         <span class="number">0</span>                 <span class="number">0</span></span><br><span class="line">...</span><br><span class="line">INFO [HANDSHAKE-/<span class="number">192.168</span><span class="number">.47</span><span class="number">.205</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">08</span>:<span class="number">01</span>,<span class="number">508</span> OutboundTcpConnection.java (line <span class="number">389</span>) Handshaking version with /<span class="number">192.168</span><span class="number">.47</span><span class="number">.205</span></span><br><span class="line">INFO [RequestResponseStage:<span class="number">17</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">08</span>:<span class="number">01</span>,<span class="number">621</span> Gossiper.java (line <span class="number">948</span>) InetAddress /<span class="number">192.168</span><span class="number">.47</span><span class="number">.206</span> is now UP</span><br><span class="line">...</span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">08</span>:<span class="number">02</span>,<span class="number">180</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ConcurrentMarkSweep: <span class="number">26194</span> ms <span class="keyword">for</span> <span class="number">2</span> collections, <span class="number">504</span>,<span class="number">0721624</span> used; max is <span class="number">16750</span>,<span class="number">411776</span></span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">08</span>:<span class="number">02</span>,<span class="number">295</span> StatusLogger.java (line <span class="number">70</span>) ReadStage                         <span class="number">4</span>         <span class="number">4</span>       <span class="number">80127252</span>         <span class="number">0</span>                 <span class="number">0</span></span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">08</span>:<span class="number">02</span>,<span class="number">305</span> StatusLogger.java (line <span class="number">70</span>) MutationStage                    <span class="number">64</span>     <span class="number">16652</span>     <span class="number">7568222241</span>         <span class="number">0</span>                 <span class="number">0</span></span><br><span class="line">...</span><br><span class="line">INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">08</span>:<span class="number">07</span>,<span class="number">358</span> MessagingService.java (line <span class="number">876</span>) <span class="number">1503</span> MUTATION messages dropped in last <span class="number">5000</span>ms</span><br></pre></td></tr></table></figure>
<p>1) 这里的GC for ParNew或者for CMS使用的堆内存只有3G,5G. 和上一个Problem接近堆的最大内存16G不可同日而语的. 不过CMS的时间仍然很长:26s.<br>2) Gossiper在17:08:01,502为DOWN, 在17:08:01,621又恢复为UP, 说明当前节点并不是真正的挂掉的.  </p>
<p>看看这个时候的gc日志:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grep -C <span class="number">10</span> <span class="string">"2015-10-22T17:07"</span> gc-<span class="number">1442722105.l</span>og</span><br><span class="line">grep -C <span class="number">20</span> <span class="string">"2015-10-22T17:07.*Class Histogram (before full gc)"</span> gc-<span class="number">1442722105.l</span>og</span><br></pre></td></tr></table></figure></p>
<p>ParNew收集器提升失败, 可以看到ParNew后面的反而比之前要大. (是这个原因吗, 不是吧)<br>为什么提升失败: 因为总的才有3374, 提升前是3364, 提升后是3378, 显然不能成功提升. 这里提升的含义是?</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span>T17:<span class="number">07</span>:<span class="number">25.975</span>+<span class="number">0800</span>: <span class="number">2782739.528</span>: [ParNew (promotion failed): <span class="number">3364</span>,<span class="number">001</span>K-&gt;<span class="number">3378</span>,<span class="number">480</span>K(<span class="number">3774</span>,<span class="number">912</span>K), <span class="number">1.6523860</span> secs]</span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span>T17:<span class="number">07</span>:<span class="number">33.314</span>+<span class="number">0800</span>: <span class="number">2782746.867</span>: [CMSCMS: Large block <span class="number">0x00000007c440e128</span></span><br><span class="line">CMS: Large Block: <span class="number">0x00000007fdb07000</span>; Proximity: <span class="number">0x00000007fda93b90</span> -&gt; <span class="number">0x00000007fda93b90</span></span><br><span class="line">CMS: Large block <span class="number">0x00000007fdb07000</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">22</span>T17:<span class="number">07</span>:<span class="number">35.001</span>+<span class="number">0800</span>: <span class="number">2782748.554</span>: [CMS-concurrent-sweep: <span class="number">5.884</span>/<span class="number">13.956</span> secs] [Times: user=<span class="number">86.82</span> sys=<span class="number">2.11</span>, real=<span class="number">13.96</span> secs]</span><br><span class="line"> (concurrent mode failure)CMS: Large block <span class="number">0x00000005d4d31cb8</span></span><br></pre></td></tr></table></figure>
<p>分析GC信息:<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@<span class="number">192-168-47-229</span> ~]$ sudo -u admin jstat -gc `sudo -u admin jps | grep CassandraDaemon |awk '&#123;print $1&#125;'` 1000</span><br><span class="line"> S0C    S1C    S0U    S1U      EC       EU        OC         OU       PC     PU    YGC     YGCT    FGC    FGCT     GCT</span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  0.0   <span class="number">15686.1 33</span><span class="number">55520.0 32</span><span class="number">93381.8 125</span><span class="number">82912.0 29</span><span class="number">88456.0</span>  <span class="number">46492.0 27</span><span class="number">854.7 34</span><span class="number">12928 97557</span>.<span class="number">349 11677</span> <span class="number">2883.959</span> <span class="number">100441.307</span></span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  0.0   <span class="number">24851.0 33</span><span class="number">55520.0 143</span><span class="number">9211.8 125</span><span class="number">82912.0 30</span><span class="number">06161.3</span>  <span class="number">46492.0 27</span><span class="number">854.7 34</span><span class="number">12930 97557</span>.<span class="number">397 11677</span> <span class="number">2883.959</span> <span class="number">100441.355</span></span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  0.0   <span class="number">1109.9 33</span><span class="number">55520.0 194</span><span class="number">9350.9 125</span><span class="number">82912.0 30</span><span class="number">27253.7</span>  <span class="number">46492.0 27</span><span class="number">854.7 34</span><span class="number">12932 97557</span>.<span class="number">452 11677</span> <span class="number">2883.959</span> <span class="number">100441.410</span></span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  0.0   <span class="number">17355.3 33</span><span class="number">55520.0 70</span><span class="number">5937.7 125</span><span class="number">82912.0 30</span><span class="number">39274.0</span>  <span class="number">46492.0 27</span><span class="number">854.7 34</span><span class="number">12934 97557</span>.<span class="number">493 11677</span> <span class="number">2883.959</span> <span class="number">100441.452</span></span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  0.0   594.5  <span class="number">3355520.0</span> <span class="number">2404209.8 125</span><span class="number">82912.0 30</span><span class="number">59189.1</span>  <span class="number">46492.0 27</span><span class="number">854.7 34</span><span class="number">12936 97557</span>.<span class="number">547 11677</span> <span class="number">2883.959</span> <span class="number">100441.505</span></span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  0.0   <span class="number">14354.7 33</span><span class="number">55520.0 66</span><span class="number">9799.8 125</span><span class="number">82912.0 30</span><span class="number">70693.1</span>  <span class="number">46492.0 27</span><span class="number">854.7 34</span><span class="number">12938 97557</span>.<span class="number">600 11677</span> <span class="number">2883.959</span> <span class="number">100441.559</span></span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span> <span class="number">24026.0</span>  0.0   <span class="number">3355520.0</span> <span class="number">2792573.7</span> <span class="number">12582912.0</span> <span class="number">3078149.1</span>  <span class="number">46492.0 27</span><span class="number">854.7 34</span><span class="number">12939 97557</span>.<span class="number">626 11677</span> <span class="number">2883.959</span> <span class="number">100441.585</span></span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span> <span class="number">12748.3</span>  0.0   <span class="number">3355520.0</span> <span class="number">1614777.9</span> <span class="number">12582912.0</span> <span class="number">3097754.2</span>  <span class="number">46492.0 27</span><span class="number">854.7 34</span><span class="number">12941 97557</span>.<span class="number">677 11677</span> <span class="number">2883.959</span> <span class="number">100441.636</span></span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span> <span class="number">15593.8</span>  0.0   <span class="number">3355520.0</span> <span class="number">171442.8 125</span><span class="number">82912.0 31</span><span class="number">16802.0</span>  <span class="number">46492.0 27</span><span class="number">854.7 34</span><span class="number">12943 97557</span>.<span class="number">727 11677</span> <span class="number">2883.959</span> <span class="number">100441.686</span></span><br><span class="line"><span class="number">419392.0</span> <span class="number">419392.0</span>  0.0   <span class="number">12975.7 33</span><span class="number">55520.0 209</span><span class="number">6091.2 125</span><span class="number">82912.0 31</span><span class="number">25678.7</span>  <span class="number">46492.0 27</span><span class="number">854.7 34</span><span class="number">12944 97557</span>.<span class="number">753 11677</span> <span class="number">2883.959</span> <span class="number">100441.712</span></span><br></pre></td></tr></table></figure></p>
<p>看到一秒钟就发生了2次YGC. 使用-gccause查看:输出-gcutil提供的信息以及最后一次(Last)执行GC的发生原因和当前(Current)所执行的GC的发生原因<br><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047202 ~]$ sudo -<span class="keyword">u</span> admin jstat -gccause `sudo -<span class="keyword">u</span> admin jps | grep CassandraDaemon |awk '&#123;<span class="keyword">print</span> <span class="label">$1&#125;</span>'`</span><br><span class="line">  S0     S1     <span class="keyword">E</span>      O      P     YGC     YGCT    FGC    FGCT     GCT    LGCC                 GCC</span><br><span class="line">  5.69   0.00  21.40  74.47  59.86 147180 5728.515   328   95.091 5823.606 Allocation Failure   <span class="keyword">No</span> GC</span><br><span class="line"></span><br><span class="line">[qihuang.zheng@192-168-47-229 ~]$ sudo -<span class="keyword">u</span> admin jstat -gccause `sudo -<span class="keyword">u</span> admin jps | grep CassandraDaemon |awk '&#123;<span class="keyword">print</span> <span class="label">$1&#125;</span>'`</span><br><span class="line">  S0     S1     <span class="keyword">E</span>      O      P     YGC     YGCT    FGC    FGCT     GCT    LGCC                 GCC</span><br><span class="line">  4.14   0.00  41.68  40.57  59.91 3410825 97506.253 11671 2882.208 100388.461 Allocation Failure   <span class="keyword">No</span> GC</span><br></pre></td></tr></table></figure></p>
<p>虽然229节点的YGC和FGC次数很多, 但是总体来说每次的GC时间并不是很长.  </p>
<table>
<thead>
<tr>
<th>Node</th>
<th>GC Type</th>
<th>Count</th>
<th>Time</th>
<th>Avg=Time/Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>202</td>
<td>YGC</td>
<td>147180</td>
<td>5728.515</td>
<td>0.0389s(38ms)</td>
</tr>
<tr>
<td>202</td>
<td>FGC</td>
<td>328</td>
<td>95.091</td>
<td>0.289s(289ms)</td>
</tr>
<tr>
<td>229</td>
<td>YGC</td>
<td>3410825</td>
<td>97506.253</td>
<td>0.028s(28ms)</td>
</tr>
<tr>
<td>229</td>
<td>FGC</td>
<td>11671</td>
<td>2882.208</td>
<td>0.246s(246ms)</td>
</tr>
</tbody>
</table>
<p>上面GC失败的原因提示<code>Allocation Failure</code>,通常是Eden空间不够.<br><a href="http://stackoverflow.com/questions/28342736/java-gc-allocation-failure" target="_blank" rel="external">http://stackoverflow.com/questions/28342736/java-gc-allocation-failure</a>  </p>
<p>由于229节点有64G内存, 而设置的<code>-Xms16G -Xmx16G -Xmn4G</code>, 可以考虑增加这三个值的大小.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/usr/install/cassandra/conf/cassandra-env.sh</span><br><span class="line">JVM_OPTS=<span class="string">"<span class="variable">$JVM_OPTS</span> -Xms16G"</span></span><br><span class="line">JVM_OPTS=<span class="string">"<span class="variable">$JVM_OPTS</span> -Xmx16G"</span></span><br><span class="line">JVM_OPTS=<span class="string">"<span class="variable">$JVM_OPTS</span> -Xmn4G"</span></span><br><span class="line">JVM_OPTS=<span class="string">"<span class="variable">$JVM_OPTS</span> -XX:MaxDirectMemorySize=6G"</span></span><br></pre></td></tr></table></figure></p>
<h2 id="GC_Question">GC Question</h2><h3 id="Tombstone">Tombstone</h3><p>查询超过1000个tombstone的记录:<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="link_label">qihuang.zheng@cass047202 cassandra</span>]$ cat system.log | grep "live and [<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>] tombstone cells in forseti.velocity" | grep "2015-10-26" | wc -l</span><br><span class="line">576</span><br><span class="line">[<span class="link_label">qihuang.zheng@cass047202 cassandra</span>]$ cat system.log | grep "live and [<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>] tombstone cells in forseti.velocity" | grep "2015-10-26" | head</span><br><span class="line"> WARN [ReadStage:751] 2015-10-26 00:13:14,986 SliceQueryFilter.java (line 231) Read 654 live and 5751 tombstone cells in forseti.velocity (see tombstone<span class="emphasis">_warn_</span>threshold). 1000 columns was requested, slices=[koudai:weidian:payeeIdNumber:!-koudai:weidian:payeeIdNumber]</span><br><span class="line"> WARN [ReadStage:752] 2015-10-26 00:13:19,420 SliceQueryFilter.java (line 231) Read 0 live and 5079 tombstone cells in forseti.velocity (see tombstone<span class="emphasis">_warn_</span>threshold). 1000 columns was requested, slices=[fyzhibo:fyzhibo<span class="emphasis">_web:accountLogin:!-fyzhibo:fyzhibo_</span>web:accountLogin]</span><br></pre></td></tr></table></figure></p>
<p>Read 0 live and 5079 tombstone是的记录表示要查询的记录都已经过期了. 但是还剩把tombstone都返回到客户端.  </p>
<h3 id="CMS_and_Heap_Memory_Relation">CMS and Heap Memory Relation</h3><p>如果直接查看system.log的GC日志, 会发现used增长到一定程序比如10G左右, 就会下降, 那么这个下降之前的一定发生了CMS.<br>虽然在system.log中我们并没有看到GC for CMS的日志. 但是在gc.log中一定是发生了CMS, 否则堆内存是不会减少的.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047202 ~]$ cat /var/<span class="built_in">log</span>/cassandra/system.<span class="built_in">log</span> | grep <span class="string">"GC"</span> | grep <span class="string">"2015-10-26"</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span> <span class="number">15</span>:<span class="number">55</span>:<span class="number">08</span>,<span class="number">627</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">224</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">7815460192</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">08</span>:<span class="number">52</span>,<span class="number">236</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">230</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">9134796832</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">10</span>:<span class="number">58</span>,<span class="number">390</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">371</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">9907968408</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">01</span>,<span class="number">391</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">475</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">10142946312</span> used; max is <span class="number">16750411776</span></span><br><span class="line"></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">08</span>,<span class="number">499</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">225</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">3120763368</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">13</span>:<span class="number">27</span>,<span class="number">580</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">214</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">3887484088</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">13</span>:<span class="number">49</span>,<span class="number">744</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">308</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">3189094392</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">13</span>:<span class="number">51</span>,<span class="number">883</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">556</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">3412395496</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">13</span>:<span class="number">54</span>,<span class="number">884</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">231</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">3724150752</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">35</span>:<span class="number">13</span>,<span class="number">382</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">402</span> ms <span class="keyword">for</span> <span class="number">2</span> collections, <span class="number">7429717728</span> used; max is <span class="number">16750411776</span></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">45</span>:<span class="number">07</span>,<span class="number">388</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">228</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">10338198528</span> used; max is <span class="number">16750411776</span></span><br><span class="line"></span><br><span class="line"> INFO [ScheduledTasks:<span class="number">1</span>] <span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span> <span class="number">16</span>:<span class="number">52</span>:<span class="number">28</span>,<span class="number">976</span> GCInspector.java (line <span class="number">116</span>) GC <span class="keyword">for</span> ParNew: <span class="number">303</span> ms <span class="keyword">for</span> <span class="number">1</span> collections, <span class="number">3099641168</span> used; max is <span class="number">16750411776</span></span><br></pre></td></tr></table></figure></p>
<p>选取16:45这个点之后的16:52, 堆内存的used从10338198528下降到3099641168, 下面是对应的system.log对应这个时间段的日志, 发生了一次CMS:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">06.703</span>+<span class="number">0800</span>: <span class="number">323508.048</span>: [👉ParNew: <span class="number">3535452</span>K-&gt;<span class="number">126895</span>K(<span class="number">3774912</span>K), <span class="number">0.2282630</span> secs] <span class="number">12898898</span>K-&gt;<span class="number">9656082</span>K(<span class="number">16357824</span>K)After GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">06.937</span>+<span class="number">0800</span>: <span class="number">323508.282</span>: [GC [<span class="number">1</span> CMS-initial-mark: <span class="number">9529186</span>K(<span class="number">12582912</span>K)] <span class="number">9656178</span>K(<span class="number">16357824</span>K), <span class="number">0.0232120</span> secs] [Times: user=<span class="number">0.02</span> sys=<span class="number">0.00</span>, real=<span class="number">0.02</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">06.960</span>+<span class="number">0800</span>: <span class="number">323508.305</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.0271990</span> seconds</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">06.961</span>+<span class="number">0800</span>: <span class="number">323508.305</span>: [CMS-concurrent-mark-start]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">08.299</span>+<span class="number">0800</span>: <span class="number">323509.643</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.0034360</span> seconds</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">08.660</span>+<span class="number">0800</span>: <span class="number">323510.004</span>: [CMS-concurrent-mark: <span class="number">1.695</span>/<span class="number">1.699</span> secs] [Times: user=<span class="number">7.69</span> sys=<span class="number">0.52</span>, real=<span class="number">1.70</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">08.660</span>+<span class="number">0800</span>: <span class="number">323510.004</span>: [CMS-concurrent-preclean-start]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">08.711</span>+<span class="number">0800</span>: <span class="number">323510.055</span>: [CMS-concurrent-preclean: <span class="number">0.049</span>/<span class="number">0.051</span> secs] [Times: user=<span class="number">0.17</span> sys=<span class="number">0.03</span>, real=<span class="number">0.05</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">08.711</span>+<span class="number">0800</span>: <span class="number">323510.055</span>: [CMS-concurrent-abortable-preclean-start]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">09.696</span>+<span class="number">0800</span>: <span class="number">323511.040</span>: [👉ParNew: <span class="number">3482415</span>K-&gt;<span class="number">33672</span>K(<span class="number">3774912</span>K), <span class="number">0.0383290</span> secs] <span class="number">13011602</span>K-&gt;<span class="number">9568814</span>K(<span class="number">16357824</span>K)After GC:</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">11.023</span>+<span class="number">0800</span>: <span class="number">323512.367</span>: [CMS-concurrent-abortable-preclean: <span class="number">2.256</span>/<span class="number">2.312</span> secs] [Times: user=<span class="number">8.44</span> sys=<span class="number">0.82</span>, real=<span class="number">2.31</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">11.026</span>+<span class="number">0800</span>: <span class="number">323512.371</span>: [GC[YG occupancy: <span class="number">1833528</span> K (<span class="number">3774912</span> K)]<span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">11.027</span>+<span class="number">0800</span>: <span class="number">323512.371</span>: [Rescan (parallel) , <span class="number">0.3862770</span> secs]<span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">11.413</span>+<span class="number">0800</span>: <span class="number">323512.758</span>: [weak refs processing, <span class="number">0.0027200</span> secs]<span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">11.416</span>+<span class="number">0800</span>: <span class="number">323512.760</span>: [<span class="keyword">class</span> unloading, <span class="number">0.0165040</span> secs]<span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">11.432</span>+<span class="number">0800</span>: <span class="number">323512.777</span>: [scrub symbol table, <span class="number">0.0027290</span> secs]<span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">11.435</span>+<span class="number">0800</span>: <span class="number">323512.780</span>: [scrub <span class="built_in">string</span> table, <span class="number">0.0044760</span> secs] [<span class="number">1</span> CMS-remark: <span class="number">9535141</span>K(<span class="number">12582912</span>K)] <span class="number">11368669</span>K(<span class="number">16357824</span>K), <span class="number">0.4202100</span> secs] [Times: user=<span class="number">2.83</span> sys=<span class="number">0.00</span>, real=<span class="number">0.42</span> secs]</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">11.447</span>+<span class="number">0800</span>: <span class="number">323512.792</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.4239320</span> seconds</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">11.447</span>+<span class="number">0800</span>: <span class="number">323512.792</span>: [CMS-concurrent-sweep-start]</span><br><span class="line">CMS: Large Block: <span class="number">0x00000007d0aedfc0</span>; Proximity: <span class="number">0x00000007c96cce08</span> -&gt; <span class="number">0x00000007c96cce08</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">11.571</span>+<span class="number">0800</span>: <span class="number">323512.915</span>: Total time <span class="keyword">for</span> which application threads were stopped: <span class="number">0.0038340</span> seconds</span><br><span class="line"><span class="number">2015</span>-<span class="number">10</span>-<span class="number">26</span>T16:<span class="number">45</span>:<span class="number">12.641</span>+<span class="number">0800</span>: <span class="number">323513.986</span>: [👉ParNew: <span class="number">3389192</span>K-&gt;<span class="number">24343</span>K(<span class="number">3774912</span>K), <span class="number">0.0435410</span> secs] <span class="number">12284258</span>K-&gt;<span class="number">8924627</span>K(<span class="number">16357824</span>K)After GC:</span><br></pre></td></tr></table></figure></p>
<p>可以看到这几次的ParNew夹杂着CMS, 总体的对内存变化是: 9656082K -&gt; 9568814K -&gt; 8924627K  </p>
<h3 id="ParNew_promotion_failed">ParNew promotion failed</h3><p>在查询是否有较多的ExpiringColumn时, 发现了一个新生代晋升的异常:<br>虽然在失败的前一个ParNew回收后, 堆内存是7700182K, 还剩有3个G左右. 但是因为碎片数太多:9599. 导致这次ParNew失败.<br><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047202 cassandra]$ grep "org.apache.cassandra.db.ExpiringColumn" -C 50 gc-<span class="number">1445525597</span>.log &gt; ~/ttl2.log</span><br><span class="line"><span class="number">2015-10-24</span>T<span class="number">00:55:39.33</span>3+0800: <span class="number">93740.677</span>: [ParNew: <span class="number">3369116</span>K-&gt;9027K(<span class="number">3774912</span>K), <span class="number">0.0332100</span> secs] <span class="number">11053952</span>K-&gt;<span class="number">7700182</span>K(<span class="number">16357824</span>K)After GC:</span><br><span class="line"><span class="number">2015-10-24</span>T<span class="number">00:55:40.97</span>6+0800: <span class="number">93742.320</span>: [GCBefore GC:</span><br><span class="line">Statistics for BinaryTreeDictionary:</span><br><span class="line">------------------------------------</span><br><span class="line">Total Free Space: <span class="number">28243524</span></span><br><span class="line">Max   Chunk Size: 100283        最大的连续块只有100KB! </span><br><span class="line">Number of Blocks: 9599</span><br><span class="line">Av.  Block  Size: 2942</span><br><span class="line">Tree      Height: 110</span><br><span class="line">Before GC:</span><br><span class="line">Statistics for BinaryTreeDictionary:</span><br><span class="line">------------------------------------</span><br><span class="line">Total Free Space: <span class="number">2371623</span></span><br><span class="line">Max   Chunk Size: <span class="number">2362880</span></span><br><span class="line">Number of Blocks: 15</span><br><span class="line">Av.  Block  Size: 158108</span><br><span class="line">Tree      Height: 5</span><br><span class="line"><span class="number">2015-10-24</span>T<span class="number">00:55:40.97</span>7+0800: <span class="number">93742.321</span>: [ParNew (promotion failed): <span class="number">3364547</span>K-&gt;<span class="number">3364509</span>K(<span class="number">3774912</span>K), <span class="number">1.0112290</span> secs]<span class="number">2015-10-24</span>T<span class="number">00:55:41.98</span>8+0800: <span class="number">93743.332</span>: [Class Histogram (before full gc):</span><br><span class="line"> num     #instances         #bytes  class name</span><br><span class="line">----------------------------------------------</span><br><span class="line">   1:      <span class="number">94519264</span>     <span class="number">4536924672</span>  java.nio.HeapByteBuffer</span><br><span class="line">   2:      <span class="number">16569138</span>     <span class="number">4369282384</span>  [B</span><br><span class="line">   3:      <span class="number">10057767</span>      <span class="number">482772816</span>  edu.stanford.ppl.concurrent.SnapTreeMap$Node</span><br><span class="line">   4:       <span class="number">8381272</span>      <span class="number">335250880</span>  org.apache.cassandra.db.ExpiringColumn</span><br><span class="line">   5:       <span class="number">5873894</span>      <span class="number">234955760</span>  org.apache.cassandra.io.sstable.IndexHelper$IndexInfo</span><br><span class="line">   6:       <span class="number">1515892</span>      <span class="number">145525632</span>  edu.stanford.ppl.concurrent.CopyOnWriteManager$COWEpoch</span><br><span class="line"></span><br><span class="line"><span class="number">2015-10-24</span>T<span class="number">00:55:50.56</span>9+0800: <span class="number">93751.914</span>: [CMSCMS: Large block <span class="number">0x00000005</span>ae76daa8</span><br><span class="line">: <span class="number">7693343</span>K-&gt;<span class="number">2942262</span>K(<span class="number">12582912</span>K), <span class="number">15.9221830</span> secs]<span class="number">2015-10-24</span>T<span class="number">00:56:06.49</span>1+0800: <span class="number">93767.836</span>: [Class Histogram (after full gc):</span><br><span class="line"> num     #instances         #bytes  class name</span><br><span class="line">----------------------------------------------</span><br><span class="line">   1:      <span class="number">12886333</span>     <span class="number">1316594672</span>  [B</span><br><span class="line">   2:      <span class="number">15611806</span>      <span class="number">749366688</span>  java.nio.HeapByteBuffer</span><br><span class="line">   3:       <span class="number">5714973</span>      <span class="number">228598920</span>  org.apache.cassandra.io.sstable.IndexHelper$IndexInfo</span><br><span class="line">   4:       <span class="number">1853317</span>       <span class="number">88959216</span>  edu.stanford.ppl.concurrent.SnapTreeMap$Node</span><br><span class="line">   5:       <span class="number">2448358</span>       <span class="number">78347456</span>  java.util.concurrent.ConcurrentHashMap$HashEntry</span><br><span class="line">   6:       <span class="number">2427734</span>       <span class="number">77687488</span>  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node</span><br><span class="line">   7:       <span class="number">1850344</span>       <span class="number">74013760</span>  org.apache.cassandra.db.ExpiringColumn</span><br><span class="line">   8:       <span class="number">2433031</span>       <span class="number">58392744</span>  org.apache.cassandra.db.RowIndexEntry</span><br><span class="line">   9:       <span class="number">2427734</span>       <span class="number">58265616</span>  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue</span><br><span class="line">  10:       <span class="number">2427651</span>       <span class="number">58263624</span>  org.apache.cassandra.cache.KeyCacheKey</span><br><span class="line">  11:           886       <span class="number">47000280</span>  [Ljava.util.concurrent.ConcurrentHashMap$HashEntry<span class="comment">;</span></span><br><span class="line">  12:         47596       <span class="number">29251952</span>  [Ljava.lang.Object<span class="comment">;</span></span><br></pre></td></tr></table></figure></p>
<p><a href="http://stackoverflow.com/questions/21992943/persistant-gc-issues-with-cassandra-long-app-pauses" target="_blank" rel="external">http://stackoverflow.com/questions/21992943/persistant-gc-issues-with-cassandra-long-app-pauses</a>  </p>
<p>在20:49的时候有三个节点几乎同时在做CMS,只持续了20s.并且在21:55分又开始同时CMS,但这个时候每个节点都做了多次的CMS.<br>这次的CMS都差不多在22:10分结束, 但是最后204在22:44分又做了一次41次collection,花费了1600s=30分钟! 报警发生在10:55.<br><img src="http://img.blog.csdn.net/20151104104302323" alt="c_gc3"></p>
<p>观察202, 在21:55分开始CMS后的一端时间貌似监控数据丢失了. 直到CMS在22:11完成后, 监控数据才有.<br><img src="http://img.blog.csdn.net/20151104104319137" alt="c_202"></p>
<p>在225上, 可以看到21:55分确实做了CMS(圆圈圈起来的):<br><img src="http://img.blog.csdn.net/20151104104335018" alt="c_225"></p>
<p>当GC发生时, read请求不会路由到这些发生GC的节点, 如果有请求过来, 因为JVM正在做GC, 响应会很慢, 所以可以看到第四张图read request latency变长.<br>即使请求路由到这些节点,也会超时,所以C<em>的Driver会选择其他节点, 所以请求到这些节点的数量也会减少, 所以看到第三张图read requests数量减少.<br>同样因为GC的原因, 节点响应很慢, 即使有数据写到这些节点, 成功完成一次写请求需要的时间变长, 即第三张图write request latency变长.<br>正如 <strong>读请求数变少, 读延迟变长, 当写延迟变长, 写请求数也同样减少</strong>(即第一张图) 图中粉色的是202, 黄色的是225.<br>或者 <em>*GC的时候, 响应变慢, 导致读写延迟都增加, 路由到节点的读写请求数也变少</em></em>.<br><img src="http://img.blog.csdn.net/20151104113357712" alt="gc_read_slow"></p>
<p>注意第三图虽然202,225读请求数减少, 但是这时却有其他节点的读请求数增加. 这也解释了从这些节点读数据被转移到了从其他有副本的节点读取.  </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://aryanet.com/blog/cassandra-garbage-collector-tuning&quot;&gt;http://aryanet.com/blog/cassandra-garbage-collector-tuning&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://tech.shift.com/post/74311817513/cassandra-tuning-the-jvm-for-read-heavy-workloads&quot;&gt;http://tech.shift.com/post/74311817513/cassandra-tuning-the-jvm-for-read-heavy-workloads&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html&quot;&gt;http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="cassandra" scheme="http://github.com/zqhxuyuan/categories/cassandra/"/>
    
    
      <category term="cassandra" scheme="http://github.com/zqhxuyuan/tags/cassandra/"/>
    
  </entry>
  
</feed>
