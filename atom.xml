<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zqhxuyuan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://github.com/zqhxuyuan/"/>
  <updated>2017-09-20T16:53:13.000Z</updated>
  <id>http://github.com/zqhxuyuan/</id>
  
  <author>
    <name>任何忧伤,都抵不过世界的美丽</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kafka技术内幕</title>
    <link href="http://github.com/zqhxuyuan/2018/01/01/Kafka-Code-Index/"/>
    <id>http://github.com/zqhxuyuan/2018/01/01/Kafka-Code-Index/</id>
    <published>2017-12-31T16:00:00.000Z</published>
    <updated>2017-09-20T16:53:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>《Kafka技术内幕》<br><a id="more"></a></p>
<h2 id="本书介绍：">本书介绍：</h2><p>本书主要以0.10版本的Kafka源码为基础，并通过图文详解的方式分析Kafka内部组件的实现细节，全书原创的图片有近400幅。对于Kafka流处理的一些新特性，也会分析0.11版本的相关源码。本书各个章节的主要内容如下。</p>
<ul>
<li>第一章首先介绍了Kafka作为流式数据平台的三个组成，包括消息系统、存储系统、流处理系统。Kafka基本概念的三种模型，包括分区模型、消费模型、分布式模型。然后介绍了Kafka几个比较重要的设计思路，最后介绍了如何在一台机器上模拟单机模式与分布式模式，以及如何搭建源码开发环境。</li>
<li>第二章从一个生产者的示例开始，引出了新版本生产者的两种消息发送方式。生产者客户端利用记录收集器、发送线程，对消息集进行分组和缓存，并为目标节点创建生产请求，发送到不同的代理节点。接着介绍了与网络相关的Kafka通道、选择器、轮询等NIO操作。另外还介绍了Scala版本的旧生产者，它使用阻塞通道的方式发送请求。最后介绍了服务端采用<code>Reactor</code>模式处理客户端的请求。</li>
<li>第三章首先介绍了消费者相关的基础概念，然后从一个消费者的示例开始，引出了基于ZooKeeper的高级消费者API，理解高级API主要是要理解消费线程的模型以及变量的传递方式。接着介绍了消费者提交分区偏移量的两种方式。最后，我们举了一个低级API的示例，开发者需要自己实现一些比较复杂的逻辑处理才能保证消费程序的健壮性和稳定性。</li>
<li>第四章介绍了新版本的消费者，不同于旧版本的消费者，新版本去除了ZooKeeper的依赖，统一了旧版本的高级API和低级API，并提供了两种消费方式：订阅和分配。新版本引入订阅状态来管理消费者的订阅信息、并使用拉取器拉取消息。新版本的消费者没有使用拉取线程，而是采用轮询的方式拉取消息，它的性能比旧版本的消费者更好。另外还介绍了消费者采用回调器、处理器、监听器、适配器、组合模式、链式调用等实现不同类型的异步请求。最后，我们介绍了新消费者的心跳任务、提交偏移量以及三种消息处理语义的使用方式。</li>
<li>第五章介绍了新版本消费者相关的协调者实现，主要包括“加入组”与“同步组”。每个消费者都有一个客户端的协调者，服务端也有一个消费组级别的协调者负责处理所有消费者客户端的请求。当消费组触发再平衡操作，服务端的协调者会记录消费组元数据的变化，并通过状态机保证消费组状态的正常转换。本章会通过很多不同的示例场景来帮助读者理解消费组相关的实现。</li>
<li>第六章介绍了Kafka的存储层实现，包括日志的读写、日志的管理、日志的压缩等一些常用的日志操作。服务端通过副本管理器处理客户端的生产请求和拉取请求。接着介绍了副本机制相关的分区、副本、最高水位、复制点等一些概念。最后介绍了延迟操作接口与延迟缓存。服务端如果不能立即返回响应结果给客户端，会先将延迟操作缓存起来，直到请求处理完成或超时。</li>
<li>第七章介绍了作为服务端核心的Kafka控制器，它主要负责管理分区状态机和副本状态机，以及多种类型的监听器，比如代理节点上线和下线、删除主题、重新分配分区等。控制器的一个重要职责是选举分区的主副本。不同代理节点根据控制器下发的请求，决定成为分区的主副本还是从副本。另外，我们还分析了本地副本与远程副本的区别，以及元数据缓存的作用。</li>
<li>第八章首先介绍了两种集群的同步工具：Kafka内置的MirrorMaker和Uber开源的uReplicator。接着介绍了新版本Kafka提供的连接器框架，以及如何开发一个自定义的连接器。然后介绍了连接器的架构模型的具体实现，主要包括数据模型、Connector模型、Worker模型。</li>
<li>第九章介绍了Kafka流处理的两种API：低级Processor API和高级DSL。本章我们重点介绍了流处理的线程模型，主要包括流实例、流线程、流任务。我们还介绍了流处理的本地状态存储，它主要用来作为备份任务的数据恢复。高级DSL包括两个组件：<code>KStream</code>与<code>KTable</code>，它们都定义了一些常用的流处理算子操作，比如无状态的操作（过滤、映射等）、有状态的操作（连接、窗口等）。</li>
<li>第十章介绍了Kafka的一些高级特性，比如客户端的配额、新的消息格式、事务特性。</li>
</ul>
<p>本书相关的示例代码在笔者的Github主页<a href="https://github.com/zqhxuyuan/kafka-book">https://github.com/zqhxuyuan/kafka-book</a>上，另外，限于篇幅，本书的附录部分会放在个人博客上。由于个人能力有限，文中的错误在所难免，读者在阅读本书的过程中，发现不妥之处，可以私信笔者的微博：<a href="http://weibo.com/xuyuantree" target="_blank" rel="external">http://weibo.com/xuyuantree</a>，笔者会定期将勘误表更新到个人博客上。</p>
<h2 id="English_Introduce">English Introduce</h2><p>《Apache Kafka Internal》</p>
<p>This book mostly based on Kafka-0.10, and some part of 0.11 for streaming. It has nearly 400 pictures to analysis Kafka internal implementation. The book written from client to coordinator, from storage to controller, and also including Kafka Connect and Kafka Streams. Here is content introduction of each chapter:</p>
<p>Chapter 1: Being a streaming platform, kafka composed of message system, storage and streaming processing. There are three model of Kafka basic concepts: Partition model, Consumer model and Distributed model. We also introduce some important design ideas of kafka, such as file system persistent, data transformation, producer and consumer, replication and HA.</p>
<p>Chapter 2: From a producer example into how client send message. The whole workflow include record accumulator, sender thread, grouping message, create request and at last send to different target broker. Then we introduce Kafka channel, selector and also how server use NIO reactor to handle client request.</p>
<p>Chapter 3: From a old high-level consumer example into zookeeper based api. The most important of high-level consuemr is consumer thread model. Then we introduce two approach to commit consumer offset which is zookeeper or internal topic. After that, we illustrate how to write low-level consumer to ensure processing messages stability and robust.</p>
<p>Chapter 4: New version consumer client use subscription state and polling fetch instead of fetcher thread. We also introduce how consumer use callback, handler, listener, adapter, chain to implement different asynchronous request mode. Last we introduce heartbeat, offset commit and three consumer processing semantic: at-most-once,at-least-once,exactly-once.</p>
<p>Chapter 5: New consumer communicate with server coordinator by ConsumerCoordinator, there’re mainly two request/response involved: Join-group and Sync-group. This process also called consumer group rebalance. We also discussed how server coordinator use state machine to ensure group state transformation, such as PreparingRebalance,AwaitingSync,Stable. This chapter also give some different scene to help reader understand how consumer group worked in production environment.</p>
<p>Chapter 6: Kafka’s storage layer process include log read/write, log manager, log compaction. In server side, ReplicationManager is responsible for client’s request. Then we introduce Replication mechanism concepts, such as Partition, Replication, HW, LEO. Last we introduce delayed operation and delayed purgatory. If server can’t response immediately to client, they have to cache request and send response to client some times later.</p>
<p>Chapter 7: Kafka Controller component is in charge of managing PartitionState, ReplicationState, and some listeners, such as broker up/down, topic deletion, partition reassign. The main duty of controller is selecting partition’s leader and sent LeaderAndIsr request down to brokers. Target brokers receiving request will decide to be partition leader or follower. Furthermore, we introduce the different between local replication and remote replication, also the function of metadata cache.</p>
<p>Chapter 8: First we introduce two kind of cluster synchronization: Kafka internal MirrorMaker and Uber open sourced uReplicator, we also show how apache helix build replicated uReplicator. Next we introduce new build-in kafka connect framework and how to develop a custom connector plugin. Then we deep into connector’s architecture, mainly concentrate on data model, connector model, worker model.</p>
<p>Chapter 9: Introduce Kafka Streams two api: low-level processor and high-level DSL. This chapter focus on streaming thread model, including stream instance, thread and task. We also introduce local state store used by standby task for recovery. After that, we introduce two abstract components in High-level DSL: KStream and KTable, they both based on low-level processor, support common operator and advance function, such as window, join and so on.</p>
<p>Chapter 10: Introduce some advanced features. such as client quota, new message format in 0.11 and also  transaction support.</p>
<h2 id="目录">目录</h2><p><img src="http://img.blog.csdn.net/20170624225547302" alt="0"></p>
<p><img src="http://img.blog.csdn.net/20170624225600513" alt="0"></p>
<hr>
<p>下面是以前写的一些博客，当然实际的书籍已经改动很大了，下面的一些博文仅供参考。</p>
<h2 id="Introduce">Introduce</h2><ul>
<li><a href="http://zqhxuyuan.github.io/2016/01/04/2016-01-04-Kafka-Intro/" target="_blank" rel="external">Kafka介绍</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/05/2016-01-05-Kafka-Unix/" target="_blank" rel="external">使用Unix管道解释Kafka</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/13/2016-01-13-Kafka-Picture/" target="_blank" rel="external">Kafka图文理解</a></li>
</ul>
<h2 id="源码分析汇总">源码分析汇总</h2><ul>
<li><a href="http://zqhxuyuan.github.io/2016/01/06/2016-01-06-Kafka_Producer/" target="_blank" rel="external">生产者(java)</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/07/2016-01-07-Kafka_Producer-scala/" target="_blank" rel="external">生产者(scala)</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/08/2016-01-08-Kafka_SocketServer/" target="_blank" rel="external">网络层SocketServer</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/10/2016-01-10-Kafka_LogAppend/" target="_blank" rel="external">消息存储到日志文件中</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/14/2016-01-14-Kafka-ISR/" target="_blank" rel="external">Partition的ISR工作机制</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/19/2016-01-19-Kafka-Consumer-scala/" target="_blank" rel="external">消费者初始化(scala)</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/20/2016-01-20-Kafka-Consumer-fetcher/" target="_blank" rel="external">消费者抓取流程</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/05/26/2016-05-13-Kafka-Book-Sample/" target="_blank" rel="external">旧的样章</a>  </li>
</ul>
<h2 id="Kafka_Connect">Kafka Connect</h2><ul>
<li><a href="http://zqhxuyuan.github.io/2016/02/21/Kafka-Connect/" target="_blank" rel="external">使用Kafka Connect构建一个可扩展的ETL管道</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/02/21/Kafka-connect-pipeline/" target="_blank" rel="external">使用Kafka Connect构建大规模低延迟的数据管道</a></li>
</ul>
<h2 id="Kafka_Streams">Kafka Streams</h2><ul>
<li><a href="http://zqhxuyuan.github.io/2016/11/18/Kafka-CQRS-Streams/" target="_blank" rel="external">译：Kafka事件驱动和流处理</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/11/02/Kafka-Streams-cn/" target="_blank" rel="external">Kafka Streams中文翻译</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/10/29/Kafka-Interactive-Query/" target="_blank" rel="external">译：Kafka交互式查询和流处理的统一</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《Kafka技术内幕》&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>深入解析中间件之-Canal</title>
    <link href="http://github.com/zqhxuyuan/2017/10/11/Midd-canal/"/>
    <id>http://github.com/zqhxuyuan/2017/10/11/Midd-canal/</id>
    <published>2017-10-10T16:00:00.000Z</published>
    <updated>2017-10-13T03:38:10.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/alibaba/canal">canal</a>: 阿里巴巴mysql数据库binlog的增量订阅&amp;消费组件<br><a id="more"></a></p>
<h2 id="MySQL_binlog">MySQL binlog</h2><h3 id="MySQL主从复制">MySQL主从复制</h3><p>mysql服务端修改配置并重启</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/my.cnf</span><br><span class="line">[mysqld]</span><br><span class="line">log-bin=mysql-bin</span><br><span class="line">binlog-format=ROW</span><br><span class="line">server_id=1</span><br><span class="line"></span><br><span class="line">$ mysql -uroot</span><br><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">USER</span> canal <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'canal'</span>;</span>  </span><br><span class="line"><span class="operator"><span class="keyword">GRANT</span> ALL <span class="keyword">PRIVILEGES</span> <span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'canal'</span>@<span class="string">'%'</span> ;</span></span><br><span class="line"><span class="operator"><span class="keyword">FLUSH</span> <span class="keyword">PRIVILEGES</span>;</span></span><br><span class="line"></span><br><span class="line">$ sudo service mysqld <span class="operator"><span class="keyword">start</span></span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>问题：创建canal用户的目的是什么？直接使用现有的用户名可以吗，比如root。<br>答案：有些用户没有REPLICATION SLAVE, REPLICATION CLIENT的权限，用这些用户连接canal时，无法获取到binlog。<br>这里的canal用户授权了全部权限，所以客户端可以从canal中获取binlog。</p>
</blockquote>
<p>明确两个概念：canal server连接mysql，客户端连接canal server。</p>
<ul>
<li>canal指的是canal server，它会读取mysql的binlog，解析后存储起来</li>
<li>客户端指的是消费canal server的binlog</li>
</ul>
<p>本机连接服务端，验证binlog的格式是ROW</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -h192.168.6.52 -ucanal -pcanal</span><br><span class="line"><span class="header">mysql&gt; show variables like '%binlog_format%';</span><br><span class="line">+---------------+-------+</span></span><br><span class="line"><span class="header">| Variable_name | Value |</span><br><span class="line">+---------------+-------+</span></span><br><span class="line"><span class="header">| binlog_format | ROW   |</span><br><span class="line">+---------------+-------+</span></span><br></pre></td></tr></table></figure>
<p>mysql主从复制的原理：</p>
<ul>
<li>master将改变记录到二进制日志(binary log)中；</li>
<li>slave将master的binary log events拷贝到它的中继日志(relay log)；</li>
<li>slave重做中继日志中的事件，将改变反映它自己的数据。</li>
</ul>
<p><img src="http://img.blog.csdn.net/20160914112042547" alt="mysql replication"></p>
<h3 id="binlog">binlog</h3><p>在启动canal之前，先来了解下什么是mysql的binlog:</p>
<figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show binlog events;</span><br><span class="line">| Log_name         | Pos   | Event_type  | Server_id | End_log_pos | Info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |</span><br><span class="line">+<span class="comment">------------------+-------+-------------+-----------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span></span><br><span class="line">| mysql-bin.<span class="number">000001</span> |     <span class="number">4</span> | Format_desc |         <span class="number">1</span> |         <span class="number">106</span> | Server ver: <span class="number">5.1</span>.<span class="number">73</span>-log, Binlog ver: <span class="number">4</span>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |</span><br><span class="line">| mysql-bin.<span class="number">000001</span> |   <span class="number">106</span> | Query       |         <span class="number">1</span> |        <span class="number">1864</span> | <span class="keyword">use</span> `mysql`; CREATE TABLE <span class="keyword">IF</span> <span class="keyword">NOT</span> EXISTS db (   Host char(<span class="number">60</span>) binary <span class="keyword">DEFAULT</span> '' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Db char(<span class="number">64</span>) binary <span class="keyword">DEFAULT</span> '' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, User char(<span class="number">16</span>) binary <span class="keyword">DEFAULT</span> '' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Select_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Insert_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Update_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Delete_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Create_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Drop_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Grant_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, References_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Index_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Alter_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Create_tmp_table_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Lock_tables_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Create_view_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Show_view_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Create_routine_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Alter_routine_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Execute_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Event_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Trigger_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, PRIMARY KEY Host (Host,Db,User), KEY User (User) ) engine=MyISAM <span class="typename">CHARACTER</span> SET utf8 COLLATE utf8_bin comment=<span class="attribute">'Database</span> privileges' |</span><br><span class="line">| mysql-bin.<span class="number">000001</span> |  <span class="number">1864</span> | Query       |         <span class="number">1</span> |        <span class="number">3518</span> | <span class="keyword">use</span> `mysql`; CREATE TABLE <span class="keyword">IF</span> <span class="keyword">NOT</span> EXISTS host (  Host char(<span class="number">60</span>) binary <span class="keyword">DEFAULT</span> '' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Db char(<span class="number">64</span>) binary <span class="keyword">DEFAULT</span> '' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Select_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Insert_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Update_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Delete_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Create_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Drop_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Grant_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, References_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Index_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Alter_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Create_tmp_table_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Lock_tables_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Create_view_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Show_view_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Create_routine_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Alter_routine_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Execute_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, Trigger_priv enum(<span class="attribute">'N</span>',<span class="attribute">'Y</span>') COLLATE utf8_general_ci <span class="keyword">DEFAULT</span> <span class="attribute">'N</span>' <span class="keyword">NOT</span> <span class="keyword">NULL</span>, PRIMARY KEY Host (Host,Db) ) engine=MyISAM <span class="typename">CHARACTER</span> SET utf8 COLLATE utf8_bin comment=<span class="attribute">'Host</span> privileges;  Merged <span class="keyword">with</span> database privileges' |</span><br></pre></td></tr></table></figure>
<p>mysql数据文件下会生成mysql-bin.xxx的binlog文件，以及索引文件</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 canal]$ ll /var/lib/mysql/</span><br><span class="line">总用量 <span class="number">26228</span></span><br><span class="line">drwx------ <span class="number">2</span> mysql mysql     <span class="number">4096</span> <span class="number">10</span>月 <span class="number">11</span> <span class="number">14</span>:<span class="number">05</span> canal_test</span><br><span class="line">-rw-rw---- <span class="number">1</span> mysql mysql <span class="number">10485760</span> <span class="number">9</span>月  <span class="number">30</span> <span class="number">22</span>:<span class="number">12</span> ibdata1</span><br><span class="line">-rw-rw---- <span class="number">1</span> mysql mysql  <span class="number">5242880</span> <span class="number">10</span>月 <span class="number">11</span> <span class="number">09</span>:<span class="number">57</span> ib_logfile0</span><br><span class="line">-rw-rw---- <span class="number">1</span> mysql mysql  <span class="number">5242880</span> <span class="number">10</span>月 <span class="number">11</span> <span class="number">09</span>:<span class="number">57</span> ib_logfile1</span><br><span class="line">drwx------ <span class="number">2</span> mysql mysql     <span class="number">4096</span> <span class="number">8</span>月   <span class="number">2</span> <span class="number">11</span>:<span class="number">01</span> mysql</span><br><span class="line">-rw-rw---- <span class="number">1</span> mysql mysql    <span class="number">18451</span> <span class="number">8</span>月   <span class="number">2</span> <span class="number">11</span>:<span class="number">01</span> mysql-bin<span class="number">.000001</span></span><br><span class="line">-rw-rw---- <span class="number">1</span> mysql mysql   <span class="number">929226</span> <span class="number">8</span>月   <span class="number">2</span> <span class="number">11</span>:<span class="number">01</span> mysql-bin<span class="number">.000002</span></span><br><span class="line">-rw-rw---- <span class="number">1</span> mysql mysql  <span class="number">4890698</span> <span class="number">9</span>月  <span class="number">30</span> <span class="number">22</span>:<span class="number">12</span> mysql-bin<span class="number">.000003</span></span><br><span class="line">-rw-rw---- <span class="number">1</span> mysql mysql      <span class="number">897</span> <span class="number">10</span>月 <span class="number">11</span> <span class="number">14</span>:<span class="number">06</span> mysql-bin<span class="number">.000004</span></span><br><span class="line">-rw-rw---- <span class="number">1</span> mysql mysql       <span class="number">76</span> <span class="number">10</span>月 <span class="number">11</span> <span class="number">09</span>:<span class="number">57</span> mysql-bin.index</span><br><span class="line">srwxrwxrwx <span class="number">1</span> mysql mysql        <span class="number">0</span> <span class="number">10</span>月 <span class="number">11</span> <span class="number">09</span>:<span class="number">57</span> mysql.sock</span><br></pre></td></tr></table></figure>
<p>针对mysql的操作都会有二进制的事件记录到binlog文件中。下面的一些操作包括创建用户，授权，创建数据库，创建表，插入一条记录。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 canal]$ sudo strings /var/lib/mysql/mysql-bin.000004</span><br><span class="line">5.1.73-log</span><br><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">USER</span> canal <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'canal'</span></span><br><span class="line">root    localhost</span><br><span class="line"><span class="keyword">GRANT</span> ALL <span class="keyword">PRIVILEGES</span> <span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'canal'</span>@<span class="string">'%'</span></span><br><span class="line"><span class="keyword">FLUSH</span> <span class="keyword">PRIVILEGES</span></span><br><span class="line">canal_test</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> canal_test    ===》创建数据库</span><br><span class="line">canal_test</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span> (   uid <span class="built_in">int</span> (<span class="number">4</span>) primary <span class="keyword">key</span> <span class="keyword">not</span> <span class="literal">null</span> auto_increment,   <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">10</span>) <span class="keyword">not</span> <span class="literal">null</span>)  ==》创建表</span><br><span class="line">canal_test</span><br><span class="line"><span class="keyword">BEGIN</span>     ==》插入记录，这里有事务。但是没有把具体的语句打印出来</span><br><span class="line">canal_test</span><br><span class="line"><span class="keyword">test</span></span><br><span class="line">canal_test</span><br><span class="line"><span class="keyword">COMMIT</span></span></span><br></pre></td></tr></table></figure>
<h2 id="Canal_QuickStart">Canal QuickStart</h2><h3 id="canal_&amp;_config">canal &amp; config</h3><p>部署canal server到6.52，并启动。查看canal的日志：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 canal]$ cat logs/canal/canal.<span class="built_in">log</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">11</span>:<span class="number">31</span>:<span class="number">52.076</span> [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - <span class="preprocessor">## start the canal server.</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">11</span>:<span class="number">31</span>:<span class="number">52.151</span> [main] INFO  com.alibaba.otter.canal.deployer.CanalController - <span class="preprocessor">## start the canal server[<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">11111</span>]</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">11</span>:<span class="number">31</span>:<span class="number">52.644</span> [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - <span class="preprocessor">## the canal server is running now ......</span></span><br></pre></td></tr></table></figure>
<p>查看instance的日志：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 canal]$ cat logs/example/example<span class="class">.log</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">11</span>:<span class="number">31</span>:<span class="number">52.435</span> [main] INFO  c<span class="class">.a</span><span class="class">.o</span><span class="class">.c</span><span class="class">.i</span><span class="class">.spring</span><span class="class">.support</span><span class="class">.PropertyPlaceholderConfigurer</span> - Loading properties file from class path resource [canal.properties]</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">11</span>:<span class="number">31</span>:<span class="number">52.444</span> [main] INFO  c<span class="class">.a</span><span class="class">.o</span><span class="class">.c</span><span class="class">.i</span><span class="class">.spring</span><span class="class">.support</span><span class="class">.PropertyPlaceholderConfigurer</span> - Loading properties file from class path resource [example/instance.properties]</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">11</span>:<span class="number">31</span>:<span class="number">52.587</span> [main] INFO  c<span class="class">.a</span><span class="class">.otter</span><span class="class">.canal</span><span class="class">.instance</span><span class="class">.spring</span><span class="class">.CanalInstanceWithSpring</span> - start CannalInstance <span class="keyword">for</span> <span class="number">1</span>-example</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">11</span>:<span class="number">31</span>:<span class="number">52.599</span> [main] INFO  c<span class="class">.a</span><span class="class">.otter</span><span class="class">.canal</span><span class="class">.instance</span><span class="class">.core</span><span class="class">.AbstractCanalInstance</span> - start successful....</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">11</span>:<span class="number">31</span>:<span class="number">52.679</span> [destination = example , <span class="tag">address</span> = /<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">3306</span> , EventParser] WARN  c<span class="class">.a</span><span class="class">.otter</span><span class="class">.canal</span><span class="class">.parse</span><span class="class">.inbound</span><span class="class">.mysql</span><span class="class">.MysqlEventParser</span> - prepare to find start <span class="attribute">position</span> just show master status</span><br></pre></td></tr></table></figure>
<p>canal server的conf下有几个配置文件</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜  canal<span class="class">.deployer-1</span>.<span class="number">0.24</span> tree conf</span><br><span class="line">conf</span><br><span class="line">├── canal<span class="class">.properties</span></span><br><span class="line">├── example</span><br><span class="line">│   └── instance<span class="class">.properties</span></span><br><span class="line">├── logback<span class="class">.xml</span></span><br><span class="line">└── spring</span><br><span class="line">    ├── default-instance<span class="class">.xml</span></span><br><span class="line">    ├── file-instance<span class="class">.xml</span></span><br><span class="line">    ├── group-instance<span class="class">.xml</span></span><br><span class="line">    ├── local-instance<span class="class">.xml</span></span><br><span class="line">    └── memory-instance.xml</span><br></pre></td></tr></table></figure>
<p>先来看canal.properties的common属性前四个配置项：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">canal.id= <span class="number">1</span></span><br><span class="line">canal.ip=</span><br><span class="line">canal.port= <span class="number">11111</span></span><br><span class="line">canal.zkServers=</span><br></pre></td></tr></table></figure>
<p>canal.id是canal的编号，在集群环境下，不同canal的id不同，注意它和mysql的server_id不同。<br>ip这里不指定，默认为本机，比如上面是192.168.6.52，端口号是11111。zk用于canal cluster。</p>
<p>再看下canal.properties下destinations相关的配置：</p>
<figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">#</span></span><br><span class="line"><span class="comment">######</span><span class="comment">###       destinations        ###</span><span class="comment">######</span><span class="comment">#### </span><br><span class="line">###</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">####</span><br><span class="line">canal.destinations = example</span><br><span class="line">canal.conf.dir = ../conf</span><br><span class="line">canal.auto.scan = true</span><br><span class="line">canal.auto.scan.interval = 5</span><br><span class="line"></span><br><span class="line">canal.instance.global.mode = spring </span><br><span class="line">canal.instance.global.lazy = false</span><br><span class="line">canal.instance.global.spring.xml = classpath:spring/file-instance.xml</span></span><br></pre></td></tr></table></figure>
<p>这里的canal.destinations = example可以设置多个，比如example1,example2，<br>则需要创建对应的两个文件夹，并且每个文件夹下都有一个instance.properties文件。</p>
<p>全局的canal实例管理用spring，这里的file-instance.xml最终会实例化所有的destinations instances:</p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;bean class=<span class="string">"com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer"</span> lazy-init=<span class="string">"false"</span>&gt;</span><br><span class="line">    &lt;property name=<span class="string">"ignoreResourceNotFound"</span> value=<span class="string">"true"</span> /&gt;</span><br><span class="line">    &lt;property name=<span class="string">"systemPropertiesModeName"</span> value=<span class="string">"SYSTEM_PROPERTIES_MODE_OVERRIDE"</span>/&gt;&lt;!-- 允许system覆盖 --&gt;</span><br><span class="line">    &lt;property name=<span class="string">"locationNames"</span>&gt;</span><br><span class="line">        &lt;list&gt;</span><br><span class="line">            &lt;value&gt;classpath:canal.properties&lt;/value&gt;</span><br><span class="line">            &lt;value&gt;classpath:$&#123;canal.instance.destination:&#125;/instance.properties&lt;/value&gt;</span><br><span class="line">        &lt;/list&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/bean&gt;</span><br><span class="line">&lt;bean id=<span class="string">"instance"</span> class=<span class="string">"com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring"</span>&gt;</span><br><span class="line">    &lt;property name=<span class="string">"destination"</span> value=<span class="string">"$&#123;canal.instance.destination&#125;"</span> /&gt;</span><br><span class="line">    &lt;property name=<span class="string">"eventParser"</span>&gt;&lt;<span class="keyword">ref</span> local=<span class="string">"eventParser"</span> /&gt;&lt;/property&gt;</span><br><span class="line">    &lt;property name=<span class="string">"eventSink"</span>&gt;&lt;<span class="keyword">ref</span> local=<span class="string">"eventSink"</span> /&gt;&lt;/property&gt;</span><br><span class="line">    &lt;property name=<span class="string">"eventStore"</span>&gt;&lt;<span class="keyword">ref</span> local=<span class="string">"eventStore"</span> /&gt;&lt;/property&gt;</span><br><span class="line">    &lt;property name=<span class="string">"metaManager"</span>&gt;&lt;<span class="keyword">ref</span> local=<span class="string">"metaManager"</span> /&gt;&lt;/property&gt;</span><br><span class="line">    &lt;property name=<span class="string">"alarmHandler"</span>&gt;&lt;<span class="keyword">ref</span> local=<span class="string">"alarmHandler"</span> /&gt;&lt;/property&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure>
<p>比如canal.instance.destination等于example，就会加载example/instance.properties配置文件</p>
<p>example下instance.properties配置文件不需要修改。一个canal server可以运行多个canal instance。</p>
<figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">#</span></span><br><span class="line"><span class="comment">## mysql serverId，这里的slaveId不能和myql集群中已有的server_id一样</span></span><br><span class="line">canal.instance.mysql.slaveId = <span class="number">1234</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># position info 这里连接的是mysql master的地址。</span></span><br><span class="line">canal.instance.master.address = <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">3306</span></span><br><span class="line">canal.instance.master.journal.name = </span><br><span class="line">canal.instance.master.position = </span><br><span class="line">canal.instance.master.timestamp = </span><br><span class="line"></span><br><span class="line"><span class="comment">#canal.instance.standby.address = </span></span><br><span class="line"><span class="comment">#canal.instance.standby.journal.name =</span></span><br><span class="line"><span class="comment">#canal.instance.standby.position = </span></span><br><span class="line"><span class="comment">#canal.instance.standby.timestamp = </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># username/password</span></span><br><span class="line">canal.instance.dbUsername = canal</span><br><span class="line">canal.instance.dbPassword = canal</span><br><span class="line">canal.instance.defaultDatabaseName =</span><br><span class="line">canal.instance.connectionCharset = UTF-<span class="number">8</span></span><br><span class="line"></span><br><span class="line">canal.instance.filter.regex = .*\\..*</span><br><span class="line">canal.instance.filter.black.regex =  </span><br><span class="line"><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">#</span></span><br></pre></td></tr></table></figure>
<h3 id="simple_client">simple client</h3><p>在mysql上创建数据库，创建表，插入一条记录，再修改记录。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">database</span> canal_test;</span></span><br><span class="line"><span class="operator"><span class="keyword">use</span> canal_test;</span></span><br><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">test</span> (   uid <span class="built_in">int</span> (<span class="number">4</span>) primary <span class="keyword">key</span> <span class="keyword">not</span> <span class="literal">null</span> auto_increment,   <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">10</span>) <span class="keyword">not</span> <span class="literal">null</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span> (<span class="keyword">name</span>) <span class="keyword">values</span>(<span class="string">'10'</span>);</span></span><br></pre></td></tr></table></figure>
<p>修改客户端测试例子的连接信息。其中example对应了canal实例的名称。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">String destination = <span class="string">"example"</span>;</span><br><span class="line">CanalConnector connector = CanalConnectors.newSingleConnector(</span><br><span class="line">    <span class="keyword">new</span> InetSocketAddress(<span class="string">"192.168.6.52"</span>, <span class="number">11111</span>), destination, <span class="string">"canal"</span>, <span class="string">"canal"</span>);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：如果连接有错误，客户端测试例子会立即结束，打印## stop the canal client。正常的话，终端不会退出，会一直运行。</p>
</blockquote>
<p>SimpleCanalClientTest控制台的结果如下：</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"><span class="keyword">*</span> Batch Id: [1] ,count : [2] , memsize : [263] , Time : 2017-10-11 14:06:06</span><br><span class="line"><span class="keyword">*</span> Start : [mysql-bin.000004:396:1507701897000(2017-10-11 14:04:57)] </span><br><span class="line"><span class="keyword">*</span> End : [mysql-bin.000004:491:1507701904000(2017-10-11 14:05:04)] </span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"></span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:396] , name[canal_test,] , eventType : QUERY , executeTime : 1507701897000 , delay : 69710ms</span><br><span class="line"> sql ----&gt; create database canal_test</span><br><span class="line"></span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:491] , name[canal_test,test] , eventType : CREATE , executeTime : 1507701904000 , delay : 62723ms</span><br><span class="line"> sql ----&gt; create table test (   uid int (4) primary key not null auto_increment,   name varchar(10) not null)</span><br></pre></td></tr></table></figure>
<p>插入一条记录：（其中uid和name的update都等于true）</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"><span class="keyword">*</span> Batch Id: [2] ,count : [3] , memsize : [186] , Time : 2017-10-11 14:06:32</span><br><span class="line"><span class="keyword">*</span> Start : [mysql-bin.000004:659:1507701989000(2017-10-11 14:06:29)] </span><br><span class="line"><span class="keyword">*</span> End : [mysql-bin.000004:822:1507701989000(2017-10-11 14:06:29)] </span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"></span><br><span class="line">================&gt; binlog[mysql-bin.000004:659] , executeTime : 1507701989000 , delay : 3142ms</span><br><span class="line"> BEGIN ----&gt; Thread id: 11</span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:785] , name[canal_test,test] , eventType : INSERT , executeTime : 1507701989000 , delay : 3154ms</span><br><span class="line">uid : 1    type=int(4)    update=true</span><br><span class="line">name : 10    type=varchar(10)    update=true</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:822] , executeTime : 1507701989000 , delay : 3179ms</span><br></pre></td></tr></table></figure>
<p>修改记录：（其中name的update等于true）</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"><span class="keyword">*</span> Batch Id: [3] ,count : [3] , memsize : [202] , Time : 2017-10-11 14:49:11</span><br><span class="line"><span class="keyword">*</span> Start : [mysql-bin.000004:897:1507704547000(2017-10-11 14:49:07)] </span><br><span class="line"><span class="keyword">*</span> End : [mysql-bin.000004:1076:1507704547000(2017-10-11 14:49:07)] </span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"></span><br><span class="line">================&gt; binlog[mysql-bin.000004:897] , executeTime : 1507704547000 , delay : 4048ms</span><br><span class="line"> BEGIN ----&gt; Thread id: 13</span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:1023] , name[canal_test,test] , eventType : UPDATE , executeTime : 1507704547000 , delay : 4059ms</span><br><span class="line">uid : 1    type=int(4)</span><br><span class="line">name : zqhxuyuan    type=varchar(10)    update=true</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:1076] , executeTime : 1507704547000 , delay : 4096ms</span><br></pre></td></tr></table></figure>
<p>canal安装包下的example instance下除了example.log外，还有一个meta.log</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 canal]$ cat logs/example/meta.<span class="built_in">log</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">06</span>:<span class="number">03.728</span> - clientId:<span class="number">1001</span> cursor:[mysql-bin<span class="number">.000004</span>,<span class="number">396</span>,<span class="number">1507701897000</span>] address[/<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">3306</span>]</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">06</span>:<span class="number">04.589</span> - clientId:<span class="number">1001</span> cursor:[mysql-bin<span class="number">.000004</span>,<span class="number">491</span>,<span class="number">1507701904000</span>] address[localhost/<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">3306</span>]</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">06</span>:<span class="number">29.589</span> - clientId:<span class="number">1001</span> cursor:[mysql-bin<span class="number">.000004</span>,<span class="number">822</span>,<span class="number">1507701989000</span>] address[localhost/<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">3306</span>]</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">11</span> <span class="number">14</span>:<span class="number">49</span>:<span class="number">08.589</span> - clientId:<span class="number">1001</span> cursor:[mysql-bin<span class="number">.000004</span>,<span class="number">1076</span>,<span class="number">1507704547000</span>] address[localhost/<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">3306</span>]</span><br></pre></td></tr></table></figure>
<h2 id="Cannal_Internal_Overview">Cannal Internal Overview</h2><h3 id="canal_client_&amp;_server">canal client &amp; server</h3><p>canal client与canal server之间是C/S模式的通信，客户端采用NIO，服务端采用Netty。<br>canal server启动后，如果没有canal client，那么canal server不会去mysql拉取binlog。</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    connector.connect();</span><br><span class="line">    connector.subscribe();</span><br><span class="line">    <span class="keyword">while</span> (running) &#123;</span><br><span class="line">        Message message = connector.getWithoutAck(batchSize); <span class="comment">// 获取指定数量的数据</span></span><br><span class="line">        <span class="keyword">long</span> batchId = message.getId();</span><br><span class="line">        <span class="keyword">int</span> <span class="keyword">size</span> = message.getEntries().<span class="keyword">size</span>();</span><br><span class="line">        printSummary(message, batchId, <span class="keyword">size</span>);</span><br><span class="line">        printEntry(message.getEntries());</span><br><span class="line">        connector.ack(batchId); <span class="comment">// 提交确认</span></span><br><span class="line">        connector.rollback(batchId); <span class="comment">// 处理失败, 回滚数据</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    connector.disconnect();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>canal client与canal server之间属于增量订阅/消费，流程图如下：（其中C端是canal client，S端是canal server）</p>
<p><img src="https://camo.githubusercontent.com/db1debcfa50f4ebea1f56a1fa0e18a4e960cafcc/687474703a2f2f646c2e69746579652e636f6d2f75706c6f61642f6174746163686d656e742f303038302f333239372f39643765643133652d366138362d333836642d393266342d3835323233386334373562662e6a7067" alt="canal protocol"></p>
<p>canal client调用connect()方法时，类型为PacketType.HANDSHAKE，接着写入CLIENTAUTHENTICATION。然后调用subscribe()方法，类型为SUBSCRIPTION。</p>
<p>对应服务端采用netty处理RPC请求（CanalServerWithNetty）:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">bootstrap.setPipelineFactory(<span class="keyword">new</span> ChannelPipelineFactory() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ChannelPipeline <span class="title">getPipeline</span><span class="params">()</span> throws Exception </span>&#123;</span><br><span class="line">        ChannelPipeline pipelines = Channels.pipeline();</span><br><span class="line">        pipelines.addLast(FixedHeaderFrameDecoder.<span class="keyword">class</span>.getName(), <span class="keyword">new</span> FixedHeaderFrameDecoder());</span><br><span class="line">        <span class="comment">// 处理客户端的HANDSHAKE请求</span></span><br><span class="line">        pipelines.addLast(HandshakeInitializationHandler.<span class="keyword">class</span>.getName(),</span><br><span class="line">            <span class="keyword">new</span> HandshakeInitializationHandler(childGroups));</span><br><span class="line">        <span class="comment">// 处理客户端的CLIENTAUTHENTICATION请求</span></span><br><span class="line">        pipelines.addLast(ClientAuthenticationHandler.<span class="keyword">class</span>.getName(),</span><br><span class="line">            <span class="keyword">new</span> ClientAuthenticationHandler(embeddedServer));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 处理客户端的会话请求，包括SUBSCRIPTION，GET等</span></span><br><span class="line">        SessionHandler sessionHandler = <span class="keyword">new</span> SessionHandler(embeddedServer);</span><br><span class="line">        pipelines.addLast(SessionHandler.<span class="keyword">class</span>.getName(), sessionHandler);</span><br><span class="line">        <span class="keyword">return</span> pipelines;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>ClientAuthenticationHandler处理鉴权后，会移除HandshakeInitializationHandler和ClientAuthenticationHandler</p>
<p>以client发送GET，server从mysql得到binlog后，返回MESSAGES给client为例，说明client和server的rpc交互过程：</p>
<p>SimpleCanalConnector发送GET请求，并读取响应结果的流程：</p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">public <span class="type">Message</span> getWithoutAck(<span class="type">int</span> batchSize, <span class="type">Long</span> timeout, <span class="type">TimeUnit</span> unit) throws <span class="type">CanalClientException</span> &#123;</span><br><span class="line">    waitClientRunning();</span><br><span class="line">    <span class="type">int</span> size = (batchSize &lt;= <span class="number">0</span>) ? <span class="number">1000</span> : batchSize;</span><br><span class="line">    long time = (timeout == null || timeout &lt; <span class="number">0</span>) ? -<span class="number">1</span> : timeout; // -<span class="number">1</span>代表不做timeout控制</span><br><span class="line">    <span class="keyword">if</span> (unit == null) unit = <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>;</span><br><span class="line"></span><br><span class="line">    // client发送<span class="type">GET</span>请求</span><br><span class="line">    writeWithHeader(<span class="type">Packet</span>.newBuilder()</span><br><span class="line">        .setType(<span class="type">PacketType</span>.<span class="type">GET</span>)</span><br><span class="line">        .setBody(<span class="type">Get</span>.newBuilder()</span><br><span class="line">            .setAutoAck(<span class="literal">false</span>)</span><br><span class="line">            .setDestination(clientIdentity.getDestination())</span><br><span class="line">            .setClientId(<span class="type">String</span>.valueOf(clientIdentity.getClientId()))</span><br><span class="line">            .setFetchSize(size)</span><br><span class="line">            .setTimeout(time)</span><br><span class="line">            .setUnit(unit.ordinal())</span><br><span class="line">            .build()</span><br><span class="line">            .toByteString())</span><br><span class="line">        .build()</span><br><span class="line">        .toByteArray());</span><br><span class="line">    // client获取<span class="type">GET</span>结果    </span><br><span class="line">    <span class="keyword">return</span> receiveMessages();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private <span class="type">Message</span> receiveMessages() throws <span class="type">IOException</span> &#123;</span><br><span class="line">    // 读取server发送的数据包</span><br><span class="line">    <span class="type">Packet</span> p = <span class="type">Packet</span>.parseFrom(readNextPacket());</span><br><span class="line">    switch (p.getType()) &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">MESSAGES</span>: &#123;</span><br><span class="line">            <span class="type">Messages</span> messages = <span class="type">Messages</span>.parseFrom(p.getBody());</span><br><span class="line">            <span class="type">Message</span> <span class="literal">result</span> = new <span class="type">Message</span>(messages.getBatchId());</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">ByteString</span> byteString : messages.getMessagesList()) &#123;</span><br><span class="line">                <span class="literal">result</span>.addEntry(<span class="type">Entry</span>.parseFrom(byteString));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">result</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>服务端SessionHandler处理客户端发送的GET请求流程：</p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> GET:</span><br><span class="line">    <span class="comment">// 读取客户端发送的数据包，封装为Get对象</span></span><br><span class="line">    Get <span class="keyword">get</span> = CanalPacket.Get.parseFrom(packet.getBody());</span><br><span class="line">    <span class="comment">// destination表示canal instance</span></span><br><span class="line">    <span class="keyword">if</span> (StringUtils.isNotEmpty(<span class="keyword">get</span>.getDestination()) &amp;&amp; StringUtils.isNotEmpty(<span class="keyword">get</span>.getClientId())) &#123;</span><br><span class="line">        clientIdentity = <span class="keyword">new</span> ClientIdentity(<span class="keyword">get</span>.getDestination(), Short.valueOf(<span class="keyword">get</span>.getClientId()));</span><br><span class="line">        Message message = <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">get</span>.getTimeout() == -<span class="number">1</span>) &#123;<span class="comment">// 是否是初始值</span></span><br><span class="line">            message = embeddedServer.getWithoutAck(clientIdentity, <span class="keyword">get</span>.getFetchSize());</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            TimeUnit unit = convertTimeUnit(<span class="keyword">get</span>.getUnit());</span><br><span class="line">            message = embeddedServer.getWithoutAck(clientIdentity, <span class="keyword">get</span>.getFetchSize(), <span class="keyword">get</span>.getTimeout(), unit);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 设置返回给客户端的数据包类型为MESSAGES   </span></span><br><span class="line">        Packet.Builder packetBuilder = CanalPacket.Packet.newBuilder();</span><br><span class="line">        packetBuilder.setType(PacketType.MESSAGES);</span><br><span class="line">        <span class="comment">// 构造Message</span></span><br><span class="line">        Messages.Builder messageBuilder = CanalPacket.Messages.newBuilder();</span><br><span class="line">        messageBuilder.setBatchId(message.getId());</span><br><span class="line">        <span class="keyword">if</span> (message.getId() != -<span class="number">1</span> &amp;&amp; !CollectionUtils.isEmpty(message.getEntries())) &#123;</span><br><span class="line">            <span class="keyword">for</span> (Entry entry : message.getEntries()) &#123;</span><br><span class="line">                messageBuilder.addMessages(entry.toByteString());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        packetBuilder.setBody(messageBuilder.build().toByteString());</span><br><span class="line">        <span class="comment">// 输出数据，返回给客户端</span></span><br><span class="line">        NettyUtils.write(ctx.getChannel(), packetBuilder.build().toByteArray(), <span class="literal">null</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>get/ack/rollback协议介绍：</p>
<ul>
<li>Message getWithoutAck(int batchSize)，允许指定batchSize，一次可以获取多条，每次返回的对象为Message，包含的内容为：<br>– batch id 唯一标识<br>– entries 具体的数据对象，对应的数据对象格式：<a href="https://github.com/alibaba/canal/blob/master/protocol/src/main/java/com/alibaba/otter/canal/protocol/EntryProtocol.proto">EntryProtocol.proto</a></li>
<li>void rollback(long batchId)，顾命思议，回滚上次的get请求，重新获取数据。基于get获取的batchId进行提交，避免误操作</li>
<li>void ack(long batchId)，顾命思议，确认已经消费成功，通知server删除数据。基于get获取的batchId进行提交，避免误操作</li>
</ul>
<p>EntryProtocol.protod对应的canal消息结构如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Entry  </span><br><span class="line">    Header  </span><br><span class="line">        logfileName [binlog文件名]  </span><br><span class="line">        logfileOffset [binlog position]  </span><br><span class="line">        executeTime [binlog里记录变更发生的时间戳,精确到秒]  </span><br><span class="line">        schemaName   </span><br><span class="line">        tableName  </span><br><span class="line">        eventType [<span class="operator"><span class="keyword">insert</span>/<span class="keyword">update</span>/<span class="keyword">delete</span>类型]  </span><br><span class="line">    entryType   [事务头<span class="keyword">BEGIN</span>/事务尾<span class="keyword">END</span>/数据ROWDATA]  </span><br><span class="line">    storeValue  [<span class="keyword">byte</span>数据,可展开，对应的类型为RowChange]  </span><br><span class="line">      </span><br><span class="line">RowChange  </span><br><span class="line">    isDdl       [是否是<span class="keyword">ddl</span>变更操作，比如<span class="keyword">create</span> <span class="keyword">table</span>/<span class="keyword">drop</span> <span class="keyword">table</span>]  </span><br><span class="line">    <span class="keyword">sql</span>         [具体的<span class="keyword">ddl</span> <span class="keyword">sql</span>]  </span><br><span class="line">    rowDatas    [具体<span class="keyword">insert</span>/<span class="keyword">update</span>/<span class="keyword">delete</span>的变更数据，可为多条，<span class="number">1</span>个<span class="keyword">binlog</span> <span class="keyword">event</span>事件可对应多条变更，比如批处理]  </span><br><span class="line">        beforeColumns [<span class="keyword">Column</span>类型的数组，变更前的数据字段]  </span><br><span class="line">        afterColumns [<span class="keyword">Column</span>类型的数组，变更后的数据字段]  </span><br><span class="line">          </span><br><span class="line"><span class="keyword">Column</span>   </span><br><span class="line">    <span class="keyword">index</span>         </span><br><span class="line">    sqlType     [jdbc <span class="keyword">type</span>]  </span><br><span class="line">    <span class="keyword">name</span>        [<span class="keyword">column</span> <span class="keyword">name</span>]  </span><br><span class="line">    isKey       [是否为主键]  </span><br><span class="line">    <span class="keyword">updated</span>     [是否发生过变更]  </span><br><span class="line">    <span class="keyword">isNull</span>      [值是否为<span class="literal">null</span>]  </span><br><span class="line">    <span class="keyword">value</span>       [具体的内容，注意为<span class="keyword">string</span>文本]</span></span><br></pre></td></tr></table></figure>
<p>SessionHandler中服务端处理客户端的其他类型请求，都会调用CanalServerWithEmbedded的相关方法：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> SUBSCRIPTION:</span><br><span class="line">        Sub sub = Sub.parseFrom(packet.getBody());</span><br><span class="line">        embeddedServer.subscribe(clientIdentity);</span><br><span class="line"><span class="keyword">case</span> GET:</span><br><span class="line">        Get get = CanalPacket.Get.parseFrom(packet.getBody());</span><br><span class="line">        message = embeddedServer.getWithoutAck(clientIdentity, get.getFetchSize());</span><br><span class="line"><span class="keyword">case</span> CLIENTACK:</span><br><span class="line">        ClientAck ack = CanalPacket.ClientAck.parseFrom(packet.getBody());</span><br><span class="line">        embeddedServer.ack(clientIdentity, ack.getBatchId());</span><br><span class="line"><span class="keyword">case</span> CLIENTROLLBACK:</span><br><span class="line">        ClientRollback rollback = CanalPacket.ClientRollback.parseFrom(packet.getBody());</span><br><span class="line">        embeddedServer.rollback(clientIdentity);<span class="comment">// 回滚所有批次</span></span><br></pre></td></tr></table></figure>
<h3 id="CanalServerWithEmbedded">CanalServerWithEmbedded</h3><p>CanalServer包含多个Instance，它的成员变量canalInstances记录了instance名称与实例的映射关系。<br>因为是一个Map，所以同一个Server不允许出现相同instance名称，比如不能同时有两个example在一个server上。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">CanalServerWithEmbedded</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">AbstractCanalLifeCycle</span> <span class="title">implements</span> <span class="title">CanalServer</span>, <span class="title">CanalService</span> &#123;</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">Map</span>&lt;<span class="type">String</span>, <span class="type">CanalInstance</span>&gt; canalInstances;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">CanalInstanceGenerator</span>     canalInstanceGenerator;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下图表示一个server有两个instance，每个Client连接一个Instance。<br>每个Canal实例模拟为一个MySQL的slave，所以每个Instance的slaveId必须不一样。比如图中两个Instance的id分别是1234和1235。</p>
<p><img src="http://img.blog.csdn.net/20171011202259253" alt="instances"></p>
<p>注意这里每个Canal Client都对应一个Instance，每个Client在启动时，都会指定一个Destination，这个Destination就表示Instance的名称。<br>所以CanalServerWithEmbedded处理各种请求时的参数都有ClientIdentity，从ClientIdentity中获取destination，就可以获取出对应的CanalInstance</p>
<p>下面以CanalServerWithEmbedded的订阅方法为例：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(ClientIdentity clientIdentity)</span> <span class="keyword">throws</span> CanalServerException </span>&#123;</span><br><span class="line">    <span class="comment">// ClientIdentity表示Canal Client客户端，从中可以获取出客户端指定连接的Destination</span></span><br><span class="line">    <span class="comment">// 由于CanalServerWithEmbedded记录了每个Destination对应的Instance，可以获取客户端对应的Instance</span></span><br><span class="line">    CanalInstance canalInstance = canalInstances.get(clientIdentity.getDestination());</span><br><span class="line">    <span class="keyword">if</span> (!canalInstance.getMetaManager().isStart()) &#123;</span><br><span class="line">        canalInstance.getMetaManager().start(); <span class="comment">// 启动Instance的元数据管理器</span></span><br><span class="line">    &#125;</span><br><span class="line">    canalInstance.getMetaManager().subscribe(clientIdentity); <span class="comment">// 执行一下meta订阅</span></span><br><span class="line">    Position position = canalInstance.getMetaManager().getCursor(clientIdentity);</span><br><span class="line">    <span class="keyword">if</span> (position == <span class="keyword">null</span>) &#123;</span><br><span class="line">        position = canalInstance.getEventStore().getFirstPosition();<span class="comment">// 获取一下store中的第一条</span></span><br><span class="line">        <span class="keyword">if</span> (position != <span class="keyword">null</span>) &#123;</span><br><span class="line">            canalInstance.getMetaManager().updateCursor(clientIdentity, position); <span class="comment">// 更新一下cursor</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 通知下订阅关系变化</span></span><br><span class="line">    canalInstance.subscribeChange(clientIdentity);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个CanalInstance中包括了四个组件：EventParser、EventSink、EventStore、MetaManager。</p>
<p>服务端主要的处理方法包括get/ack/rollback，这三个方法都会用到Instance上面的几个内部组件，主要还是EventStore和MetaManager：</p>
<p>在这之前，要先理解EventStore的含义，EventStore是一个RingBuffer，有三个指针：Put、Get、Ack。</p>
<ul>
<li>Put: Canal Server从MySQL拉取到数据后，放到内存中，Put增加</li>
<li>Get: 消费者（Canal Client）从内存中消费数据，Get增加</li>
<li>Ack: 消费者消费完成，Ack增加。并且会删除Put中已经被Ack的数据</li>
</ul>
<p>这三个操作与Instance组件的关系如下：</p>
<p><img src="http://img.blog.csdn.net/20171011211529169" alt="ops"></p>
<p>客户端通过canal server获取mysql binlog有几种方式（get方法和getWithoutAck）：</p>
<ul>
<li>如果timeout为null，则采用tryGet方式，即时获取  </li>
<li>如果timeout不为null  <ol>
<li>timeout为0，则采用get阻塞方式，获取数据，不设置超时，直到有足够的batchSize数据才返回  </li>
<li>timeout不为0，则采用get+timeout方式，获取数据，超时还没有batchSize足够的数据，有多少返回多少  </li>
</ol>
</li>
</ul>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Events&lt;Event&gt; getEvents(CanalEventStore eventStore, Position <span class="built_in">start</span>, int batchSize, Long timeout,</span><br><span class="line">                                TimeUnit unit) &#123;</span><br><span class="line">    <span class="keyword">if</span> (timeout == <span class="constant">null</span>) &#123;</span><br><span class="line">        <span class="constant">return</span> eventStore.tryGet(<span class="built_in">start</span>, batchSize);<span class="comment"> // 即时获取</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (timeout &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="constant">return</span> eventStore.<span class="built_in">get</span>(<span class="built_in">start</span>, batchSize);<span class="comment"> // 阻塞获取</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="constant">return</span> eventStore.<span class="built_in">get</span>(<span class="built_in">start</span>, batchSize, timeout, unit);<span class="comment"> // 异步获取</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：EventStore的实现采用了类似Disruptor的RingBuffer环形缓冲区。RingBuffer的实现类是MemoryEventStoreWithBuffer</p>
</blockquote>
<p>get方法和getWithoutAck方法的区别是：</p>
<ul>
<li>get方法会立即调用ack</li>
<li>getWithoutAck方法不会调用ack</li>
</ul>
<h3 id="EventStore">EventStore</h3><p>以10条数据为例，初始时current=-1，第一个元素起始next=0，end=9，循环<code>[0,9]</code>所有元素。<br>List元素为(A,B,C,D,E,F,G,H,I,J)</p>
<table>
<thead>
<tr>
<th>next</th>
<th>entries[next]</th>
<th>next-current-1</th>
<th>list element</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>entries[0]</td>
<td>0-(-1)-1=0</td>
<td>A</td>
</tr>
<tr>
<td>1</td>
<td>entries[1]</td>
<td>1-(-1)-1=1</td>
<td>B</td>
</tr>
<tr>
<td>2</td>
<td>entries[2]</td>
<td>2-(-1)-1=2</td>
<td>C</td>
</tr>
<tr>
<td>3</td>
<td>entries[3]</td>
<td>3-(-1)-1=3</td>
<td>D</td>
</tr>
<tr>
<td>.</td>
<td>……….</td>
<td>……….</td>
<td>.</td>
</tr>
<tr>
<td>9</td>
<td>entries[9]</td>
<td>9-(-1)-1=9</td>
<td>J</td>
</tr>
</tbody>
</table>
<p>第一批10个元素put完成后，putSequence设置为end=9。假设第二批又Put了5个元素:(K,L,M,N,O)</p>
<p>current=9，起始next=9+1=10，end=9+5=14，在Put完成后，putSequence设置为end=14。</p>
<table>
<thead>
<tr>
<th>next</th>
<th>entries[next]</th>
<th>next-current-1</th>
<th>list element</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>entries[10]</td>
<td>10-(9)-1=0</td>
<td>K</td>
</tr>
<tr>
<td>11</td>
<td>entries[11]</td>
<td>11-(9)-1=1</td>
<td>L</td>
</tr>
<tr>
<td>12</td>
<td>entries[12]</td>
<td>12-(9)-1=2</td>
<td>M</td>
</tr>
<tr>
<td>13</td>
<td>entries[13]</td>
<td>13-(9)-1=3</td>
<td>N</td>
</tr>
<tr>
<td>14</td>
<td>entries[14]</td>
<td>14-(9)-1=3</td>
<td>O</td>
</tr>
</tbody>
</table>
<p>这里假设环形缓冲区的最大大小为15个（源码中是16MB），那么上面两批一共产生了15个元素，刚好填满了环形缓冲区。<br>如果又有Put事件进来，由于环形缓冲区已经满了，没有可用的slot，则Put操作会被阻塞，直到被消费掉。</p>
<p>下面是Put填充环形缓冲区的代码，检查可用slot（checkFreeSlotAt方法）在几个put方法中。</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> MemoryEventStoreWithBuffer <span class="keyword">extends</span> AbstractCanalStoreScavenge <span class="keyword">implements</span> CanalEventStore&lt;Event&gt;, CanalStoreScavenge &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> INIT_SQEUENCE = -<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span>               bufferSize    = <span class="number">16</span> * <span class="number">1024</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span>               bufferMemUnit = <span class="number">1024</span>;                         <span class="comment">// memsize的单位，默认为1kb大小</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span>               indexMask;</span><br><span class="line">    <span class="keyword">private</span> Event[]           entries;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 记录下put/get/ack操作的三个下标</span></span><br><span class="line">    <span class="keyword">private</span> AtomicLong        putSequence   = <span class="keyword">new</span> AtomicLong(INIT_SQEUENCE); <span class="comment">// 代表当前put操作最后一次写操作发生的位置</span></span><br><span class="line">    <span class="keyword">private</span> AtomicLong        getSequence   = <span class="keyword">new</span> AtomicLong(INIT_SQEUENCE); <span class="comment">// 代表当前get操作读取的最后一条的位置</span></span><br><span class="line">    <span class="keyword">private</span> AtomicLong        ackSequence   = <span class="keyword">new</span> AtomicLong(INIT_SQEUENCE); <span class="comment">// 代表当前ack操作的最后一条的位置</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动EventStore时，创建指定大小的缓冲区，Event数组的大小是16*1024</span></span><br><span class="line">    <span class="comment">// 也就是说算个数的话，数组可以容纳16000个事件。算内存的话，大小为16MB</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> start() <span class="keyword">throws</span> CanalStoreException &#123;</span><br><span class="line">        <span class="keyword">super</span>.start();</span><br><span class="line">        indexMask = bufferSize - <span class="number">1</span>;</span><br><span class="line">        entries = <span class="keyword">new</span> Event[bufferSize];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// EventParser解析后，会放入内存中（Event数组，缓冲区）</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">void</span> doPut(List&lt;Event&gt; data) &#123;</span><br><span class="line">        <span class="keyword">long</span> current = putSequence.get(); <span class="comment">// 取得当前的位置，初始时为-1，第一个元素为-1+1=0</span></span><br><span class="line">        <span class="keyword">long</span> end = current + data.<span class="keyword">size</span>(); <span class="comment">// 最末尾的位置，假设Put了10条数据，end=-1+10=9</span></span><br><span class="line">        <span class="comment">// 先写数据，再更新对应的cursor,并发度高的情况，putSequence会被get请求可见，拿出了ringbuffer中的老的Entry值</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">long</span> <span class="keyword">next</span> = current + <span class="number">1</span>; <span class="keyword">next</span> &lt;= end; <span class="keyword">next</span>++) &#123;</span><br><span class="line">            entries[getIndex(<span class="keyword">next</span>)] = data.get((<span class="keyword">int</span>) (<span class="keyword">next</span> - current - <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        putSequence.set(end);</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Put是生产数据，Get是消费数据，Get一定不会超过Put。比如Put了10条数据，Get最多只能获取到10条数据。但有时候为了保证Get处理的速度，Put和Get并不会相等。<br>可以把Put看做是生产者，Get看做是消费者。生产者速度可以很快，消费者则可以慢慢地消费。比如Put了1000条，而Get我们只需要每次处理10条数据。</p>
<p>仍然以前面的示例来说明Get的流程，初始时current=-1，假设Put了两批数据一共15条，maxAbleSequence=14，而Get的BatchSize假设为10。<br>初始时next=current=-1，end=-1。通过startPosition，会设置next=0。最后end又被赋值为9，即循环缓冲区[0,9]一共10个元素。</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Events&lt;Event&gt; doGet(Position start, <span class="keyword">int</span> batchSize) <span class="keyword">throws</span> CanalStoreException &#123;</span><br><span class="line">    LogPosition startPosition = (LogPosition) start;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">long</span> current = getSequence.get();</span><br><span class="line">    <span class="keyword">long</span> maxAbleSequence = putSequence.get();</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">next</span> = current;</span><br><span class="line">    <span class="keyword">long</span> end = current;</span><br><span class="line">    <span class="comment">// 如果startPosition为null，说明是第一次，默认+1处理</span></span><br><span class="line">    <span class="keyword">if</span> (startPosition == <span class="keyword">null</span> || !startPosition.getPostion().isIncluded()) &#123; <span class="comment">// 第一次订阅之后，需要包含一下start位置，防止丢失第一条记录</span></span><br><span class="line">        <span class="keyword">next</span> = <span class="keyword">next</span> + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    end = (<span class="keyword">next</span> + batchSize - <span class="number">1</span>) &lt; maxAbleSequence ? (<span class="keyword">next</span> + batchSize - <span class="number">1</span>) : maxAbleSequence;</span><br><span class="line">    <span class="comment">// 提取数据并返回</span></span><br><span class="line">    <span class="keyword">for</span> (; <span class="keyword">next</span> &lt;= end; <span class="keyword">next</span>++) &#123;</span><br><span class="line">        Event event = entries[getIndex(<span class="keyword">next</span>)];</span><br><span class="line">        <span class="keyword">if</span> (ddlIsolation &amp;&amp; isDdl(event.getEntry().getHeader().getEventType())) &#123;</span><br><span class="line">            <span class="comment">// 如果是ddl隔离，直接返回</span></span><br><span class="line">            <span class="keyword">if</span> (entrys.<span class="keyword">size</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">                entrys.add(event);<span class="comment">// 如果没有DML事件，加入当前的DDL事件</span></span><br><span class="line">                end = <span class="keyword">next</span>; <span class="comment">// 更新end为当前</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 如果之前已经有DML事件，直接返回了，因为不包含当前next这记录，需要回退一个位置</span></span><br><span class="line">                end = <span class="keyword">next</span> - <span class="number">1</span>; <span class="comment">// next-1一定大于current，不需要判断</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            entrys.add(event);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 处理PositionRange，然后设置getSequence为end</span></span><br><span class="line">    getSequence.compareAndSet(current, end)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ack操作的上限是Get，假设Put了15条数据，Get了10条数据，最多也只能Ack10条数据。Ack的目的是清空缓冲区中已经被Get过的数据</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> ack(Position position) <span class="keyword">throws</span> CanalStoreException &#123;</span><br><span class="line">    cleanUntil(position);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> cleanUntil(Position position) <span class="keyword">throws</span> CanalStoreException &#123;</span><br><span class="line">    <span class="keyword">long</span> sequence = ackSequence.get();</span><br><span class="line">    <span class="keyword">long</span> maxSequence = getSequence.get();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> hasMatch = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">long</span> memsize = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">long</span> <span class="keyword">next</span> = sequence + <span class="number">1</span>; <span class="keyword">next</span> &lt;= maxSequence; <span class="keyword">next</span>++) &#123;</span><br><span class="line">        Event event = entries[getIndex(<span class="keyword">next</span>)];</span><br><span class="line">        memsize += calculateSize(event);</span><br><span class="line">        <span class="keyword">boolean</span> match = CanalEventUtils.checkPosition(event, (LogPosition) position);</span><br><span class="line">        <span class="keyword">if</span> (match) &#123;<span class="comment">// 找到对应的position，更新ack seq</span></span><br><span class="line">            hasMatch = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (batchMode.isMemSize()) &#123;</span><br><span class="line">                ackMemSize.addAndGet(memsize);</span><br><span class="line">                <span class="comment">// 尝试清空buffer中的内存，将ack之前的内存全部释放掉</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">long</span> index = sequence + <span class="number">1</span>; index &lt; <span class="keyword">next</span>; index++) &#123;</span><br><span class="line">                    entries[getIndex(index)] = <span class="keyword">null</span>;<span class="comment">// 设置为null</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            ackSequence.compareAndSet(sequence, <span class="keyword">next</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>rollback回滚方法的实现则比较简单，将getSequence回退到ack位置。</p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">rollback</span>(<span class="params"></span>) throws CanalStoreException </span>&#123;</span><br><span class="line">    getSequence.<span class="keyword">set</span>(ackSequence.<span class="keyword">get</span>());</span><br><span class="line">    getMemSize.<span class="keyword">set</span>(ackMemSize.<span class="keyword">get</span>());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下图展示了RingBuffer的几个操作示例：</p>
<p><img src="http://img.blog.csdn.net/20171011225116791" alt="ringbuffer"></p>
<h3 id="EventParser_WorkFlow">EventParser WorkFlow</h3><p>EventStore负责存储解析后的Binlog事件，而解析动作负责拉取Binlog，它的流程比较复杂。需要和MetaManager进行交互。<br>比如要记录每次拉取的Position，这样下一次就可以从上一次的最后一个位置继续拉取。所以MetaManager应该是有状态的。</p>
<p>EventParser的流程如下：</p>
<ol>
<li>Connection获取上一次解析成功的位置 (如果第一次启动，则获取初始指定的位置或者是当前数据库的binlog位点)</li>
<li>Connection建立链接，发送BINLOG_DUMP指令</li>
<li>Mysql开始推送Binaly Log</li>
<li>接收到的Binaly Log的通过Binlog parser进行协议解析，补充一些特定信息</li>
<li>传递给EventSink模块进行数据存储，是一个阻塞操作，直到存储成功</li>
<li>存储成功后，定时记录Binaly Log位置</li>
</ol>
<p><img src="https://camo.githubusercontent.com/031db3aa27461d13faa2dea479ef639f93386a00/687474703a2f2f646c2e69746579652e636f6d2f75706c6f61642f6174746163686d656e742f303038302f333134332f37393531633136392d663764662d336362332d616562622d6439323466353733313163622e6a7067" alt="parser"></p>
<p>上面提到的Connection指的是实现了<code>ErosaConnection</code>接口的<code>MysqlConnection</code>。<br><code>EventParser</code>的实现类是实现了<code>AbstractEventParser</code>的<code>MysqlEventParser</code>。</p>
<p><code>EventParser</code>解析binlog后通过<code>EventSink</code>写入到<code>EventStore</code>，这条链路可以通过EventStore的put方法串联起来：</p>
<p><img src="http://img.blog.csdn.net/20171011234800632" alt="put"></p>
<p>其实这里还有一个EventTransactionBuffer缓冲区，即Parser解析后先放到缓冲区中，<br>当事务发生时或者数据超过阈值，就会执行刷新操作：即消费缓冲区的数据，放到EventStore中。<br>这个缓冲区有两个偏移量指针：putSequence和flushSequence。</p>
<h2 id="Canal_HA">Canal HA</h2><p>单机模拟两个Canal Server，将单机模式复制出两个文件夹，并修改相关配置</p>
<p>canal_m/conf/canal.properties</p>
<figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">canal.id= <span class="number">2</span></span><br><span class="line">canal.ip=</span><br><span class="line">canal.port= <span class="number">11112</span></span><br><span class="line">canal.zkServers=localhost:<span class="number">2181</span></span><br><span class="line">canal.<span class="keyword">instance</span>.<span class="keyword">global</span>.<span class="keyword">spring</span>.xml = classpath:<span class="keyword">spring</span>/<span class="keyword">default</span>-<span class="keyword">instance</span>.xml</span><br></pre></td></tr></table></figure>
<p>canal_m/conf/example/instance.properties</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">canal<span class="class">.instance</span><span class="class">.mysql</span><span class="class">.slaveId</span> = <span class="number">1235</span></span><br></pre></td></tr></table></figure>
<p>canal_s</p>
<figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">canal.id= <span class="number">3</span></span><br><span class="line">canal.ip=</span><br><span class="line">canal.port= <span class="number">11113</span></span><br><span class="line">canal.zkServers=localhost:<span class="number">2181</span></span><br><span class="line">canal.<span class="keyword">instance</span>.<span class="keyword">global</span>.<span class="keyword">spring</span>.xml = classpath:<span class="keyword">spring</span>/<span class="keyword">default</span>-<span class="keyword">instance</span>.xml</span><br></pre></td></tr></table></figure>
<p>canal_s/conf/example/instance.properties</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">canal<span class="class">.instance</span><span class="class">.mysql</span><span class="class">.slaveId</span> = <span class="number">1236</span></span><br></pre></td></tr></table></figure>
<p>启动canal_m</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2017<span class="tag">-10-12</span> 14<span class="pseudo">:51</span><span class="pseudo">:45</span><span class="class">.202</span> <span class="attr_selector">[main]</span> <span class="tag">INFO</span>  <span class="tag">com</span><span class="class">.alibaba</span><span class="class">.otter</span><span class="class">.canal</span><span class="class">.deployer</span><span class="class">.CanalLauncher</span> <span class="tag">-</span> ## <span class="tag">start</span> <span class="tag">the</span> <span class="tag">canal</span> <span class="tag">server</span>.</span><br><span class="line">2017<span class="tag">-10-12</span> 14<span class="pseudo">:51</span><span class="pseudo">:45</span><span class="class">.776</span> <span class="attr_selector">[main]</span> <span class="tag">INFO</span>  <span class="tag">com</span><span class="class">.alibaba</span><span class="class">.otter</span><span class="class">.canal</span><span class="class">.deployer</span><span class="class">.CanalController</span> <span class="tag">-</span> ## <span class="tag">start</span> <span class="tag">the</span> <span class="tag">canal</span> <span class="tag">server</span><span class="attr_selector">[192.168.6.52:11112]</span></span><br><span class="line">2017<span class="tag">-10-12</span> 14<span class="pseudo">:51</span><span class="pseudo">:46</span><span class="class">.687</span> <span class="attr_selector">[main]</span> <span class="tag">INFO</span>  <span class="tag">com</span><span class="class">.alibaba</span><span class="class">.otter</span><span class="class">.canal</span><span class="class">.deployer</span><span class="class">.CanalLauncher</span> <span class="tag">-</span> ## <span class="tag">the</span> <span class="tag">canal</span> <span class="tag">server</span> <span class="tag">is</span> <span class="tag">running</span> <span class="tag">now</span> ......</span><br></pre></td></tr></table></figure>
<p>启动canal_s</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2017<span class="tag">-10-12</span> 14<span class="pseudo">:52</span><span class="pseudo">:18</span><span class="class">.999</span> <span class="attr_selector">[main]</span> <span class="tag">INFO</span>  <span class="tag">com</span><span class="class">.alibaba</span><span class="class">.otter</span><span class="class">.canal</span><span class="class">.deployer</span><span class="class">.CanalLauncher</span> <span class="tag">-</span> ## <span class="tag">start</span> <span class="tag">the</span> <span class="tag">canal</span> <span class="tag">server</span>.</span><br><span class="line">2017<span class="tag">-10-12</span> 14<span class="pseudo">:52</span><span class="pseudo">:19</span><span class="class">.208</span> <span class="attr_selector">[main]</span> <span class="tag">INFO</span>  <span class="tag">com</span><span class="class">.alibaba</span><span class="class">.otter</span><span class="class">.canal</span><span class="class">.deployer</span><span class="class">.CanalController</span> <span class="tag">-</span> ## <span class="tag">start</span> <span class="tag">the</span> <span class="tag">canal</span> <span class="tag">server</span><span class="attr_selector">[192.168.6.52:11113]</span></span><br><span class="line">2017<span class="tag">-10-12</span> 14<span class="pseudo">:52</span><span class="pseudo">:19</span><span class="class">.364</span> <span class="attr_selector">[main]</span> <span class="tag">INFO</span>  <span class="tag">com</span><span class="class">.alibaba</span><span class="class">.otter</span><span class="class">.canal</span><span class="class">.deployer</span><span class="class">.CanalLauncher</span> <span class="tag">-</span> ## <span class="tag">the</span> <span class="tag">canal</span> <span class="tag">server</span> <span class="tag">is</span> <span class="tag">running</span> <span class="tag">now</span> ......</span><br></pre></td></tr></table></figure>
<p>master提供服务，canal_m/logs/example/example.log下有日志，而canal_s/logs没有example文件夹</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 ~]$ tail -f canal_m/logs/example/example<span class="class">.log</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">12</span> <span class="number">14</span>:<span class="number">51</span>:<span class="number">46.453</span> [main] INFO  c<span class="class">.a</span><span class="class">.o</span><span class="class">.c</span><span class="class">.i</span><span class="class">.spring</span><span class="class">.support</span><span class="class">.PropertyPlaceholderConfigurer</span> - Loading properties file from class path resource [canal.properties]</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">12</span> <span class="number">14</span>:<span class="number">51</span>:<span class="number">46.463</span> [main] INFO  c<span class="class">.a</span><span class="class">.o</span><span class="class">.c</span><span class="class">.i</span><span class="class">.spring</span><span class="class">.support</span><span class="class">.PropertyPlaceholderConfigurer</span> - Loading properties file from class path resource [example/instance.properties]</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">12</span> <span class="number">14</span>:<span class="number">51</span>:<span class="number">46.624</span> [main] INFO  c<span class="class">.a</span><span class="class">.otter</span><span class="class">.canal</span><span class="class">.instance</span><span class="class">.spring</span><span class="class">.CanalInstanceWithSpring</span> - start CannalInstance <span class="keyword">for</span> <span class="number">1</span>-example</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">12</span> <span class="number">14</span>:<span class="number">51</span>:<span class="number">46.644</span> [main] INFO  c<span class="class">.a</span><span class="class">.otter</span><span class="class">.canal</span><span class="class">.instance</span><span class="class">.core</span><span class="class">.AbstractCanalInstance</span> - start successful....</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">12</span> <span class="number">14</span>:<span class="number">51</span>:<span class="number">46.658</span> [destination = example , <span class="tag">address</span> = /<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">3306</span> , EventParser] WARN  c<span class="class">.a</span><span class="class">.otter</span><span class="class">.canal</span><span class="class">.parse</span><span class="class">.inbound</span><span class="class">.mysql</span><span class="class">.MysqlEventParser</span> - prepare to find start <span class="attribute">position</span> just show master status</span><br></pre></td></tr></table></figure>
<p>查看Canal HA记录在ZK的信息</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">7</span>] ls /otter/canal/destinations/example/cluster</span><br><span class="line">[<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">11112</span>, <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">11113</span>]</span><br><span class="line"></span><br><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">10</span>] get /otter/canal/destinations/example/running</span><br><span class="line">&#123;<span class="string">"active"</span>:<span class="literal">true</span>,<span class="string">"address"</span>:<span class="string">"192.168.6.52:11112"</span>,<span class="string">"cid"</span>:<span class="number">2</span>&#125;</span><br></pre></td></tr></table></figure>
<p>启动example的ClusterCanalClientTest</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">CanalConnector</span> connector = CanalConnectors.newClusterConnector(<span class="string">"192.168.6.52:2181"</span>, destination, <span class="string">"canal"</span>, <span class="string">"canal"</span>);</span><br></pre></td></tr></table></figure>
<p>执行SQL：<code>update test set name = &#39;zqh&#39; where uid=1;</code>，控制台打印日志如下：</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"><span class="keyword">*</span> Batch Id: [1] ,count : [3] , memsize : [203] , Time : 2017-10-12 15:05:20</span><br><span class="line"><span class="keyword">*</span> Start : [mysql-bin.000004:1151:1507791918000(2017-10-12 15:05:18)] </span><br><span class="line"><span class="keyword">*</span> End : [mysql-bin.000004:1331:1507791918000(2017-10-12 15:05:18)] </span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"></span><br><span class="line">================&gt; binlog[mysql-bin.000004:1151] , executeTime : 1507791918000 , delay : 2080ms</span><br><span class="line"> BEGIN ----&gt; Thread id: 763</span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:1277] , name[canal_test,test] , eventType : UPDATE , executeTime : 1507791918000 , delay : 2092ms</span><br><span class="line">uid : 1    type=int(4)</span><br><span class="line">name : zqh    type=varchar(10)    update=true</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:1331] , executeTime : 1507791918000 , delay : 2130ms</span><br></pre></td></tr></table></figure>
<p>再次查看ZK中记录的客户端信息：</p>
<ul>
<li>一个Instance对应一个Client，这里的Instance名称为example，对应的客户端编号是1001</li>
<li>为了验证Instance确实是由指定的Client连接，在Server上查看11112端口</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">18</span>] get /otter/canal/destinations/example/<span class="number">1001</span>/running</span><br><span class="line">&#123;<span class="string">"active"</span>:<span class="literal">true</span>,<span class="string">"address"</span>:<span class="string">"10.57.241.44:53942"</span>,<span class="string">"clientId"</span>:<span class="number">1001</span>&#125;</span><br><span class="line"></span><br><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">19</span>] get /otter/canal/destinations/example/<span class="number">1001</span>/cursor</span><br><span class="line">&#123;<span class="string">"@type"</span>:<span class="string">"com.alibaba.otter.canal.protocol.position.LogPosition"</span>,</span><br><span class="line"><span class="string">"identity"</span>:&#123;<span class="string">"slaveId"</span>:-<span class="number">1</span>,<span class="string">"sourceAddress"</span>:&#123;<span class="string">"address"</span>:<span class="string">"localhost"</span>,<span class="string">"port"</span>:<span class="number">3306</span>&#125;&#125;,</span><br><span class="line"><span class="string">"postion"</span>:&#123;<span class="string">"included"</span>:<span class="literal">false</span>,<span class="string">"journalName"</span>:<span class="string">"mysql-bin.000004"</span>,<span class="string">"position"</span>:<span class="number">1331</span>,<span class="string">"serverId"</span>:<span class="number">1</span>,<span class="string">"timestamp"</span>:<span class="number">1507791918000</span>&#125;&#125; ==》serverId表示MySQL的server_id</span><br><span class="line"></span><br><span class="line">[qihuang.zheng@dp0652 ~]$ netstat -anpt|grep <span class="number">11112</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">11112</span>               <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:*                   LISTEN      <span class="number">27816</span>/java   ==》Canal服务端</span><br><span class="line">tcp        <span class="number">0</span>     <span class="number">19</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">11112</span>          <span class="number">10.57</span><span class="number">.241</span><span class="number">.44</span>:<span class="number">53942</span>          ESTABLISHED <span class="number">27816</span>/java   ==》Canal客户端</span><br></pre></td></tr></table></figure>
<p>停止canal_m</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng<span class="annotation">@dp</span>0652 canal_m]$ bin/stop.sh</span><br><span class="line"><span class="string">dp0652:</span> stopping canal <span class="number">27816</span> ...</span><br><span class="line">Oook! <span class="string">cost:</span><span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>Instance会在slave节点即canal_s上启动</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 ~]$ tail -f canal_s/logs/example/example.<span class="built_in">log</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">12</span> <span class="number">15</span>:<span class="number">17</span>:<span class="number">21.452</span> [New I/O server worker <span class="preprocessor">#<span class="number">1</span>-<span class="number">1</span>] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:<span class="number">400</span> , Caused by :</span></span><br><span class="line">something goes wrong with channel:[id: <span class="number">0x0c182149</span>, /<span class="number">10.57</span><span class="number">.241</span><span class="number">.44</span>:<span class="number">54008</span> =&gt; /<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">11113</span>], exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:example should start first</span><br><span class="line"></span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">12</span> <span class="number">15</span>:<span class="number">17</span>:<span class="number">21.661</span> [pool-<span class="number">1</span>-thread-<span class="number">1</span>] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from <span class="keyword">class</span> path resource [canal.properties]</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">12</span> <span class="number">15</span>:<span class="number">17</span>:<span class="number">21.663</span> [pool-<span class="number">1</span>-thread-<span class="number">1</span>] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from <span class="keyword">class</span> path resource [example/instance.properties]</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">12</span> <span class="number">15</span>:<span class="number">17</span>:<span class="number">21.767</span> [pool-<span class="number">1</span>-thread-<span class="number">1</span>] WARN  org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider <span class="keyword">using</span> a more isolated form of registration, e.g. on the BeanWrapper/BeanFactory!</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">12</span> <span class="number">15</span>:<span class="number">17</span>:<span class="number">21.968</span> [pool-<span class="number">1</span>-thread-<span class="number">1</span>] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance <span class="keyword">for</span> <span class="number">1</span>-example</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">12</span> <span class="number">15</span>:<span class="number">17</span>:<span class="number">21.998</span> [pool-<span class="number">1</span>-thread-<span class="number">1</span>] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful....</span><br><span class="line"><span class="number">2017</span>-<span class="number">10</span>-<span class="number">12</span> <span class="number">15</span>:<span class="number">17</span>:<span class="number">22.071</span> [destination = example , address = /<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">3306</span> , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position</span><br><span class="line"> &#123;<span class="string">"identity"</span>:&#123;<span class="string">"slaveId"</span>:-<span class="number">1</span>,<span class="string">"sourceAddress"</span>:&#123;<span class="string">"address"</span>:<span class="string">"localhost"</span>,<span class="string">"port"</span>:<span class="number">3306</span>&#125;&#125;,<span class="string">"postion"</span>:&#123;<span class="string">"included"</span>:<span class="literal">false</span>,<span class="string">"journalName"</span>:<span class="string">"mysql-bin.000004"</span>,<span class="string">"position"</span>:<span class="number">1331</span>,<span class="string">"serverId"</span>:<span class="number">1</span>,<span class="string">"timestamp"</span>:<span class="number">1507791918000</span>&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>停止canal_m后，只剩下canal_s，所以Canal集群只有一个节点了：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">14</span>] ls /otter/canal/cluster</span><br><span class="line">[<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">11113</span>]</span><br><span class="line"></span><br><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">5</span>] get /otter/canal/destinations/example/running</span><br><span class="line">&#123;<span class="string">"active"</span>:<span class="literal">true</span>,<span class="string">"address"</span>:<span class="string">"192.168.6.52:11113"</span>,<span class="string">"cid"</span>:<span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure>
<p>切换过程中，Client的日志</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">2017-10-12 15:17:22.524 [Thread-2] WARN  c.alibaba.otter.canal.client.impl.ClusterCanalConnector - failed to connect to:/192.168.6.52:11113 after retry 0 times</span><br><span class="line">2017-10-12 15:17:22.529 [Thread-2] WARN  c.a.otter.canal.client.impl.running.ClientRunningMonitor - canal is not run any in node</span><br><span class="line">2017-10-12 15:17:27.695 [Thread-2] INFO  c.alibaba.otter.canal.client.impl.ClusterCanalConnector - restart the connector for next round retry.</span><br><span class="line"></span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"><span class="keyword">*</span> Batch Id: [1] ,count : [1] , memsize : [75] , Time : 2017-10-12 15:17:27</span><br><span class="line"><span class="keyword">*</span> Start : [mysql-bin.000004:1331:1507791918000(2017-10-12 15:05:18)] </span><br><span class="line"><span class="keyword">*</span> End : [mysql-bin.000004:1331:1507791918000(2017-10-12 15:05:18)] </span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:1331] , executeTime : 1507791918000 , delay : 729763ms</span><br></pre></td></tr></table></figure>
<p>再次执行SQL语句</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"><span class="keyword">*</span> Batch Id: [2] ,count : [3] , memsize : [198] , Time : 2017-10-12 15:20:56</span><br><span class="line"><span class="keyword">*</span> Start : [mysql-bin.000004:1406:1507792855000(2017-10-12 15:20:55)] </span><br><span class="line"><span class="keyword">*</span> End : [mysql-bin.000004:1581:1507792855000(2017-10-12 15:20:55)] </span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"></span><br><span class="line">================&gt; binlog[mysql-bin.000004:1406] , executeTime : 1507792855000 , delay : 1539ms</span><br><span class="line"> BEGIN ----&gt; Thread id: 763</span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:1532] , name[canal_test,test] , eventType : UPDATE , executeTime : 1507792855000 , delay : 1539ms</span><br><span class="line">uid : 1    type=int(4)</span><br><span class="line">name : zqhx    type=varchar(10)    update=true</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:1581] , executeTime : 1507792855000 , delay : 1540ms</span><br></pre></td></tr></table></figure>
<p>停止客户端后，查询ZK中的客户端信息。注意，仍然有cursor信息，但是没有running，因为instance没有对应的client了。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">1</span>] ls /otter/canal/destinations/example</span><br><span class="line">[running, cluster, <span class="number">1001</span>]</span><br><span class="line"></span><br><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">0</span>] ls /otter/canal/destinations/example/<span class="number">1001</span></span><br><span class="line">[cursor]</span><br><span class="line"></span><br><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">6</span>] get /otter/canal/destinations/example/<span class="number">1001</span>/cursor</span><br><span class="line">&#123;<span class="string">"@type"</span>:<span class="string">"com.alibaba.otter.canal.protocol.position.LogPosition"</span>,</span><br><span class="line"><span class="string">"identity"</span>:&#123;<span class="string">"slaveId"</span>:-<span class="number">1</span>,<span class="string">"sourceAddress"</span>:&#123;<span class="string">"address"</span>:<span class="string">"localhost"</span>,<span class="string">"port"</span>:<span class="number">3306</span>&#125;&#125;,</span><br><span class="line"><span class="string">"postion"</span>:&#123;<span class="string">"included"</span>:<span class="literal">false</span>,<span class="string">"journalName"</span>:<span class="string">"mysql-bin.000004"</span>,<span class="string">"position"</span>:<span class="number">1581</span>,<span class="string">"serverId"</span>:<span class="number">1</span>,<span class="string">"timestamp"</span>:<span class="number">1507792855000</span>&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>cursor信息是instance消费binlog的位置，即使客户端停掉了，也仍然保留在zk中。</p>
<blockquote>
<p>注意：1001是ClientIdentity的固定编号，相关源码在SimpleCanalConnector的构造方法里。</p>
</blockquote>
<p>下面总结下zk中的相关记录：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/otter/canal/</span><br><span class="line">  |- cluster          ==&gt; [<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">11112</span>, <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">11113</span>]</span><br><span class="line">  |- destinations     ==&gt; instances</span><br><span class="line">     |- example1/     ==&gt; instance name</span><br><span class="line">     |  |- cluster    ==&gt; [<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">11112</span>, <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">11113</span>]</span><br><span class="line">     |  |- running    ==&gt; &#123;<span class="string">"active"</span>:<span class="literal">true</span>,<span class="string">"address"</span>:<span class="string">"192.168.6.52:11112"</span>,<span class="string">"cid"</span>:<span class="number">2</span>&#125;</span><br><span class="line">     |  |- <span class="number">1001</span></span><br><span class="line">     |     |-running  ==&gt; &#123;<span class="string">"active"</span>:<span class="literal">true</span>,<span class="string">"address"</span>:<span class="string">"10.57.241.44:53942"</span>,<span class="string">"clientId"</span>:<span class="number">1001</span>&#125;</span><br><span class="line">     |     |- cursor  ==&gt; &#123;localhost:<span class="number">3306</span>,<span class="string">"journalName"</span>:<span class="string">"mysql-bin.000004"</span>,<span class="string">"position"</span>:<span class="number">1331</span>,<span class="string">"serverId"</span>:<span class="number">1</span>&#125;</span><br><span class="line">     |- example2/</span><br><span class="line">     |  |- cluster    ==&gt; [<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">11112</span>, <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">11113</span>]</span><br><span class="line">     |  |- running    ==&gt; &#123;<span class="string">"active"</span>:<span class="literal">true</span>,<span class="string">"address"</span>:<span class="string">"192.168.6.52:11112"</span>,<span class="string">"cid"</span>:<span class="number">2</span>&#125;</span><br><span class="line">     |  |- <span class="number">1001</span></span><br><span class="line">     |     |-running  ==&gt; &#123;<span class="string">"active"</span>:<span class="literal">true</span>,<span class="string">"address"</span>:<span class="string">"10.57.241.44:53942"</span>,<span class="string">"clientId"</span>:<span class="number">1001</span>&#125;</span><br><span class="line">     |     |- cursor  ==&gt; &#123;localhost:<span class="number">3306</span>,<span class="string">"journalName"</span>:<span class="string">"mysql-bin.000004"</span>,<span class="string">"position"</span>:<span class="number">1331</span>,<span class="string">"serverId"</span>:<span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>
<p>下图是Canal Server HA的流程图：</p>
<ol>
<li>canal server要启动某个canal instance时都先向zookeeper进行一次尝试启动判断 (实现：创建EPHEMERAL节点，谁创建成功就允许谁启动)</li>
<li>创建zookeeper节点成功后，对应的canal server就启动对应的canal instance，没有创建成功的canal instance就会处于standby状态</li>
<li>一旦zookeeper发现canal server A创建的节点消失后，立即通知其他的canal server再次进行步骤1的操作，重新选出一个canal server启动instance.</li>
<li>canal client每次进行connect时，会首先向zookeeper询问当前是谁启动了canal instance，然后和其建立链接，一旦链接不可用，会重新尝试connect.</li>
</ol>
<p><img src="https://camo.githubusercontent.com/c8f1d98268a307821273e94e7eefcd29a26f9b78/687474703a2f2f646c2e69746579652e636f6d2f75706c6f61642f6174746163686d656e742f303038302f333330332f64333230326332362d653935342d333563302d613331392d3537363034313032633537642e6a7067" alt="server ha"></p>
<h2 id="Canal_Client_HA">Canal Client HA</h2><p>Canal Client的方式和canal server方式类似，也是利用zookeeper的抢占EPHEMERAL节点的方式进行控制。</p>
<p>关于Canal Client HA的验证，可以参考：<a href="http://blog.csdn.net/xiaolinzi007/article/details/52933909" target="_blank" rel="external">http://blog.csdn.net/xiaolinzi007/article/details/52933909</a></p>
<ul>
<li>在IDEA中同时启动多个客户端，执行一条SQL语句，其中一个客户端会打印日志，另一个不会打印。</li>
<li>停止该客户端。</li>
<li>再次执行SQL语句，另外一个客户端会打印日志</li>
</ul>
<p>Client1的日志：</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"><span class="keyword">*</span> Batch Id: [3] ,count : [3] , memsize : [198] , Time : 2017-10-12 17:59:59</span><br><span class="line"><span class="keyword">*</span> Start : [mysql-bin.000004:1656:1507802398000(2017-10-12 17:59:58)] </span><br><span class="line"><span class="keyword">*</span> End : [mysql-bin.000004:1831:1507802398000(2017-10-12 17:59:58)] </span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"></span><br><span class="line">================&gt; binlog[mysql-bin.000004:1656] , executeTime : 1507802398000 , delay : 1188ms</span><br><span class="line"> BEGIN ----&gt; Thread id: 768</span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:1782] , name[canal_test,test] , eventType : UPDATE , executeTime : 1507802398000 , delay : 1199ms</span><br><span class="line">uid : 1    type=int(4)</span><br><span class="line">name : zqh    type=varchar(10)    update=true</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:1831] , executeTime : 1507802398000 , delay : 1236ms</span><br><span class="line"><span class="comment">## stop the canal client## canal client is down.</span></span><br></pre></td></tr></table></figure>
<p>停止Client1后，Client2的日志：</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"><span class="keyword">*</span> Batch Id: [4] ,count : [3] , memsize : [198] , Time : 2017-10-12 18:02:15</span><br><span class="line"><span class="keyword">*</span> Start : [mysql-bin.000004:1906:1507802534000(2017-10-12 18:02:14)] </span><br><span class="line"><span class="keyword">*</span> End : [mysql-bin.000004:2081:1507802534000(2017-10-12 18:02:14)] </span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line"></span><br><span class="line">================&gt; binlog[mysql-bin.000004:1906] , executeTime : 1507802534000 , delay : 1807ms</span><br><span class="line"> BEGIN ----&gt; Thread id: 768</span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:2032] , name[canal_test,test] , eventType : UPDATE , executeTime : 1507802534000 , delay : 1819ms</span><br><span class="line">uid : 1    type=int(4)</span><br><span class="line">name : zqhx    type=varchar(10)    update=true</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:2081] , executeTime : 1507802534000 , delay : 1855ms</span><br></pre></td></tr></table></figure>
<p>观察ZK节点中instance对应的client节点，在Client切换时，会进行变更。<br>比如下面的客户端从56806端口切换到了56842端口。<br>把所有客户端都关闭后，1001下没有running。表示instance没有客户端消费binlog了。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">29</span>] get /otter/canal/destinations/example/<span class="number">1001</span>/running</span><br><span class="line">&#123;<span class="string">"active"</span>:<span class="literal">true</span>,<span class="string">"address"</span>:<span class="string">"10.57.241.44:56806"</span>,<span class="string">"clientId"</span>:<span class="number">1001</span>&#125;</span><br><span class="line"></span><br><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">30</span>] get /otter/canal/destinations/example/<span class="number">1001</span>/running</span><br><span class="line">Node does not exist: /otter/canal/destinations/example/<span class="number">1001</span>/running</span><br><span class="line"></span><br><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">31</span>] get /otter/canal/destinations/example/<span class="number">1001</span>/running</span><br><span class="line">&#123;<span class="string">"active"</span>:<span class="literal">true</span>,<span class="string">"address"</span>:<span class="string">"10.57.241.44:56842"</span>,<span class="string">"clientId"</span>:<span class="number">1001</span>&#125;</span><br><span class="line"></span><br><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">2181</span>(CONNECTED) <span class="number">32</span>] ls /otter/canal/destinations/example/<span class="number">1001</span></span><br><span class="line">[cursor]</span><br></pre></td></tr></table></figure>
<p>具体实现相关类有：ClientRunningMonitor/ClientRunningListener/ClientRunningData。</p>
<p>client running相关控制，主要为解决client自身的failover机制。<br>canal client允许同时启动多个canal client，通过running机制，可保证只有一个client在工作，其他client做为冷备.<br>当运行中的client挂了，running会控制让冷备中的client转为工作模式，<br>这样就可以确保canal client也不会是单点. 保证整个系统的高可用性.</p>
<p>下图左边是客户端的HA实现，右边是服务端的HA实现</p>
<p><img src="http://img.blog.csdn.net/20171012184033228" alt="ha"></p>
<h2 id="Develop_Canal_Client">Develop Canal Client</h2><p>先理解：<a href="https://github.com/alibaba/canal/wiki/ClientAPI">https://github.com/alibaba/canal/wiki/ClientAPI</a></p>
<p><img src="https://camo.githubusercontent.com/8cc684cf92e22d738d57b002c356afba96bcc4f5/687474703a2f2f646c322e69746579652e636f6d2f75706c6f61642f6174746163686d656e742f303039302f363435332f39326233343335302d323566632d333162332d626361362d3865326131653763356532322e6a7067" alt="client"></p>
<h3 id="subscribe_change">subscribe change</h3><p>重新看下CanalServerWithEmbedded的订阅方法。我们知道客户端在连接服务端的某个destination之后，会紧接着调用subscribe()方法。</p>
<p>客户端连接服务端时，必须指定destination名称，因为一个服务端可能有多个destination。比如服务端启动了两个Instance，它们的destination名称分别是example1和example2。<br>假设有两个客户端A和B，A连接example1，B连接example2。 服务端的canalInstances字典为：{example1=&gt;Instance1，example2-&gt;Instance2}。<br>那么ClientA的destination等于example1，对应的服务端实例为Instance1。ClientB的destination等于example2，对应的服务端实例为Instance3。</p>
<p><img src="http://img.blog.csdn.net/20171012230738279" alt="clients"></p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span><br><span class="line"> * 客户端订阅，重复订阅时会更新对应的filter信息</span><br><span class="line"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span>(<span class="params">ClientIdentity clientIdentity</span>) throws CanalServerException </span>&#123;</span><br><span class="line">    CanalInstance canalInstance = canalInstances.<span class="keyword">get</span>(clientIdentity.getDestination());</span><br><span class="line">    <span class="keyword">if</span> (!canalInstance.getMetaManager().isStart()) &#123;</span><br><span class="line">        canalInstance.getMetaManager().start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    canalInstance.getMetaManager().subscribe(clientIdentity); <span class="comment">// 执行一下meta订阅</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据Client从MetaManager中获取最近一次的Cursor</span></span><br><span class="line">    Position position = canalInstance.getMetaManager().getCursor(clientIdentity);</span><br><span class="line">    <span class="keyword">if</span> (position == <span class="keyword">null</span>) &#123; <span class="comment">// 如果没有</span></span><br><span class="line">        position = canalInstance.getEventStore().getFirstPosition();<span class="comment">// 获取一下store中的第一条</span></span><br><span class="line">        <span class="keyword">if</span> (position != <span class="keyword">null</span>) &#123;</span><br><span class="line">            canalInstance.getMetaManager().updateCursor(clientIdentity, position); <span class="comment">// 更新一下cursor</span></span><br><span class="line">        &#125;</span><br><span class="line">        logger.info(<span class="string">"subscribe successfully, &#123;&#125; with first position:&#123;&#125; "</span>, clientIdentity, position);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">// 有就直接使用</span></span><br><span class="line">        logger.info(<span class="string">"subscribe successfully, use last cursor position:&#123;&#125; "</span>, clientIdentity, position);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 通知下订阅关系变化</span></span><br><span class="line">    canalInstance.subscribeChange(clientIdentity);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里面关于订阅方法有两个地方，CanalInstance本身调用了subscribeChange，它关联的MetaManager也调用了subscribe方法。</p>
<p>一个CanalServer可以有多个CanalInstance，每个Instance都会有一个MetaManager。而一个Instance对应一个Client。<br>那么，这么说来，一个MetaManager也就只会有一个Client了。但是从下面的数据结构来看的话，一个MetaManager貌似可以有多个Destination。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">MemoryMetaManager</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">AbstractCanalLifeCycle</span> <span class="title">implements</span> <span class="title">CanalMetaManager</span> &#123;</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="type">Map</span>&lt;<span class="type">String</span>, <span class="type">List</span>&lt;<span class="type">ClientIdentity</span>&gt;&gt;              destinations;</span><br><span class="line">    <span class="keyword">protected</span> <span class="type">Map</span>&lt;<span class="type">ClientIdentity</span>, <span class="type">MemoryClientIdentityBatch</span>&gt; batches;</span><br><span class="line">    <span class="keyword">protected</span> <span class="type">Map</span>&lt;<span class="type">ClientIdentity</span>, <span class="type">Position</span>&gt;                  cursors;</span><br><span class="line"></span><br><span class="line">    public synchronized void subscribe(<span class="type">ClientIdentity</span> clientIdentity) <span class="keyword">throws</span> <span class="type">CanalMetaManagerException</span> &#123;</span><br><span class="line">        <span class="type">List</span>&lt;<span class="type">ClientIdentity</span>&gt; clientIdentitys = destinations.get(clientIdentity.getDestination());</span><br><span class="line">        <span class="keyword">if</span> (clientIdentitys.contains(clientIdentity)) &#123;</span><br><span class="line">            clientIdentitys.remove(clientIdentity);</span><br><span class="line">        &#125;</span><br><span class="line">        clientIdentitys.add(clientIdentity);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>猜测：多个Client可以连接到同一个Instance（虽然只会有一个Instance起作用），所以一个MetaManager可以管理多个Client。<br>NO！Client的HA与MetaManager记录的Client是不一样的。HA表示同一时间只有一个Client起作用，那么MetaManager不可能同时记录两个Client。</p>
</blockquote>
<p>官方ClientAPI文档上：ClientIdentity是canal client和server交互之间的身份标识，目前clientId写死为1001.<br>(目前canal server上的一个instance只能有一个client消费，clientId的设计是为1个instance多client消费模式而预留的，暂时不需要理会)</p>
<p>也就是说：一个Instance还是有可能有多个Client连接上来的。</p>
<p><img src="http://img.blog.csdn.net/20171012234337736" alt="subscribes"></p>
<p>这里的数据结构为什么这么设计，还需要参考AbstractMetaManagerTest的doSubscribeTest方法来理解。</p>
<p>对于相同的destination，可以订阅不同的client。下面的示例分别订阅了[client1,client2]和[client1,client3]。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doSubscribeTest</span><span class="params">(CanalMetaManager metaManager)</span> </span>&#123;</span><br><span class="line">    ClientIdentity client1 = <span class="keyword">new</span> ClientIdentity(destination, (<span class="keyword">short</span>) <span class="number">1</span>);</span><br><span class="line">    metaManager.subscribe(client1);</span><br><span class="line">    metaManager.subscribe(client1); <span class="comment">// 重复调用：删除旧的client1，并继续增加新的client1</span></span><br><span class="line">    ClientIdentity client2 = <span class="keyword">new</span> ClientIdentity(destination, (<span class="keyword">short</span>) <span class="number">2</span>);</span><br><span class="line">    metaManager.subscribe(client2);</span><br><span class="line"></span><br><span class="line">    List&lt;ClientIdentity&gt; clients = metaManager.listAllSubscribeInfo(destination);</span><br><span class="line">    Assert.assertEquals(Arrays.asList(client1, client2), clients);</span><br><span class="line"></span><br><span class="line">    metaManager.unsubscribe(client2);</span><br><span class="line">    ClientIdentity client3 = <span class="keyword">new</span> ClientIdentity(destination, (<span class="keyword">short</span>) <span class="number">3</span>);</span><br><span class="line">    metaManager.subscribe(client3);</span><br><span class="line"></span><br><span class="line">    clients = metaManager.listAllSubscribeInfo(destination);</span><br><span class="line">    Assert.assertEquals(Arrays.asList(client1, client3), clients);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>CanalServerWithEmbedded的订阅方法最后还会调用AbstractCanalInstance的subscribeChange方法。<br>这里会设置表名的filter，以及黑名单。配置项在instance.properties中。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># table regex</span></span><br><span class="line">canal.instance.<span class="built_in">filter</span>.regex = .*\\..*</span><br><span class="line"><span class="comment"># table black regex</span></span><br><span class="line">canal.instance.<span class="built_in">filter</span>.<span class="keyword">black</span>.regex =</span><br></pre></td></tr></table></figure>
<p>filter表示客户端要通过Canal Server获取MySQL哪些表的binlog，上面配置项表示获取所有表。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">AbstractCanalInstance</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">AbstractCanalLifeCycle</span> <span class="title">implements</span> <span class="title">CanalInstance</span> &#123;</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="type">Long</span>                                   canalId;                                                      <span class="comment">// 和manager交互唯一标示</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="type">String</span>                                 destination;                                                  <span class="comment">// 队列名字</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="type">CanalEventStore</span>&lt;<span class="type">Event</span>&gt;                 eventStore;                                                   <span class="comment">// 有序队列</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> <span class="type">CanalEventParser</span>                       eventParser;                                                  <span class="comment">// 解析对应的数据信息</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="type">CanalEventSink</span>&lt;<span class="type">List</span>&lt;<span class="type">CanalEntry</span>.<span class="type">Entry</span>&gt;&gt; eventSink;                                                    <span class="comment">// 链接parse和store的桥接器</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="type">CanalMetaManager</span>                       metaManager;                                                  <span class="comment">// 消费信息管理器</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="type">CanalAlarmHandler</span>                      alarmHandler;                                                 <span class="comment">// alarm报警机制</span></span><br><span class="line"></span><br><span class="line">    <span class="annotation">@Override</span></span><br><span class="line">    public boolean subscribeChange(<span class="type">ClientIdentity</span> identity) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="type">StringUtils</span>.isNotEmpty(identity.getFilter())) &#123;</span><br><span class="line">            logger.info(<span class="string">"subscribe filter change to "</span> + identity.getFilter());</span><br><span class="line">            <span class="type">AviaterRegexFilter</span> aviaterFilter = <span class="keyword">new</span> <span class="type">AviaterRegexFilter</span>(identity.getFilter());</span><br><span class="line"></span><br><span class="line">            boolean isGroup = (eventParser instanceof <span class="type">GroupEventParser</span>);</span><br><span class="line">            <span class="keyword">if</span> (isGroup) &#123;</span><br><span class="line">                <span class="comment">// 处理group的模式</span></span><br><span class="line">                <span class="type">List</span>&lt;<span class="type">CanalEventParser</span>&gt; eventParsers = ((<span class="type">GroupEventParser</span>) eventParser).getEventParsers();</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">CanalEventParser</span> singleEventParser : eventParsers) &#123;<span class="comment">// 需要遍历启动</span></span><br><span class="line">                    ((<span class="type">AbstractEventParser</span>) singleEventParser).setEventFilter(aviaterFilter);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                ((<span class="type">AbstractEventParser</span>) eventParser).setEventFilter(aviaterFilter);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// filter的处理规则</span></span><br><span class="line">        <span class="comment">// a. parser处理数据过滤处理</span></span><br><span class="line">        <span class="comment">// b. sink处理数据的路由&amp;分发,一份parse数据经过sink后可以分发为多份，每份的数据可以根据自己的过滤规则不同而有不同的数据</span></span><br><span class="line">        <span class="comment">// 后续内存版的一对多分发，可以考虑</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应在EventParser中，存在两个Filter的引用。比如上面eventParser.setEventFilter()方法会设置AbstractEventParser的eventFilter。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">AbstractEventParser&lt;EVENT&gt;</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">AbstractCanalLifeCycle</span> <span class="title">implements</span> <span class="title">CanalEventParser&lt;EVENT&gt;</span> &#123;</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="type">CanalLogPositionManager</span>                logPositionManager         = <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">protected</span> <span class="type">CanalEventSink</span>&lt;<span class="type">List</span>&lt;<span class="type">CanalEntry</span>.<span class="type">Entry</span>&gt;&gt; eventSink                  = <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">protected</span> <span class="type">CanalEventFilter</span>                       eventFilter                = <span class="literal">null</span>;</span><br><span class="line">    <span class="keyword">protected</span> <span class="type">CanalEventFilter</span>                       eventBlackFilter           = <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="EventParser_Implement">EventParser Implement</h3><p>AbstractEventParser的start()方法是解析binlog的主要方法。在启动transactionBuffer和BinLogParser后，会启动一个后台的工作线程parseThread一直运行：</p>
<p>注意：下面的几个步骤是嵌套在一个while死循环里，最后会进行sleep。</p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开始执行replication</span></span><br><span class="line"><span class="comment">// 1. 构造Erosa连接</span></span><br><span class="line">erosaConnection = buildErosaConnection();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 启动一个心跳线程</span></span><br><span class="line">startHeartBeat(erosaConnection);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 执行dump前的准备工作</span></span><br><span class="line">preDump(erosaConnection);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. 连接MySQL数据库</span></span><br><span class="line">erosaConnection.connect(); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 5. 获取最后的位置信息</span></span><br><span class="line">EntryPosition startPosition = findStartPosition(erosaConnection);</span><br><span class="line">logger.info(<span class="string">"find start position : &#123;&#125;"</span>, startPosition.toString());</span><br><span class="line"><span class="comment">// 重新链接，因为在找position过程中可能有状态，需要断开后重建</span></span><br><span class="line">erosaConnection.reconnect();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义回调函数，当解析成功后，sink()方法会暂存到缓冲区transactionBuffer中。缓冲区的数据会通过心跳线程放入EventSink</span></span><br><span class="line">final SinkFunction sinkHandler = <span class="keyword">new</span> SinkFunction&lt;EVENT&gt;() &#123;</span><br><span class="line">    <span class="keyword">private</span> LogPosition lastPosition;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sink</span>(<span class="params">EVENT <span class="keyword">event</span></span>) </span>&#123;</span><br><span class="line">        CanalEntry.Entry entry = parseAndProfilingIfNecessary(<span class="keyword">event</span>);</span><br><span class="line">        <span class="keyword">if</span> (entry != <span class="keyword">null</span>) &#123;</span><br><span class="line">            transactionBuffer.add(entry);</span><br><span class="line">            <span class="keyword">this</span>.lastPosition = buildLastPosition(entry);  <span class="comment">// 记录一下对应的positions</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 6. 开始dump数据</span></span><br><span class="line"><span class="keyword">if</span> (StringUtils.isEmpty(startPosition.getJournalName()) &amp;&amp; startPosition.getTimestamp() != <span class="keyword">null</span>) &#123;</span><br><span class="line">    erosaConnection.dump(startPosition.getTimestamp(), sinkHandler);</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    erosaConnection.dump(startPosition.getJournalName(), startPosition.getPosition(), sinkHandler);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的erosaConnection指的是Canal Server到MySQL的连接。<br>而前面我们说的客户端（CanalClient）连接CanalConnector指的是CanalClient到CanalServer的连接。</p>
<p>CanalServer到MySQL的连接是要获取binlog的dump数据包。而CanalClient到CanalServer有多种请求（GET/ACK等）。</p>
<p>我们不会具体分析dump的流程，不过看下erosaConnection的MySQL实现MysqlConnection是如何在获取到事件后调用回调函数。</p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">dump</span>(<span class="params">String binlogfilename, Long binlogPosition, SinkFunction func</span>) throws IOException </span>&#123;</span><br><span class="line">    updateSettings();</span><br><span class="line">    sendBinlogDump(binlogfilename, binlogPosition);</span><br><span class="line">    <span class="comment">// connector指的是CanalServer到MySQL Master服务器的连接，创建一个拉取线程拉取MySQL的binlog</span></span><br><span class="line">    DirectLogFetcher fetcher = <span class="keyword">new</span> DirectLogFetcher(connector.getReceiveBufferSize());</span><br><span class="line">    fetcher.start(connector.getChannel());</span><br><span class="line">    LogDecoder decoder = <span class="keyword">new</span> LogDecoder(LogEvent.UNKNOWN_EVENT, LogEvent.ENUM_END_EVENT);</span><br><span class="line">    LogContext context = <span class="keyword">new</span> LogContext();</span><br><span class="line">    <span class="keyword">while</span> (fetcher.fetch()) &#123; <span class="comment">// 由于设置了缓冲区的大小，每次dump都只会拉取一批数据</span></span><br><span class="line">        LogEvent <span class="keyword">event</span> = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">event</span> = decoder.decode(fetcher, context);</span><br><span class="line">        <span class="keyword">if</span> (!func.sink(<span class="keyword">event</span>)) <span class="keyword">break</span>; <span class="comment">// 调用回调方法</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>服务端有一个心跳线程，它的目的是消费transactionBuffer，并写入到EventSink中。</p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">protected boolean consumeTheEventAndProfilingIfNecessary(<span class="type">List</span>&lt;<span class="type">CanalEntry</span>.<span class="type">Entry</span>&gt; entrys) &#123;</span><br><span class="line">    boolean <span class="literal">result</span> = eventSink.sink(entrys, </span><br><span class="line">        (runningInfo == null) ? null : runningInfo.getAddress(), destination);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">result</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>EventSink最终会将数据写入到EventStore中，即Put到RingBuffer中。</p>
<h2 id="eunomia">eunomia</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">2181</span>(CONNECTED) <span class="number">3</span>] ls /otter/canal/destinations</span><br><span class="line">[octopus_demeter, example_bak, namelist_test, xiaopang2, namelist2, xiaopang3, namelist1, example, xiaopang]</span><br><span class="line"></span><br><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">2181</span>(CONNECTED) <span class="number">4</span>] ls /otter/canal/destinations/xiaopang</span><br><span class="line">[eunomia, cluster, <span class="number">1001</span>, running]</span><br><span class="line"></span><br><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">2181</span>(CONNECTED) <span class="number">5</span>] ls /otter/canal/destinations/xiaopang/eunomia</span><br><span class="line">[_c_2a900d4e-<span class="number">75f</span>b-<span class="number">4445</span>-b30c-<span class="number">04e1</span>bdb2e5d9-lock-<span class="number">0001381746</span>, runnning, _c_ea33db37-<span class="number">9193</span>-<span class="number">4</span>c75-<span class="number">9e61</span>-<span class="number">85e59</span>e123109-lock-<span class="number">0001381738</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// Eunomia Server？还是Canal Client？</span></span><br><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">2181</span>(CONNECTED) <span class="number">7</span>] get /otter/canal/destinations/xiaopang/eunomia/runnning</span><br><span class="line"><span class="number">10.57</span><span class="number">.17</span><span class="number">.100</span></span><br><span class="line"></span><br><span class="line">[zk: <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">2181</span>(CONNECTED) <span class="number">18</span>] get /otter/canal/destinations/xiaopang/<span class="number">1001</span>/running</span><br><span class="line">&#123;<span class="string">"active"</span>:<span class="literal">true</span>,<span class="string">"address"</span>:<span class="string">"10.57.17.100:60661"</span>,<span class="string">"clientId"</span>:<span class="number">1001</span>&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/alibaba/canal&quot;&gt;canal&lt;/a&gt;: 阿里巴巴mysql数据库binlog的增量订阅&amp;amp;消费组件&lt;br&gt;
    
    </summary>
    
      <category term="midd" scheme="http://github.com/zqhxuyuan/categories/midd/"/>
    
    
      <category term="midd" scheme="http://github.com/zqhxuyuan/tags/midd/"/>
    
  </entry>
  
  <entry>
    <title>Spark DataSources Implementation</title>
    <link href="http://github.com/zqhxuyuan/2017/09/15/2017-09-15-Spark-DataSources/"/>
    <id>http://github.com/zqhxuyuan/2017/09/15/2017-09-15-Spark-DataSources/</id>
    <published>2017-09-14T16:00:00.000Z</published>
    <updated>2017-10-17T06:43:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>Spark数据源扩展与实践(40行代码实现一个自定义的DataSource)<br><a id="more"></a></p>
<h2 id="简单示例">简单示例</h2><p>Spark的DataSource API可以方便地扩展。如果没有使用META-INFO这种ServiceLocator机制，则自定义的数据源名称必须是DefaultSource.<br>并且必须实现RelationProvider接口。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">RelationProvider</span> &#123;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span>(</span>sqlContext: <span class="type">SQLContext</span>,</span><br><span class="line">                              parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line">    ???</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通常自定义数据源都有不同的配置文件，所以我们也要实现自己的BaseRelation</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">RelationProvider</span>&#123;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span>(</span>sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line">    <span class="type">EmptyRelation</span>()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">EmptyRelation</span>(</span>) <span class="keyword">extends</span> <span class="type">BaseRelation</span> &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span>:</span> <span class="type">SQLContext</span> = ???</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">schema</span>:</span> <span class="type">StructType</span> = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主要的起始还是BaseRelation的实现类，但是这里怎么获取schema和SQLContext呢。由于DefaultSource的createRelation方法中已经有SQLContext。所以我们可以改成</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">RelationProvider</span>&#123;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span>(</span>sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line">    <span class="type">EmptyRelation</span>()(sqlContext)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">EmptyRelation</span>(</span>)(<span class="annotation">@transient</span> <span class="keyword">val</span> sc: <span class="type">SQLContext</span>) <span class="keyword">extends</span> <span class="type">BaseRelation</span> &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span>:</span> <span class="type">SQLContext</span> = sc</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">schema</span>:</span> <span class="type">StructType</span> = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那么Schema怎么确定呢？通常它需要从DefaultSource的createRelation方法的parameters确定。<br>所以通常我们会给自定义的BaseRelation加上一个参数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultSource</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">RelationProvider</span>&#123;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createRelation</span>(</span>sqlContext: <span class="type">SQLContext</span>, parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]): <span class="type">BaseRelation</span> = &#123;</span><br><span class="line">    <span class="type">EmptyRelation</span>(parameters)(sqlContext)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">EmptyRelation</span>(</span>parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>])(<span class="annotation">@transient</span> <span class="keyword">val</span> sc: <span class="type">SQLContext</span>) <span class="keyword">extends</span> <span class="type">BaseRelation</span> &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span>:</span> <span class="type">SQLContext</span> = sc</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">schema</span>:</span> <span class="type">StructType</span> = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个schema的具体实现必须依赖于如何读取数据源。所以EmptyRelation还需要实现另外一个接口：TableScan</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">EmptyRelation</span>(</span>parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>])</span><br><span class="line">                        (<span class="annotation">@transient</span> <span class="keyword">val</span> sc: <span class="type">SQLContext</span>) </span><br><span class="line">  <span class="keyword">extends</span> <span class="type">BaseRelation</span> <span class="keyword">with</span> <span class="type">TableScan</span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span>:</span> <span class="type">SQLContext</span> = sc</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">schema</span>:</span> <span class="type">StructType</span> = ???</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span>(</span>): <span class="type">RDD</span>[<span class="type">Row</span>] = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在有两个方法需要我们自己实现。buildScan表示如何读取数据源，并生成<code>RDD[ROW]</code>。<br>下面以一个简单的示例入门：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">EmptyRelation</span>(</span>parameters: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>])</span><br><span class="line">                        (<span class="annotation">@transient</span> <span class="keyword">val</span> sc: <span class="type">SQLContext</span>) </span><br><span class="line">  <span class="keyword">extends</span> <span class="type">BaseRelation</span> <span class="keyword">with</span> <span class="type">TableScan</span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">sqlContext</span>:</span> <span class="type">SQLContext</span> = sc</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">schema</span>:</span> <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">List</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"id"</span>, <span class="type">IntegerType</span>), </span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"name"</span>, <span class="type">StringType</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"age"</span>, <span class="type">IntegerType</span>)</span><br><span class="line">    ))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">buildScan</span>(</span>): <span class="type">RDD</span>[<span class="type">Row</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> rdd = sqlContext.sparkContext.parallelize(</span><br><span class="line">      <span class="type">List</span>(</span><br><span class="line">        (<span class="number">1</span>, <span class="string">"A"</span>, <span class="number">20</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="string">"B"</span>, <span class="number">25</span>)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    rdd.map(row =&gt; <span class="type">Row</span>.fromSeq(<span class="type">Seq</span>(row._1, row._2, row._3)))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来就可以运行测试例子了：</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">object</span> TestExample &#123;</span><br><span class="line"></span><br><span class="line">  def <span class="function">main</span>(args<span class="value">: Array[String]) &#123;</span><br><span class="line">    val spark = SparkSession.<span class="function">builder</span>().<span class="function">master</span>(<span class="string">"local"</span>).<span class="function">getOrCreate</span>()</span><br><span class="line">    val df = spark.read.<span class="function">format</span>(<span class="string">"com.zqh.spark.connectors.test.empty"</span>).<span class="function">load</span>()</span><br><span class="line">    df.<span class="function">printSchema</span>()</span><br><span class="line">    df.<span class="function">show</span>()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure>
<p>什么，只有40行代码，就实现了自定义的DataSource!!!</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"><span class="code"> |-- id: integer (nullable = true)</span></span><br><span class="line"><span class="code"> |-- name: string (nullable = true)</span></span><br><span class="line"><span class="code"> |-- age: integer (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="code">+---+</span>----<span class="code">+---+</span></span><br><span class="line"><span class="header">| id|name|age|</span><br><span class="line">+---+----+---+</span></span><br><span class="line"><span class="header">|  1|   A| 20|</span><br><span class="line">+---+----+---+</span></span><br></pre></td></tr></table></figure>
<p>上面示例EmptyRelation中，schema方法和buildScan方法有如下特点：</p>
<ul>
<li>schema定义了三个字段，则buildScan中每一行Row都必须有三个元素</li>
<li>RDD的每一行Row是数据，而schema对应了数据的元数据，schema可以任意指定</li>
</ul>
<p>总结下自定义数据源相关的类：</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">RelationProvider                  BaseRelation    TableScan</span><br><span class="line">       /|<span class="string">\                            /</span>|<span class="string">\            /</span>|<span class="string">\                   spark</span><br><span class="line">        </span>|<span class="string">                              </span>|<span class="string">              </span>|<span class="string">        ------------------</span><br><span class="line">        </span>|<span class="string">                              </span>|<span class="string">              </span>|<span class="string">                    user</span><br><span class="line">        </span>|<span class="string">                           schema()     buildScan()    </span><br><span class="line">DefaultSource                          </span>|<span class="string">              </span>|</span><br><span class="line">        |<span class="string">                              </span>|<span class="string">              </span>|</span><br><span class="line">        |<span class="string">                              </span>|<span class="string">              </span>|</span><br><span class="line">        ·                              |<span class="string">              </span>|</span><br><span class="line">createRelation()  --------------------&gt; EmptyRelation</span><br></pre></td></tr></table></figure>
<h2 id="JDBC_DataSource">JDBC DataSource</h2><p>jdbc数据源的定义类是：JdbcRelationProvider</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark数据源扩展与实践(40行代码实现一个自定义的DataSource)&lt;br&gt;
    
    </summary>
    
      <category term="spark" scheme="http://github.com/zqhxuyuan/categories/spark/"/>
    
    
      <category term="hadoop" scheme="http://github.com/zqhxuyuan/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>StreamingPro</title>
    <link href="http://github.com/zqhxuyuan/2017/09/04/2017-09-04-StreamingPro/"/>
    <id>http://github.com/zqhxuyuan/2017/09/04/2017-09-04-StreamingPro/</id>
    <published>2017-09-03T16:00:00.000Z</published>
    <updated>2017-09-05T13:14:53.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/allwefantasy/streamingpro/">https://github.com/allwefantasy/streamingpro/</a><br><a id="more"></a></p>
<p>单个Job的配置示例：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "<span class="attribute">you-first-streaming-job</span>": <span class="value">&#123;</span><br><span class="line">    "<span class="attribute">desc</span>": <span class="value"><span class="string">"just a example"</span></span>,</span><br><span class="line">    "<span class="attribute">strategy</span>": <span class="value"><span class="string">"spark"</span></span>,</span><br><span class="line">    "<span class="attribute">algorithm</span>": <span class="value">[]</span>,</span><br><span class="line">    "<span class="attribute">ref</span>": <span class="value">[</span><br><span class="line">    ]</span>,</span><br><span class="line">    "<span class="attribute">compositor</span>": <span class="value">[</span><br><span class="line">      &#123;</span><br><span class="line">        "<span class="attribute">name</span>": <span class="value"><span class="string">"stream.sources"</span></span>,</span><br><span class="line">        "<span class="attribute">params</span>": <span class="value">[</span><br><span class="line">          &#123;</span><br><span class="line">            "<span class="attribute">format</span>": <span class="value"><span class="string">"socket"</span></span>,</span><br><span class="line">            "<span class="attribute">outputTable</span>": <span class="value"><span class="string">"test"</span></span>,</span><br><span class="line">            "<span class="attribute">port</span>": <span class="value"><span class="string">"9999"</span></span>,</span><br><span class="line">            "<span class="attribute">host</span>": <span class="value"><span class="string">"localhost"</span></span>,</span><br><span class="line">            "<span class="attribute">path</span>": <span class="value"><span class="string">"-"</span></span><br><span class="line">          </span>&#125;</span><br><span class="line">        ]</span><br><span class="line">      </span>&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        "<span class="attribute">name</span>": <span class="value"><span class="string">"stream.sql"</span></span>,</span><br><span class="line">        "<span class="attribute">params</span>": <span class="value">[</span><br><span class="line">          &#123;</span><br><span class="line">            "<span class="attribute">sql</span>": <span class="value"><span class="string">"select avg(value) avgAge from test"</span></span>,</span><br><span class="line">            "<span class="attribute">outputTableName</span>": <span class="value"><span class="string">"test3"</span></span><br><span class="line">          </span>&#125;</span><br><span class="line">        ]</span><br><span class="line">      </span>&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        "<span class="attribute">name</span>": <span class="value"><span class="string">"stream.sql"</span></span>,</span><br><span class="line">        "<span class="attribute">params</span>": <span class="value">[</span><br><span class="line">          &#123;</span><br><span class="line">            "<span class="attribute">sql</span>": <span class="value"><span class="string">"select count(value) as nameCount from test"</span></span>,</span><br><span class="line">            "<span class="attribute">outputTableName</span>": <span class="value"><span class="string">"test1"</span></span><br><span class="line">          </span>&#125;</span><br><span class="line">        ]</span><br><span class="line">      </span>&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        "<span class="attribute">name</span>": <span class="value"><span class="string">"stream.sql"</span></span>,</span><br><span class="line">        "<span class="attribute">params</span>": <span class="value">[</span><br><span class="line">          &#123;</span><br><span class="line">            "<span class="attribute">sql</span>": <span class="value"><span class="string">"select sum(value) ageSum from test"</span></span>,</span><br><span class="line">            "<span class="attribute">outputTableName</span>": <span class="value"><span class="string">"test2"</span></span><br><span class="line">          </span>&#125;</span><br><span class="line">        ]</span><br><span class="line">      </span>&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        "<span class="attribute">name</span>": <span class="value"><span class="string">"stream.sql"</span></span>,</span><br><span class="line">        "<span class="attribute">params</span>": <span class="value">[</span><br><span class="line">          &#123;</span><br><span class="line">            "<span class="attribute">sql</span>": <span class="value"><span class="string">"select * from test1 union select * from test2 union select * from test3"</span></span>,</span><br><span class="line">            "<span class="attribute">outputTableName</span>": <span class="value"><span class="string">"test4"</span></span><br><span class="line">          </span>&#125;</span><br><span class="line">        ]</span><br><span class="line">      </span>&#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        "<span class="attribute">name</span>": <span class="value"><span class="string">"stream.outputs"</span></span>,</span><br><span class="line">        "<span class="attribute">params</span>": <span class="value">[</span><br><span class="line">          &#123;</span><br><span class="line">            "<span class="attribute">name</span>": <span class="value"><span class="string">"jack"</span></span>,</span><br><span class="line">            "<span class="attribute">format</span>": <span class="value"><span class="string">"console"</span></span>,</span><br><span class="line">            "<span class="attribute">path</span>": <span class="value"><span class="string">"-"</span></span>,</span><br><span class="line">            "<span class="attribute">inputTableName</span>": <span class="value"><span class="string">"test4"</span></span>,</span><br><span class="line">            "<span class="attribute">mode</span>": <span class="value"><span class="string">"Overwrite"</span></span><br><span class="line">          </span>&#125;</span><br><span class="line">        ]</span><br><span class="line">      </span>&#125;</span><br><span class="line">    ]</span>,</span><br><span class="line">    "<span class="attribute">configParams</span>": <span class="value">&#123;</span><br><span class="line">    &#125;</span><br><span class="line">  </span>&#125;</span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>多个Job的配置示例：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   "<span class="attribute">you-first-streaming-job</span>": <span class="value">&#123;</span><br><span class="line">     "<span class="attribute">desc</span>": <span class="value"><span class="string">"just a example"</span></span>,</span><br><span class="line">     "<span class="attribute">strategy</span>": <span class="value"><span class="string">"spark"</span></span>,</span><br><span class="line">     "<span class="attribute">algorithm</span>": <span class="value">[]</span>,</span><br><span class="line">     "<span class="attribute">ref</span>": <span class="value">[</span><br><span class="line">     ]</span>,</span><br><span class="line">     "<span class="attribute">compositor</span>": <span class="value">[</span><br><span class="line">     ]</span>,</span><br><span class="line">     "<span class="attribute">configParams</span>": <span class="value">&#123;</span><br><span class="line">     &#125;</span><br><span class="line">   </span>&#125;</span>,</span><br><span class="line">   "<span class="attribute">you-second-streaming-job</span>": <span class="value">&#123;</span><br><span class="line">        "<span class="attribute">desc</span>": <span class="value"><span class="string">"just a example"</span></span>,</span><br><span class="line">        "<span class="attribute">strategy</span>": <span class="value"><span class="string">"spark"</span></span>,</span><br><span class="line">        "<span class="attribute">algorithm</span>": <span class="value">[]</span>,</span><br><span class="line">        "<span class="attribute">ref</span>": <span class="value">[</span><br><span class="line">        ]</span>,</span><br><span class="line">        "<span class="attribute">compositor</span>": <span class="value">[</span><br><span class="line">        ]</span>,</span><br><span class="line">        "<span class="attribute">configParams</span>": <span class="value">&#123;</span><br><span class="line">        &#125;</span><br><span class="line">      </span>&#125;</span><br><span class="line"> </span>&#125;</span><br></pre></td></tr></table></figure>
<p>StreamingPro支持Spark、SparkStreaming、SparkStruncture、Flink。入口类都是统一的<code>StreamingApp</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamingApp</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span>(</span>args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> params = <span class="keyword">new</span> <span class="type">ParamsUtil</span>(args)</span><br><span class="line">    require(params.hasParam(<span class="string">"streaming.name"</span>), <span class="string">"Application name should be set"</span>)</span><br><span class="line">    <span class="type">PlatformManager</span>.getOrCreate.run(params)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过streaming.platform可以指定不同的运行平台。当然，不同的运行引擎的jar包也不同。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">SHome=/Users/allwefantasy/streamingpro</span><br><span class="line"></span><br><span class="line">./bin/spark-submit   --class streaming.core.StreamingApp \</span><br><span class="line">--master <span class="built_in">local</span>[<span class="number">2</span>] \</span><br><span class="line">--name <span class="built_in">test</span> \</span><br><span class="line"><span class="variable">$SHome</span>/streamingpro-spark-<span class="number">2.0</span>-<span class="number">0.4</span>.<span class="number">15</span>-SNAPSHOT.jar    \</span><br><span class="line">-streaming.name <span class="built_in">test</span>    \</span><br><span class="line">-streaming.platform spark_streaming \</span><br><span class="line">-streaming.job.file.path file://<span class="variable">$SHome</span>/spark-streaming.json</span><br><span class="line"></span><br><span class="line">bin/flink run -c streaming.core.StreamingApp \ </span><br><span class="line">/Users/allwefantasy/streamingpro/streamingpro.flink-<span class="number">0.4</span>.<span class="number">14</span>-SNAPSHOT-online-<span class="number">1.2</span>.<span class="number">0</span>.jar \</span><br><span class="line">-streaming.name god \</span><br><span class="line">-streaming.platform flink_streaming \</span><br><span class="line">-streaming.job.file.path file:///Users/allwefantasy/streamingpro/flink.json</span><br></pre></td></tr></table></figure>
<p>jar包会被用来加载不同的Runtime。Runtime运行的映射关系定义在<code>PlatformManager</code>的<code>platformNameMapping</code>变量中。<br>Runtime是一个接口，最主要的是startRuntime方法和params方法。后面我们把Runtime叫做<strong>执行引擎</strong>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">StreamingRuntime</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">startRuntime</span>:</span> <span class="type">StreamingRuntime</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">destroyRuntime</span>(</span>stopGraceful: <span class="type">Boolean</span>, stopContext: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">Boolean</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">streamingRuntimeInfo</span>:</span> <span class="type">StreamingRuntimeInfo</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">resetRuntimeOperator</span>(</span>runtimeOperator: <span class="type">RuntimeOperator</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configureStreamingRuntimeInfo</span>(</span>streamingRuntimeInfo: <span class="type">StreamingRuntimeInfo</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">awaitTermination</span></span><br><span class="line"></span>  <span class="function"><span class="keyword">def</span> <span class="title">startThriftServer</span></span><br><span class="line"></span>  <span class="function"><span class="keyword">def</span> <span class="title">startHttpServer</span></span><br><span class="line"></span>  <span class="function"><span class="keyword">def</span> <span class="title">params</span>:</span> <span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StreamingPro本质上还是通过spark-submit运行。框架的整体运行流程在<code>PlatformManager</code>的<code>run</code>方法中。主要的步骤有：</p>
<ol>
<li>设置配置信息</li>
<li>根据反射机制，创建并获取运行时环境</li>
<li>获取dispatcher以及所有的strategies</li>
<li>启动REST服务、Thrift服务、注册ZK（可选）</li>
<li>启动执行引擎，并等待作业完成</li>
</ol>
<blockquote>
<p>关于Dispatcher、Strategy的概念，参考作者的ServiceframeworkDispatcher项目。<br>反射创建执行引擎，调用的是对应Object类的getOrCreate方法，并传入params参数，最后实例化为StreamingRuntime。</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">platformNameMapping</span> =</span> <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">  <span class="type">SPAKR_S_S</span> -&gt; <span class="string">"streaming.core.strategy.platform.SparkStructuredStreamingRuntime"</span>,</span><br><span class="line">  <span class="type">SPAKR_STRUCTURED_STREAMING</span> -&gt; <span class="string">"streaming.core.strategy.platform.SparkStructuredStreamingRuntime"</span>,</span><br><span class="line">  <span class="type">FLINK_STREAMING</span> -&gt; <span class="string">"streaming.core.strategy.platform.FlinkStreamingRuntime"</span>,</span><br><span class="line">  <span class="type">SPAKR_STREAMING</span> -&gt; <span class="string">"streaming.core.strategy.platform.SparkStreamingRuntime"</span>,</span><br><span class="line">  <span class="type">SPARK</span> -&gt; <span class="string">"streaming.core.strategy.platform.SparkRuntime"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>注意：StreamingPro的Runtime只是Spark作业的执行引擎，具体根据配置文件加载策略是ServiceframeworkDispatcher的工作。<br>假设我们定义了下面的一个配置文件，由于采用了shortName，需要定义一个ShortNameMapping</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"convert-multi-csv-to-json"</span>: &#123;</span><br><span class="line">    <span class="string">"desc"</span>: <span class="string">"测试"</span>,</span><br><span class="line">    <span class="string">"strategy"</span>: <span class="string">"spark"</span>,</span><br><span class="line">    <span class="string">"algorithm"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"testProcessor"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"ref"</span>: [],</span><br><span class="line">    <span class="string">"compositor"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"testCompositor"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"configParams"</span>: &#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>DefaultShortNameMapping的定义如下。这样配置文件中的spark就和ServiceframeworkDispatcher的加载过程对应起来了。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultShortNameMapping</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">ShortNameMapping</span> &#123;</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> compositorNameMap: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">    <span class="string">"spark"</span> -&gt; <span class="string">"serviceframework.dispatcher.test.DefaultStrategy"</span>,</span><br><span class="line">    <span class="string">"testProcessor"</span> -&gt; <span class="string">"serviceframework.dispatcher.test.TestProcessor"</span>,</span><br><span class="line">    <span class="string">"testCompositor"</span> -&gt; <span class="string">"serviceframework.dispatcher.test.TestCompositor"</span></span><br><span class="line">  )</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">forName</span>(</span>shortName: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (compositorNameMap.contains(shortName)) compositorNameMap(shortName)</span><br><span class="line">    <span class="keyword">else</span> shortName</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ServiceframeworkDispatcher的核心是StrategyDispatcher，这个类在创建的时候，会读取配置文件。<br>然后解析配置文件中的strategy、algorithm(processor)、ref、compositor、configParams等配置项，并构造对应的对象。<br>ServiceframeworkDispatcher是一个模块组合框架，它主要定义了Compositor、Processor、Strategy三个接口。</p>
<p>Strategy接口包含了processor、ref、compositor，以及初始化和result方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Strategy</span>[</span><span class="type">T</span>] <span class="keyword">extends</span> <span class="type">ServiceInj</span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">processor</span>:</span><span class="type">JList</span>[<span class="type">Processor</span>[<span class="type">T</span>]]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">ref</span>:</span><span class="type">JList</span>[<span class="type">Strategy</span>[<span class="type">T</span>]]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compositor</span>:</span><span class="type">JList</span>[<span class="type">Compositor</span>[<span class="type">T</span>]]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">name</span>:</span><span class="type">String</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span>(</span>name:<span class="type">String</span>,alg:<span class="type">JList</span>[<span class="type">Processor</span>[<span class="type">T</span>]],ref:<span class="type">JList</span>[<span class="type">Strategy</span>[<span class="type">T</span>]],com:<span class="type">JList</span>[<span class="type">Compositor</span>[<span class="type">T</span>]],params:<span class="type">JMap</span>[<span class="type">Any</span>,<span class="type">Any</span>])</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">result</span>(</span>params:<span class="type">JMap</span>[<span class="type">Any</span>,<span class="type">Any</span>]):<span class="type">JList</span>[<span class="type">T</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configParams</span>:</span>util.<span class="type">Map</span>[<span class="type">Any</span>, <span class="type">Any</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span> =</span> &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Strategy策略的初始化需要算法、引用、组合器，以及配置信息，对应的方法是StrategyDispatcher的createStrategy方法。</p>
<p>注意下面的initialize方法，createAlgorithms和createCompositors初始化时<br>会读取params配置，这是一个嵌套了Map的列表：<code>JList[JMap[String, Any]]</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createStrategy</span>(</span>name: <span class="type">String</span>, desc: <span class="type">JMap</span>[_, _]): <span class="type">Option</span>[<span class="type">Strategy</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (_strategies.contains(name)) <span class="keyword">return</span> <span class="type">None</span>;</span><br><span class="line">  <span class="comment">// 实例化策略，如果有shortName，则先获取fullName，并通过Class.forName实例化具体的策略类</span></span><br><span class="line">  <span class="keyword">val</span> strategy = <span class="type">Class</span>.forName(shortNameMapping.forName(desc.get(<span class="string">"strategy"</span>).asInstanceOf[<span class="type">String</span>])).newInstance().asInstanceOf[<span class="type">Strategy</span>[<span class="type">T</span>]]</span><br><span class="line">  <span class="comment">// 读取配置信息，并实例化为Map[Any,Any]</span></span><br><span class="line">  <span class="keyword">val</span> configParams: <span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>] = <span class="keyword">if</span> (desc.containsKey(<span class="string">"configParams"</span>)) desc.get(<span class="string">"configParams"</span>).asInstanceOf[<span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>]] <span class="keyword">else</span> <span class="keyword">new</span> java.util.<span class="type">HashMap</span>()</span><br><span class="line">  <span class="comment">// 初始化策略，需要创建算法、引用、组合器</span></span><br><span class="line">  strategy.initialize(name, createAlgorithms(desc), createRefs(desc), createCompositors(desc), configParams)</span><br><span class="line">  _strategies.put(name, strategy)</span><br><span class="line">  <span class="type">Option</span>(strategy)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建算法。一个策略由0个或者多个算法提供结果</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createAlgorithms</span>(</span>jobJMap: <span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]): <span class="type">JList</span>[<span class="type">Processor</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (!jobJMap.contains(<span class="string">"algorithm"</span>) &amp;&amp; !jobJMap.contains(<span class="string">"processor"</span>)) <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">AList</span>[<span class="type">Processor</span>[<span class="type">T</span>]]()</span><br><span class="line">  <span class="keyword">val</span> processors = <span class="keyword">if</span> (jobJMap.contains(<span class="string">"algorithm"</span>)) jobJMap(<span class="string">"algorithm"</span>) <span class="keyword">else</span> jobJMap(<span class="string">"processor"</span>)</span><br><span class="line">  processors.asInstanceOf[<span class="type">JList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]]].map &#123;</span><br><span class="line">    alg =&gt;</span><br><span class="line">      <span class="keyword">val</span> name = shortName2FullName(alg)</span><br><span class="line">      <span class="keyword">val</span> processor = <span class="type">Class</span>.forName(name).newInstance().asInstanceOf[<span class="type">Processor</span>[<span class="type">T</span>]]</span><br><span class="line">      <span class="keyword">val</span> params: <span class="type">JList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]] = <span class="keyword">if</span> (alg.contains(<span class="string">"params"</span>)) alg(<span class="string">"params"</span>).asInstanceOf[<span class="type">JList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]]] <span class="keyword">else</span> <span class="keyword">new</span> <span class="type">AList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]]()</span><br><span class="line">      processor.initialize(name, params)</span><br><span class="line">      processor</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建组合器，可以多个，按顺序调用。有点类似过滤器链。第一个过滤器会接受算法或者策略的结果。后续的组合器就只能处理上一阶段的组合器吐出的结果</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createCompositors</span>(</span>jobJMap: <span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]): <span class="type">JList</span>[<span class="type">Compositor</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (!jobJMap.contains(<span class="string">"compositor"</span>)) <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">AList</span>()</span><br><span class="line">  <span class="keyword">val</span> compositors = jobJMap.get(<span class="string">"compositor"</span>)</span><br><span class="line">  compositors.asInstanceOf[<span class="type">JList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]]].map &#123;</span><br><span class="line">    f =&gt;</span><br><span class="line">      <span class="keyword">val</span> compositor = <span class="type">Class</span>.forName(shortName2FullName(f)).newInstance().asInstanceOf[<span class="type">Compositor</span>[<span class="type">T</span>]]</span><br><span class="line">      <span class="keyword">val</span> params: <span class="type">JList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]] = <span class="keyword">if</span> (f.contains(<span class="string">"params"</span>)) f.get(<span class="string">"params"</span>).asInstanceOf[<span class="type">JList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]]] <span class="keyword">else</span> <span class="keyword">new</span> <span class="type">AList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]]()</span><br><span class="line">      compositor.initialize(f.get(<span class="string">"typeFilter"</span>).asInstanceOf[<span class="type">JList</span>[<span class="type">String</span>]], params)</span><br><span class="line">      compositor</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ServiceframeworkDispatcher的核心是StrategyDispatcher，而StrategyDispatcher的核心是其dispatch方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dispatch</span>(</span>params: <span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>]): <span class="type">JList</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  findStrategies(clientType) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(strategies) =&gt;</span><br><span class="line">      strategies.flatMap &#123; f =&gt; f.result(params) &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>不同执行引擎的启动方法实现不同：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkRuntime</span>(</span>_params: <span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>]) <span class="keyword">extends</span> <span class="type">StreamingRuntime</span> <span class="keyword">with</span> <span class="type">PlatformManagerListener</span> &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">startRuntime</span>:</span> <span class="type">StreamingRuntime</span> = <span class="keyword">this</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> sparkSession: <span class="type">SparkSession</span> = createRuntime</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createRuntime</span> =</span> &#123;</span><br><span class="line">    <span class="comment">//...创建SparkSession，这里会根据参数判断是否支持Hive、Carbondata</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  params.put(<span class="string">"_session_"</span>, sparkSession) <span class="comment">//将SparkSession放入params中</span></span><br><span class="line">  registerUDF  </span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">params</span>:</span> <span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>] = _params</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingRuntime</span>(</span>_params: <span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>]) <span class="keyword">extends</span> <span class="type">StreamingRuntime</span> <span class="keyword">with</span> <span class="type">PlatformManagerListener</span> &#123; self =&gt;</span><br><span class="line">  <span class="keyword">var</span> streamingContext: <span class="type">StreamingContext</span> = createRuntime</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createRuntime</span> =</span> &#123;</span><br><span class="line">    <span class="comment">//创建StreamingContext，并将SparkSession放入params中</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">startRuntime</span> =</span> &#123;</span><br><span class="line">    streamingContext.start()</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">awaitTermination</span> =</span> streamingContext.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但真正执行StreamingPro主流程在streamingpro-commons下的SparkStreamingStrategy类。<br>注意：如果是spark-1.6，则streamingpro-spark下也有一个SparkStreamingStrategy类。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingStrategy</span>[</span><span class="type">T</span>] <span class="keyword">extends</span> <span class="type">Strategy</span>[<span class="type">T</span>] <span class="keyword">with</span> <span class="type">DebugTrait</span> <span class="keyword">with</span> <span class="type">JobStrategy</span> &#123;</span><br><span class="line">  <span class="keyword">var</span> _ref: util.<span class="type">List</span>[<span class="type">Strategy</span>[<span class="type">T</span>]] = _</span><br><span class="line">  <span class="keyword">var</span> _compositor: util.<span class="type">List</span>[<span class="type">Compositor</span>[<span class="type">T</span>]] = _</span><br><span class="line">  <span class="keyword">var</span> _processor: util.<span class="type">List</span>[<span class="type">Processor</span>[<span class="type">T</span>]] = _</span><br><span class="line">  <span class="keyword">var</span> _configParams: util.<span class="type">Map</span>[<span class="type">Any</span>, <span class="type">Any</span>] = _</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">result</span>(</span>params: util.<span class="type">Map</span>[<span class="type">Any</span>, <span class="type">Any</span>]): util.<span class="type">List</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    ref.foreach &#123; r =&gt; r.result(params) &#125; <span class="comment">// 先执行ref</span></span><br><span class="line">    <span class="keyword">if</span> (compositor != <span class="literal">null</span> &amp;&amp; compositor.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// 第一个Compositor, 产生第一个中间结果</span></span><br><span class="line">      <span class="keyword">var</span> middleR = compositor.get(<span class="number">0</span>).result(processor, ref, <span class="literal">null</span>, params)</span><br><span class="line">      <span class="comment">// 将新的中间结果运用到下一个Compositor</span></span><br><span class="line">      <span class="comment">// 第一个Compositor的结果运用到第二个的输入, 第二个Compositor的结果运用到第三个Compositor的输入...</span></span><br><span class="line">      <span class="comment">// 所以不同Compositor是链式执行的</span></span><br><span class="line">      <span class="keyword">for</span> (i &lt;- <span class="number">1</span> until compositor.size()) &#123;</span><br><span class="line">        middleR = compositor.get(i).result(processor, ref, middleR, params)</span><br><span class="line">      &#125;</span><br><span class="line">      middleR</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">T</span>]()</span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意：配置文件中每个Job都有一个<code>strategy</code>级别的<code>configParams</code>，<code>ref</code>也会使用这个全局的<code>configParams</code>。<br>它是一个<code>Map[String, Any]</code>的结构。每个Compositor和Processor内部也有一个<code>params</code>配置，这是一个数组。</p>
<blockquote>
<p>实际上，全局的<code>configParams</code>参数会被用在Strategy、Ref/Processor和Compositor的result()方法的最后一个参数。</p>
</blockquote>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"compositor"</span>: [</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"testCompositor"</span>,</span><br><span class="line">    <span class="string">"params"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"sql"</span>: <span class="string">"select avg(value) avgAge from test"</span>,</span><br><span class="line">        <span class="string">"outputTableName"</span>: <span class="string">"test3"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"sql"</span>: <span class="string">"select sum(value) sumAge from test"</span>,</span><br><span class="line">        <span class="string">"outputTableName"</span>: <span class="string">"test4"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">],</span><br></pre></td></tr></table></figure>
<p>接下来以读取多个数据源的Compositor实现类为例：</p>
<ul>
<li><code>_configParams</code>是在创建Compositor时初始化调用的，这是一个<code>List[Map[String, Any]]</code>的结构，对应了<code>params</code>列表配置</li>
<li>如果需要替换，则会先处理配置信息</li>
<li>接着，从params中获取SparkSession（还记得之前创建Runtime时放入Map中吗？），</li>
<li>然后，执行sparkSession.read.format(xx).options(Map).load(path)</li>
<li>最后，通过df.createOrReplaceTempView创建Spark SQL的临时表，名称为<code>outputTable</code></li>
</ul>
<figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiSQLSourceCompositor</span>[<span class="title">T</span>] <span class="keyword">extends</span> <span class="title">Compositor</span>[<span class="title">T</span>] <span class="title">with</span> <span class="title">CompositorHelper</span> </span>&#123;</span><br><span class="line">  private <span class="keyword">var</span> _configParams: util.<span class="built_in">List</span>[util.<span class="built_in">Map</span>[Any, Any]] = _</span><br><span class="line"></span><br><span class="line">  override def initialize(typeFilters: util.<span class="built_in">List</span>[<span class="built_in">String</span>], configParams: util.<span class="built_in">List</span>[util.<span class="built_in">Map</span>[Any, Any]]): Unit = &#123;</span><br><span class="line">    <span class="keyword">this</span>._configParams = configParams</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def result(alg: util.<span class="built_in">List</span>[Processor[T]], ref: util.<span class="built_in">List</span>[Strategy[T]], middleResult: util.<span class="built_in">List</span>[T], params: util.<span class="built_in">Map</span>[Any, Any]): util.<span class="built_in">List</span>[T] = &#123;</span><br><span class="line"></span><br><span class="line">    _configParams.foreach &#123; sourceConfig =&gt;</span><br><span class="line">      val name = sourceConfig.getOrElse(<span class="string">"name"</span>, <span class="string">""</span>).toString</span><br><span class="line"></span><br><span class="line">      val _cfg = sourceConfig.map(f =&gt; (f._1.toString, f._2.toString)).map &#123; f =&gt;</span><br><span class="line">        (f._1, params.getOrElse(s<span class="string">"streaming.sql.source.<span class="subst">$&#123;name&#125;</span>.<span class="subst">$&#123;f._1&#125;</span>"</span>, f._2).toString)</span><br><span class="line">      &#125;.toMap</span><br><span class="line"></span><br><span class="line">      val sourcePath = _cfg(<span class="string">"path"</span>)</span><br><span class="line">      val df = sparkSession(params).read.format(sourceConfig(<span class="string">"format"</span>).toString).options(</span><br><span class="line">        (_cfg - <span class="string">"format"</span> - <span class="string">"path"</span> - <span class="string">"outputTable"</span>).map(f =&gt; (f._1.toString, f._2.toString))).load(sourcePath)</span><br><span class="line">      df.createOrReplaceTempView(_cfg.getOrElse(<span class="string">"outputTable"</span>, _cfg.getOrElse(<span class="string">"outputTableName"</span>, <span class="string">""</span>)))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">List</span>()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>为了支持配置的动态替换，<code>_cfg</code>参数会做一些处理，比如上面的<code>s&quot;streaming.sql.source.${name}.${f._1}&quot;</code>如果需要被替换，则会被替换为<code>f._2</code>。<br>下表列举了StreamingPro支持的几种替换方式。</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>配置示例</th>
<th>动态传参数</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>streaming.sql.source.[name].[参数]</code></td>
<td>“path”: “file:///tmp/sample_article.txt”</td>
<td>-streaming.sql.source.firstSource.path  file:///tmp/wow.txt</td>
</tr>
<tr>
<td><code>streaming.sql.out.[name].[参数]</code></td>
<td>“path”: “file:///tmp/sample_article.txt”</td>
<td>-streaming.sql.source.firstSink.path  file:///tmp/wow_20170101.txt</td>
</tr>
<tr>
<td><code>streaming.sql.params.[param-name]</code></td>
<td>“sql”: “select * from test where hp_time=:today”</td>
<td>-streaming.sql.params.today “20170101”</td>
</tr>
</tbody>
</table>
<p>假设有两个数据输入源和一个输出目标的配置如下：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"batch.sources"</span>,</span><br><span class="line">  <span class="string">"params"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>:<span class="string">"firstSource"</span>,</span><br><span class="line">      <span class="string">"path"</span>: <span class="string">"file:///tmp/sample_article.txt"</span>,</span><br><span class="line">      <span class="string">"format"</span>: <span class="string">"com.databricks.spark.csv"</span>,</span><br><span class="line">      <span class="string">"outputTable"</span>: <span class="string">"article"</span>,</span><br><span class="line">      <span class="string">"header"</span>:true</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">"name"</span>:<span class="string">"secondSource"</span>,</span><br><span class="line">        <span class="string">"path"</span>: <span class="string">"file:///tmp/sample_article2.txt"</span>,</span><br><span class="line">        <span class="string">"format"</span>: <span class="string">"com.databricks.spark.csv"</span>,</span><br><span class="line">        <span class="string">"outputTable"</span>: <span class="string">"article2"</span>,</span><br><span class="line">        <span class="string">"header"</span>:true</span><br><span class="line">      &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"batch.outputs"</span>,</span><br><span class="line">  <span class="string">"params"</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"name"</span>:<span class="string">"firstSink"</span>,</span><br><span class="line">      <span class="string">"path"</span>: <span class="string">"file:///tmp/sample_article.txt"</span>,</span><br><span class="line">      <span class="string">"format"</span>: <span class="string">"com.databricks.spark.csv"</span>,</span><br><span class="line">      <span class="string">"outputTable"</span>: <span class="string">"article"</span>,</span><br><span class="line">      <span class="string">"header"</span>:true</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Source的功能是：读取输入源形成DataFrame，然后创建临时表。其他组件比如SQL也是类似的。至此StreamingPro的大致流程就分析完了。 </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/allwefantasy/streamingpro/&quot;&gt;https://github.com/allwefantasy/streamingpro/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/categories/bigdata/"/>
    
    
      <category term="spark" scheme="http://github.com/zqhxuyuan/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Akka并发图解</title>
    <link href="http://github.com/zqhxuyuan/2017/08/16/Akka-Concurrenty/"/>
    <id>http://github.com/zqhxuyuan/2017/08/16/Akka-Concurrenty/</id>
    <published>2017-08-15T16:00:00.000Z</published>
    <updated>2017-08-16T08:04:31.000Z</updated>
    
    <content type="html"><![CDATA[<p>Akka并发图解（Akka Concurrenty）：<br><a id="more"></a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Akka并发图解（Akka Concurrenty）：&lt;br&gt;
    
    </summary>
    
      <category term="akka" scheme="http://github.com/zqhxuyuan/categories/akka/"/>
    
    
      <category term="akka" scheme="http://github.com/zqhxuyuan/tags/akka/"/>
    
  </entry>
  
  <entry>
    <title>SnappyData</title>
    <link href="http://github.com/zqhxuyuan/2017/07/13/SnappyData-In-Action/"/>
    <id>http://github.com/zqhxuyuan/2017/07/13/SnappyData-In-Action/</id>
    <published>2017-07-12T16:00:00.000Z</published>
    <updated>2017-09-14T08:21:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>SnappyData®</p>
<a id="more"></a>
<h2 id="SnappyData">SnappyData</h2><h3 id="开发模式">开发模式</h3><p>由于下载的snappydata已经带了spark，所以不需要使用–packges</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cd snappydata-<span class="number">0.9</span>-bin</span><br><span class="line">$ bin/spark-shell --driver-memory=<span class="number">4</span>g \</span><br><span class="line">  --conf spark<span class="class">.snappydata</span><span class="class">.store</span><span class="class">.sys-disk-dir</span>=quickstartdatadir \</span><br><span class="line">  --conf spark<span class="class">.snappydata</span><span class="class">.store</span><span class="class">.log-file</span>=quickstartdatadir/quickstart<span class="class">.log</span> \</span><br><span class="line">  --driver-java-options=<span class="string">"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g"</span></span><br><span class="line">Spark context Web UI available at http:<span class="comment">//192.168.6.52:4042</span></span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
<p>执行CRUD操作：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">val snappy = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.SnappySession</span>(spark.sparkContext)</span><br><span class="line">import snappy<span class="class">.implicits</span>._</span><br><span class="line">import org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.types</span>._</span><br><span class="line">import org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.Row</span></span><br><span class="line"></span><br><span class="line">val ds = <span class="function"><span class="title">Seq</span><span class="params">((<span class="number">1</span>,<span class="string">"a"</span>)</span></span>, (<span class="number">2</span>, <span class="string">"b"</span>), (<span class="number">3</span>, <span class="string">"c"</span>)).<span class="function"><span class="title">toDS</span><span class="params">()</span></span></span><br><span class="line">val tableSchema = <span class="function"><span class="title">StructType</span><span class="params">(Array(StructField(<span class="string">"CustKey"</span>, IntegerType, false)</span></span>,<span class="function"><span class="title">StructField</span><span class="params">(<span class="string">"CustName"</span>, StringType, false)</span></span>))</span><br><span class="line"></span><br><span class="line">snappy.<span class="function"><span class="title">createTable</span><span class="params">(tableName = <span class="string">"colTable"</span>, provider = <span class="string">"column"</span>, schema = tableSchema, options = Map.empty[String, String], allowExisting = false)</span></span></span><br><span class="line">snappy.<span class="function"><span class="title">createTable</span><span class="params">(tableName = <span class="string">"rowTable"</span>, provider = <span class="string">"row"</span>, schema = tableSchema, options = Map.empty[String, String], allowExisting = false)</span></span></span><br><span class="line"></span><br><span class="line">ds<span class="class">.write</span><span class="class">.insertInto</span>(<span class="string">"colTable"</span>)</span><br><span class="line">ds<span class="class">.write</span><span class="class">.insertInto</span>(<span class="string">"rowTable"</span>)</span><br><span class="line"></span><br><span class="line">snappy.<span class="function"><span class="title">insert</span><span class="params">(<span class="string">"colTable"</span>, Row(<span class="number">10</span>, <span class="string">"f"</span>)</span></span>)</span><br><span class="line">snappy.<span class="function"><span class="title">insert</span><span class="params">(<span class="string">"rowTable"</span>, Row(<span class="number">4</span>, <span class="string">"d"</span>)</span></span>)</span><br><span class="line"></span><br><span class="line">snappy.<span class="function"><span class="title">table</span><span class="params">(<span class="string">"colTable"</span>)</span></span><span class="class">.count</span></span><br><span class="line">snappy.<span class="function"><span class="title">table</span><span class="params">(<span class="string">"colTable"</span>)</span></span>.<span class="function"><span class="title">orderBy</span><span class="params">(<span class="string">"CustKey"</span>)</span></span><span class="class">.show</span></span><br><span class="line">snappy.<span class="function"><span class="title">table</span><span class="params">(<span class="string">"rowTable"</span>)</span></span><span class="class">.count</span></span><br><span class="line">snappy.<span class="function"><span class="title">table</span><span class="params">(<span class="string">"rowTable"</span>)</span></span>.<span class="function"><span class="title">orderBy</span><span class="params">(<span class="string">"CUSTKEY"</span>)</span></span><span class="class">.show</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// update and delete on row table. current version did't support update and delete on column table.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// update rowTable set custname='d' where custkey=1</span></span><br><span class="line">snappy.<span class="function"><span class="title">update</span><span class="params">(tableName = <span class="string">"rowTable"</span>, filterExpr = <span class="string">"CUSTKEY=1"</span>, newColumnValues = Row(<span class="string">"d"</span>)</span></span>, updateColumns = <span class="string">"CUSTNAME"</span>)</span><br><span class="line">snappy.<span class="function"><span class="title">table</span><span class="params">(<span class="string">"rowTable"</span>)</span></span>.<span class="function"><span class="title">orderBy</span><span class="params">(<span class="string">"CUSTKEY"</span>)</span></span><span class="class">.show</span></span><br><span class="line"><span class="comment">// delete rowTable where custkey=1</span></span><br><span class="line">snappy.<span class="function"><span class="title">delete</span><span class="params">(tableName = <span class="string">"rowTable"</span>, filterExpr = <span class="string">"CUSTKEY=1"</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>打开<a href="http://192.168.6.52:4042/dashboard/" target="_blank" rel="external">http://192.168.6.52:4042/dashboard/</a>，查看web-ui的dashboard页面</p>
<p><img src="http://img.blog.csdn.net/20170714115456658" alt="snappy"></p>
<p>查看quickstartdir,索引采用GF(GemFire)</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ tree quickstartdatadir/</span><br><span class="line">quickstartdatadir/</span><br><span class="line">├── BACKUPGFXD-DEFAULT-DISKSTORE_1<span class="class">.crf</span></span><br><span class="line">├── BACKUPGFXD-DEFAULT-DISKSTORE_1<span class="class">.drf</span></span><br><span class="line">├── BACKUPGFXD-DEFAULT-DISKSTORE<span class="class">.if</span></span><br><span class="line">├── datadictionary</span><br><span class="line">│   ├── BACKUPGFXD-DD-DISKSTORE_1<span class="class">.crf</span></span><br><span class="line">│   ├── BACKUPGFXD-DD-DISKSTORE_1<span class="class">.drf</span></span><br><span class="line">│   ├── BACKUPGFXD-DD-DISKSTORE<span class="class">.if</span></span><br><span class="line">│   └── DRLK_IFGFXD-DD-DISKSTORE<span class="class">.lk</span></span><br><span class="line">├── DRLK_IFGFXD-DEFAULT-DISKSTORE<span class="class">.lk</span></span><br><span class="line">├── gemfirexdtemp_1015622261<span class="class">.d</span></span><br><span class="line">└── quickstart.log</span><br></pre></td></tr></table></figure>
<p>简单的性能测试：</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def <span class="function">benchmark</span>(name<span class="value">: String, times: Int = <span class="number">10</span>, warmups: Int = <span class="number">6</span>)(f: =&gt; Unit) &#123;</span><br><span class="line">  for (i &lt;- <span class="number">1</span> to warmups) &#123;</span><br><span class="line">    f</span><br><span class="line">  &#125;</span><br><span class="line">  val startTime = System.nanoTime</span><br><span class="line">  for (i &lt;- <span class="number">1</span> to times) &#123;</span><br><span class="line">    f</span><br><span class="line">  &#125;</span><br><span class="line">  val endTime = System.nanoTime</span><br><span class="line">  <span class="function">println</span>(s<span class="string">"Average time taken in $name for $times runs: "</span> +</span><br><span class="line">    (endTime - startTime).toDouble / (times * <span class="number">1000000.0</span>) + <span class="string">" millis"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val snappy = new org.apache.spark.sql.<span class="function">SnappySession</span>(spark.sparkContext)</span><br><span class="line">val testDF = snappy.<span class="function">range</span>(<span class="number">100000000</span>).<span class="function">selectExpr</span>(<span class="string">"id"</span>, <span class="string">"concat('sym', cast((id % 100) as varchar(10))) as sym"</span>)</span><br><span class="line">snappy.<span class="function">sql</span>(<span class="string">"drop table if exists snappyTable"</span>)</span><br><span class="line">snappy.<span class="function">sql</span>(<span class="string">"create table snappyTable (id bigint not null, sym varchar(10) not null) using column"</span>)</span><br><span class="line"><span class="function">benchmark</span>(<span class="string">"Snappy insert perf"</span>, <span class="number">1</span>, <span class="number">0</span>) &#123;testDF.write.<span class="function">insertInto</span>(<span class="string">"snappyTable"</span>) &#125;</span><br><span class="line"><span class="function">benchmark</span>(<span class="string">"Snappy perf"</span>) &#123;snappy.<span class="function">sql</span>(<span class="string">"select sym, avg(id) from snappyTable group by sym"</span>).<span class="function">collect</span>()&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="单机模式">单机模式</h3><p>左图为本地模式，右图为伪分布式模式：分别启动locator（左下）、server（DataServer，右上）、<br>leader（左上），quickstartdir为右下（share-nothing store）.</p>
<p><img src="http://snappydatainc.github.io/snappydata/Images/SnappyLocalMode.png" alt="snappy"><br><img src="http://snappydatainc.github.io/snappydata/Images/SnappyEmbeddedMode.png" alt="snappy"></p>
<p>伪分布式模式的三个组件都在本机启动，使用不同的文件夹。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ cd snappydata-0.9-bin</span><br><span class="line">$ mkdir -p node-a/locator1 node-b/server1 node-c/lead1</span><br><span class="line"></span><br><span class="line">$ bin/snappy locator <span class="operator"><span class="keyword">start</span> -dir=node-a/locator1</span><br><span class="line"></span><br><span class="line"><span class="keyword">Starting</span> SnappyData <span class="keyword">Locator</span> <span class="keyword">using</span> peer discovery <span class="keyword">on</span>: <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>[<span class="number">10334</span>]</span><br><span class="line"><span class="keyword">Starting</span> Thrift <span class="keyword">server</span> <span class="keyword">for</span> SnappyData <span class="keyword">at</span> address localhost/<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>[<span class="number">1527</span>]</span><br><span class="line"><span class="keyword">Logs</span> <span class="keyword">generated</span> <span class="keyword">in</span> /home/qihuang.zheng/snappydata-<span class="number">0.9</span>-<span class="keyword">bin</span>/node-a/locator1/snappylocator.<span class="keyword">log</span></span><br><span class="line">SnappyData <span class="keyword">Locator</span> pid: <span class="number">27651</span> <span class="keyword">status</span>: running</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">bin</span>/snappy <span class="keyword">server</span> <span class="keyword">start</span> -dir=node-b/server1 -locators=dp0652:<span class="number">10334</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">Starting</span> SnappyData <span class="keyword">Server</span> <span class="keyword">using</span> locators <span class="keyword">for</span> peer discovery: dp0652:<span class="number">10334</span></span><br><span class="line"><span class="keyword">Starting</span> Thrift <span class="keyword">server</span> <span class="keyword">for</span> SnappyData <span class="keyword">at</span> address localhost/<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>[<span class="number">1528</span>]</span><br><span class="line"><span class="keyword">Logs</span> <span class="keyword">generated</span> <span class="keyword">in</span> /home/qihuang.zheng/snappydata-<span class="number">0.9</span>-<span class="keyword">bin</span>/node-b/server1/snappyserver.<span class="keyword">log</span></span><br><span class="line">SnappyData <span class="keyword">Server</span> pid: <span class="number">29595</span> <span class="keyword">status</span>: running</span><br><span class="line">  <span class="keyword">Distributed</span> <span class="keyword">system</span> <span class="keyword">now</span> has <span class="number">2</span> members.</span><br><span class="line">  Other members: dp0652(<span class="number">27651</span>:<span class="keyword">locator</span>)&lt;v0&gt;:<span class="number">32709</span></span><br><span class="line"></span><br><span class="line">$ <span class="keyword">bin</span>/snappy leader <span class="keyword">start</span> -dir=node-<span class="keyword">c</span>/lead1 -locators=dp0652:<span class="number">10334</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">Starting</span> SnappyData Leader <span class="keyword">using</span> locators <span class="keyword">for</span> peer discovery: localhost:<span class="number">10334</span></span><br><span class="line"><span class="keyword">Logs</span> <span class="keyword">generated</span> <span class="keyword">in</span> /home/qihuang.zheng/snappydata-<span class="number">0.9</span>-<span class="keyword">bin</span>/node-<span class="keyword">c</span>/lead1/snappyleader.<span class="keyword">log</span></span><br><span class="line">SnappyData Leader pid: <span class="number">29860</span> <span class="keyword">status</span>: running</span><br><span class="line">  <span class="keyword">Distributed</span> <span class="keyword">system</span> <span class="keyword">now</span> has <span class="number">3</span> members.</span><br><span class="line">  Other members: dp0652(<span class="number">27651</span>:<span class="keyword">locator</span>)&lt;v0&gt;:<span class="number">32709</span>, dp0652(<span class="number">29595</span>:datastore)&lt;v7&gt;:<span class="number">9553</span></span></span><br></pre></td></tr></table></figure>
<p>如果要修改地址，可以用xx=xx的方式，<br>比如(修改locator的地址)[<a href="https://snappydatainc.github.io/snappydata/reference/configuration_parameters/start-locator/" target="_blank" rel="external">https://snappydatainc.github.io/snappydata/reference/configuration_parameters/start-locator/</a>]</p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/snappy locator <span class="literal">start</span> -<span class="variable">dir=</span><span class="keyword">node</span><span class="identifier"></span><span class="title">-a</span>/locator1 -<span class="variable">start-locator=</span><span class="number">192.168</span>.<span class="number">6.52</span>[<span class="number">1529</span>]</span><br></pre></td></tr></table></figure>
<p>关闭各个组件：</p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/snappy locator <span class="literal">stop</span> -<span class="variable">dir=</span><span class="keyword">node</span><span class="identifier"></span><span class="title">-a</span>/locator1</span><br><span class="line">bin/snappy server <span class="literal">stop</span> -<span class="variable">dir=</span><span class="keyword">node</span><span class="identifier"></span><span class="title">-b</span>/server1</span><br><span class="line">bin/snappy leader <span class="literal">stop</span> -<span class="variable">dir=</span><span class="keyword">node</span><span class="identifier"></span><span class="title">-c</span>/lead1</span><br></pre></td></tr></table></figure>
<p>执行spark-shell，并指定snappydata的连接地址为<code>localhost:1527</code>.</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --driver-memory=<span class="number">4</span>g \</span><br><span class="line">  --conf spark<span class="class">.snappydata</span><span class="class">.connection</span>=localhost:<span class="number">1527</span> \</span><br><span class="line">  --conf spark<span class="class">.snappydata</span><span class="class">.store</span><span class="class">.sys-disk-dir</span>=quickstartdatadir2 \</span><br><span class="line">  --conf spark<span class="class">.snappydata</span><span class="class">.store</span><span class="class">.log-file</span>=quickstartdatadir2/quickstart<span class="class">.log</span> \</span><br><span class="line">  --driver-java-options=<span class="string">"-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g"</span></span><br></pre></td></tr></table></figure>
<p>如果打开<a href="http://192.168.6.52:4042" target="_blank" rel="external">http://192.168.6.52:4042</a>，有spark app的页面，但是没有dashboard的页面。<br>打开<a href="http://192.168.6.52:5050/dashboard/" target="_blank" rel="external">http://192.168.6.52:5050/dashboard/</a>，可以查看snappydata的web ui。</p>
<blockquote>
<p>5050类似于spark standalone的8082 web-ui，4040类似于spark app的ui。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20170714120810617" alt="snappy2"></p>
<h3 id="一键启动三个组件">一键启动三个组件</h3><p>上面三个启动脚本可以用一个脚本执行,这种情况默认的文件夹在work下。</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sbin/snappy-start-<span class="keyword">all</span>.<span class="keyword">sh</span></span><br><span class="line">sbin/snappy-<span class="keyword">stop</span>-<span class="keyword">all</span>.<span class="keyword">sh</span></span><br><span class="line">sbin/snappy-status-<span class="keyword">all</span>.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>
<p>snappy-start-all.sh会在本地启动一个locator,一个server,一个leader.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/snappy-<span class="operator"><span class="keyword">start</span>-all.sh</span><br><span class="line"><span class="keyword">Starting</span> SnappyData <span class="keyword">Locator</span> <span class="keyword">using</span> peer discovery <span class="keyword">on</span>: localhost[<span class="number">10334</span>], other locators: localhost[<span class="number">10334</span>]</span><br><span class="line"><span class="keyword">Starting</span> Thrift <span class="keyword">server</span> <span class="keyword">for</span> SnappyData <span class="keyword">at</span> address localhost/<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>[<span class="number">1527</span>]</span><br><span class="line"><span class="keyword">Logs</span> <span class="keyword">generated</span> <span class="keyword">in</span> /home/qihuang.zheng/snappydata-<span class="number">0.9</span>-<span class="keyword">bin</span>/<span class="keyword">work</span>/localhost-<span class="keyword">locator</span>-<span class="number">1</span>/snappylocator.<span class="keyword">log</span></span><br><span class="line">SnappyData <span class="keyword">Locator</span> pid: <span class="number">7949</span> <span class="keyword">status</span>: running</span><br><span class="line"></span><br><span class="line"><span class="keyword">Starting</span> SnappyData <span class="keyword">Server</span> <span class="keyword">using</span> locators <span class="keyword">for</span> peer discovery: localhost[<span class="number">10334</span>]</span><br><span class="line"><span class="keyword">Starting</span> Thrift <span class="keyword">server</span> <span class="keyword">for</span> SnappyData <span class="keyword">at</span> address localhost/<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>[<span class="number">1528</span>]</span><br><span class="line"><span class="keyword">Logs</span> <span class="keyword">generated</span> <span class="keyword">in</span> /home/qihuang.zheng/snappydata-<span class="number">0.9</span>-<span class="keyword">bin</span>/<span class="keyword">work</span>/localhost-<span class="keyword">server</span>-<span class="number">1</span>/snappyserver.<span class="keyword">log</span></span><br><span class="line">SnappyData <span class="keyword">Server</span> pid: <span class="number">8176</span> <span class="keyword">status</span>: running</span><br><span class="line">  <span class="keyword">Distributed</span> <span class="keyword">system</span> <span class="keyword">now</span> has <span class="number">2</span> members.</span><br><span class="line">  Other members: localhost(<span class="number">7949</span>:<span class="keyword">locator</span>)&lt;v0&gt;:<span class="number">37846</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">Starting</span> SnappyData Leader <span class="keyword">using</span> locators <span class="keyword">for</span> peer discovery: localhost[<span class="number">10334</span>]</span><br><span class="line"><span class="keyword">Logs</span> <span class="keyword">generated</span> <span class="keyword">in</span> /home/qihuang.zheng/snappydata-<span class="number">0.9</span>-<span class="keyword">bin</span>/<span class="keyword">work</span>/localhost-<span class="keyword">lead</span>-<span class="number">1</span>/snappyleader.<span class="keyword">log</span></span><br><span class="line">SnappyData Leader pid: <span class="number">8488</span> <span class="keyword">status</span>: running</span><br><span class="line">  <span class="keyword">Distributed</span> <span class="keyword">system</span> <span class="keyword">now</span> has <span class="number">3</span> members.</span><br><span class="line">  Other members: localhost(<span class="number">7949</span>:<span class="keyword">locator</span>)&lt;v0&gt;:<span class="number">37846</span>, dp0652(<span class="number">8176</span>:datastore)&lt;v1&gt;:<span class="number">24462</span></span></span><br></pre></td></tr></table></figure>
<p>查看默认work下的目录</p>
<ul>
<li>lead：类似于Spark的Driver，文件夹是spark-jobserver，放了作业和jar包</li>
<li>locator：</li>
<li>server：</li>
</ul>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">$ tree work/</span><br><span class="line">work/</span><br><span class="line">├── localhost-lead-<span class="number">1</span></span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE<span class="class">.if</span></span><br><span class="line">│   ├── DRLK_IFGFXD-DEFAULT-DISKSTORE<span class="class">.lk</span></span><br><span class="line">│   ├── snappyleader<span class="class">.gfs</span></span><br><span class="line">│   ├── snappyleader<span class="class">.log</span></span><br><span class="line">│   ├── snappyleader<span class="class">.pid</span></span><br><span class="line">│   ├── spark-jobserver</span><br><span class="line">│   │   ├── filedao</span><br><span class="line">│   │   │   └── data</span><br><span class="line">│   │   │       ├── configs<span class="class">.data</span></span><br><span class="line">│   │   │       ├── jars<span class="class">.data</span></span><br><span class="line">│   │   │       └── jobs<span class="class">.data</span></span><br><span class="line">│   │   └── upload</span><br><span class="line">│   │       └── files<span class="class">.data</span></span><br><span class="line">│   └── start_snappyleader<span class="class">.log</span></span><br><span class="line">├── localhost-locator-<span class="number">1</span></span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE_1<span class="class">.crf</span></span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE_1<span class="class">.drf</span></span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE<span class="class">.if</span></span><br><span class="line">│   ├── datadictionary</span><br><span class="line">│   │   ├── BACKUPGFXD-DD-DISKSTORE_1<span class="class">.crf</span></span><br><span class="line">│   │   ├── BACKUPGFXD-DD-DISKSTORE_1<span class="class">.drf</span></span><br><span class="line">│   │   ├── BACKUPGFXD-DD-DISKSTORE<span class="class">.if</span></span><br><span class="line">│   │   └── DRLK_IFGFXD-DD-DISKSTORE<span class="class">.lk</span></span><br><span class="line">│   ├── DRLK_IFGFXD-DEFAULT-DISKSTORE<span class="class">.lk</span></span><br><span class="line">│   ├── locator10334state<span class="class">.dat</span></span><br><span class="line">│   ├── locator10334views<span class="class">.log</span></span><br><span class="line">│   ├── snappylocator<span class="class">.gfs</span></span><br><span class="line">│   ├── snappylocator<span class="class">.log</span></span><br><span class="line">│   ├── snappylocator<span class="class">.pid</span></span><br><span class="line">│   └── start_snappylocator<span class="class">.log</span></span><br><span class="line">├── localhost-server-<span class="number">1</span></span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE_1<span class="class">.crf</span></span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE_1<span class="class">.drf</span></span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE<span class="class">.if</span></span><br><span class="line">│   ├── datadictionary</span><br><span class="line">│   │   ├── BACKUPGFXD-DD-DISKSTORE_1<span class="class">.crf</span></span><br><span class="line">│   │   ├── BACKUPGFXD-DD-DISKSTORE_1<span class="class">.drf</span></span><br><span class="line">│   │   ├── BACKUPGFXD-DD-DISKSTORE<span class="class">.if</span></span><br><span class="line">│   │   └── DRLK_IFGFXD-DD-DISKSTORE<span class="class">.lk</span></span><br><span class="line">│   ├── DRLK_IFGFXD-DEFAULT-DISKSTORE<span class="class">.lk</span></span><br><span class="line">│   ├── snappyserver<span class="class">.gfs</span></span><br><span class="line">│   ├── snappyserver<span class="class">.log</span></span><br><span class="line">│   ├── snappyserver<span class="class">.pid</span></span><br><span class="line">│   └── start_snappyserver<span class="class">.log</span></span><br><span class="line">└── members.txt</span><br></pre></td></tr></table></figure>
<h2 id="client">client</h2><p>先停止snappydata，然后修改远程机器conf下的servers, locators, leads.<br>将localhost改为主机地址:192.168.6.52，再重启snappydata。</p>
<p>注意：默认启动时，使用的是localhost，work下的文件夹页是localhost开头。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 snappydata-<span class="number">0.9</span>-bin]$ sbin/snappy-start-all.sh</span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>: Starting SnappyData Locator <span class="keyword">using</span> peer discovery on: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>[<span class="number">10334</span>], other locators: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">10334</span></span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>: Starting Thrift server <span class="keyword">for</span> SnappyData at address /<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>[<span class="number">1527</span>]</span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>: Logs generated in /home/qihuang.zheng/snappydata-<span class="number">0.9</span>-bin/work/<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>-locator-<span class="number">1</span>/snappylocator.<span class="built_in">log</span></span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>: SnappyData Locator pid: <span class="number">45151</span> status: running</span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>: Starting SnappyData Server <span class="keyword">using</span> locators <span class="keyword">for</span> peer discovery: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">10334</span></span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>: Starting Thrift server <span class="keyword">for</span> SnappyData at address /<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>[<span class="number">1528</span>]</span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>: Logs generated in /home/qihuang.zheng/snappydata-<span class="number">0.9</span>-bin/work/<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>-server-<span class="number">1</span>/snappyserver.<span class="built_in">log</span></span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>: SnappyData Server pid: <span class="number">45860</span> status: running</span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:   Distributed system now has <span class="number">2</span> members.</span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:   Other members: dp0652(<span class="number">45151</span>:locator)&lt;v0&gt;:<span class="number">48205</span></span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>: Starting SnappyData Leader <span class="keyword">using</span> locators <span class="keyword">for</span> peer discovery: <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">10334</span></span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>: Logs generated in /home/qihuang.zheng/snappydata-<span class="number">0.9</span>-bin/work/<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>-lead-<span class="number">1</span>/snappyleader.<span class="built_in">log</span></span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>: SnappyData Leader pid: <span class="number">46726</span> status: running</span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:   Distributed system now has <span class="number">3</span> members.</span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:   Other members: dp0652(<span class="number">45860</span>:datastore)&lt;v1&gt;:<span class="number">8287</span>, dp0652(<span class="number">45151</span>:locator)&lt;v0&gt;:<span class="number">48205</span></span><br></pre></td></tr></table></figure>
<p>查看进程</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">45860</span> io.snappydata.tools.ServerLauncher server -critical-heap-percentage=<span class="number">90</span> -eviction-heap-percentage=<span class="number">81</span> locators=<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">10334</span> <span class="built_in">log</span>-file=snappyserver.<span class="built_in">log</span> -client-bind-address=<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span></span><br><span class="line"><span class="number">46726</span> io.snappydata.tools.LeaderLauncher server locators=<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">10334</span> <span class="built_in">log</span>-file=snappyleader.<span class="built_in">log</span> -run-netserver=<span class="literal">false</span></span><br><span class="line"><span class="number">45151</span> io.snappydata.tools.LocatorLauncher server locators=<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">10334</span> start-locator=<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">10334</span> <span class="built_in">log</span>-file=snappylocator.<span class="built_in">log</span> -client-bind-address=<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span> -peer-discovery-address=<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span> jmx-manager=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>本机下载snappydata的二进制包，并启动snappy脚本，通过thrift/jdbc连接远程的snappydata cluster</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">➜  snappydata-0.9-bin bin/snappy</span><br><span class="line">SnappyData 版本 0.9</span><br><span class="line">snappy&gt; connect client '192.168.6.52:1527';</span><br><span class="line">九月 14, 2017 3:43:43 下午 java.util.logging.LogManager$RootLogger log</span><br><span class="line">信息: Starting client on '10.57.4.219' with ID='7059|<span class="string">2017/09/14 15:43:43.185 CST'</span><br><span class="line">Using CONNECTION0</span><br><span class="line">snappy&gt; show connections ;</span><br><span class="line">CONNECTION0* -  jdbc:snappydata:thrift://192.168.6.52[1527]</span><br><span class="line">* = 当前连接</span><br><span class="line">snappy&gt; show tables;</span><br><span class="line">TABLE_SCHEM          </span>|<span class="string">TABLE_NAME                    </span>|<span class="string">TABLE_TYPE  </span>|REMARKS</span><br><span class="line">--------------------------------------------------------------------------------------</span><br><span class="line">SYS                  |<span class="string">ASYNCEVENTLISTENERS           </span>|<span class="string">SYSTEM TABLE</span>|</span><br><span class="line">SYS                  |<span class="string">GATEWAYRECEIVERS              </span>|<span class="string">SYSTEM TABLE</span>|</span><br><span class="line">SYS                  |<span class="string">GATEWAYSENDERS                </span>|<span class="string">SYSTEM TABLE</span>|</span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SnappyData®&lt;/p&gt;
    
    </summary>
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/categories/bigdata/"/>
    
    
      <category term="spark" scheme="http://github.com/zqhxuyuan/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>CarbonData</title>
    <link href="http://github.com/zqhxuyuan/2017/07/13/CarbonData-In-Action/"/>
    <id>http://github.com/zqhxuyuan/2017/07/13/CarbonData-In-Action/</id>
    <published>2017-07-12T16:00:00.000Z</published>
    <updated>2017-09-13T07:33:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>Apache CarbonData</p>
<a id="more"></a>
<h2 id="Apache_CarbonData">Apache CarbonData</h2><p>版本：carbondata-1.1.0，spark-2.1.1，hadoop-2.6.0</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ mvn -DskipTests -Pspark-<span class="number">2.1</span> -Dspark.version=<span class="number">2.1</span><span class="number">.1</span> -Dhadoop.version=<span class="number">2.6</span><span class="number">.0</span> clean package</span><br><span class="line"></span><br><span class="line">$ ll assembly/target/scala-<span class="number">2.11</span></span><br><span class="line"><span class="number">8.9</span>M  <span class="number">7</span> <span class="number">12</span> <span class="number">16</span>:<span class="number">14</span> carbondata_2<span class="number">.11</span>-<span class="number">1.1</span><span class="number">.1</span>-shade-hadoop2<span class="number">.6</span><span class="number">.0</span>.jar</span><br></pre></td></tr></table></figure>
<p>本地模式测试，创建CarbonSession的第一个参数为本地文件系统</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --jars ~/Github/carbondata-parent-<span class="number">1.1</span>.<span class="number">0</span>/assembly/target/scala-<span class="number">2.11</span>/carbondata_2.<span class="number">11</span>-<span class="number">1.1</span>.<span class="number">1</span>-shade-hadoop2.<span class="number">6.0</span><span class="class">.jar</span></span><br><span class="line"></span><br><span class="line">import org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.SparkSession</span></span><br><span class="line">import org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.CarbonSession</span>._</span><br><span class="line">val carbon = SparkSession.<span class="function"><span class="title">builder</span><span class="params">()</span></span>.<span class="function"><span class="title">config</span><span class="params">(sc.getConf)</span></span>.<span class="function"><span class="title">getOrCreateCarbonSession</span><span class="params">(<span class="string">"/tmp/carbon"</span>)</span></span></span><br><span class="line">carbon.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"CREATE TABLE IF NOT EXISTS test_table(id string,name string,city string,age Int)STORED BY 'carbondata'"</span>)</span></span></span><br><span class="line">carbon.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"LOAD DATA INPATH '/Users/zhengqh/Downloads/spark-2.1.1-bin-hadoop2.7/sample.csv' INTO TABLE test_table"</span>)</span></span></span><br><span class="line">carbon.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT city, avg(age), sum(age) FROM test_table GROUP BY city"</span>)</span></span>.<span class="function"><span class="title">show</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<p>本地文件系统的文件夹包括Fact（表数据）、Metadata(表结构)</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">➜  carbondata-parent-<span class="number">1.1</span><span class="number">.0</span> tree /tmp/carbon</span><br><span class="line">/tmp/carbon</span><br><span class="line">├── <span class="keyword">default</span></span><br><span class="line">│   └── test_table</span><br><span class="line">│       ├── Fact</span><br><span class="line">│       │   └── Part0</span><br><span class="line">│       │       └── Segment_0</span><br><span class="line">│       │           ├── <span class="number">0</span>_batchno0-<span class="number">0</span>-<span class="number">1499845043969.</span>carbonindex</span><br><span class="line">│       │           └── part-<span class="number">0</span>-<span class="number">0</span>_batchno0-<span class="number">0</span>-<span class="number">1499845043969.</span>carbondata</span><br><span class="line">│       └── Metadata</span><br><span class="line">│           ├── <span class="number">3</span>d8bd318-a620-<span class="number">419</span>b-b0fd-c276936375e2.dict</span><br><span class="line">│           ├── <span class="number">3</span>d8bd318-a620-<span class="number">419</span>b-b0fd-c276936375e2.dictmeta</span><br><span class="line">│           ├── <span class="number">3</span>d8bd318-a620-<span class="number">419</span>b-b0fd-c276936375e2_27.sortindex</span><br><span class="line">│           ├── f2f45986-<span class="number">6f</span>b6-<span class="number">42</span>af-b991-<span class="number">513</span>ee43aad01.dict</span><br><span class="line">│           ├── f2f45986-<span class="number">6f</span>b6-<span class="number">42</span>af-b991-<span class="number">513</span>ee43aad01.dictmeta</span><br><span class="line">│           ├── f2f45986-<span class="number">6f</span>b6-<span class="number">42</span>af-b991-<span class="number">513</span>ee43aad01_18.sortindex</span><br><span class="line">│           ├── f93ce55d-b82a-<span class="number">4</span>eca-<span class="number">9076</span>-e21dcd819218.dict</span><br><span class="line">│           ├── f93ce55d-b82a-<span class="number">4</span>eca-<span class="number">9076</span>-e21dcd819218.dictmeta</span><br><span class="line">│           ├── f93ce55d-b82a-<span class="number">4</span>eca-<span class="number">9076</span>-e21dcd819218_30.sortindex</span><br><span class="line">│           ├── schema</span><br><span class="line">│           └── tablestatus</span><br><span class="line">└── modifiedTime.mdt</span><br></pre></td></tr></table></figure>
<p>yarn模式按照官网部署<a href="http://carbondata.apache.org/installation-guide.html" target="_blank" rel="external">http://carbondata.apache.org/installation-guide.html</a></p>
<blockquote>
<p>注意：使用yarn模式，不需要把carbondata通过scp分发到各个节点，只需要在Driver端有就可以。另外，当前版本不依赖kettle</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cd spark-<span class="number">2.1</span><span class="number">.1</span>*</span><br><span class="line">mkdir carbonlib</span><br><span class="line">cp ~/carbondata_2<span class="number">.11</span>-<span class="number">1.1</span><span class="number">.1</span>-shade-hadoop2<span class="number">.6</span><span class="number">.0</span>.jar carbonlib</span><br><span class="line">cp ~/carbon.properties conf</span><br><span class="line"></span><br><span class="line">tar -zcvf carbondata.tar.gz carbonlib/</span><br><span class="line">mv carbondata.tar.gz carbonlib/</span><br><span class="line"></span><br><span class="line">$ vi conf/spark-defaults.conf</span><br><span class="line">spark.executor.extraJavaOptions -Dcarbon.properties.filepath=/usr/install/spark-<span class="number">2.1</span><span class="number">.1</span>-bin-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.9</span><span class="number">.0</span>/conf/carbon.properties</span><br><span class="line">spark.driver.extraJavaOptions   -Dcarbon.properties.filepath=/usr/install/spark-<span class="number">2.1</span><span class="number">.1</span>-bin-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.9</span><span class="number">.0</span>/conf/carbon.properties</span><br><span class="line">spark.driver.extraClassPath     /usr/install/spark-<span class="number">2.1</span><span class="number">.1</span>-bin-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.9</span><span class="number">.0</span>/carbonlib<span class="comment">/*</span><br><span class="line">spark.executor.extraClassPath   /usr/install/spark-2.1.1-bin-2.6.0-cdh5.9.0/carbonlib<span class="comment">/*</span><br><span class="line">spark.yarn.dist.files           /usr/install/spark-2.1.1-bin-2.6.0-cdh5.9.0/conf/carbon.properties</span><br><span class="line">spark.yarn.dist.archives        /usr/install/spark-2.1.1-bin-2.6.0-cdh5.9.0/carbonlib/carbondata.tar.gz</span></span></span><br></pre></td></tr></table></figure>
<p>启动spark-shell还需要加上<code>--jars</code>。注意创建CarbonSession时第一个参数必须加上hdfs前缀，否则会报错找不到文件</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell <span class="comment">--jars /home/admin/carbondata_2.11-1.1.1-shade-hadoop2.6.0.jar</span></span><br><span class="line"></span><br><span class="line">sql("<span class="operator"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> test_table1(<span class="keyword">id</span> <span class="keyword">string</span>,<span class="keyword">name</span> <span class="keyword">string</span>,city <span class="keyword">string</span>,age <span class="built_in">Int</span>)<span class="string">")</span><br><span class="line">sql("</span><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> test_table1 <span class="keyword">values</span>(<span class="string">'1'</span>,<span class="string">'david'</span>,<span class="string">'shenzhen'</span>,<span class="number">31</span>)<span class="string">")</span><br><span class="line">sql("</span><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> test_table1 <span class="keyword">values</span>(<span class="string">'2'</span>,<span class="string">'eason'</span>,<span class="string">'shenzhen'</span>,<span class="number">20</span>)<span class="string">")</span><br><span class="line">sql("</span><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> test_table1 <span class="keyword">values</span>(<span class="string">'3'</span>,<span class="string">'jarry'</span>,<span class="string">'wuhan'</span>,<span class="number">35</span>)<span class="string">")</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.CarbonSession._</span><br><span class="line">val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession("</span>hdfs://tdhdfs/<span class="keyword">user</span>/tongdun/carbon<span class="string">","</span>/home/<span class="keyword">admin</span>/carbon<span class="string">")</span><br><span class="line"></span><br><span class="line">carbon.sql("</span><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> test_table2(<span class="keyword">id</span> <span class="keyword">string</span>,<span class="keyword">name</span> <span class="keyword">string</span>,city <span class="keyword">string</span>,age <span class="built_in">Int</span>)<span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'carbondata'</span><span class="string">")</span><br><span class="line">carbon.sql("</span><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_table2 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> test_table1<span class="string">") // insert #1</span><br><span class="line">carbon.sql("</span><span class="keyword">select</span> * <span class="keyword">from</span> test_table2<span class="string">").show</span><br><span class="line">carbon.sql("</span><span class="keyword">INSERT</span> <span class="keyword">INTO</span> test_table2 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> test_table1<span class="string">") // insert again</span><br><span class="line">carbon.sql("</span><span class="keyword">select</span> * <span class="keyword">from</span> test_table2<span class="string">").show</span><br><span class="line"></span><br><span class="line">carbon.sql("</span><span class="keyword">INSERT</span> overwrite <span class="keyword">table</span> test_table2 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> test_table1<span class="string">") // overwrite</span></span></span><br></pre></td></tr></table></figure>
<p>carbondata运行在HDFS时，它的事实数据与元数据保存在HDFS上。</p>
<p><img src="http://img.blog.csdn.net/20170713222930582" alt="carbon"></p>
<p>将hdfs表数据导入到carbondata建立的表后，执行一些查询语句，观察ui。</p>
<blockquote>
<p>注意：导入数据时，carbondata分为两个步骤：全局字典（GlobalDictionary）和CarbonDataRDD。<br>其中全局字典会在Metadata下生产索引文件，CarbonDataRDD会在Fact下生成数据文件。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20170713222957886" alt="carbon1"></p>
<h3 id="CarbonData数据导入与查询">CarbonData数据导入与查询</h3><p>建立crosspartner carbondata表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.CarbonSession._</span><br><span class="line">val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession("hdfs://tdhdfs/user/tongdun/carbon","/home/admin/carbon")</span><br><span class="line"></span><br><span class="line">carbon.sql("<span class="operator"><span class="keyword">drop</span> <span class="keyword">table</span> cross_partner_carbon<span class="string">")</span><br><span class="line">carbon.sql("""</span><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> cross_partner_carbon(</span><br><span class="line">  partnerCode <span class="keyword">string</span>,</span><br><span class="line">  eventType <span class="keyword">string</span>,</span><br><span class="line">  idNumber <span class="keyword">string</span>,</span><br><span class="line">  accountMobile <span class="keyword">string</span>,</span><br><span class="line">  accountEmail <span class="keyword">string</span>,</span><br><span class="line">  accountPhone <span class="keyword">string</span>,</span><br><span class="line">  deviceId <span class="keyword">string</span>,</span><br><span class="line">  cardNumber <span class="keyword">string</span>,</span><br><span class="line">  contact1Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact2Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact3Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact4Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact5Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact1IdNumber <span class="keyword">string</span>,</span><br><span class="line">  contact2IdNumber <span class="keyword">string</span>,</span><br><span class="line">  contact3IdNumber <span class="keyword">string</span>,</span><br><span class="line">  contact4IdNumber <span class="keyword">string</span>,</span><br><span class="line">  contact5IdNumber <span class="keyword">string</span>,</span><br><span class="line">  sequenceId <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'carbondata'</span></span><br><span class="line">TBLPROPERTIES (<span class="string">'DICTIONARY_EXCLUDE'</span>=<span class="string">'sequenceId'</span>)</span><br><span class="line"><span class="string">""")</span></span></span><br></pre></td></tr></table></figure>
<p>再生成carbondata表：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">carbon.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"insert into cross_partner_carbon select * from crosspartner"</span>)</span></span></span><br><span class="line"></span><br><span class="line">spark.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select count(*) from cross2partner_dt"</span>)</span></span><span class="class">.show</span></span><br><span class="line">carbon.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select count(*) from cross_partner_carbon_dm"</span>)</span></span><span class="class">.show</span></span><br><span class="line"></span><br><span class="line">spark.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select * from cross2partner_dt"</span>)</span></span><span class="class">.show</span></span><br><span class="line">carbon.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select * from cross_partner_carbon_dm"</span>)</span></span><span class="class">.show</span></span><br><span class="line"></span><br><span class="line">val idnumber=<span class="string">""</span></span><br><span class="line">spark.<span class="function"><span class="title">sql</span><span class="params">(s<span class="string">"select sequenceId from cross2partner_dt where partnerCode='007fenqi' and eventType='Loan' and idNumber='$idnumber'"</span>)</span></span><span class="class">.show</span></span><br><span class="line">carbon.<span class="function"><span class="title">sql</span><span class="params">(s<span class="string">"select sequenceId from cross_partner_carbon_dm where partnerCode='007fenqi' and eventType='Loan' and idNumber='$idnumber'"</span>)</span></span>.show</span><br></pre></td></tr></table></figure>
<p>比较crosspartner_hdfs的过滤与carbondata的查询</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">carbon.sql("<span class="operator"><span class="keyword">select</span> sequenceId <span class="keyword">from</span> cross_partner_carbon <span class="keyword">where</span> partnerCode=<span class="string">'qufenqi'</span> <span class="keyword">and</span> eventType=<span class="string">'Loan'</span> <span class="keyword">and</span> idNumber=<span class="string">''</span><span class="string">").show</span></span></span><br></pre></td></tr></table></figure>
<h3 id="实验结果">实验结果</h3><p>创建carbondata表时，如果默认所有字段都加上索引，导入数据时Executor会报错OOM。<br>如果去掉所有字段的索引，导入数据很快，但是查询速度就满了。</p>
<p>比较磁盘空间的大小，没有索引下，Parquet和Carbondata差不多</p>
<p><img src="http://img.blog.csdn.net/20170721100001741" alt="1"></p>
<h3 id="问题">问题</h3><h4 id="1-_Hive表与CarbonData表">1. Hive表与CarbonData表</h4><p>activity事件数据,只取借贷和放贷的数据，并保存成临时表crosspartner_hdfs</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">spark.sql("""<span class="operator"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> crosspartner_hdfs(</span><br><span class="line">  partnerCode <span class="keyword">string</span>,</span><br><span class="line">  eventType <span class="keyword">string</span>,</span><br><span class="line">  idNumber <span class="keyword">string</span>,</span><br><span class="line">  accountMobile <span class="keyword">string</span>,</span><br><span class="line">  accountEmail <span class="keyword">string</span>,</span><br><span class="line">  accountPhone <span class="keyword">string</span>,</span><br><span class="line">  deviceId <span class="keyword">string</span>,</span><br><span class="line">  cardNumber <span class="keyword">string</span>,</span><br><span class="line">  contact1Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact2Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact3Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact4Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact5Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact1IdNumber <span class="keyword">string</span>,</span><br><span class="line">  contact2IdNumber <span class="keyword">string</span>,</span><br><span class="line">  contact3IdNumber <span class="keyword">string</span>,</span><br><span class="line">  contact4IdNumber <span class="keyword">string</span>,</span><br><span class="line">  contact5IdNumber <span class="keyword">string</span>,</span><br><span class="line">  sequenceId <span class="keyword">string</span></span><br><span class="line">) partitioned <span class="keyword">by</span>(ds <span class="keyword">string</span>)</span><br><span class="line"><span class="string">""")</span><br><span class="line"></span><br><span class="line">spark.sql("""</span><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> crosspartner_hdfs <span class="keyword">partition</span>(ds=<span class="string">'201706'</span>)</span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">  activity_map.partnerCode <span class="keyword">as</span> partnerCode,</span><br><span class="line">  activity_map.eventType <span class="keyword">as</span> eventType,</span><br><span class="line">  activity_map.idNumber <span class="keyword">as</span> idNumber,</span><br><span class="line">  activity_map.accountMobile <span class="keyword">as</span> accountMobile,</span><br><span class="line">  activity_map.accountEmail <span class="keyword">as</span> accountEmail,</span><br><span class="line">  activity_map.accountPhone <span class="keyword">as</span> accountPhone,</span><br><span class="line">  activity_map.deviceId <span class="keyword">as</span> deviceId,</span><br><span class="line">  activity_map.cardNumber <span class="keyword">as</span> cardNumber,</span><br><span class="line">  activity_map.contact1Mobile <span class="keyword">as</span> contact1Mobile,</span><br><span class="line">  activity_map.contact2Mobile <span class="keyword">as</span> contact2Mobile,</span><br><span class="line">  activity_map.contact3Mobile <span class="keyword">as</span> contact3Mobile,</span><br><span class="line">  activity_map.contact4Mobile <span class="keyword">as</span> contact4Mobile,</span><br><span class="line">  activity_map.contact5Mobile <span class="keyword">as</span> contact5Mobile,</span><br><span class="line">  activity_map.contact1IdNumber <span class="keyword">as</span> contact1IdNumber,</span><br><span class="line">  activity_map.contact2IdNumber <span class="keyword">as</span> contact2IdNumber,</span><br><span class="line">  activity_map.contact3IdNumber <span class="keyword">as</span> contact3IdNumber,</span><br><span class="line">  activity_map.contact4IdNumber <span class="keyword">as</span> contact4IdNumber,</span><br><span class="line">  activity_map.contact5IdNumber <span class="keyword">as</span> contact5IdNumber,</span><br><span class="line">  activity_map.sequenceId <span class="keyword">as</span> sequenceId</span><br><span class="line"><span class="keyword">from</span> activity </span><br><span class="line"><span class="keyword">where</span> <span class="keyword">year</span>=<span class="number">2017</span> <span class="keyword">and</span> <span class="keyword">month</span>=<span class="number">6</span></span><br><span class="line"><span class="keyword">and</span> activity_map.eventType <span class="keyword">in</span>(<span class="string">'Loan'</span>,<span class="string">'Lending'</span>)</span><br><span class="line"><span class="string">""")</span></span></span><br></pre></td></tr></table></figure>
<p>上面如果建表时没有指定存储为parquet,最后是part-xxx。<br>而且即使指定了parquet,insert sql也不能指定分区数量。  </p>
<p><strong>下面改用parquet文件夹加上手动分区的形式:cross_partner_hdfs</strong>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">Calendar</span>,<span class="type">Date</span>&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">year</span>(</span>ymd: <span class="type">String</span>) = ymd.substring(<span class="number">0</span>,<span class="number">4</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">month</span>(</span>ymd: <span class="type">String</span>) = &#123;</span><br><span class="line">  <span class="keyword">var</span> month=ymd.substring(<span class="number">4</span>,<span class="number">6</span>)</span><br><span class="line">  <span class="keyword">if</span>(month.startsWith(<span class="string">"0"</span>)) month=ymd.substring(<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line">  month</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">day</span>(</span>ymd: <span class="type">String</span>) = &#123;</span><br><span class="line">  <span class="keyword">var</span> d=ymd.substring(<span class="number">6</span>,<span class="number">8</span>)</span><br><span class="line">  <span class="keyword">if</span>(d.startsWith(<span class="string">"0"</span>)) d=ymd.substring(<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line">  d</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//写成parquet文件夹</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genCrossData</span>(</span>beg: <span class="type">String</span>, end: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">var</span> cal = <span class="type">Calendar</span>.getInstance()</span><br><span class="line">    <span class="keyword">var</span> datef=<span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyyMMdd"</span>)</span><br><span class="line">    <span class="keyword">var</span> beginTime=datef.parse(beg)</span><br><span class="line">    <span class="keyword">var</span> endTime=datef.parse(end)</span><br><span class="line">    <span class="keyword">while</span>(beginTime.compareTo(endTime)&lt;=<span class="number">0</span>)&#123;</span><br><span class="line">      cal.setTime(beginTime);</span><br><span class="line">      <span class="keyword">var</span> ymd=datef.format(beginTime)</span><br><span class="line">      println(ymd)</span><br><span class="line">      <span class="keyword">var</span> y=year(ymd)</span><br><span class="line">      <span class="keyword">var</span> m=month(ymd)</span><br><span class="line">      <span class="keyword">var</span> d=day(ymd)</span><br><span class="line">      spark.sql(s<span class="string">"""</span><br><span class="line">        select </span><br><span class="line">          activity_map.partnerCode as partnerCode,</span><br><span class="line">          activity_map.eventType as eventType,</span><br><span class="line">          activity_map.idNumber as idNumber,</span><br><span class="line">          activity_map.accountMobile as accountMobile,</span><br><span class="line">          activity_map.accountEmail as accountEmail,</span><br><span class="line">          activity_map.accountPhone as accountPhone,</span><br><span class="line">          activity_map.deviceId as deviceId,</span><br><span class="line">          activity_map.cardNumber as cardNumber,</span><br><span class="line">          activity_map.contact1Mobile as contact1Mobile,</span><br><span class="line">          activity_map.contact2Mobile as contact2Mobile,</span><br><span class="line">          activity_map.contact3Mobile as contact3Mobile,</span><br><span class="line">          activity_map.contact4Mobile as contact4Mobile,</span><br><span class="line">          activity_map.contact5Mobile as contact5Mobile,</span><br><span class="line">          activity_map.contact1IdNumber as contact1IdNumber,</span><br><span class="line">          activity_map.contact2IdNumber as contact2IdNumber,</span><br><span class="line">          activity_map.contact3IdNumber as contact3IdNumber,</span><br><span class="line">          activity_map.contact4IdNumber as contact4IdNumber,</span><br><span class="line">          activity_map.contact5IdNumber as contact5IdNumber,</span><br><span class="line">          activity_map.sequenceId as sequenceId</span><br><span class="line">        from activity </span><br><span class="line">        where year=$y and month=$m and day=$d </span><br><span class="line">        and activity_map.eventType in('Loan','Lending')</span><br><span class="line">        """</span>).repartition(<span class="number">1</span>).write.mode(<span class="string">"overwrite"</span>).parquet(s<span class="string">"/user/hive/warehouse/cross_partner_hdfs/ds=$ymd"</span>)</span><br><span class="line">      cal.add(<span class="type">Calendar</span>.<span class="type">DATE</span>,<span class="number">1</span>);</span><br><span class="line">      beginTime=cal.getTime();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">genCrossData(<span class="string">"20170101"</span>,<span class="string">"20170630"</span>)</span><br><span class="line"></span><br><span class="line">genCrossData(<span class="string">"20170621"</span>,<span class="string">"20170630"</span>)</span><br></pre></td></tr></table></figure>
<p>查询parquet，建立临时表，使用SparkSQL查询</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val df=spark<span class="class">.read</span><span class="class">.parquet</span>(<span class="string">"/user/hive/warehouse/cross_partner_hdfs/*"</span>)</span><br><span class="line">df.<span class="function"><span class="title">createOrReplaceTempView</span><span class="params">(<span class="string">"cross_partner_hdfs"</span>)</span></span></span><br><span class="line"></span><br><span class="line">spark.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select * from cross_partner_hdfs"</span>)</span></span><span class="class">.show</span></span><br><span class="line"></span><br><span class="line">spark.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select sequenceId from cross_partner_hdfs where partnerCode='qufenqi' and eventType='Loan' and idNumber=''"</span>)</span></span>.show</span><br></pre></td></tr></table></figure>
<p>使用临时表的数据插入到carbondata table</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val df=spark<span class="class">.read</span><span class="class">.parquet</span>(<span class="string">"/user/hive/warehouse/cross_partner_hdfs/*"</span>)</span><br><span class="line">df.<span class="function"><span class="title">createOrReplaceTempView</span><span class="params">(<span class="string">"cross_partner_hdfs"</span>)</span></span></span><br><span class="line"></span><br><span class="line">carbon.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"insert into cross_partner_carbon select * from cross_partner_hdfs"</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>carbondata不认识用df注册的临时表：</p>
<p><img src="http://img.blog.csdn.net/20170714085833872" alt="10"></p>
<p>创建hive表时指定parquet格式，并从parquet文件夹的数据直接生成表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">spark.sql("""<span class="operator"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> crosspartner(</span><br><span class="line">  partnerCode <span class="keyword">string</span>,</span><br><span class="line">  eventType <span class="keyword">string</span>,</span><br><span class="line">  idNumber <span class="keyword">string</span>,</span><br><span class="line">  accountMobile <span class="keyword">string</span>,</span><br><span class="line">  accountEmail <span class="keyword">string</span>,</span><br><span class="line">  accountPhone <span class="keyword">string</span>,</span><br><span class="line">  deviceId <span class="keyword">string</span>,</span><br><span class="line">  cardNumber <span class="keyword">string</span>,</span><br><span class="line">  contact1Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact2Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact3Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact4Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact5Mobile <span class="keyword">string</span>,</span><br><span class="line">  contact1IdNumber <span class="keyword">string</span>,</span><br><span class="line">  contact2IdNumber <span class="keyword">string</span>,</span><br><span class="line">  contact3IdNumber <span class="keyword">string</span>,</span><br><span class="line">  contact4IdNumber <span class="keyword">string</span>,</span><br><span class="line">  contact5IdNumber <span class="keyword">string</span>,</span><br><span class="line">  sequenceId <span class="keyword">string</span></span><br><span class="line">) partitioned <span class="keyword">by</span>(ds <span class="keyword">string</span>) <span class="keyword">stored</span> <span class="keyword">as</span> parquet</span><br><span class="line"><span class="string">""")</span><br><span class="line"></span><br><span class="line">import java.text.SimpleDateFormat</span><br><span class="line">import java.util.&#123;Calendar,Date&#125;</span><br><span class="line">def genCrossData(beg: String, end: String) = &#123;</span><br><span class="line">    var cal = Calendar.getInstance()</span><br><span class="line">    var datef=new SimpleDateFormat("</span>yyyyMMdd<span class="string">")</span><br><span class="line">    var beginTime=datef.parse(beg)</span><br><span class="line">    var endTime=datef.parse(end)</span><br><span class="line">    while(beginTime.compareTo(endTime)&lt;=0)&#123;</span><br><span class="line">      cal.setTime(beginTime);</span><br><span class="line">      var ymd=datef.format(beginTime)</span><br><span class="line">      var df = spark.read.parquet(s"</span>/<span class="keyword">user</span>/hive/warehouse/cross_partner_hdfs/ds=$ymd<span class="string">")</span><br><span class="line">      df.repartition(1).write.mode("</span>overwrite<span class="string">").parquet(s"</span>/<span class="keyword">user</span>/hive/warehouse/crosspartner/ds=$ymd<span class="string">")</span><br><span class="line">      spark.sql(s"</span><span class="keyword">alter</span> <span class="keyword">table</span> crosspartner <span class="keyword">add</span> <span class="keyword">partition</span>(ds=<span class="string">'$ymd'</span>)<span class="string">")</span><br><span class="line">      cal.add(Calendar.DATE,1);</span><br><span class="line">      beginTime=cal.getTime();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">genCrossData("</span><span class="number">20170101</span><span class="string">","</span><span class="number">20170630</span><span class="string">")</span></span></span><br></pre></td></tr></table></figure>
<p>或者直接用parquet文件创建外部表：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"""</span><br><span class="line">create external table cross2partner_dt(</span><br><span class="line">  partnerCode string,</span><br><span class="line">  eventType string,</span><br><span class="line">  idNumber string,</span><br><span class="line">  accountMobile string,</span><br><span class="line">  accountEmail string,</span><br><span class="line">  accountPhone string,</span><br><span class="line">  deviceId string,</span><br><span class="line">  cardNumber string,</span><br><span class="line">  contact1Mobile string,</span><br><span class="line">  contact2Mobile string,</span><br><span class="line">  contact3Mobile string,</span><br><span class="line">  contact4Mobile string,</span><br><span class="line">  contact5Mobile string,</span><br><span class="line">  contact1IdNumber string,</span><br><span class="line">  contact2IdNumber string,</span><br><span class="line">  contact3IdNumber string,</span><br><span class="line">  contact4IdNumber string,</span><br><span class="line">  contact5IdNumber string,</span><br><span class="line">  sequenceId string    </span><br><span class="line">) </span><br><span class="line">partitioned by (ds string)</span><br><span class="line">stored as parquet</span><br><span class="line">location '/user/hive/warehouse/cross_partner_hdfs'</span><br><span class="line">"""</span>)</span><br><span class="line">spark.sql(s<span class="string">"alter table cross2partner_dt add partition(ds='20170101')"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">Calendar</span>,<span class="type">Date</span>&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genCrossData</span>(</span>beg: <span class="type">String</span>, end: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">var</span> cal = <span class="type">Calendar</span>.getInstance()</span><br><span class="line">    <span class="keyword">var</span> datef=<span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyyMMdd"</span>)</span><br><span class="line">    <span class="keyword">var</span> beginTime=datef.parse(beg)</span><br><span class="line">    <span class="keyword">var</span> endTime=datef.parse(end)</span><br><span class="line">    <span class="keyword">while</span>(beginTime.compareTo(endTime)&lt;=<span class="number">0</span>)&#123;</span><br><span class="line">      cal.setTime(beginTime);</span><br><span class="line">      <span class="keyword">var</span> ymd=datef.format(beginTime)</span><br><span class="line">      spark.sql(s<span class="string">"alter table cross2partner_dt add partition(ds='$ymd')"</span>)</span><br><span class="line">      cal.add(<span class="type">Calendar</span>.<span class="type">DATE</span>,<span class="number">1</span>);</span><br><span class="line">      beginTime=cal.getTime();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">genCrossData(<span class="string">"20170102"</span>,<span class="string">"20170630"</span>)</span><br></pre></td></tr></table></figure>
<p>一次性将所有数据插入carbondata太慢了</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">carbon.sql(s"<span class="operator"><span class="keyword">insert</span> <span class="keyword">into</span> cross_partner_carbon <span class="keyword">select</span> * <span class="keyword">from</span> crosspartner <span class="keyword">where</span> ds <span class="keyword">like</span> <span class="string">'$ymd%'</span><span class="string">")</span></span></span><br></pre></td></tr></table></figure>
<p><strong>改用按月/天插入carbondata表</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">Calendar</span>,<span class="type">Date</span>&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genCrossCarbonData</span>(</span>beg: <span class="type">String</span>, end: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">var</span> cal = <span class="type">Calendar</span>.getInstance()</span><br><span class="line">    <span class="keyword">var</span> datef=<span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyyMM"</span>)</span><br><span class="line">    <span class="keyword">var</span> beginTime=datef.parse(beg)</span><br><span class="line">    <span class="keyword">var</span> endTime=datef.parse(end)</span><br><span class="line">    <span class="keyword">while</span>(beginTime.compareTo(endTime)&lt;=<span class="number">0</span>)&#123;</span><br><span class="line">      cal.setTime(beginTime);</span><br><span class="line">      <span class="keyword">var</span> ymd=datef.format(beginTime)</span><br><span class="line">      println(ymd) </span><br><span class="line">      carbon.sql(s<span class="string">"insert into cross_partner_carbon select * from cross2partner_dt where ds like '$ymd%'"</span>)</span><br><span class="line">      cal.add(<span class="type">Calendar</span>.<span class="type">DATE</span>,<span class="number">1</span>);</span><br><span class="line">      beginTime=cal.getTime();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">genCrossCarbonData(<span class="string">"201701"</span>,<span class="string">"201706"</span>)</span><br></pre></td></tr></table></figure>
<p>导入数据时还是会报错：</p>
<p>增加内存：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark/shell \</span><br><span class="line">--conf spark<span class="class">.executor</span><span class="class">.instances</span>=<span class="number">15</span> \</span><br><span class="line">--conf spark<span class="class">.executor</span><span class="class">.cores</span>=<span class="number">2</span> \</span><br><span class="line">--conf spark<span class="class">.executor</span><span class="class">.memory</span>=<span class="number">8</span>g \</span><br><span class="line">--conf spark<span class="class">.driver</span><span class="class">.memory</span>=<span class="number">8</span>g \</span><br></pre></td></tr></table></figure>
<h4 id="2-_carbondata其他设置">2. carbondata其他设置</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">carbon.sql("""<span class="operator"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> crosspartner1(</span><br><span class="line">...</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">BY</span> <span class="string">'carbondata'</span></span><br><span class="line">TBLPROPERTIES (<span class="string">'DICTIONARY_EXCLUDE'</span>=<span class="string">'sequenceId,idNumber,accountMobile,accountEmail,accountPhone,deviceId,cardNumber,contact1Mobile,contact2Mobile,contact3Mobile,contact4Mobile,contact5Mobile,contact1IdNumber,contact2IdNumber,contact3IdNumber,contact4IdNumber,contact5IdNumber'</span>)</span><br><span class="line"><span class="string">""")</span><br><span class="line"></span><br><span class="line">carbon.sql("</span><span class="keyword">insert</span> <span class="keyword">into</span> crosspartner1 <span class="keyword">select</span> * <span class="keyword">from</span> cross_partner_hdfs<span class="string">")</span></span></span><br></pre></td></tr></table></figure>
<h4 id="3-_carbon_thrift_server">3. carbon thrift server</h4><figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="stylus">bin/spark-submit \</span><br><span class="line">--conf spark<span class="class">.sql</span><span class="class">.hive</span><span class="class">.thriftServer</span><span class="class">.singleSession</span>=true \</span><br><span class="line">--hiveconf hive<span class="class">.server2</span><span class="class">.thrift</span><span class="class">.port</span>=<span class="number">10002</span> \</span><br><span class="line">--hiveconf hive<span class="class">.server2</span><span class="class">.thrift</span><span class="class">.bind</span><span class="class">.host</span>=<span class="string">"192.168.39.25"</span> \</span><br><span class="line">--class org<span class="class">.apache</span><span class="class">.carbondata</span><span class="class">.spark</span><span class="class">.thriftserver</span><span class="class">.CarbonThriftServer</span> \</span><br><span class="line">carbonlib/carbondata_2.<span class="number">11</span>-<span class="number">1.1</span>.<span class="number">1</span>-shade-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> \</span><br><span class="line">hdfs:<span class="comment">//tdhdfs/user/tongdun/carbon</span></span><br><span class="line">hdfs:<span class="comment">//tdhdfs/user/hive/warehouse/carbon.store</span></span><br><span class="line">hdfs:<span class="comment">//tdhdfs/user/tongdun/carbondata/CarbonStore</span></span></span><br></pre></td></tr></table></figure>
<h4 id="4-_spark-2-2-0">4. spark-2.2.0</h4><p>carbondata-1.1.1目前不支持spark2.2。如果加上profile，更改spark版本为2.2.0，编译不通过</p>
<figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">$ mvn -DskipTests -Pspark-2.2 -Dspark.version=2.2.0 -Dhadoop.version=2.6.0 clean package</span><br><span class="line"></span><br><span class="line"><span class="comment">[WARNING]</span> /Users/zhengqh/Github/carbondata-parent-1.1.1/integration/spark-common/src/main/scala/org/apache/carbondata/spark/rdd/UpdateCoalescedRDD.scala:23: warning: imported `RDD' <span class="keyword">is</span> permanently hidden by definition <span class="keyword">of</span> class RDD in package rdd</span><br><span class="line"><span class="comment">[INFO]</span> import org.apache.spark.rdd.&#123;CoalescedRDDPartition, DataLoadPartitionCoalescer, RDD&#125;</span><br><span class="line"><span class="comment">[INFO]</span>                                                                                 ^</span><br><span class="line"><span class="comment">[WARNING]</span> /Users/zhengqh/Github/carbondata-parent-1.1.1/integration/spark-common/src/main/scala/org/apache/carbondata/spark/util/CarbonScalaUtil.scala:125: warning: non-variable type argument Any in type pattern scala.collection.Map<span class="comment">[Any,Any]</span> <span class="keyword">is</span> unchecked since it <span class="keyword">is</span> eliminated by erasure</span><br><span class="line"><span class="comment">[INFO]</span>         case m: scala.collection.Map<span class="comment">[Any, Any]</span> =&gt;</span><br><span class="line"><span class="comment">[INFO]</span>                                  ^</span><br><span class="line"><span class="comment">[ERROR]</span> /Users/zhengqh/Github/carbondata-parent-1.1.1/integration/spark-common/src/main/scala/org/apache/spark/sql/optimizer/CarbonDecoderOptimizerHelper.scala:87: error: value child <span class="keyword">is</span> not a member <span class="keyword">of</span> org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable</span><br><span class="line"><span class="comment">[INFO]</span>       case i: InsertIntoTable =&gt; process(i.child, nodeList)</span><br><span class="line"><span class="comment">[INFO]</span>                                            ^</span><br><span class="line"><span class="comment">[WARNING]</span> 11 warnings found</span><br><span class="line"><span class="comment">[ERROR]</span> one error found</span><br><span class="line"><span class="comment">[INFO]</span> ------------------------------------------------------------------------</span><br><span class="line"><span class="comment">[INFO]</span> Reactor Summary:</span><br><span class="line"><span class="comment">[INFO]</span></span><br><span class="line"><span class="comment">[INFO]</span> Apache CarbonData :: Parent ........................ SUCCESS <span class="comment">[  5.140 s]</span></span><br><span class="line"><span class="comment">[INFO]</span> Apache CarbonData :: Common ........................ SUCCESS <span class="comment">[ 10.114 s]</span></span><br><span class="line"><span class="comment">[INFO]</span> Apache CarbonData :: Core .......................... SUCCESS <span class="comment">[ 29.232 s]</span></span><br><span class="line"><span class="comment">[INFO]</span> Apache CarbonData :: Processing .................... SUCCESS <span class="comment">[  9.828 s]</span></span><br><span class="line"><span class="comment">[INFO]</span> Apache CarbonData :: Hadoop ........................ SUCCESS <span class="comment">[  5.719 s]</span></span><br><span class="line"><span class="comment">[INFO]</span> Apache CarbonData :: Spark Common .................. FAILURE <span class="comment">[01:10 min]</span></span><br><span class="line"><span class="comment">[INFO]</span> Apache CarbonData :: Spark Common Test ............. SKIPPED</span><br><span class="line"><span class="comment">[INFO]</span> Apache CarbonData :: Assembly ...................... SKIPPED</span><br><span class="line"><span class="comment">[INFO]</span> Apache CarbonData :: Spark2 ........................ SKIPPED</span><br><span class="line"><span class="comment">[INFO]</span> Apache CarbonData :: Spark2 Examples ............... SKIPPED</span><br><span class="line"><span class="comment">[INFO]</span> ------------------------------------------------------------------------</span><br><span class="line"><span class="comment">[INFO]</span> BUILD FAILURE</span><br><span class="line"><span class="comment">[INFO]</span> ------------------------------------------------------------------------</span><br><span class="line"><span class="comment">[INFO]</span> Total time: 02:10 min</span><br><span class="line"><span class="comment">[INFO]</span> Finished at: 2017-08-03T14:39:55+08:00</span><br><span class="line"><span class="comment">[INFO]</span> Final Memory: 72M/786M</span><br><span class="line"><span class="comment">[INFO]</span> ------------------------------------------------------------------------</span><br><span class="line"><span class="comment">[ERROR]</span> Failed to execute goal org.scala-tools:maven-scala-plugin:2.15.2:compile (default) on project carbondata-spark-common: wrap: org.apache.commons.exec.ExecuteException: Process exited with an error: 1(Exit value: 1) -&gt; <span class="comment">[Help 1]</span></span><br><span class="line"><span class="comment">[ERROR]</span></span><br><span class="line"><span class="comment">[ERROR]</span> To see the full stack trace <span class="keyword">of</span> the errors, re-run Maven with the -e switch.</span><br><span class="line"><span class="comment">[ERROR]</span> Re-run Maven using the -X switch to enable full debug logging.</span><br><span class="line"><span class="comment">[ERROR]</span></span><br><span class="line"><span class="comment">[ERROR]</span> For more information about the errors and possible solutions, please read the following articles:</span><br><span class="line"><span class="comment">[ERROR]</span> <span class="comment">[Help 1]</span> http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException</span><br><span class="line"><span class="comment">[ERROR]</span></span><br><span class="line"><span class="comment">[ERROR]</span> After correcting the problems, you can resume the build with the command</span><br><span class="line"><span class="comment">[ERROR]</span>   mvn &lt;goals&gt; -rf :carbondata-spark-common</span><br></pre></td></tr></table></figure>
<p>如果使用spark2.1.1编译的二进制包，放到spark2.2.0下，也会报错：</p>
<p><img src="http://img.blog.csdn.net/20170803143703134" alt="car"></p>
<p>spark-1.6.2</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">InsertIntoTable</span>(</span></span><br><span class="line">    table: <span class="type">LogicalPlan</span>,</span><br><span class="line">    partition: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Option</span>[<span class="type">String</span>]],</span><br><span class="line">    child: <span class="type">LogicalPlan</span>,</span><br><span class="line">    overwrite: <span class="type">Boolean</span>,</span><br><span class="line">    ifNotExists: <span class="type">Boolean</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">LogicalPlan</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">children</span>:</span> <span class="type">Seq</span>[<span class="type">LogicalPlan</span>] = child :: <span class="type">Nil</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">output</span>:</span> <span class="type">Seq</span>[<span class="type">Attribute</span>] = <span class="type">Seq</span>.empty</span><br><span class="line"></span><br><span class="line">  assert(overwrite || !ifNotExists)</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">lazy</span> <span class="keyword">val</span> resolved: <span class="type">Boolean</span> = childrenResolved &amp;&amp; child.output.zip(table.output).forall &#123;</span><br><span class="line">    <span class="keyword">case</span> (childAttr, tableAttr) =&gt;</span><br><span class="line">      <span class="type">DataType</span>.equalsIgnoreCompatibleNullability(childAttr.dataType, tableAttr.dataType)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>spark-2.2.0</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">InsertIntoTable</span>(</span></span><br><span class="line">    table: <span class="type">LogicalPlan</span>,</span><br><span class="line">    partition: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Option</span>[<span class="type">String</span>]],</span><br><span class="line">    query: <span class="type">LogicalPlan</span>,</span><br><span class="line">    overwrite: <span class="type">Boolean</span>,</span><br><span class="line">    ifPartitionNotExists: <span class="type">Boolean</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">LogicalPlan</span> &#123;</span><br><span class="line">  <span class="comment">// We don't want `table` in children as sometimes we don't want to transform it.</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">children</span>:</span> <span class="type">Seq</span>[<span class="type">LogicalPlan</span>] = query :: <span class="type">Nil</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">output</span>:</span> <span class="type">Seq</span>[<span class="type">Attribute</span>] = <span class="type">Seq</span>.empty</span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">lazy</span> <span class="keyword">val</span> resolved: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>更改为i.query后，重新编译：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[INFO] Apache CarbonData :: Assembly ...................... FAILURE [  2.180 s]</span><br><span class="line">[INFO] Apache CarbonData :: Spark2 ........................ SKIPPED</span><br><span class="line">[INFO] Apache CarbonData :: Spark2 Examples ............... SKIPPED</span><br><span class="line">[INFO] <span class="comment">------------------------------------------------------------------------</span></span><br><span class="line">[INFO] BUILD FAILURE</span><br><span class="line">[INFO] <span class="comment">------------------------------------------------------------------------</span></span><br><span class="line">[INFO] Total time: 01:57 min</span><br><span class="line">[INFO] Finished at: 2017-08-03T15:33:59+08:00</span><br><span class="line">[INFO] Final Memory: 83M/728M</span><br><span class="line">[INFO] <span class="comment">------------------------------------------------------------------------</span></span><br><span class="line">[ERROR] Failed to <span class="operator"><span class="keyword">execute</span> goal <span class="keyword">on</span> <span class="keyword">project</span> carbondata-<span class="keyword">assembly</span>: Could <span class="keyword">not</span> resolve dependencies <span class="keyword">for</span> <span class="keyword">project</span> org.apache.carbondata:carbondata-<span class="keyword">assembly</span>:pom:<span class="number">1.1</span><span class="number">.1</span>: Could <span class="keyword">not</span> find artifact org.apache.carbondata:carbondata-spark:jar:<span class="number">1.1</span><span class="number">.1</span> <span class="keyword">in</span> central (<span class="keyword">http</span>://repo1.maven.org/maven2) -&gt; [<span class="keyword">Help</span> <span class="number">1</span>]</span><br><span class="line">[<span class="keyword">ERROR</span>]</span><br><span class="line">[<span class="keyword">ERROR</span>] <span class="keyword">To</span> see the <span class="keyword">full</span> stack <span class="keyword">trace</span> <span class="keyword">of</span> the <span class="keyword">errors</span>, re-run Maven <span class="keyword">with</span> the -<span class="keyword">e</span> <span class="keyword">switch</span>.</span><br><span class="line">[<span class="keyword">ERROR</span>] Re-run Maven <span class="keyword">using</span> the -X <span class="keyword">switch</span> <span class="keyword">to</span> <span class="keyword">enable</span> <span class="keyword">full</span> debug <span class="keyword">logging</span>.</span><br><span class="line">[<span class="keyword">ERROR</span>]</span><br><span class="line">[<span class="keyword">ERROR</span>] <span class="keyword">For</span> more information about the <span class="keyword">errors</span> <span class="keyword">and</span> possible solutions, please <span class="keyword">read</span> the <span class="keyword">following</span> articles:</span><br><span class="line">[<span class="keyword">ERROR</span>] [<span class="keyword">Help</span> <span class="number">1</span>] <span class="keyword">http</span>://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException</span><br><span class="line">[<span class="keyword">ERROR</span>]</span><br><span class="line">[<span class="keyword">ERROR</span>] <span class="keyword">After</span> correcting the problems, you can <span class="keyword">resume</span> the <span class="keyword">build</span> <span class="keyword">with</span> the command</span><br><span class="line">[<span class="keyword">ERROR</span>]   mvn &lt;goals&gt; -rf :carbondata-<span class="keyword">assembly</span></span></span><br></pre></td></tr></table></figure>
<p>默认1.6版本的assembly无法下载1.1.1的pom,将默认版本改为(添加)2.2.0</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">profile</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">id</span>&gt;</span>spark-2.2<span class="tag">&lt;/<span class="title">id</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">activation</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">activeByDefault</span>&gt;</span>true<span class="tag">&lt;/<span class="title">activeByDefault</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">activation</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.carbondata<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>carbondata-spark2<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="title">version</span>&gt;</span>$&#123;project.version&#125;<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="title">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">profile</span>&gt;</span></span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache CarbonData&lt;/p&gt;
    
    </summary>
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/categories/bigdata/"/>
    
    
      <category term="spark" scheme="http://github.com/zqhxuyuan/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Deep into Apache Gearpump</title>
    <link href="http://github.com/zqhxuyuan/2017/06/25/2017-06-24-Gearpump/"/>
    <id>http://github.com/zqhxuyuan/2017/06/25/2017-06-24-Gearpump/</id>
    <published>2017-06-24T16:00:00.000Z</published>
    <updated>2017-06-25T15:24:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>Deep into Apache Gearpump<br><a id="more"></a></p>
<p><strong>Prefix</strong>: I’ve heard Gearpump nearly one or two years ago, but never take a deep look inside. Until recently I’m almost done writing my chinese book about kafka internal implimentation, and decide to add some kafka relation opensouce system to my book’s appendix, such as spark streaming,storm,flink, and gearpump! So I finaly have a chance to deep into Gearpump.</p>
<h2 id="Introduce">Introduce</h2><p>According to offical documentation: “Gearpump is a 100% Akka based platform. We model big data streaming within the Akka actor hierarchy”. Below It’s Gearpump Actor Hierarchy architecture. PS: If you don’t know Actor right now, It’s fine, just think that’s another RPC layer or message transformer.</p>
<p><img src="http://gearpump.apache.org/releases/latest/img/actor_hierarchy.png" alt="geararch"></p>
<p>Everything in the diagram is an actor; they fall into two categories, Cluster Actors and Application Actors.</p>
<p><strong>Cluster Actors</strong></p>
<p><em>Worker</em>: Maps to a physical worker machine. It is responsible for <span style="border-bottom:1px dashed red;">managing resources</span> and report metrics on that machine.</p>
<p><em>Master</em>: Heart of the cluster, which <span style="border-bottom:1px dashed red;">manages workers, resources, and applications</span>. The main function is delegated to three child actors, App Manager, Worker Manager, and Resource Scheduler.</p>
<p><strong>Application Actors</strong></p>
<p><em>AppMaster</em>: Responsible to <span style="border-bottom:1px dashed red;">schedule the tasks to workers</span> and manage the state of the application. Different applications have different AppMaster instances and are isolated.</p>
<p><em>Executor</em>: Child of AppMaster, represents a JVM process. Its job is to <span style="border-bottom:1px dashed red;">manage the life cycle of tasks</span> and recover the tasks in case of failure.</p>
<p><em>Task</em>: Child of Executor, does the real job. Every task actor has a global unique address. One task actor can send data to any other task actors. This gives us great flexibility of how the computation DAG is distributed.</p>
<blockquote>
<p>All actors in the graph are weaved together with actor supervision, and actor watching and every error is handled properly via supervisors. In a master, a risky job is isolated and delegated to child actors, so it’s more robust. In the application, an extra intermediate layer “Executor” is created so that we can do fine-grained and fast recovery in case of task failure. A master watches the lifecycle of AppMaster and worker to handle the failures, but the life cycle of Worker and AppMaster are not bound to a Master Actor by supervision, so that Master node can fail independently. Several Master Actors form an Akka cluster, the Master state is exchanged using the Gossip protocol in a conflict-free consistent way so that there is no single point of failure. With this hierarchy design, we are able to achieve high availability.</p>
</blockquote>
<p>Next It’s a good entrance to knowing some <a href="http://gearpump.apache.org/releases/latest/introduction/basic-concepts/index.html" target="_blank" rel="external">basic concepts</a>. It’s very necessary, you should first take a detail/serious look at if you want to know how gearpump works.</p>
<p><strong>Master &amp; Worker</strong></p>
<blockquote>
<p>Gearpump follow <strong>master slave architecture</strong>. Every cluster contains one or more Master node, and several worker nodes. Worker node is responsible to manage local resources on single machine, and Master node is responsible to manage global resources of the whole cluster.</p>
</blockquote>
<p>If you have already know hadoop/spark such bigdata system, you should familiar those terminology. Here is the first comparison about gearpump and other system.</p>
<table>
<thead>
<tr>
<th>bigdata system</th>
<th>Master</th>
<th>Slave</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop HDFS</td>
<td>NameNode</td>
<td>DataNode</td>
</tr>
<tr>
<td>Hadoop YARN</td>
<td>ReourceManager</td>
<td>NodeManager</td>
</tr>
<tr>
<td>Spark</td>
<td>ClusterManagement</td>
<td>Worker</td>
</tr>
<tr>
<td>Storm</td>
<td>Nimbus</td>
<td>Supervisor</td>
</tr>
<tr>
<td>Gearpump</td>
<td>Master</td>
<td>Worker</td>
</tr>
</tbody>
</table>
<p><strong>Application &amp; AppMaster &amp; Executor</strong></p>
<blockquote>
<p><strong>Application</strong> is what we want to parallel and run on the cluster. There are different application types, for example MapReduce application and streaming application are different application types. Gearpump natively supports Streaming Application types, it also contains several templates to help user to create custom application types, like distributedShell.</p>
<p>In runtime, every application instance is represented by a single <strong>AppMaster</strong> and a list of <strong>Executors</strong>. AppMaster represents the command and controls center of the Application instance. It communicates with user, master, worker, and executor to get the job done. Each executor is a parallel unit for distributed application. Typically AppMaster and Executor will be started as JVM processes on worker nodes.</p>
</blockquote>
<p>Now we have talking all important components in gearpump. Notice here we did’t mentioned Task as appeared in previous actor hierarchy. Also notice that Application is not an actor but an Java main class. Next take a look at Application Submission Flow in gearpump.</p>
<blockquote>
<p>When user submits an application to Master, Master will first find an available worker to start the AppMaster. After AppMaster is started, AppMaster will request Master for more resources (worker) to start executors. The Executor now is only an empty container. After the executors are started, the AppMaster will then distribute real computation tasks to the executor and run them in parallel way.</p>
<p>To submit an application, a Gearpump client specifies a computation defined within a DAG and submits this to an active master. The SubmitApplication message is sent to the Master who then forwards this to an AppManager. </p>
</blockquote>
<p><img src="http://gearpump.apache.org/releases/latest/img/submit.png" alt="submit app"> </p>
<blockquote>
<p>The AppManager locates an available worker and launches an AppMaster in a sub-process JVM of the worker. The AppMaster will then negotiate with the Master for Resource allocation in order to distribute the DAG as defined within the Application. The allocated workers will then launch Executors (new JVMs).</p>
</blockquote>
<p><img src="http://gearpump.apache.org/releases/latest/img/submit2.png" alt="launch"></p>
<p>Here I summary basic steps of submit application. notice the step number below are’t corresponding to the official pictures above.</p>
<ol>
<li>User(client) submits an streaming application to gearpump Master;</li>
<li>Master forward <code>SubmitApplication</code> request to AppManager;</li>
<li>Master will first find an available worker to start the AppMaster;</li>
<li>AppMaster started(as Executor) on one of worker which master specified, until now, AppManager on Master can send  <code>SubmitApplicationResult</code> to client;</li>
<li>AppMaster send <code>RequestResource</code> to master, the purpose of this step is ask resources to run/launch Tasks which doing real job. After all, AppMaster is not responsible to running job, but instead let Tasks doing the job. Notice the lifecycle of both AppMaster and Tasks all resides in Executors. So If you want to start AppMaster or Task, you first must start Executor, then let Executor start AppMaster and Task;</li>
<li>Once AppMaster receive <code>ResouceAllocated</code> response, it’ll send <code>LaunchExecutor</code> to workers which Master pointing out where to go. For ex, the ResouceAllocated response says by Master to AppMaster: you can run executors on workers #1 and #2. Then AppMaster will send LaunchExecutor request to this two workers;</li>
<li>The Workers receive LaunchExecutor request from AppMaster, it then spawn an Executor as a java process. The reason why spawn a new process here is that the Executor and Worker thread should separate, which means the working process of Executor and Worker shouldn’t affect each other;</li>
<li>Just like Worker register to Master for reporting resources, the Executor also register to AppMaster by sending <code>RegisterExecutor</code> request. If someone regist to other-one, that means someone wants to be managed/controlled by other-one. for example, students regist to school, company regist to Mainland China, employee regist to company and so on;</li>
<li>The AppMaster receive <code>RegisterExecutor</code> request from Executor on Worker, it then ask Executor to start Task;</li>
<li>As AppMaster may getting more than one resouce at step6, and each Executor all register to AppMaster, so AppMaster can start multi task on this registerd Executor;</li>
<li>Each Task reside in Executor has DAG information defined within Application, so every Task can doing real job.</li>
</ol>
<p>The workflow above was extraordinary like yarn application below. I take the picture and description from <a href="https://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/" target="_blank" rel="external">this excellent hortonworks blog</a>.</p>
<ol>
<li>A client program <em>submits</em> the application, including the necessary specifications to launch the application-specific <strong>ApplicationMaster</strong> itself.</li>
<li>The <strong>ResourceManager</strong> assumes the responsibility to negotiate a specified container in which to <em>start</em> the ApplicationMaster and then <em>launches</em> the ApplicationMaster.</li>
<li>The ApplicationMaster, on boot-up, <em>registers</em> with the ResourceManager – the registration allows the client program to query the ResourceManager for details, which allow it to  directly communicate with its own ApplicationMaster.</li>
<li>During normal operation the ApplicationMaster negotiates appropriate resource containers via the <em>resource-request </em>protocol.</li>
<li>On successful <em>container allocations</em>, the ApplicationMaster <em>launches</em> the container by providing the container launch specification to the NodeManager. The launch specification, typically, includes the necessary information to allow the container to communicate with the ApplicationMaster itself.</li>
<li>The application code executing within the container then provides necessary information (progress, status etc.) to its ApplicationMaster via an application-specific protocol.</li>
<li>During the application execution, the client that submitted the program communicates directly with the ApplicationMaster to get status, progress updates etc. via an application-specific protocol.</li>
<li>Once the application is complete, and all necessary work has been finished, the ApplicationMaster deregisters with the ResourceManager and shuts down, allowing its own container to be repurposed.</li>
</ol>
<p><img src="https://2xbbhjxc6wk3v21p62t8n4d4-wpengine.netdna-ssl.com/wp-content/uploads/2012/08/yarnflow.png" alt="yarn"></p>
<p>The picture above start two client application to yarn cluster, the ApplicationMaster reside on node2 of red one start three containers on node1 and node3, the ApplicationMaster reside on node1 of blue one only start one container. </p>
<p>In yarn, ResouceManager take responsible to launch ApplicationManager on one of container, and launching Tasks on containers is the responsibility of ApplicationManager. But as you know, the ApplicationManager did’t know cluster resources, so he ask ResouceManager to give him the information of where to start tasks. Now we summary some conclusions:</p>
<ol>
<li>ResouceManager launch ApplicationManager on one of NodeManager.</li>
<li>ApplicationManager launch Tasks on multi NodeManagers.</li>
<li>NodeManagers report resouce to ResouceManager.</li>
<li>Containers report task execution progress to ApplicationManager.  </li>
<li>ResouceManager manager ApplicationManager, and ApplicationManager manager tasks. If all tasks monitored by ApplicationManager was finished, then Application registered to ReousceManager was completed.</li>
</ol>
<p>Step into gearpump, there are similiarity idea inspired from yarn. We could take yarn’s container as gearpump’s Executor, and yarn’s NodeManager as gearpump’s Worker. Because Containers reside in NodeManager at yarn world, and Executors reside in Worker at gearpump world.</p>
<p><img src="http://img.blog.csdn.net/20170624165428321" alt="yarn-gp"></p>
<p>We could also consider yarn’s ResouceManager as gearpump’s AppManager. Note that AppManager is different from AppMaster, which the former is at Master side, and the latter is at Worker side.</p>
<p>The Master in Gearpump have three main components: AppManager,Scheduler,Worker Manager. In reality, there are non WorkerManager class around gearpump source code,but Master indeed has a map which mapping Worker ActorRef to WorkerId. </p>
<p>After oveview gearpump architecture, Let’s begin explore gearpump inside now.</p>
<h2 id="Part-1:_Application">Part-1: Application</h2><p>First given a WordCount example, We sumbit an StreamApplication through ClientContext. Inside the application() method, we create three <code>Processor</code> and connect by <code>~</code> to construct a DAG graph.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">AkkaApp</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">application</span>(</span>config: <span class="type">ParseResult</span>, system: <span class="type">ActorSystem</span>): </span><br><span class="line">      <span class="type">StreamApplication</span> = &#123;</span><br><span class="line">    <span class="keyword">implicit</span> <span class="keyword">val</span> actorSystem = system</span><br><span class="line">    <span class="keyword">val</span> split = <span class="keyword">new</span> <span class="type">Split</span></span><br><span class="line">    <span class="keyword">val</span> sourceProcessor = <span class="type">DataSourceProcessor</span>(split, <span class="number">2</span>, <span class="string">"Split"</span>)</span><br><span class="line">    <span class="keyword">val</span> sum = <span class="type">Processor</span>[<span class="type">Sum</span>](<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> computation = sourceProcessor ~ <span class="type">HashPartitioner</span> ~&gt; sum</span><br><span class="line">    <span class="keyword">val</span> app = <span class="type">StreamApplication</span>(<span class="string">"wordCount"</span>, <span class="type">Graph</span>(computation))</span><br><span class="line">    app</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span>(</span>akkaConf: <span class="type">Config</span>, args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> context: <span class="type">ClientContext</span> = <span class="type">ClientContext</span>(akkaConf)</span><br><span class="line">    <span class="keyword">val</span> app = application(config, context.system)</span><br><span class="line">    context.submit(app)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StreamApplication is one of gearpump supported application type, there’re other applications such as MapReduce could run in gearpump. Each Application type has special appMaster class, StreamApplication’s appMaster is AppMaster. There’re some other ApplicationMaster actor implementation embeded: DistShellAppMaster,DistServiceAppMaster,and AppMaster.</p>
<p>Note Application is a scala App, but ApplicationMaster is an Actor. So what’s different between an App and and Actor? Well, App normaly has a main method doing what you want, but actor doing much more complicate thing.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Application</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">name</span>:</span> <span class="type">String</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">userConfig</span>(</span><span class="keyword">implicit</span> system: <span class="type">ActorSystem</span>): <span class="type">UserConfig</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">appMaster</span>:</span> <span class="type">Class</span>[_ &lt;: <span class="type">ApplicationMaster</span>]</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ApplicationMaster</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">Actor</span></span><br><span class="line"></span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StreamApplication</span>(</span>) <span class="keyword">extends</span> <span class="type">Application</span> &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">appMaster</span>:</span> <span class="type">Class</span>[_ &lt;: <span class="type">ApplicationMaster</span>] = classOf[<span class="type">AppMaster</span>] </span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppMaster</span>(</span>appContext: <span class="type">AppMasterContext</span>, app: <span class="type">AppDescription</span>) </span><br><span class="line">  <span class="keyword">extends</span> <span class="type">ApplicationMaster</span> &#123;...&#125;</span><br></pre></td></tr></table></figure>
<p>ClientContext is a user facing util to submit/manage an application. The AppDescription describe application metadata such as appMaster name(here is AppMaster).</p>
<p>In the Akka world, Actor is the king. Client send SubmitApplication request to Master Actor, and expect get SubmitApplicationResult response from Master. Messages are sent to an Actor through one of the following methods.</p>
<ul>
<li><code>!</code> means “fire-and-forget”, e.g. send a message asynchronously and return immediately. Also known as tell.</li>
<li><code>?</code> sends a message asynchronously and returns a Future representing a possible reply. Also known as ask. That’s the way client submit application doing here.</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClientContext</span>(</span>config: <span class="type">Config</span>, sys: <span class="type">ActorSystem</span>, _master: <span class="type">ActorRef</span>) &#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">submit</span>(</span>app: <span class="type">Application</span>, jar: <span class="type">String</span>, executorNum: <span class="type">Int</span>)= &#123;</span><br><span class="line">    <span class="keyword">val</span> appName = ...</span><br><span class="line">    <span class="keyword">val</span> submissionConfig = ...</span><br><span class="line">    <span class="keyword">val</span> appDescription = <span class="type">AppDescription</span>(appName,app.appMaster.getName,...)</span><br><span class="line">    <span class="keyword">val</span> appJar = <span class="type">Option</span>(jar).map(loadFile)</span><br><span class="line">    submitApplication(<span class="type">SubmitApplication</span>(appDescription, appJar))</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitApplication</span>(</span>submitApplication: <span class="type">SubmitApplication</span>)=&#123;</span><br><span class="line">    <span class="keyword">val</span> result = <span class="type">ActorUtil</span>.askActor[<span class="type">SubmitApplicationResult</span>](</span><br><span class="line">        master, submitApplication, masterClientTimeout)</span><br><span class="line">    <span class="keyword">val</span> application = result.appId <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Success</span>(appId) =&gt;</span><br><span class="line">        <span class="type">Console</span>.println(s<span class="string">"Submit app succeed. The app id is $appId"</span>)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">RunningApplication</span>(appId, master, masterClientTimeout)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Failure</span>(ex) =&gt; <span class="keyword">throw</span> ex</span><br><span class="line">    &#125;</span><br><span class="line">    application</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Now Let’s see how Master deal with SubmitApplication. Before this, you should know that client only submit application when Master has started. Also note that when start Master, we also start some Workers to form a gearpump cluster. Only then the cluster is stabled, client then can submit application. We can see that when startup Master, in preStart() method, Master created an AppManager and Scheduler by invoking <code>context.actorOf(...)</code>. That means before client submit application, AppManager and Scheduler already exists in Master, and they both preparing to work.</p>
<p>We’re also seeing a <code>receiveHandler()</code> method return Receive object, and was invoked by <code>waitForNextWorkerId()</code> method. What <code>context.become()</code> and <code>orElse</code> meaning? well, normaly you define one receive method, but here you have seen there’re multi receive method, so become() method of ActorContext is used for switchover between different receive method.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[cluster] <span class="class"><span class="keyword">class</span> <span class="title">Master</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">Actor</span> <span class="keyword"><span class="keyword">with</span></span> <span class="title">Stash</span> &#123;</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> kvService = context.actorOf(<span class="type">Props</span>(<span class="keyword">new</span> <span class="type">InMemoryKVService</span>()))</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> appManager: <span class="type">ActorRef</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> scheduler: <span class="type">ActorRef</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> workers = <span class="keyword">new</span> immutable.<span class="type">HashMap</span>[<span class="type">ActorRef</span>, <span class="type">WorkerId</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment">// when start up Master, send GetKV to kvService immediatery</span></span><br><span class="line">  kvService ! <span class="type">GetKV</span>(<span class="type">MASTER_GROUP</span>, <span class="type">WORKER_ID</span>) </span><br><span class="line">  context.become(waitForNextWorkerId) <span class="comment">// wait for getting result </span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForNextWorkerId</span>:</span> <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">GetKVSuccess</span>(_, result) =&gt; <span class="comment">// receive GetKV response</span></span><br><span class="line">      context.become(receiveHandler) <span class="comment">// switchover to receiveHandler</span></span><br><span class="line">      unstashAll()</span><br><span class="line">    <span class="keyword">case</span> msg =&gt; stash() <span class="comment">// why do we stash here?</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receiveHandler</span>:</span> <span class="type">Receive</span> = workerMsgHandler orElse</span><br><span class="line">    appMasterMsgHandler orElse <span class="comment">// AppMaster to Master</span></span><br><span class="line">    onMasterListChange orElse <span class="comment">// Master change</span></span><br><span class="line">    clientMsgHandler orElse <span class="comment">// Client to Master. you'll see submit app here</span></span><br><span class="line">    kvServiceMsgHandler orElse <span class="type">ActorUtil</span>.defaultMsgHandler(self)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">preStart</span>(</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    appManager = context.actorOf(</span><br><span class="line">        <span class="type">Props</span>(<span class="keyword">new</span> <span class="type">AppManager</span>(kvService, <span class="type">AppMasterLauncher</span>)),</span><br><span class="line">        classOf[<span class="type">AppManager</span>].getSimpleName)</span><br><span class="line">    scheduler = context.actorOf(<span class="type">Props</span>(schedulerClass))</span><br><span class="line">    context.system.eventStream.subscribe(self,classOf[<span class="type">DisassociatedEvent</span>])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Now you have overview the main function in Master, lets see how clientMsgHandler receive method response to client’s submit application request. I have omit other unimportance request only left submit and restart application. The Master delegate/forward reqeust to AppManager.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clientMsgHandler</span>:</span> Receive = &#123;</span><br><span class="line">  case app: SubmitApplication =&gt; appManager.forward(app)</span><br><span class="line">  case app: RestartApplication =&gt; appManager.forward(app)</span><br><span class="line">  case register: RegisterAppResultListener =&gt; appManager forward register</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>AppManager is dedicated child of Master to manager all applications. The AppManager behaviour similar as Master.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[cluster] <span class="class"><span class="keyword">class</span> <span class="title">AppManager</span>(</span></span><br><span class="line">    kvService: <span class="type">ActorRef</span>, launcher: <span class="type">AppMasterLauncherFactory</span>) </span><br><span class="line">    <span class="keyword">extends</span> <span class="type">Actor</span> <span class="keyword">with</span> <span class="type">Stash</span> <span class="keyword">with</span> <span class="type">TimeOutScheduler</span> &#123;</span><br><span class="line"></span><br><span class="line">  kvService ! <span class="type">GetKV</span>(<span class="type">MASTER_GROUP</span>, <span class="type">MASTER_STATE</span>)</span><br><span class="line">  context.become(waitForMasterState)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForMasterState</span>:</span> <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">GetKVSuccess</span>(_, result) =&gt;</span><br><span class="line">      context.become(receiveHandler)</span><br><span class="line">      unstashAll()</span><br><span class="line">    <span class="keyword">case</span> msg =&gt; stash()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receiveHandler</span>:</span> <span class="type">Receive</span> = &#123;</span><br><span class="line">    clientMsgHandler orElse <span class="comment">// Client to AppManager</span></span><br><span class="line">      appMasterMessage orElse <span class="comment">// AppMaster to AppManager</span></span><br><span class="line">      selfMsgHandler orElse</span><br><span class="line">      workerMessage orElse <span class="comment">// Worker to AppManager</span></span><br><span class="line">      appDataStoreService orElse terminationWatch</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">clientMsgHandler</span>:</span> <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SubmitApplication</span>(app, jar, username) =&gt;</span><br><span class="line">      <span class="keyword">val</span> client = sender()</span><br><span class="line">      context.actorOf(launcher.props(</span><br><span class="line">        nextAppId, -<span class="number">1</span>, app, jar, username, context.parent, client))</span><br><span class="line">      <span class="comment">// ommit something like save application metadata to kv store</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Master create AppManager by invoke <code>context.actorOf(Props(...))</code>, here AppManager create AppMasterLauncher Actor by <code>context.actorOf(launcher.props(..))</code>. <strong><code>AppMasterLauncher</code> is a child Actor of <code>AppManager</code>, it is responsible to launch the <code>AppMaster</code> on the cluster.</strong> </p>
<p>When AppManager receive SubmitApplication from client, it create AppMasterLauncher, and send RequestResource to master then wait for ResourceAllocation. </p>
<p>When AppMasterLauncher receive ResourceAllocated response from master, it will Try to launch a executor for AppMaster on worker specified by ResourceAllocated response.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppMasterLauncher</span>(</span>...,master: <span class="type">ActorRef</span>, client: <span class="type">ActorRef</span>) <span class="keyword">extends</span> <span class="type">Actor</span> &#123;</span><br><span class="line">  <span class="type">LOG</span>.info(s<span class="string">"Ask Master resource to start AppMaster $appId..."</span>)</span><br><span class="line">  master ! <span class="type">RequestResource</span>(appId, <span class="type">ResourceRequest</span>(<span class="type">Resource</span>(<span class="number">1</span>))</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receive</span>:</span> <span class="type">Receive</span> = waitForResourceAllocation</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForResourceAllocation</span>:</span> <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">ResourceAllocated</span>(allocations) =&gt;</span><br><span class="line">      <span class="keyword">val</span> <span class="type">ResourceAllocation</span>(resource, worker, workerId) = allocations(<span class="number">0</span>)</span><br><span class="line">      <span class="keyword">val</span> workerInfo = <span class="type">WorkerInfo</span>(workerId, worker)</span><br><span class="line">      <span class="keyword">val</span> appMasterContext = <span class="type">AppMasterContext</span>(...)</span><br><span class="line">      <span class="comment">// Try to launch a executor for AppMaster on worker for app</span></span><br><span class="line">      <span class="keyword">val</span> name = <span class="type">ActorUtil</span>.actorNameForExecutor(appId, executorId)</span><br><span class="line">      <span class="keyword">val</span> selfPath = <span class="type">ActorUtil</span>.getFullPath(context.system, self.path)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> executorJVM = <span class="type">ExecutorJVMConfig</span>(</span><br><span class="line">        classOf[<span class="type">ActorSystemBooter</span>].getName, <span class="type">Array</span>(name, selfPath), jar,</span><br><span class="line">        username, appMasterAkkaConfig)</span><br><span class="line"></span><br><span class="line">      worker ! <span class="type">LaunchExecutor</span>(appId, executorId, resource, executorJVM)</span><br><span class="line">      context.become(waitForActorSystemToStart(worker, appMasterContext, resource))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Let’s see how Worker deal with LaunchExecutor reqeust from AppMasterLauncher.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[cluster] <span class="class"><span class="keyword">class</span> <span class="title">Worker</span>(</span>masterProxy: <span class="type">ActorRef</span>) <span class="keyword">extends</span> <span class="type">Actor</span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">service</span>:</span> <span class="type">Receive</span> = appMasterMsgHandler orElse clientMessageHandler </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">appMasterMsgHandler</span>:</span> <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> launch: <span class="type">LaunchExecutor</span> =&gt;</span><br><span class="line">      <span class="keyword">val</span> executor = context.actorOf(<span class="type">Props</span>(classOf[<span class="type">ExecutorWatcher</span>], </span><br><span class="line">        launch, masterInfo, ioPool, jarStoreClient, executorProcLauncher))</span><br><span class="line">      context.watch(executor)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The ExecutorWatcher create a java process and the main class <code>ActorSystemBooter</code> is coming from ExecutorJVMConfig which defined in AppMasterLauncher.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExecutorWatcher</span>(</span>launch: <span class="type">LaunchExecutor</span>, </span><br><span class="line">    procLauncher: <span class="type">ExecutorProcessLauncher</span>) <span class="keyword">extends</span> <span class="type">Actor</span> &#123;</span><br><span class="line">  <span class="keyword">val</span> ctx = launch.executorJvmConfig</span><br><span class="line">  procLauncher.createProcess(ctx.mainClass, ctx.arguments)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ExecutorWatcher is an Actor, ActorSystemBooter is an pure scala app. But inside ActorSystemBooter’s main method, it create another actor: Daemon.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorSystemBooter</span>(</span>config: <span class="type">Config</span>) &#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">boot</span>(</span>name: <span class="type">String</span>, reportBackActor: <span class="type">String</span>): <span class="type">ActorSystem</span> = &#123;</span><br><span class="line">    system.actorOf(<span class="type">Props</span>(classOf[<span class="type">Daemon</span>], name, reportBackActor), <span class="string">"daemon"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ActorSystemBooter</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>config: <span class="type">Config</span>): <span class="type">ActorSystemBooter</span> = <span class="keyword">new</span> <span class="type">ActorSystemBooter</span>(config)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span>(</span>args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> name = args(<span class="number">0</span>) <span class="comment">// The parameter was passed when construnct </span></span><br><span class="line">    <span class="keyword">val</span> reportBack = args(<span class="number">1</span>) <span class="comment">// ExecutorJVMConfig at AppMasterLauncher</span></span><br><span class="line">    apply(config).boot(name, reportBack)</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Daemon</span>(</span><span class="keyword">val</span> name: <span class="type">String</span>, reportBack: <span class="type">String</span>) <span class="keyword">extends</span> <span class="type">Actor</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> reportBackActor = context.actorSelection(reportBack)</span><br><span class="line">    reportBackActor ! <span class="type">RegisterActorSystem</span>(</span><br><span class="line">        <span class="type">ActorUtil</span>.getSystemAddress(context.system).toString)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Those many Actor headache me, and the invoke chain nest and nest again. So I draw a picture to help me understand what happend all the way around. To make my picture looks vividly, I use gear to indicate an Actor, you can see except ActorSystemBooter, all others are Actor. The underline character means request. Let me outlines some import steps.</p>
<ol>
<li>AppManager create AppMasterLauncher which then send RequestResource to Master</li>
<li>After AppMasterLauncher receive ResourceAllocated, it send LauncherExecutor request to Worker</li>
<li>Worker create an ExecutorWatcher and create a java Daemon process which send RegisterActorSystem request back to Master</li>
</ol>
<p><img src="http://img.blog.csdn.net/20170624205039225" alt="9"></p>
<p>Now the AppMasterLauncher is going to deal with RegisterActorSystem request. If you backward to check AppMasterLauncher, you can find that: after AppMasterLauncher send LaunchExecutor, it is waiting for ActorSystem to start.</p>
<p>After Daemon actor in Worker send <code>RegisterActorSystem</code> request to <code>AppMasterLauncher</code>, the AppMasterLauncher finally have chance to receive RegisterActorSystem event, first it send <code>ActorSystemRegistered</code> request to Daemon, and then send another request <code>CreateActor</code> to Daemon again.</p>
<ol>
<li>Daemon on Worker send RegisterActorSystem request to AppMasterLauncher</li>
<li>AppMasterLauncher on Master send ActorSystemRegistered to Daemon on Worker</li>
<li>AppMasterLauncher on Master send CreateActor to Daemon on Worker</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppMasterLauncher</span>(</span>...,master: <span class="type">ActorRef</span>, client: <span class="type">ActorRef</span>) <span class="keyword">extends</span> <span class="type">Actor</span> &#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForResourceAllocation</span>:</span> <span class="type">Receive</span> = &#123;</span><br><span class="line">    worker ! <span class="type">LaunchExecutor</span>(appId, executorId, resource, executorJVM)</span><br><span class="line">    context.become(</span><br><span class="line">        waitForActorSystemToStart(worker, appMasterContext, resource))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForActorSystemToStart</span>(</span>worker: <span class="type">ActorRef</span>, appContext: <span class="type">AppMasterContext</span>,</span><br><span class="line">      resource: <span class="type">Resource</span>): <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisterActorSystem</span>(systemPath) =&gt;</span><br><span class="line">      sender ! <span class="type">ActorSystemRegistered</span>(worker)</span><br><span class="line">      <span class="comment">// There're many masters construct Master HA in case of fault</span></span><br><span class="line">      <span class="keyword">val</span> masterAddress = systemConfig.getStringList(<span class="type">GEARPUMP_CLUSTER_MASTERS</span>)</span><br><span class="line">        .asScala.map(<span class="type">HostPort</span>(_)).map(<span class="type">ActorUtil</span>.getMasterActorPath)</span><br><span class="line"></span><br><span class="line">      sender ! <span class="type">CreateActor</span>(</span><br><span class="line">        <span class="type">AppMasterRuntimeEnvironment</span>.props(masterAddress, app, appContext))</span><br><span class="line">      context.become(waitForAppMasterToStart(worker, appMasterTimeout))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForAppMasterToStart</span>(</span>worker: <span class="type">ActorRef</span>, cancel: <span class="type">Cancellable</span>)= &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">ActorCreated</span>(appMaster, _) =&gt;</span><br><span class="line">      cancel.cancel()</span><br><span class="line">      sender ! <span class="type">BindLifeCycle</span>(appMaster)</span><br><span class="line">      <span class="type">LOG</span>.info(s<span class="string">"AppMaster is created, mission complete..."</span>)</span><br><span class="line">      replyToClient(<span class="type">SubmitApplicationResult</span>(<span class="type">Success</span>(appId)))</span><br><span class="line">      context.stop(self)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Seems AppMasterLauncher and Daemon are playing ping-pong, and they both back and forth many times. Finally after Daemon create another Actor which we’ll talk about later, it then send ActorCreated back to AppMasterLauncher. </p>
<ol>
<li>Daemon on Worker send ActorCreated reqeust to AppMasterLauncher on Master</li>
<li>AppMasterLauncher send BindLifeCycle request back to Daemon on Worker</li>
<li>and then send SubmitApplicationResult back to Client</li>
<li>Daemon on Worker receive BindLifeCycle request from AppMasterLauncher and watch the actor. this actor being watched by Daemon is AppMaster.</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Daemon</span>(</span><span class="keyword">val</span> name: <span class="type">String</span>, reportBack: <span class="type">String</span>) <span class="keyword">extends</span> <span class="type">Actor</span> &#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForRegisterResult</span>:</span> <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">ActorSystemRegistered</span>(parent) =&gt;</span><br><span class="line">      timeout.cancel()</span><br><span class="line">      context.watch(parent)</span><br><span class="line">      context.become(waitCommand)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitCommand</span>:</span> <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">BindLifeCycle</span>(actor) =&gt;</span><br><span class="line">      <span class="type">LOG</span>.info(s<span class="string">"ActorSystem $name Binding life cycle with actor: $actor"</span>)</span><br><span class="line">      context.watch(actor)</span><br><span class="line">    <span class="keyword">case</span> create<span class="annotation">@CreateActor</span>(props: <span class="type">Props</span>, name: <span class="type">String</span>) =&gt;</span><br><span class="line">      <span class="keyword">val</span> actor = <span class="type">Try</span>(context.actorOf(props, name)) <span class="comment">// create another actor</span></span><br><span class="line">      actor <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Success</span>(actor) =&gt; sender ! <span class="type">ActorCreated</span>(actor, name)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Failure</span>(e) =&gt; sender ! <span class="type">CreateActorFailed</span>(props.clazz.getName, e)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">PoisonPill</span> =&gt;</span><br><span class="line">      context.stop(self)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Terminated</span>(actor) =&gt;</span><br><span class="line">      <span class="type">LOG</span>.info(s<span class="string">"System $name Watched actor is terminated $actor"</span>)</span><br><span class="line">      context.stop(self)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20170624212741793" alt="9"></p>
<p>Daemon create an Actor which defined in RegisterActorSystem on AppMasterLauncher. This Actor is <code>AppMasterRuntimeEnvironment</code>, it’ll create AppMaster. </p>
<p>We know that create Actor can use <code>context.actorOf(props)</code> method, here the props is passed from AppMasterLauncher to Daemon, but not created on Daemon side. Why do we doing this way? Because only AppMasterLauncher know how to create an AppMaster. Passing the props inside CreateActor is just like passing other request. Now the mainpoint focus transfer to AppMasterRuntimeEnvironment.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AppMasterRuntimeEnvironment</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">props</span>(</span>masters: <span class="type">Iterable</span>[<span class="type">ActorPath</span>], </span><br><span class="line">      app: <span class="type">AppDescription</span>, appContextInput: <span class="type">AppMasterContext</span></span><br><span class="line">      ): <span class="type">Props</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> master = (appId: <span class="type">AppId</span>, masterProxy: <span class="type">MasterActorRef</span>) =&gt;</span><br><span class="line">      <span class="type">MasterWithExecutorSystemProvider</span>.props(appId, masterProxy)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> appMaster = (appContext: <span class="type">AppMasterContext</span>, app: <span class="type">AppDescription</span>) =&gt;</span><br><span class="line">      <span class="type">LazyStartAppMaster</span>.props(appContext, app)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> masterConnectionKeeper = (master: <span class="type">MasterActorRef</span>, registerAppMaster:</span><br><span class="line">      <span class="type">RegisterAppMaster</span>, listener: <span class="type">ListenerActorRef</span>) =&gt; <span class="type">Props</span>(<span class="keyword">new</span> <span class="type">MasterConnectionKeeper</span>(</span><br><span class="line">        registerAppMaster, master, masterStatusListener = listener))</span><br><span class="line"></span><br><span class="line">    <span class="type">Props</span>(<span class="keyword">new</span> <span class="type">AppMasterRuntimeEnvironment</span>(appContextInput, app, masters,</span><br><span class="line">      master, appMaster, masterConnectionKeeper))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>AppMasterRuntimeEnvironment will create three Actor once it’s created. It serves as runtime environment for AppMaster. When starting an AppMaster, we need to setup the connection to master(an MasterProxy which substitute to Master), and prepare other environments.</p>
<p>The MasterProxy also extend the function of Master, by providing a scheduler service for Executor System. AppMaster can ask Master for executor system directly. details like requesting resource, contacting worker to start a process, and then starting an executor system is hidden from AppMaster.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[appmaster] <span class="class"><span class="keyword">class</span> <span class="title">AppMasterRuntimeEnvironment</span>(</span></span><br><span class="line">    appContextInput: <span class="type">AppMasterContext</span>,</span><br><span class="line">    app: <span class="type">AppDescription</span>,</span><br><span class="line">    masters: <span class="type">Iterable</span>[<span class="type">ActorPath</span>],</span><br><span class="line">    masterFactory: (<span class="type">AppId</span>, <span class="type">MasterActorRef</span>) =&gt; <span class="type">Props</span>,</span><br><span class="line">    appMasterFactory: (<span class="type">AppMasterContext</span>, <span class="type">AppDescription</span>) =&gt; <span class="type">Props</span>,</span><br><span class="line">    masterConnectionKeeperFactory: (<span class="type">MasterActorRef</span>, <span class="type">RegisterAppMaster</span>, <span class="type">ListenerActorRef</span>) =&gt; <span class="type">Props</span>) <span class="keyword">extends</span> <span class="type">Actor</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> master = context.actorOf(</span><br><span class="line">    masterFactory(appId, context.actorOf(<span class="type">Props</span>(<span class="keyword">new</span> <span class="type">MasterProxy</span>(masters, <span class="number">30.</span>seconds)))))</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> appContext = appContextInput.copy(masterProxy = master)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create appMaster proxy to receive command and forward to appmaster</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> appMaster = context.actorOf(appMasterFactory(appContext, app))</span><br><span class="line">  context.watch(appMaster)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> registerAppMaster = <span class="type">RegisterAppMaster</span>(</span><br><span class="line">    appId, appMaster, appContext.workerInfo)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> masterConnectionKeeper = context.actorOf(</span><br><span class="line">    masterConnectionKeeperFactory(master, registerAppMaster, self))</span><br><span class="line">  context.watch(masterConnectionKeeper)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receive</span>:</span> <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">MasterConnected</span> =&gt;</span><br><span class="line">      <span class="type">LOG</span>.info(s<span class="string">"Master is connected, start AppMaster $appId..."</span>)</span><br><span class="line">      appMaster ! <span class="type">StartAppMaster</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">MasterStopped</span> =&gt;</span><br><span class="line">      <span class="type">LOG</span>.error(s<span class="string">"Master is stopped, stop AppMaster $appId..."</span>)</span><br><span class="line">      context.stop(self)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Terminated</span>(actor) =&gt; actor <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> `appMaster` =&gt;</span><br><span class="line">        <span class="type">LOG</span>.error(s<span class="string">"AppMaster $appId is stopped, shutdown myself"</span>)</span><br><span class="line">        context.stop(self)</span><br><span class="line">      <span class="keyword">case</span> `masterConnectionKeeper` =&gt;</span><br><span class="line">        <span class="type">LOG</span>.error(s<span class="string">"Master connection keeper is stopped, appId: $appId, shutdown myself"</span>)</span><br><span class="line">        context.stop(self)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="comment">// Skip</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>The workflow from creating <code>AppMasterRuntimeEnvironment</code> to create <code>AppMaster</code> is trigged through <code>MasterConnectionKeeper</code> by sending <code>RegisterAppMaster</code> request to <code>AppMasterLauncher</code>. Finally when <code>AppMasterRuntimeEnvironment</code> receive <code>MasterConnected</code> from <code>MasterConnectionKeeper</code>, it send <code>StartAppMaster</code> to <code>AppMaster</code>. happy now! Take long long way bring up to AppMaster.</p>
<p><img src="http://img.blog.csdn.net/20170624220921664" alt="9"></p>
<p>Note AppMasterRuntimeEnvironment did not send StartAppMaster directory to AppMaster but to LazyStartAppMaster. and Every message send to LazyStartAppMaster will forward to AppMaster. Why do we need a Lazy AppMaster? If you take look at LazyStartAppMaster, you’ll notice that LazyStartAppMaster is not really an AppMaster but it’s responsible to create AppMaster only when it receive StartAppMaster request from AppMasterRuntimeEnvironment. So you wont’t find StartAppMaster on AppMaster.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LazyStartAppMaster</span>(</span>appId: <span class="type">Int</span>, appMasterProps: <span class="type">Props</span>) </span><br><span class="line">    <span class="keyword">extends</span> <span class="type">Actor</span> <span class="keyword">with</span> <span class="type">Stash</span> &#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receive</span>:</span> <span class="type">Receive</span> = <span class="literal">null</span></span><br><span class="line">  context.become(startAppMaster)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">startAppMaster</span>:</span> <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">StartAppMaster</span> =&gt;</span><br><span class="line">      <span class="keyword">val</span> appMaster = context.actorOf(appMasterProps, <span class="string">"appmaster"</span>)</span><br><span class="line">      context.watch(appMaster)</span><br><span class="line">      context.become(terminationWatch(appMaster) orElse </span><br><span class="line">        appMasterService(appMaster))</span><br><span class="line">      unstashAll()</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; stash()</span><br><span class="line"> &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">appMasterService</span>(</span>appMaster: <span class="type">ActorRef</span>): <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> msg =&gt; appMaster forward msg</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">private</span>[appmaster] <span class="class"><span class="keyword">object</span> <span class="title">LazyStartAppMaster</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">props</span>(</span>appContext: <span class="type">AppMasterContext</span>, app: <span class="type">AppDescription</span>): <span class="type">Props</span> = &#123;</span><br><span class="line">    <span class="comment">// the class name of app.appMaster is AppMaster </span></span><br><span class="line">    <span class="comment">// which will create when receive StartAppMaster</span></span><br><span class="line">    <span class="keyword">val</span> appMasterProps = <span class="type">Props</span>(<span class="type">Class</span>.forName(app.appMaster), appContext, app)</span><br><span class="line">    <span class="type">Props</span>(<span class="keyword">new</span> <span class="type">LazyStartAppMaster</span>(appContext.appId, appMasterProps))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The AppMaster is the head of a streaming application. It contains:</p>
<ol>
<li>ExecutorManager to manage all executors.</li>
<li>TaskManager to manage all tasks,</li>
<li>ClockService to track the global clock for this streaming application.</li>
<li>Scheduler to decide which a task should be scheduled to.</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppMaster</span>(</span>appContext: <span class="type">AppMasterContext</span>, app: <span class="type">AppDescription</span>) </span><br><span class="line">    <span class="keyword">extends</span> <span class="type">ApplicationMaster</span> &#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> dagManager = context.actorOf(<span class="type">Props</span>(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">DagManager</span>(appContext.appId, userConfig, store,</span><br><span class="line">    <span class="type">Some</span>(getUpdatedDAG))))</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> taskManager: <span class="type">Option</span>[<span class="type">ActorRef</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> clockService: <span class="type">Option</span>[<span class="type">ActorRef</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> executorManager: <span class="type">ActorRef</span> =</span><br><span class="line">    context.actorOf(<span class="type">ExecutorManager</span>.props(userConfig, appContext, app.clusterConfig, app.name),</span><br><span class="line">      <span class="type">ActorPathUtil</span>.executorManagerActorName)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (dag &lt;- getDAG) &#123;</span><br><span class="line">    clockService = <span class="type">Some</span>(context.actorOf(<span class="type">Props</span>(<span class="keyword">new</span> <span class="type">ClockService</span>(dag, self, store))))</span><br><span class="line">    <span class="keyword">val</span> jarScheduler = <span class="keyword">new</span> <span class="type">JarScheduler</span>(appId, app.name, systemConfig, context)</span><br><span class="line">    taskManager = <span class="type">Some</span>(context.actorOf(<span class="type">Props</span>(<span class="keyword">new</span> <span class="type">TaskManager</span>(appContext.appId, dagManager,</span><br><span class="line">      jarScheduler, executorManager, clockService.get, self, app.name))))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span>:</span> <span class="type">Receive</span> = &#123;</span><br><span class="line">    taskMessageHandler orElse</span><br><span class="line">      executorMessageHandler orElse</span><br><span class="line">      ready orElse</span><br><span class="line">      recover orElse</span><br><span class="line">      appMasterService orElse</span><br><span class="line">      <span class="type">ActorUtil</span>.defaultMsgHandler(self)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>At now I lost my line of argument, as there’re no request send trigger inside AppMaster, so what’s the entry of AppMaster?</p>
<p>Keep in mind, once create AppMaster, it will create <code>ExecutorManager</code> and <code>TaskManager</code>. Althrough we did’t see request send directory from AppMaster, we could find if there’re something inside ExecutorManager or TaskManager.</p>
<p>Suddenly comeup so many Managers make me unprepared. But unlike <code>AppManager</code> reside in Master, <code>ExecutorManager</code> and <code>TaskManager</code> both reside in Worker! </p>
<p><img src="http://img.blog.csdn.net/20170624224944769" alt="9"></p>
<h2 id="Processor,_OP,_Task">Processor, OP, Task</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Planner</span> &#123;</span></span><br><span class="line">  <span class="comment">/**</span><br><span class="line">   * Converts Dag of Op to Dag of TaskDescription. TaskDescription is part of the low level Graph API.</span><br><span class="line">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">plan</span>(</span>dag: <span class="type">Graph</span>[<span class="type">Op</span>, <span class="type">OpEdge</span>])</span><br><span class="line">    (<span class="keyword">implicit</span> system: <span class="type">ActorSystem</span>): <span class="type">Graph</span>[<span class="type">Processor</span>[_ &lt;: <span class="type">Task</span>], _ &lt;: <span class="type">Partitioner</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> graph = optimize(dag)</span><br><span class="line">    graph.mapEdge &#123; (node1, edge, node2) =&gt;</span><br><span class="line">      edge <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Shuffle</span> =&gt;</span><br><span class="line">          node2 <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> op: <span class="type">GroupByOp</span>[_, _] =&gt;</span><br><span class="line">              <span class="keyword">new</span> <span class="type">GroupByPartitioner</span>(op.groupBy.groupByFn)</span><br><span class="line">            <span class="keyword">case</span> _ =&gt; <span class="keyword">new</span> <span class="type">HashPartitioner</span></span><br><span class="line">          &#125;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Direct</span> =&gt;</span><br><span class="line">          <span class="keyword">new</span> <span class="type">CoLocationPartitioner</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.mapVertex(_.getProcessor)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">optimize</span>(</span>dag: <span class="type">Graph</span>[<span class="type">Op</span>, <span class="type">OpEdge</span>])</span><br><span class="line">    (<span class="keyword">implicit</span> system: <span class="type">ActorSystem</span>): <span class="type">Graph</span>[<span class="type">Op</span>, <span class="type">OpEdge</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> graph = dag.copy</span><br><span class="line">    <span class="keyword">val</span> nodes = graph.topologicalOrderWithCirclesIterator.toList.reverse</span><br><span class="line">    <span class="keyword">for</span> (node &lt;- nodes) &#123;</span><br><span class="line">      <span class="keyword">val</span> outGoingEdges = graph.outgoingEdgesOf(node)</span><br><span class="line">      <span class="keyword">for</span> (edge &lt;- outGoingEdges) &#123;</span><br><span class="line">        merge(graph, edge._1, edge._3)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    graph</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span>(</span>graph: <span class="type">Graph</span>[<span class="type">Op</span>, <span class="type">OpEdge</span>], node1: <span class="type">Op</span>, node2: <span class="type">Op</span>)</span><br><span class="line">    (<span class="keyword">implicit</span> system: <span class="type">ActorSystem</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (graph.outDegreeOf(node1) == <span class="number">1</span> &amp;&amp;</span><br><span class="line">      graph.inDegreeOf(node2) == <span class="number">1</span> &amp;&amp;</span><br><span class="line">      <span class="comment">// For processor node, we don't allow it to merge with downstream operators</span></span><br><span class="line">      !node1.isInstanceOf[<span class="type">ProcessorOp</span>[_ &lt;: <span class="type">Task</span>]] &amp;&amp;</span><br><span class="line">      !node2.isInstanceOf[<span class="type">ProcessorOp</span>[_ &lt;: <span class="type">Task</span>]]) &#123;</span><br><span class="line">      <span class="keyword">val</span> (_, edge, _) = graph.outgoingEdgesOf(node1).head</span><br><span class="line">      <span class="keyword">if</span> (edge == <span class="type">Direct</span>) &#123;</span><br><span class="line">        <span class="keyword">val</span> chainedOp = node1.chain(node2)</span><br><span class="line">        graph.addVertex(chainedOp)</span><br><span class="line">        <span class="keyword">for</span> (incomingEdge &lt;- graph.incomingEdgesOf(node1)) &#123;</span><br><span class="line">          graph.addEdge(incomingEdge._1, incomingEdge._2, chainedOp)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (outgoingEdge &lt;- graph.outgoingEdgesOf(node2)) &#123;</span><br><span class="line">          graph.addEdge(chainedOp, outgoingEdge._2, outgoingEdge._3)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Remove the old vertex</span></span><br><span class="line">        graph.removeVertex(node1)</span><br><span class="line">        graph.removeVertex(node2)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="keyword">class</span> DataSourceOp(</span><br><span class="line">    dataSource: DataSource,</span><br><span class="line">    parallelism: <span class="built_in">Int</span> = <span class="number">1</span>,</span><br><span class="line">    userConfig: UserConfig = UserConfig.empty,</span><br><span class="line">    description: String = <span class="string">"source"</span>)</span><br><span class="line">  <span class="keyword">extends</span> Op &#123;</span><br><span class="line"></span><br><span class="line">  override def chain(other: Op)(<span class="type">implicit</span> system: ActorSystem): Op = &#123;</span><br><span class="line">    DataSourceOp(dataSource, parallelism,</span><br><span class="line">      userConfig.withValue(Constants.GEARPUMP_STREAMING_OPERATOR, other.fn),</span><br><span class="line">      description)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def getProcessor(<span class="type">implicit</span> system: ActorSystem): Processor[_ &lt;: Task] = &#123;</span><br><span class="line">    Processor[DataSourceTask[<span class="built_in">Any</span>, <span class="built_in">Any</span>]](parallelism, description,</span><br><span class="line">      userConfig.withValue(GEARPUMP_STREAMING_SOURCE, dataSource))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">TaskWrapper</span><span class="container">(</span><br><span class="line">    <span class="title">val</span> <span class="title">taskId</span>: <span class="type">TaskId</span>, <span class="title">val</span> <span class="title">taskClass</span>: <span class="type">Class</span>[<span class="title">_</span> &lt;: <span class="type">Task</span>], <span class="title">context</span>: <span class="type">TaskContextData</span>,</span><br><span class="line">    <span class="title">userConf</span>: <span class="type">UserConfig</span>)</span> extends <span class="type">TaskContext</span> with <span class="type">TaskInterface</span> &#123;</span><br><span class="line"></span><br><span class="line">  private var task: <span class="type">Option</span>[<span class="type">Task</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">  override def onStart<span class="container">(<span class="title">startTime</span>: <span class="type">Instant</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line">    val constructor = taskClass.getConstructor<span class="container">(</span><br><span class="line">        <span class="title">classOf</span>[<span class="type">TaskContext</span>], <span class="title">classOf</span>[<span class="type">UserConfig</span>])</span></span><br><span class="line">    task = <span class="type">Some</span><span class="container">(<span class="title">constructor</span>.<span class="title">newInstance</span>(<span class="title">this</span>, <span class="title">userConf</span>)</span>)</span><br><span class="line">    task.foreach<span class="container">(<span class="title">_</span>.<span class="title">onStart</span>(<span class="title">startTime</span>)</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure>
<p>Utility that helps user to create a DAG starting with [[DataSourceTask]] user should pass in a [[DataSource]]</p>
<p>Here is an example to build a DAG that reads from Kafka source followed by word count</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> source = <span class="keyword">new</span> <span class="type">KafkaSource</span>()</span><br><span class="line"><span class="keyword">val</span> sourceProcessor =  <span class="type">DataSourceProcessor</span>(source, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> split = <span class="type">Processor</span>[<span class="type">Split</span>](<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> sum = <span class="type">Processor</span>[<span class="type">Sum</span>](<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dag = sourceProcessor ~&gt; split ~&gt; sum</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataSourceProcessor</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span></span><br><span class="line">      dataSource: <span class="type">DataSource</span>,</span><br><span class="line">      parallelism: <span class="type">Int</span> = <span class="number">1</span>,</span><br><span class="line">      description: <span class="type">String</span> = <span class="string">""</span>,</span><br><span class="line">      taskConf: <span class="type">UserConfig</span> = <span class="type">UserConfig</span>.empty)(<span class="keyword">implicit</span> system: <span class="type">ActorSystem</span>)</span><br><span class="line">    : <span class="type">Processor</span>[<span class="type">DataSourceTask</span>[<span class="type">Any</span>, <span class="type">Any</span>]] = &#123;</span><br><span class="line">    <span class="type">Processor</span>[<span class="type">DataSourceTask</span>[<span class="type">Any</span>, <span class="type">Any</span>]](parallelism, description,</span><br><span class="line">      taskConf.withValue[<span class="type">DataSource</span>](<span class="type">Constants</span>.<span class="type">GEARPUMP_STREAMING_SOURCE</span>, dataSource))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Default Task container for [[org.apache.gearpump.streaming.source.DataSource]] that reads from DataSource in batch</p>
<p>DataSourceTask calls:</p>
<ul>
<li><code>DataSource.open()</code> in <code>onStart</code> and pass in [[org.apache.gearpump.streaming.task.TaskContext]]</li>
</ul>
<p>and application start time</p>
<ul>
<li><code>DataSource.read()</code> in each <code>onNext</code>, which reads a batch of messages</li>
<li><code>DataSource.close()</code> in <code>onStop</code></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataSourceTask</span>[</span><span class="type">IN</span>, <span class="type">OUT</span>] <span class="keyword">private</span>[source](</span><br><span class="line">    context: <span class="type">TaskContext</span>,</span><br><span class="line">    conf: <span class="type">UserConfig</span>,</span><br><span class="line">    source: <span class="type">DataSource</span>,</span><br><span class="line">    transform: <span class="type">Transform</span>[<span class="type">IN</span>, <span class="type">OUT</span>])</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Task</span>(context, conf) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span>(</span>context: <span class="type">TaskContext</span>, conf: <span class="type">UserConfig</span>) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(context, conf,</span><br><span class="line">      conf.getValue[<span class="type">DataSource</span>](<span class="type">GEARPUMP_STREAMING_SOURCE</span>)(context.system).get,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Transform</span>[<span class="type">IN</span>, <span class="type">OUT</span>](context,</span><br><span class="line">        conf.getValue[<span class="type">FunctionRunner</span>[<span class="type">IN</span>, <span class="type">OUT</span>]](<span class="type">GEARPUMP_STREAMING_OPERATOR</span>)(context.system))</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deep into Apache Gearpump&lt;br&gt;
    
    </summary>
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/categories/bigdata/"/>
    
    
      <category term="流处理" scheme="http://github.com/zqhxuyuan/tags/%E6%B5%81%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Apache Kafka JIRA</title>
    <link href="http://github.com/zqhxuyuan/2017/06/20/JIRA_KAFKA/"/>
    <id>http://github.com/zqhxuyuan/2017/06/20/JIRA_KAFKA/</id>
    <published>2017-06-19T16:00:00.000Z</published>
    <updated>2017-06-24T14:57:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Kafka JIRA<br><a id="more"></a></p>
<p><a href="https://issues.apache.org/jira/browse/KAFKA" target="_blank" rel="external">https://issues.apache.org/jira/browse/KAFKA</a></p>
<h2 id="2944">2944</h2><p><a href="https://github.com/apache/kafka/pull/723">https://github.com/apache/kafka/pull/723</a></p>
<p>最后来分析<code>KafkaBasedLog</code>的<code>readToLogEnd()</code>方法如何读取到日志的最末尾，具体步骤如下。</p>
<ol>
<li>定位到分区的最末尾，通过消费者的<code>seekToEnd()</code>只是声明了重置策略为<code>LATEST</code>，并没有真正定位。客户端还需要调用消费者的轮询方法，才能保证发送拉取请求，并更新消费者的当前位置；</li>
<li>比较消费者的当前位置（<code>endOffset</code>）与上一次还没定位到最末尾时的位置（<code>startOffset</code>），如果前者大于后者，客户端需要调用<code>seek()</code>方法定位到旧的位置（<code>startOffset</code>）；</li>
<li>如果步骤(2)回退到旧的位置，需要调用轮询方法消费消息，直到当前位置是分区的最末尾位置。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaBasedLog</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123; </span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">readToLogEnd</span><span class="params">()</span> </span>&#123; <span class="comment">// 读取到日志的最末尾</span></span><br><span class="line">    <span class="comment">// 1. 定位到分区的最末尾（logEndOffset）</span></span><br><span class="line">    Set&lt;TopicPartition&gt; assignment = consumer.assignment();</span><br><span class="line">    Map&lt;TopicPartition, Long&gt; offsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (TopicPartition tp : assignment) &#123;</span><br><span class="line">      <span class="keyword">long</span> offset = consumer.position(tp); <span class="comment">// 获取当前的消费位置</span></span><br><span class="line">      offsets.put(tp, offset); <span class="comment">// 暂存起来</span></span><br><span class="line">      consumer.seekToEnd(singleton(tp)); <span class="comment">// 定位到最末尾的位置</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2. 回退到开始位置</span></span><br><span class="line">    Map&lt;TopicPartition, Long&gt; endOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      poll(<span class="number">0</span>);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">for</span> (TopicPartition tp : assignment) &#123;</span><br><span class="line">        <span class="keyword">long</span> startOffset = offsets.get(tp); <span class="comment">// 旧的消费位置</span></span><br><span class="line">        <span class="keyword">long</span> endOffset = consumer.position(tp); <span class="comment">// 当前的偏移量等于最末尾的位置</span></span><br><span class="line">        <span class="keyword">if</span> (endOffset &gt; startOffset) &#123; </span><br><span class="line">          endOffsets.put(tp, endOffset); </span><br><span class="line">          consumer.seek(tp, startOffset);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3. 开始读取，直到读取到分区的最末尾位置</span></span><br><span class="line">    <span class="keyword">while</span> (!endOffsets.isEmpty()) &#123;</span><br><span class="line">      poll(Integer.MAX_VALUE);</span><br><span class="line">      Iterator it = endOffsets.entrySet().iterator();</span><br><span class="line">      <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">        Map.Entry&lt;TopicPartition, Long&gt; entry = it.next();</span><br><span class="line">        <span class="keyword">if</span> (consumer.position(entry.getKey()) &lt; entry.getValue()) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">else</span> it.remove();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>客户端调用<code>readToLogEnd()</code>之前，如果还有新的消息没有消费，当调用<code>readToLogEnd()</code>方法时，可以保证客户端会完全消费新写入的消息。如图8-31（左图）所示，偏移量从<code>3</code>到<code>6</code>是新写入的消息（比如一个连接器配置、两个任务配置、一个提交日志的配置，总共四条消息）。客户端为了读取到分区最近的位置，先定位到最近的位置（<code>7</code>）。注意这时不能立即调用轮询方法，因为如果客户端在最近的位置，调用轮询不会有任何的新消息。客户端应该再定位到上次消费的位置（<code>3</code>），然后才能调用轮询方法，直到消费者的当前位置大于等于最近位置时，就说明客户端读取到了日志的最末尾。右图中，假设客户端已经消费到了日志的最末尾，那么调用<code>readToLogEnd()</code>方法会立即返回。</p>
<p><img src="http://img.blog.csdn.net/20170520193000243" alt="8"></p>
<p>图8-31 读取到分区最末尾的位置</p>
<blockquote>
<p><strong>注意：</strong>上面的<code>readToLogEnd()</code>方法用到了Kafka新消费者的三个方法。（1）：<code>postion()</code>方法返回消费者当前的位置，即消费进度，这个值比客户端真正消费过的位置要大<code>1</code>。比如客户端消费了两条消息，<code>postion()</code>方法的返回值就等于<code>3</code>。（2）：<code>seekToEnd(tp)</code>方法定位到日志的最末尾，同样，这个值也是实际的偏移量加上<code>1</code>（即<code>nextOffset</code>）。比如分区实际只有六条消息，最末尾的偏移量等于<code>7</code>。（3）：<code>seekTo(tp,offset)</code>方法定位到日志的指定位置。客户端定位到指定位置后，下一步一般是要调用轮询方法，并从这个位置拉取消息。所以如果客户端已经消费了偏移量等于<code>1</code>和<code>2</code>的两条消息，定位的位置是<code>3</code>，表示要拉取第三条的消息。不能定位到<code>2</code>，那样的话，从位置<code>2</code>开始拉取消息，就重复拉取了第二条消息。</p>
</blockquote>
<h2 id="2500/2076/KIP-17">2500/2076/KIP-17</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Kafka JIRA&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="流处理" scheme="http://github.com/zqhxuyuan/tags/%E6%B5%81%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://github.com/zqhxuyuan/2017/05/10/2017-05-10-Spark2-SQL/"/>
    <id>http://github.com/zqhxuyuan/2017/05/10/2017-05-10-Spark2-SQL/</id>
    <published>2017-05-10T09:41:16.000Z</published>
    <updated>2017-05-10T09:41:16.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Spark Metrics</title>
    <link href="http://github.com/zqhxuyuan/2017/05/01/Spark-Metrics/"/>
    <id>http://github.com/zqhxuyuan/2017/05/01/Spark-Metrics/</id>
    <published>2017-04-30T16:00:00.000Z</published>
    <updated>2017-09-01T03:08:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>Spark Metrics<br><a id="more"></a></p>
<ul>
<li><a href="http://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/" target="_blank" rel="external">http://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/</a></li>
</ul>
<h2 id="命令行添加监控">命令行添加监控</h2><p>直接添加到命令行后</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--files=/yourPath/metrics<span class="class">.properties</span> --conf spark<span class="class">.metrics</span><span class="class">.conf</span>=metrics.properties</span><br></pre></td></tr></table></figure>
<blockquote>
<p>The –files flag will cause /path/to/metrics.properties to be sent to every executor,<br>and spark.metrics.conf=metrics.properties will tell all executors to load that file<br>when initializing their respective MetricsSystems.</p>
</blockquote>
<p>或者用conf的形式</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--conf spark<span class="class">.metrics</span><span class="class">.conf</span>.*<span class="class">.sink</span><span class="class">.graphite</span><span class="class">.class</span>=org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.metrics</span><span class="class">.sink</span><span class="class">.GraphiteSink</span> \</span><br><span class="line">--conf spark<span class="class">.metrics</span><span class="class">.conf</span>.*<span class="class">.sink</span><span class="class">.graphite</span><span class="class">.host</span>=...</span><br></pre></td></tr></table></figure>
<h2 id="Spark_Metrics">Spark Metrics</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">*<span class="class">.sink</span><span class="class">.console</span><span class="class">.class</span>=org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.metrics</span><span class="class">.sink</span><span class="class">.ConsoleSink</span></span><br><span class="line">*<span class="class">.sink</span><span class="class">.console</span><span class="class">.period</span>=<span class="number">10</span></span><br><span class="line">*<span class="class">.sink</span><span class="class">.console</span><span class="class">.unit</span>=seconds</span><br><span class="line">*<span class="class">.sink</span><span class="class">.csv</span><span class="class">.class</span>=org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.metrics</span><span class="class">.sink</span><span class="class">.CsvSink</span></span><br><span class="line">*<span class="class">.sink</span><span class="class">.csv</span><span class="class">.period</span>=<span class="number">1</span></span><br><span class="line">*<span class="class">.sink</span><span class="class">.csv</span><span class="class">.unit</span>=minutes</span><br><span class="line">*<span class="class">.sink</span><span class="class">.csv</span><span class="class">.directory</span>=/tmp/</span><br></pre></td></tr></table></figure>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-<span class="number">2.0</span><span class="number">.1</span>-bin-hadoop2<span class="number">.7</span> bin/spark-<span class="built_in">shell</span></span><br><span class="line">Spark context Web UI available <span class="keyword">at</span> <span class="keyword">http</span>://<span class="number">10.57</span><span class="number">.2</span><span class="number">.5</span>:<span class="number">4040</span></span><br><span class="line">Spark context available <span class="keyword">as</span> <span class="string">'sc'</span> (master = <span class="built_in">local</span>[*], app id = <span class="built_in">local</span>-<span class="number">1495078254084</span>).</span><br><span class="line">Spark session available <span class="keyword">as</span> <span class="string">'spark'</span>.</span><br><span class="line">scala&gt; <span class="number">17</span>-<span class="number">5</span>-<span class="number">18</span> <span class="number">11</span>:<span class="number">31</span>:<span class="number">05</span> ===============================================================</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Gauges ----------------------------------------------------------------------</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.BlockManager.disk.diskSpaceUsed_MB <span class="built_in">value</span> = <span class="number">0</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.BlockManager.memory.maxMem_MB <span class="built_in">value</span> = <span class="number">366</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.BlockManager.memory.memUsed_MB <span class="built_in">value</span> = <span class="number">0</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.BlockManager.memory.remainingMem_MB <span class="built_in">value</span> = <span class="number">366</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.DAGScheduler.job.activeJobs <span class="built_in">value</span> = <span class="number">0</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.DAGScheduler.job.allJobs <span class="built_in">value</span> = <span class="number">0</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.DAGScheduler.stage.failedStages <span class="built_in">value</span> = <span class="number">0</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.DAGScheduler.stage.runningStages <span class="built_in">value</span> = <span class="number">0</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.DAGScheduler.stage.waitingStages <span class="built_in">value</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- Histograms ------------------------------------------------------------------</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.CodeGenerator.compilationTime</span><br><span class="line">             count = <span class="number">0</span></span><br><span class="line">               <span class="built_in">min</span> = <span class="number">0</span></span><br><span class="line">               <span class="built_in">max</span> = <span class="number">0</span></span><br><span class="line">              mean = <span class="number">0.00</span></span><br><span class="line">            stddev = <span class="number">0.00</span></span><br><span class="line">            <span class="built_in">median</span> = <span class="number">0.00</span></span><br><span class="line">              <span class="number">75</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">              <span class="number">95</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">              <span class="number">98</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">              <span class="number">99</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">            <span class="number">99.9</span>% &lt;= <span class="number">0.00</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.CodeGenerator.generatedClassSize</span><br><span class="line">             count = <span class="number">0</span></span><br><span class="line">               <span class="built_in">min</span> = <span class="number">0</span></span><br><span class="line">               <span class="built_in">max</span> = <span class="number">0</span></span><br><span class="line">              mean = <span class="number">0.00</span></span><br><span class="line">            stddev = <span class="number">0.00</span></span><br><span class="line">            <span class="built_in">median</span> = <span class="number">0.00</span></span><br><span class="line">              <span class="number">75</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">              <span class="number">95</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">              <span class="number">98</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">              <span class="number">99</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">            <span class="number">99.9</span>% &lt;= <span class="number">0.00</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.CodeGenerator.generatedMethodSize</span><br><span class="line">             count = <span class="number">0</span></span><br><span class="line">               <span class="built_in">min</span> = <span class="number">0</span></span><br><span class="line">               <span class="built_in">max</span> = <span class="number">0</span></span><br><span class="line">              mean = <span class="number">0.00</span></span><br><span class="line">            stddev = <span class="number">0.00</span></span><br><span class="line">            <span class="built_in">median</span> = <span class="number">0.00</span></span><br><span class="line">              <span class="number">75</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">              <span class="number">95</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">              <span class="number">98</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">              <span class="number">99</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">            <span class="number">99.9</span>% &lt;= <span class="number">0.00</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.CodeGenerator.sourceCodeSize</span><br><span class="line">             count = <span class="number">0</span></span><br><span class="line">               <span class="built_in">min</span> = <span class="number">0</span></span><br><span class="line">               <span class="built_in">max</span> = <span class="number">0</span></span><br><span class="line">              mean = <span class="number">0.00</span></span><br><span class="line">            stddev = <span class="number">0.00</span></span><br><span class="line">            <span class="built_in">median</span> = <span class="number">0.00</span></span><br><span class="line">              <span class="number">75</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">              <span class="number">95</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">              <span class="number">98</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">              <span class="number">99</span>% &lt;= <span class="number">0.00</span></span><br><span class="line">            <span class="number">99.9</span>% &lt;= <span class="number">0.00</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- Timers ----------------------------------------------------------------------</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.DAGScheduler.messageProcessingTime</span><br><span class="line">             count = <span class="number">0</span></span><br><span class="line">         mean rate = <span class="number">0.00</span> calls/<span class="keyword">second</span></span><br><span class="line">     <span class="number">1</span>-minute rate = <span class="number">0.00</span> calls/<span class="keyword">second</span></span><br><span class="line">     <span class="number">5</span>-minute rate = <span class="number">0.00</span> calls/<span class="keyword">second</span></span><br><span class="line">    <span class="number">15</span>-minute rate = <span class="number">0.00</span> calls/<span class="keyword">second</span></span><br><span class="line">               <span class="built_in">min</span> = <span class="number">0.00</span> <span class="built_in">milliseconds</span></span><br><span class="line">               <span class="built_in">max</span> = <span class="number">0.00</span> <span class="built_in">milliseconds</span></span><br><span class="line">              mean = <span class="number">0.00</span> <span class="built_in">milliseconds</span></span><br><span class="line">            stddev = <span class="number">0.00</span> <span class="built_in">milliseconds</span></span><br><span class="line">            <span class="built_in">median</span> = <span class="number">0.00</span> <span class="built_in">milliseconds</span></span><br><span class="line">              <span class="number">75</span>% &lt;= <span class="number">0.00</span> <span class="built_in">milliseconds</span></span><br><span class="line">              <span class="number">95</span>% &lt;= <span class="number">0.00</span> <span class="built_in">milliseconds</span></span><br><span class="line">              <span class="number">98</span>% &lt;= <span class="number">0.00</span> <span class="built_in">milliseconds</span></span><br><span class="line">              <span class="number">99</span>% &lt;= <span class="number">0.00</span> <span class="built_in">milliseconds</span></span><br><span class="line">            <span class="number">99.9</span>% &lt;= <span class="number">0.00</span> <span class="built_in">milliseconds</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">17</span>-<span class="number">5</span>-<span class="number">18</span> <span class="number">11</span>:<span class="number">31</span>:<span class="number">15</span> ===============================================================</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)).count</span><br><span class="line">res1: Long = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="number">17</span>-<span class="number">5</span>-<span class="number">18</span> <span class="number">11</span>:<span class="number">33</span>:<span class="number">15</span> ===============================================================</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Timers ----------------------------------------------------------------------</span></span><br><span class="line"><span class="built_in">local</span>-<span class="number">1495078254084.</span>driver.DAGScheduler.messageProcessingTime</span><br><span class="line">             count = <span class="number">10</span></span><br><span class="line">         mean rate = <span class="number">0.07</span> calls/<span class="keyword">second</span></span><br><span class="line">     <span class="number">1</span>-minute rate = <span class="number">0.16</span> calls/<span class="keyword">second</span></span><br><span class="line">     <span class="number">5</span>-minute rate = <span class="number">0.03</span> calls/<span class="keyword">second</span></span><br><span class="line">    <span class="number">15</span>-minute rate = <span class="number">0.01</span> calls/<span class="keyword">second</span></span><br><span class="line">               <span class="built_in">min</span> = <span class="number">0.03</span> <span class="built_in">milliseconds</span></span><br><span class="line">               <span class="built_in">max</span> = <span class="number">1207.28</span> <span class="built_in">milliseconds</span></span><br><span class="line">              mean = <span class="number">125.02</span> <span class="built_in">milliseconds</span></span><br><span class="line">            stddev = <span class="number">358.42</span> <span class="built_in">milliseconds</span></span><br><span class="line">            <span class="built_in">median</span> = <span class="number">0.32</span> <span class="built_in">milliseconds</span></span><br><span class="line">              <span class="number">75</span>% &lt;= <span class="number">16.58</span> <span class="built_in">milliseconds</span></span><br><span class="line">              <span class="number">95</span>% &lt;= <span class="number">1207.28</span> <span class="built_in">milliseconds</span></span><br><span class="line">              <span class="number">98</span>% &lt;= <span class="number">1207.28</span> <span class="built_in">milliseconds</span></span><br><span class="line">              <span class="number">99</span>% &lt;= <span class="number">1207.28</span> <span class="built_in">milliseconds</span></span><br><span class="line">            <span class="number">99.9</span>% &lt;= <span class="number">1207.28</span> <span class="built_in">milliseconds</span></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ll /tmp/ -rth</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel    <span class="number">99</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.DAGScheduler.stage.waitingStages.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel    <span class="number">99</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.DAGScheduler.stage.runningStages.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel    <span class="number">99</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.DAGScheduler.stage.failedStages.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel   <span class="number">1.3</span>K  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.DAGScheduler.messageProcessingTime.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel    <span class="number">99</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.DAGScheduler.job.allJobs.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel    <span class="number">99</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.DAGScheduler.job.activeJobs.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel   <span class="number">676</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.CodeGenerator.sourceCodeSize.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel   <span class="number">676</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.CodeGenerator.generatedMethodSize.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel   <span class="number">676</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.CodeGenerator.generatedClassSize.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel   <span class="number">676</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.CodeGenerator.compilationTime.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel   <span class="number">113</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.BlockManager.memory.remainingMem_MB.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel    <span class="number">99</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.BlockManager.memory.memUsed_MB.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel   <span class="number">113</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.BlockManager.memory.maxMem_MB.csv</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  wheel    <span class="number">99</span>B  <span class="number">5</span> <span class="number">18</span> <span class="number">11</span>:<span class="number">36</span> local-<span class="number">1495078254084.</span>driver.BlockManager.disk.diskSpaceUsed_MB.csv</span><br><span class="line"></span><br><span class="line">➜  /tmp cat local-<span class="number">1495078254084.</span>driver.DAGScheduler.messageProcessingTime.csv</span><br><span class="line">t,count,max,mean,min,stddev,p50,p75,p95,p98,p99,p999,mean_rate,m1_rate,m5_rate,m15_rate,rate_unit,duration_unit</span><br><span class="line"><span class="number">1495078315</span>,<span class="number">0</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,calls/second,milliseconds</span><br><span class="line"><span class="number">1495078375</span>,<span class="number">0</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,<span class="number">0.000000</span>,calls/second,milliseconds</span><br><span class="line"><span class="number">1495078435</span>,<span class="number">10</span>,<span class="number">1207.284400</span>,<span class="number">125.017564</span>,<span class="number">0.027442</span>,<span class="number">358.422668</span>,<span class="number">0.317114</span>,<span class="number">16.580495</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">0.055257</span>,<span class="number">0.082101</span>,<span class="number">0.028931</span>,<span class="number">0.010599</span>,calls/second,milliseconds</span><br><span class="line"><span class="number">1495078495</span>,<span class="number">10</span>,<span class="number">1207.284400</span>,<span class="number">125.017564</span>,<span class="number">0.027442</span>,<span class="number">358.422668</span>,<span class="number">0.317114</span>,<span class="number">16.580495</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">0.041499</span>,<span class="number">0.030203</span>,<span class="number">0.023686</span>,<span class="number">0.009915</span>,calls/second,milliseconds</span><br><span class="line"><span class="number">1495078555</span>,<span class="number">10</span>,<span class="number">1207.284400</span>,<span class="number">125.017564</span>,<span class="number">0.027442</span>,<span class="number">358.422668</span>,<span class="number">0.317114</span>,<span class="number">16.580495</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">0.033225</span>,<span class="number">0.011111</span>,<span class="number">0.019393</span>,<span class="number">0.009276</span>,calls/second,milliseconds</span><br><span class="line"><span class="number">1495078577</span>,<span class="number">10</span>,<span class="number">1207.284400</span>,<span class="number">125.017564</span>,<span class="number">0.027442</span>,<span class="number">358.422668</span>,<span class="number">0.317114</span>,<span class="number">16.580495</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">0.030895</span>,<span class="number">0.007962</span>,<span class="number">0.018142</span>,<span class="number">0.009072</span>,calls/second,milliseconds</span><br><span class="line"><span class="number">1495078577</span>,<span class="number">10</span>,<span class="number">1207.284400</span>,<span class="number">125.017564</span>,<span class="number">0.027442</span>,<span class="number">358.422668</span>,<span class="number">0.317114</span>,<span class="number">16.580495</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">1207.284400</span>,<span class="number">0.030890</span>,<span class="number">0.007962</span>,<span class="number">0.018142</span>,<span class="number">0.009072</span>,calls/second,milliseconds</span><br></pre></td></tr></table></figure>
<h2 id="Spark_Cassandra_Metrics">Spark Cassandra Metrics</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">executor<span class="class">.source</span><span class="class">.cassandra-connector</span><span class="class">.class</span>=org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.metrics</span><span class="class">.CassandraConnectorSource</span></span><br><span class="line">driver<span class="class">.source</span><span class="class">.cassandra-connector</span><span class="class">.class</span>=org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.metrics</span><span class="class">.CassandraConnectorSource</span></span><br></pre></td></tr></table></figure>
<h2 id="Spark_Influx_Metrics">Spark Influx Metrics</h2><p><a href="https://github.com/palantir/spark-influx-sink">https://github.com/palantir/spark-influx-sink</a></p>
<p>spark.driver.extraClassPath=spark-influx-sink.jar:metrics-influxdb.jar<br>spark.executor.extraClassPath=spark-influx-sink.jar:metrics-influxdb.jar</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">*<span class="class">.sink</span><span class="class">.influx</span><span class="class">.class</span>=org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.metrics</span><span class="class">.sink</span><span class="class">.InfluxDbSink</span></span><br><span class="line">*<span class="class">.sink</span><span class="class">.influx</span><span class="class">.protocol</span>=https</span><br><span class="line">*<span class="class">.sink</span><span class="class">.influx</span><span class="class">.host</span>=localhost</span><br><span class="line">*<span class="class">.sink</span><span class="class">.influx</span><span class="class">.port</span>=<span class="number">8086</span></span><br><span class="line">*<span class="class">.sink</span><span class="class">.influx</span><span class="class">.database</span>=my_metrics</span><br><span class="line">*<span class="class">.sink</span><span class="class">.influx</span><span class="class">.auth</span>=metric_client:PASSWORD</span><br><span class="line">*<span class="class">.sink</span><span class="class">.influx</span><span class="class">.tags</span>=product:my_product,parent:my_service</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark Metrics&lt;br&gt;
    
    </summary>
    
      <category term="spark" scheme="http://github.com/zqhxuyuan/categories/spark/"/>
    
    
      <category term="spark" scheme="http://github.com/zqhxuyuan/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>开源大数据ETL工具</title>
    <link href="http://github.com/zqhxuyuan/2017/02/15/2017-02-17-BigData-ETL/"/>
    <id>http://github.com/zqhxuyuan/2017/02/15/2017-02-17-BigData-ETL/</id>
    <published>2017-02-14T16:00:00.000Z</published>
    <updated>2017-02-25T07:13:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>BigData ETL Tools<br><a id="more"></a></p>
<h2 id="datatorrent(apex)">datatorrent(apex)</h2><p>执行<code>./datatorrent-rts-community-3.7.0.bin --help</code>打印帮助项</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 <span class="operator"><span class="keyword">install</span>]$ sudo -u <span class="keyword">admin</span> ./datatorrent-rts-community-<span class="number">3.7</span><span class="number">.0</span>.<span class="keyword">bin</span> \</span><br><span class="line">-B /usr/<span class="keyword">install</span>/datatorrent-rts -<span class="keyword">g</span> <span class="number">9094</span> \</span><br><span class="line">-<span class="keyword">E</span> DT_LOG_DIR=/home/<span class="keyword">admin</span>/datatorrent \</span><br><span class="line">-<span class="keyword">E</span> DT_RUN_DIR=/home/<span class="keyword">admin</span>/run/datatorrent</span><br><span class="line"></span><br><span class="line">Verifying <span class="keyword">archive</span> integrity... All good.</span><br><span class="line">Uncompressing DataTorrent Distribution  <span class="number">100</span>%</span><br><span class="line"></span><br><span class="line">DataTorrent Platform <span class="number">3.7</span><span class="number">.0</span> will be installed <span class="keyword">under</span> /usr/<span class="keyword">install</span>/datatorrent-rts/releases/<span class="number">3.7</span><span class="number">.0</span></span><br><span class="line"></span><br><span class="line">dtGateway can be <span class="keyword">managed</span> <span class="keyword">with</span>: /usr/<span class="keyword">install</span>/datatorrent-rts/releases/<span class="number">3.7</span><span class="number">.0</span>/<span class="keyword">bin</span>/dtgateway [<span class="keyword">start</span>|<span class="keyword">stop</span>|<span class="keyword">status</span>]</span><br><span class="line">DTGateway <span class="keyword">is</span> running <span class="keyword">as</span> pid <span class="number">24571</span> <span class="keyword">and</span> listening <span class="keyword">on</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">9094</span></span><br><span class="line"></span><br><span class="line">Please <span class="keyword">finish</span> the remaining installation steps via DataTorrent Console <span class="keyword">at</span>: <span class="keyword">http</span>://dp0653:<span class="number">9094</span>/</span></span><br></pre></td></tr></table></figure>
<p>创建apex项目，并打包</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">name=salesapp</span><br><span class="line">version=3.5.0</span><br><span class="line"></span><br><span class="line">mvn -B archetype:generate \</span><br><span class="line">  -<span class="ruby"><span class="constant">DarchetypeGroupId</span>=org.apache.apex \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">DarchetypeArtifactId</span>=apex-app-archetype \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">DarchetypeVersion</span>=<span class="variable">$version</span>  \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">DgroupId</span>=com.example \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Dpackage</span>=com.example.<span class="variable">$name</span> \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">DartifactId</span>=<span class="variable">$name</span> \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Dversion</span>=<span class="number">1.0</span>-<span class="constant">SNAPSHOT</span></span></span><br></pre></td></tr></table></figure>
<p>上传到datatorrent平台</p>
<h2 id="StreamSets(https://github-com/streamsets/datacollector)">StreamSets(<a href="https://github.com/streamsets/datacollector">https://github.com/streamsets/datacollector</a>)</h2><h2 id="StreamFlow(https://github-com/lmco/streamflow)">StreamFlow(<a href="https://github.com/lmco/streamflow">https://github.com/lmco/streamflow</a>)</h2><h2 id="CDAP(https://github-com/caskdata/cdap)">CDAP(<a href="https://github.com/caskdata/cdap">https://github.com/caskdata/cdap</a>)</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;BigData ETL Tools&lt;br&gt;
    
    </summary>
    
      <category term="work" scheme="http://github.com/zqhxuyuan/categories/work/"/>
    
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>开发者构建工具</title>
    <link href="http://github.com/zqhxuyuan/2017/01/01/Tools-Build/"/>
    <id>http://github.com/zqhxuyuan/2017/01/01/Tools-Build/</id>
    <published>2016-12-31T16:00:00.000Z</published>
    <updated>2017-07-18T07:57:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>Maven,SBT构建工具<br><a id="more"></a></p>
<h2 id="Maven">Maven</h2><h3 id="assembly">assembly</h3><p>maven-assembly-plugin</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="title">descriptorRef</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">descriptorRefs</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="title">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="title">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="title">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>maven-shade-plugin</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="title">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">goal</span>&gt;</span></span><br><span class="line">                    shade</span><br><span class="line">                <span class="tag">&lt;/<span class="title">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">executions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">minimizeJar</span>&gt;</span>true<span class="tag">&lt;/<span class="title">minimizeJar</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">shadedArtifactAttached</span>&gt;</span>true<span class="tag">&lt;/<span class="title">shadedArtifactAttached</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">shadedClassifierName</span>&gt;</span>fat<span class="tag">&lt;/<span class="title">shadedClassifierName</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">relocations</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">relocation</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">pattern</span>&gt;</span>com.google<span class="tag">&lt;/<span class="title">pattern</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">shadedPattern</span>&gt;</span>shaded.guava<span class="tag">&lt;/<span class="title">shadedPattern</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">includes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">include</span>&gt;</span>com.google.**<span class="tag">&lt;/<span class="title">include</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">includes</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">excludes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">exclude</span>&gt;</span>com.google.common.base.Optional<span class="tag">&lt;/<span class="title">exclude</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">exclude</span>&gt;</span>com.google.common.base.Absent<span class="tag">&lt;/<span class="title">exclude</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">exclude</span>&gt;</span>com.google.common.base.Present<span class="tag">&lt;/<span class="title">exclude</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">excludes</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">relocation</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">relocations</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">filters</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">filter</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="title">artifact</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">excludes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="title">exclude</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="title">exclude</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="title">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="title">exclude</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="title">excludes</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">filters</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[INFO] --- maven-jar-plugin:<span class="number">2.4</span>:jar (<span class="keyword">default</span>-jar) @ test ---</span><br><span class="line">[INFO] Building jar: /Users/zhengqh/Github/test/target/test-<span class="number">1.0</span>-SNAPSHOT.jar</span><br><span class="line">[INFO] --- maven-assembly-plugin:<span class="number">2.2</span>-beta-<span class="number">5</span>:single (make-assembly) @ test ---</span><br><span class="line">...</span><br><span class="line">[INFO] Building jar: /Users/zhengqh/Github/test/target/test-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line">[INFO] Minimizing jar test:test:jar:<span class="number">1.0</span>-SNAPSHOT</span><br><span class="line"></span><br><span class="line">$ ll target</span><br><span class="line">-rw-r--r--  <span class="number">1</span> zhengqh  staff   <span class="number">8.1</span>M  <span class="number">6</span> <span class="number">22</span> <span class="number">11</span>:<span class="number">54</span> test-<span class="number">1.0</span>-SNAPSHOT-fat.jar</span><br><span class="line">-rw-r--r--  <span class="number">1</span> zhengqh  staff    <span class="number">29</span>M  <span class="number">6</span> <span class="number">22</span> <span class="number">11</span>:<span class="number">54</span> test-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line">-rw-r--r--  <span class="number">1</span> zhengqh  staff   <span class="number">9.2</span>K  <span class="number">6</span> <span class="number">22</span> <span class="number">11</span>:<span class="number">54</span> test-<span class="number">1.0</span>-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>
<p>maven-assembly-plugin生成test-1.0-SNAPSHOT-jar-with-dependencies.jar<br>maven-shade-plugin的shadedClassifierName为<code>fat</code>，结果：test-1.0-SNAPSHOT-fat.jar</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  test jar -tvf target/test-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar|grep shaded</span><br><span class="line">assembly并不会重命令，只有shade才可以</span><br><span class="line"></span><br><span class="line">➜  test jar -tvf target/test-<span class="number">1.0</span>-SNAPSHOT-fat.jar|grep shaded</span><br><span class="line">     <span class="number">0</span> Thu Jun <span class="number">22</span> <span class="number">11</span>:<span class="number">54</span>:<span class="number">42</span> CST <span class="number">2017</span> shaded/</span><br><span class="line">     <span class="number">0</span> Thu Jun <span class="number">22</span> <span class="number">11</span>:<span class="number">54</span>:<span class="number">42</span> CST <span class="number">2017</span> shaded/guava/</span><br><span class="line">     <span class="number">0</span> Thu Jun <span class="number">22</span> <span class="number">11</span>:<span class="number">54</span>:<span class="number">42</span> CST <span class="number">2017</span> shaded/guava/common/</span><br><span class="line">     ...</span><br></pre></td></tr></table></figure>
<h3 id="install">install</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">mvn <span class="operator"><span class="keyword">install</span>:<span class="keyword">install</span>-<span class="keyword">file</span> -Dfile=~/Downloads/ojdbc6-<span class="number">11.2</span><span class="number">.0</span><span class="number">.3</span>.jar -DgroupId=com.<span class="keyword">oracle</span> -DartifactId=ojdbc6 -Dversion=<span class="number">11.2</span><span class="number">.0</span> -Dpackaging=jar</span><br><span class="line"></span><br><span class="line">mvn <span class="keyword">install</span>:<span class="keyword">install</span>-<span class="keyword">file</span> -Dfile=pontus-api_2<span class="number">.11</span>-<span class="number">0.0</span><span class="number">.1</span>.jar -DgroupId=cn.fraudmetrix.pontus -DartifactId=pontus-api_2<span class="number">.11</span> -Dversion=<span class="number">0.0</span><span class="number">.1</span> -Dpackaging=jar</span><br><span class="line"></span><br><span class="line">[INFO] Scanning <span class="keyword">for</span> projects...</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] <span class="comment">------------------------------------------------------------------------</span></span><br><span class="line">[INFO] Building Maven Stub <span class="keyword">Project</span> (<span class="keyword">No</span> POM) <span class="number">1</span></span><br><span class="line">[INFO] <span class="comment">------------------------------------------------------------------------</span></span><br><span class="line">[INFO]</span><br><span class="line">[INFO] <span class="comment">--- maven-install-plugin:2.4:install-file (default-cli) @ standalone-pom ---</span></span><br><span class="line">[INFO] Installing /<span class="keyword">Users</span>/zhengqh/pontus-api_2<span class="number">.11</span>-<span class="number">0.0</span><span class="number">.1</span>.jar <span class="keyword">to</span> /<span class="keyword">Users</span>/zhengqh/.m2/repository/cn/fraudmetrix/pontus/pontus-api_2<span class="number">.11</span>/<span class="number">0.0</span><span class="number">.1</span>/pontus-api_2<span class="number">.11</span>-<span class="number">0.0</span><span class="number">.1</span>.jar</span><br><span class="line">[INFO] Installing /<span class="keyword">var</span>/folders/xc/x0b8crk9667ddh1zhfs29_zr0000gn/<span class="keyword">T</span>/mvninstall1940592568391629100.pom <span class="keyword">to</span> /<span class="keyword">Users</span>/zhengqh/.m2/repository/cn/fraudmetrix/pontus/pontus-api_2<span class="number">.11</span>/<span class="number">0.0</span><span class="number">.1</span>/pontus-api_2<span class="number">.11</span>-<span class="number">0.0</span><span class="number">.1</span>.pom</span><br><span class="line">[INFO] <span class="comment">------------------------------------------------------------------------</span></span><br><span class="line">[INFO] <span class="keyword">BUILD</span> <span class="keyword">SUCCESS</span></span><br><span class="line">[INFO] <span class="comment">------------------------------------------------------------------------</span></span><br><span class="line">[INFO] Total <span class="keyword">time</span>: <span class="number">1.088</span> s</span><br><span class="line">[INFO] Finished <span class="keyword">at</span>: <span class="number">2017</span>-<span class="number">07</span>-<span class="number">17</span>T11:<span class="number">50</span>:<span class="number">49</span>+<span class="number">08</span>:<span class="number">00</span></span><br><span class="line">[INFO] <span class="keyword">Final</span> <span class="keyword">Memory</span>: <span class="number">6</span><span class="keyword">M</span>/<span class="number">64</span><span class="keyword">M</span></span><br><span class="line">[INFO] <span class="comment">------------------------------------------------------------------------</span></span></span><br></pre></td></tr></table></figure>
<h3 id="deploy">deploy</h3><p>源码包上传</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">mvn </span>deploy</span><br></pre></td></tr></table></figure>
<p>本地包上传到nexus</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">mvn deploy:deploy-file -DgroupId=&lt;group-id&gt; \</span><br><span class="line">  -<span class="ruby"><span class="constant">DartifactId</span>=&lt;artifact-id&gt; \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Dversion</span>=&lt;version&gt; \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Dpackaging</span>=&lt;type-of-packaging&gt; \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Dfile</span>=&lt;path-to-file&gt; \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">DrepositoryId</span>=&lt;id-to-map-on-server-section-of-settings.xml&gt; \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Durl</span>=&lt;url-of-the-repository-to-deploy&gt;</span><br><span class="line"></span></span><br><span class="line">mvn deploy:deploy-file -DgroupId=依赖项的GroupID \</span><br><span class="line">  -<span class="ruby"><span class="constant">DartifactId</span>=依赖项名称 \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Dversion</span>=依赖版本 \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Dpackaging</span>=jar \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Dfile</span>=三方库的文件路径 \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">DrepositoryId</span>=fraudmetrixRepo \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Durl</span>=<span class="symbol">http:</span>/<span class="regexp">/maven.fraudmetrix.cn/nexus</span><span class="regexp">/content/repositories</span><span class="regexp">/releases/</span></span><br><span class="line"></span></span><br><span class="line">repositoryId对应~/.m2/setting.xml中的server配置</span><br><span class="line"></span><br><span class="line">&lt;server&gt;</span><br><span class="line">  &lt;id&gt;fraudmetrixRepo&lt;/id&gt;</span><br><span class="line">  &lt;username&gt;xxx&lt;/username&gt;</span><br><span class="line">  &lt;password&gt;xxx&lt;/password&gt;</span><br><span class="line">&lt;/server&gt;</span><br><span class="line"></span><br><span class="line">部署pontus-api.jar</span><br><span class="line">mvn deploy:deploy-file -DgroupId=cn.fraudmetrix.pontus -DartifactId=pontus-api_2.11 -Dversion=0.0.1 \</span><br><span class="line"> -<span class="ruby"><span class="constant">Dpackaging</span>=jar -<span class="constant">Dfile</span>=pontus-api_2.<span class="number">11</span>-<span class="number">0</span>.<span class="number">0</span>.<span class="number">1</span>.jar \</span><br><span class="line"></span> -<span class="ruby"><span class="constant">Durl</span>=<span class="symbol">http:</span>/<span class="regexp">/maven.fraudmetrix.cn/nexus</span><span class="regexp">/content/repositories</span><span class="regexp">/releases/</span> -<span class="constant">DrepositoryId</span>=fraudmetrixRepo  </span><br><span class="line"></span></span><br><span class="line">部署ojdbc.jar</span><br><span class="line">mvn deploy:deploy-file -Dfile=/Users/zhengqh/Downloads/install/ojdbc6-11.2.0.3.jar \</span><br><span class="line">  -<span class="ruby"><span class="constant">DgroupId</span>=com.oracle -<span class="constant">DartifactId</span>=ojdbc6 -<span class="constant">Dversion</span>=<span class="number">11.2</span>.<span class="number">0</span> -<span class="constant">Dpackaging</span>=jar \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Durl</span>=<span class="symbol">http:</span>/<span class="regexp">/maven.fraudmetrix.cn/nexus</span><span class="regexp">/content/repositories</span><span class="regexp">/releases/</span> -<span class="constant">DrepositoryId</span>=fraudmetrixRepo</span></span><br></pre></td></tr></table></figure>
<p>以-数字开头或者-V开头生成准备文件：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> files = <span class="keyword">new</span> java.io.<span class="type">File</span>(<span class="string">"/Users/zhengqh/Downloads/V100R002C60U20CP003/common/lib"</span>).listFiles.map(_.getName).filter(_.startsWith(<span class="string">"h"</span>)).toList</span><br><span class="line"><span class="keyword">import</span> scala.util.matching.<span class="type">Regex</span></span><br><span class="line"><span class="keyword">val</span> numitemPattern = <span class="string">"(.*)(-[0-9|V].*)"</span>.r</span><br><span class="line">files.foreach(file =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> numitemPattern(art, version) = file</span><br><span class="line">  println(file + <span class="string">" "</span> + art + <span class="string">" "</span> + version.substring(<span class="number">1</span>).replace(<span class="string">".jar"</span>,<span class="string">""</span>))    </span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>导入到maven仓库：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat genMaven.txt | <span class="keyword">while</span> <span class="built_in">read</span> line</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  jar=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut <span class="operator">-d</span><span class="string">" "</span> <span class="operator">-f</span>1`</span><br><span class="line">  art=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut <span class="operator">-d</span><span class="string">" "</span> <span class="operator">-f</span>2`</span><br><span class="line">  ver=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut <span class="operator">-d</span><span class="string">" "</span> <span class="operator">-f</span>3`</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$jar</span> <span class="variable">$art</span> <span class="variable">$ver</span>"</span></span><br><span class="line">  mvn deploy:deploy-file -Dfile=<span class="variable">$jar</span> \</span><br><span class="line">  -DgroupId=com.huawei.fusion -DartifactId=<span class="variable">$art</span> -Dversion=<span class="string">"<span class="variable">$ver</span>-FSV100R002C60U20CP003"</span> -Dpackaging=jar \</span><br><span class="line">  -Durl=http://maven.fraudmetrix.cn/nexus/content/repositories/releases/ -DrepositoryId=fraudmetrixRepo</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>不更改groupId,从MANIFEST中获取groupId</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">cat genMaven.txt | <span class="keyword">while</span> <span class="built_in">read</span> line</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  jarName=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut <span class="operator">-d</span><span class="string">" "</span> <span class="operator">-f</span>1`</span><br><span class="line">  art=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut <span class="operator">-d</span><span class="string">" "</span> <span class="operator">-f</span>2`</span><br><span class="line">  ver=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut <span class="operator">-d</span><span class="string">" "</span> <span class="operator">-f</span>3`</span><br><span class="line">  <span class="built_in">printf</span> <span class="string">"<span class="variable">$jarName</span> <span class="variable">$art</span> <span class="variable">$ver</span> "</span></span><br><span class="line">  jar xf <span class="variable">$jarName</span> META-INF/MANIFEST.MF</span><br><span class="line">  group=$(cat META-INF/MANIFEST.MF |grep Implementation-Vendor-Id |cut <span class="operator">-d</span><span class="string">" "</span> <span class="operator">-f</span>2)</span><br><span class="line">  <span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$group</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">   <span class="built_in">printf</span> <span class="string">"<span class="variable">$group</span>"</span></span><br><span class="line">   <span class="built_in">print</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line">  rm -rf META-INF</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">cat genFS.txt | <span class="keyword">while</span> <span class="built_in">read</span> line</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  jarName=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut <span class="operator">-d</span><span class="string">" "</span> <span class="operator">-f</span>1`</span><br><span class="line">  art=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut <span class="operator">-d</span><span class="string">" "</span> <span class="operator">-f</span>2`</span><br><span class="line">  ver=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut <span class="operator">-d</span><span class="string">" "</span> <span class="operator">-f</span>3`</span><br><span class="line">  group=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut <span class="operator">-d</span><span class="string">" "</span> <span class="operator">-f</span>4`</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$jarName</span> <span class="variable">$art</span> <span class="variable">$ver</span> <span class="variable">$group</span>"</span></span><br><span class="line">  mvn deploy:deploy-file -Dfile=<span class="variable">$jarName</span> \</span><br><span class="line">    -DgroupId=<span class="variable">$group</span> -DartifactId=<span class="variable">$art</span> -Dversion=<span class="string">"<span class="variable">$ver</span>-FSV100R002C60U20CP003"</span> -Dpackaging=jar \</span><br><span class="line">    -Durl=http://maven.fraudmetrix.cn/nexus/content/repositories/releases/ -DrepositoryId=fraudmetrixRepo  </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>找不到的jar包改版本后重新上传</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mvn deploy:deploy-file -Dfile=hadoop-yarn-server-tests-2.7.2.jar \</span><br><span class="line">  -<span class="ruby"><span class="constant">DgroupId</span>=org.apache.hadoop -<span class="constant">DartifactId</span>=hadoop-yarn-server-tests -<span class="constant">Dversion</span>=<span class="number">2.7</span>.<span class="number">2</span>-<span class="constant">FSV100R002C60U20CP003</span> -<span class="constant">Dpackaging</span>=jar \</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">Durl</span>=<span class="symbol">http:</span>/<span class="regexp">/maven.fraudmetrix.cn/nexus</span><span class="regexp">/content/repositories</span><span class="regexp">/releases/</span> -<span class="constant">DrepositoryId</span>=fraudmetrixRepo</span><br><span class="line"></span>  </span><br><span class="line">find ~/.m2/ -name "*.lastUpdated" | xargs rm</span><br></pre></td></tr></table></figure>
<h2 id="SBT">SBT</h2><h2 id="Ref">Ref</h2><ul>
<li><a href="https://stackoverflow.com/questions/13620281/what-is-the-maven-shade-plugin-used-for-and-why-would-you-want-to-relocate-java" target="_blank" rel="external">https://stackoverflow.com/questions/13620281/what-is-the-maven-shade-plugin-used-for-and-why-would-you-want-to-relocate-java</a></li>
<li><a href="https://www.elastic.co/blog/to-shade-or-not-to-shade#sthash.CRl8HKfN.dpbs" target="_blank" rel="external">https://www.elastic.co/blog/to-shade-or-not-to-shade#sthash.CRl8HKfN.dpbs</a></li>
<li><a href="http://ju.outofmemory.cn/entry/67085" target="_blank" rel="external">http://ju.outofmemory.cn/entry/67085</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Maven,SBT构建工具&lt;br&gt;
    
    </summary>
    
      <category term="tool" scheme="http://github.com/zqhxuyuan/categories/tool/"/>
    
    
      <category term="work" scheme="http://github.com/zqhxuyuan/tags/work/"/>
    
  </entry>
  
  <entry>
    <title>Daily Work</title>
    <link href="http://github.com/zqhxuyuan/2016/12/31/Daily/"/>
    <id>http://github.com/zqhxuyuan/2016/12/31/Daily/</id>
    <published>2016-12-30T16:00:00.000Z</published>
    <updated>2017-07-17T10:36:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>日常工作常用命令（Linux、Mac）<br><a id="more"></a></p>
<h1 id="Linux_Daily">Linux Daily</h1><h2 id="1-_rpm简单命令">1. rpm简单命令</h2><p>查看已经安装的软件 <figure class="highlight"><figcaption><span>-qa | grep mysql```  </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#24378;&#21147;&#21368;&#36733;&#36719;&#20214; ```rpm -e --nodeps mysql```  &#10;&#23433;&#35013;&#36719;&#20214; ```rpm -ivy xxx.rpm```  &#10;&#10;## 2. yum&#19979;&#36733;&#28304;</span><br></pre></td></tr></table></figure></p>
<p>cd /etc/yum.repos.d<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="blockquote">&gt; 如果是RHEL, 则/etc/yum.repos.d下没有任何源.   </span></span><br><span class="line"><span class="blockquote">&gt; 可以通过rpm -ivh epel*.rpm安装源. 安装后会在/etc/yum.repos.d下生成repl.repo文件.   </span></span><br><span class="line"><span class="blockquote">&gt; 如果是CentOS, 则有CentOS-Base.repo. 在确保虚拟机能够ping通外网, 可以直接通过wget获取文件.   </span></span><br><span class="line"></span><br><span class="line"><span class="header">### RHEL使用163源</span></span><br><span class="line"></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="title">http:</span>//<span class="attribute">boris05.blog.51cto.com</span>/<span class="attribute">1073705</span>/<span class="attribute">1439865</span>&gt;</span></span></span><br><span class="line"></span><br><span class="line">删除redhat原有的yum源 <span class="code">```</span>rpm -aq|grep yum|xargs rpm -e --nodeps</span><br></pre></td></tr></table></figure></p>
<p>下载yum安装文件 163 6.5</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget <span class="symbol">http:</span>/<span class="regexp">/mirrors.163.com/centos</span><span class="regexp">/6.5/os</span><span class="regexp">/x86_64/</span><span class="constant">Packages</span>/yum-<span class="number">3.2</span>.<span class="number">29</span>-<span class="number">40</span>.el6.centos.noarch.rpm </span><br><span class="line">wget <span class="symbol">http:</span>/<span class="regexp">/mirrors.163.com/centos</span><span class="regexp">/6.5/os</span><span class="regexp">/x86_64/</span><span class="constant">Packages</span>/yum-metadata-parser-<span class="number">1.1</span>.<span class="number">2</span>-<span class="number">16</span>.el6.x86_64.rpm</span><br><span class="line">wget <span class="symbol">http:</span>/<span class="regexp">/mirrors.163.com/centos</span><span class="regexp">/6.5/os</span><span class="regexp">/x86_64/</span><span class="constant">Packages</span>/yum-plugin-fastestmirror-<span class="number">1.1</span>.<span class="number">30</span>-<span class="number">14</span>.el6.noarch.rpm</span><br><span class="line">wget <span class="symbol">http:</span>/<span class="regexp">/mirrors.163.com/centos</span><span class="regexp">/6.5/os</span><span class="regexp">/x86_64/</span><span class="constant">Packages</span>/python-iniparse-<span class="number">0.3</span>.<span class="number">1</span>-<span class="number">2.1</span>.el6.noarch.rpm</span><br></pre></td></tr></table></figure>
<p>进行安装yum</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh python<span class="keyword">*</span></span><br><span class="line">rpm -ivh yum<span class="keyword">*</span></span><br></pre></td></tr></table></figure>
<p>更改yum源</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/yum<span class="class">.repos</span><span class="class">.d</span>/</span><br><span class="line">vim /etc/yum<span class="class">.repos</span><span class="class">.d</span>/rhel<span class="class">.repo</span> </span><br><span class="line"></span><br><span class="line">[base]</span><br><span class="line">name=CentOS-<span class="variable">$releasever</span> - Base</span><br><span class="line">baseurl=http:<span class="comment">//mirrors.163.com/centos/6.5/os/$basearch/</span></span><br><span class="line">gpgcheck=<span class="number">1</span></span><br><span class="line">gpgkey=http:<span class="comment">//mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6</span></span><br><span class="line"><span class="id">#released</span> updates</span><br><span class="line">[updates]</span><br><span class="line">name=CentOS-<span class="variable">$releasever</span> - Updates</span><br><span class="line">baseurl=http:<span class="comment">//mirrors.163.com/centos/6.5/updates/$basearch/</span></span><br><span class="line">gpgcheck=<span class="number">1</span></span><br><span class="line">gpgkey=http:<span class="comment">//mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6</span></span><br><span class="line"><span class="id">#packages</span> used/produced <span class="keyword">in</span> the build but not released</span><br><span class="line">#[addons]</span><br><span class="line">#name=CentOS-<span class="variable">$releasever</span> - Addons</span><br><span class="line">#baseurl=http:<span class="comment">//mirrors.163.com/centos/$releasever/addons/$basearch/</span></span><br><span class="line">#gpgcheck=<span class="number">1</span></span><br><span class="line">#gpgkey=http:<span class="comment">//mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6</span></span><br><span class="line"><span class="hexcolor">#add</span>itional packages that may be useful</span><br><span class="line">[extras]</span><br><span class="line">name=CentOS-<span class="variable">$releasever</span> - Extras</span><br><span class="line">baseurl=http:<span class="comment">//mirrors.163.com/centos/6.5/extras/$basearch/</span></span><br><span class="line">gpgcheck=<span class="number">1</span></span><br><span class="line">gpgkey=http:<span class="comment">//mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6</span></span><br><span class="line"><span class="hexcolor">#add</span>itional packages that extend functionality of existing packages</span><br><span class="line">[centosplus]</span><br><span class="line">name=CentOS-<span class="variable">$releasever</span> - Plus</span><br><span class="line">baseurl=http:<span class="comment">//mirrors.163.com/centos/6.5/centosplus/$basearch/</span></span><br><span class="line">gpgcheck=<span class="number">1</span></span><br><span class="line">enabled=<span class="number">0</span></span><br><span class="line">gpgkey=http:<span class="comment">//mirrors.163.com/centos/RPM-GPG-KEY-CentOS-6</span></span><br><span class="line"></span><br><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br><span class="line">yum update</span><br></pre></td></tr></table></figure>
<h3 id="EPEL-7_下载源">EPEL-7 下载源</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># wget http:<span class="comment">//dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-1.noarch.rpm </span></span></span><br><span class="line"><span class="preprocessor"># rpm -ivh epel-release-<span class="number">7</span>-<span class="number">1.</span>noarch.rpm </span></span><br><span class="line"><span class="preprocessor"># rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-<span class="number">7</span></span></span><br></pre></td></tr></table></figure>
<h3 id="EPEL-6_下载源">EPEL-6 下载源</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># wget http:<span class="comment">//dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm   </span></span></span><br><span class="line"><span class="preprocessor"># rpm -ivh epel-release-<span class="number">6</span>-<span class="number">8.</span>noarch.rpm </span></span><br><span class="line"><span class="preprocessor"># rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-<span class="number">6</span></span></span><br></pre></td></tr></table></figure>
<p>注意如果是CentOS, 则最后的rpm –import要修改成CentOS-6 </p>
<h3 id="CentOS使用阿里云源">CentOS使用阿里云源</h3><h3 id="CentOS-163源">CentOS-163源</h3><p>对于CentOS, /etc/yum.repos.d下有CentOS-Base.repo, 可以直接用别的源替换默认的, 或者新增加源. </p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># wget http:<span class="comment">//mirrors.163.com/.help/CentOS6-Base-163.repo </span></span><br><span class="line"></span><br><span class="line">mv /etc/yum.repos.<span class="keyword">d</span>/CentOS-Base.repo /etc/yum.repos.<span class="keyword">d</span>/CentOS-Base.repo.backup</span><br><span class="line">wget -O /etc/yum.repos.<span class="keyword">d</span>/CentOS-Base.repo http:<span class="comment">//mirrors.aliyun.com/repo/Centos-6.repo</span></span><br><span class="line">mv /etc/yum.repos.<span class="keyword">d</span>/epel.repo /etc/yum.repos.<span class="keyword">d</span>/epel.repo.backup</span><br><span class="line">mv /etc/yum.repos.<span class="keyword">d</span>/epel-testing.repo /etc/yum.repos.<span class="keyword">d</span>/epel-testing.repo.backup</span><br><span class="line">wget -O /etc/yum.repos.<span class="keyword">d</span>/epel.repo http:<span class="comment">//mirrors.aliyun.com/repo/epel-6.repo</span></span><br><span class="line">yum makecache</span><br></pre></td></tr></table></figure>
<p>更新源 </p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># yum clean all </span></span><br><span class="line"><span class="preprocessor"># yum makecache </span></span><br><span class="line"><span class="preprocessor"># yum update </span></span><br><span class="line"><span class="preprocessor"># yum repolist</span></span><br></pre></td></tr></table></figure>
<h2 id="3-_安装基本软件">3. 安装基本软件</h2><p><strong>1. 确保能够上网, 并且yum repolist有数据, 比如先下个163的源. 当服务器稳定之后, 可以禁用源(文件后缀改下即可)</strong></p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># yum install wget</span></span><br></pre></td></tr></table></figure>
<p><strong>2.英文环境 </strong></p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># vi ~/.bashrc </span></span><br><span class="line">export LANG=en_US.UTF-<span class="number">8</span></span><br><span class="line"><span class="preprocessor"># source ~/.bashrc </span></span><br><span class="line"><span class="preprocessor"># vi /etc/sysconfig/i18n </span></span><br><span class="line">LANG=<span class="string">"en_US.UTF-8"</span></span><br></pre></td></tr></table></figure>
<p><strong>3. 安装gcc, git等</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># yum group <span class="operator"><span class="keyword">install</span> <span class="string">"Development Tools"</span>     <span class="comment">--&gt; CentOS6要使用yum groupinstall中间没有空格!</span></span><br><span class="line"># yum grouplist </span><br><span class="line">Loaded plugins: fastestmirror, product-<span class="keyword">id</span>, subscription-manager </span><br><span class="line">This <span class="keyword">system</span> <span class="keyword">is</span> <span class="keyword">not</span> registered <span class="keyword">to</span> Red Hat Subscription <span class="keyword">Management</span>. You can <span class="keyword">use</span> subscription-manager <span class="keyword">to</span> <span class="keyword">register</span>. </span><br><span class="line">Setting up <span class="keyword">Group</span> Process </span><br><span class="line">Loading mirror speeds <span class="keyword">from</span> cached hostfile </span><br><span class="line">Installed <span class="keyword">Groups</span>: </span><br><span class="line">   Console internet tools </span><br><span class="line">   Development tools </span><br><span class="line">   <span class="keyword">E</span>-mail <span class="keyword">server</span> </span><br><span class="line">   Perl Support </span><br><span class="line">   <span class="keyword">Security</span> Tools</span></span><br></pre></td></tr></table></figure>
<p>这里已经安装了Development tools, 所以会显示在Installed Groups里.  注意不是yum group list<br>如果没有安装, 会显示在Available Groups里.  上一步的英文环境很重要, 否则如果是中文环境, 你就不知道要安装哪个组了. </p>
<p><strong>4.yum安装MySQL客户端和服务器</strong> </p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># yum info mysql </span><br><span class="line"># yum <span class="keyword">list</span> | <span class="keyword">grep</span> mysql </span><br><span class="line"># yum groupinfo <span class="string">"MySQL Database server"</span> </span><br><span class="line"># yum groupinfo <span class="string">"MySQL Database client"</span> </span><br><span class="line"> Mandatory Package<span class="variable">s:</span> </span><br><span class="line">   mysql </span><br><span class="line"> Default Package<span class="variable">s:</span> </span><br><span class="line">   MySQL-<span class="keyword">python</span> </span><br><span class="line">   mysql-connector-odbc </span><br><span class="line"> Optional Package<span class="variable">s:</span> </span><br><span class="line">   libdbi-dbd-mysql </span><br><span class="line">   mysql-connector-java </span><br><span class="line">   <span class="keyword">perl</span>-DBD-MySQL</span><br></pre></td></tr></table></figure>
<p>当然也可以单独一个一个安装. 同样要注意在/etc/yum.repos.d中要存在163, 或者epel等源. 否则如果没有源, 安装任何软件都没有数据.</p>
<p><a href="http://www.cnblogs.com/xiaoluo501395377/archive/2013/04/07/3003278.html" target="_blank" rel="external">http://www.cnblogs.com/xiaoluo501395377/archive/2013/04/07/3003278.html</a>  </p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># yum install mysql mysql-server mysql-devel </span></span><br><span class="line"><span class="preprocessor"># service mysqld start </span></span><br><span class="line"><span class="preprocessor"># netstat -anpt | grep 3306 </span></span><br><span class="line"><span class="preprocessor"># chkconfig --add mysqld </span></span><br><span class="line"><span class="preprocessor"># chkconfig --list | grep mysqld </span></span><br><span class="line"><span class="preprocessor"># chkconfig mysqld on </span></span><br><span class="line"><span class="preprocessor"># mysqladmin -u root password 'root' </span></span><br><span class="line"><span class="preprocessor"># mysql -u root -p </span></span><br><span class="line">&gt; show databases;</span><br></pre></td></tr></table></figure>
<p><strong>5. nginx</strong></p>
<p>源码安装方式: <a href="http://network810.blog.51cto.com/2212549/1264669" target="_blank" rel="external">http://network810.blog.51cto.com/2212549/1264669</a><br>yum源安装:  nginx默认不在源里. 需要自己去nginx官网下载repo文件. 下载完后可以删除或者备份. </p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="constant">CentOS</span> </span><br><span class="line"><span class="comment"># vi /etc/yum.repos.d/nginx.repo </span></span><br><span class="line">[nginx]</span><br><span class="line">name=nginx repo</span><br><span class="line">baseurl=<span class="symbol">http:</span>/<span class="regexp">/nginx.org/packages</span><span class="regexp">/centos/</span><span class="variable">$releasever</span>/<span class="variable">$basearch</span>/</span><br><span class="line">gpgcheck=<span class="number">0</span></span><br><span class="line">enabled=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="constant">RHEL</span></span><br><span class="line"><span class="comment"># vi /etc/yum.repos.d/nginx.repo </span></span><br><span class="line">[nginx]</span><br><span class="line">name=nginx repo</span><br><span class="line">baseurl=<span class="symbol">http:</span>/<span class="regexp">/nginx.org/packages</span><span class="regexp">/rhel/</span><span class="variable">$releasever</span>/<span class="variable">$basearch</span>/</span><br><span class="line">gpgcheck=<span class="number">0</span></span><br><span class="line">enabled=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># yum install nginx</span></span><br></pre></td></tr></table></figure>
<p>如果服务器开启防火墙, 则要开放80端口 </p>
<figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi /etc/sysconfig/iptables </span></span><br><span class="line">-A INPUT -m <span class="keyword">state</span> --state NEW -m    tcp -p tcp --dport <span class="number">80</span> -j ACCEPT</span><br></pre></td></tr></table></figure>
<p>重启防火墙 </p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># service iptables restart</span></span><br></pre></td></tr></table></figure>
<p>启动nginx方法. 显然第一种方法最快 </p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># service nginx start </span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># cd /usr/local/nginx/sbin </span></span><br><span class="line"><span class="preprocessor"># ./nginx </span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf</span></span><br></pre></td></tr></table></figure>
<p>同mysqld加入到chkconfig的方式开机启动, 也可以把nginx加入开机启动项中.<br>如果提示没有nginx这个service, 参考 <a href="http://www.01happy.com/centos-nginx-shell-chkconfig" target="_blank" rel="external">http://www.01happy.com/centos-nginx-shell-chkconfig</a><br>可以在主机的浏览器上查看, 或者查看端口号80是否开启   </p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># netstat –na|grep 80 </span></span><br><span class="line"><span class="preprocessor"># ps -ef | grep nginx</span></span><br></pre></td></tr></table></figure>
<p>默认配置</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">user</span>  nginx;</span><br><span class="line"><span class="title">worker_processes</span>  <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="title">error_log</span>  /var/log/nginx/error.log <span class="built_in">warn</span>;</span><br><span class="line"><span class="title">pid</span>        /var/run/nginx.pid;</span><br><span class="line"></span><br><span class="line"><span class="title">events</span> &#123;</span><br><span class="line">    <span class="title">worker_connections</span>  <span class="number">1024</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="title">http</span> &#123;</span><br><span class="line">    <span class="title">include</span>       /etc/nginx/mime.types;</span><br><span class="line">    <span class="title">default_type</span>  application/octet-stream;</span><br><span class="line"></span><br><span class="line">    <span class="title">log_format</span>  main  <span class="string">'<span class="variable">$remote_addr</span> - <span class="variable">$remote_user</span> [<span class="variable">$time_local</span>] "<span class="variable">$request</span>" '</span></span><br><span class="line">                      <span class="string">'<span class="variable">$status</span> <span class="variable">$body_bytes_sent</span> "<span class="variable">$http_referer</span>" '</span></span><br><span class="line">                      <span class="string">'"<span class="variable">$http_user_agent</span>" "<span class="variable">$http_x_forwarded_for</span>"'</span>;</span><br><span class="line">    <span class="title">access_log</span>  /var/log/nginx/access.log  main;</span><br><span class="line">    <span class="title">sendfile</span>        <span class="built_in">on</span>;</span><br><span class="line">    <span class="title">keepalive_timeout</span>  <span class="number">65</span>;</span><br><span class="line">    <span class="title">include</span> /etc/nginx/conf.d/<span class="regexp">*.conf</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">user  www www;</span><br><span class="line">worker_processes  <span class="number">2</span>;</span><br><span class="line">error_<span class="built_in">log</span>  logs/error.log;</span><br><span class="line">pid        logs/nginx.pid;</span><br><span class="line">events &#123;</span><br><span class="line">    use epoll;</span><br><span class="line">    worker_connections  <span class="number">2048</span>;</span><br><span class="line">&#125;</span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_<span class="built_in">type</span>  application/octet-stream;</span><br><span class="line">    sendfile        on;</span><br><span class="line">    keepalive_timeout  <span class="number">65</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># gzip压缩功能设置</span></span><br><span class="line">    gzip on;</span><br><span class="line">    gzip_min_length <span class="number">1</span>k;</span><br><span class="line">    gzip_buffers    <span class="number">4</span> <span class="number">16</span>k;</span><br><span class="line">    gzip_http_version <span class="number">1.0</span>;</span><br><span class="line">    gzip_comp_level <span class="number">6</span>;</span><br><span class="line">    gzip_types text/html text/plain text/css text/javascript application/json application/javascript application/x-javascript application/xml;</span><br><span class="line">    gzip_vary on;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># http_proxy 设置</span></span><br><span class="line">    client_max_body_size   <span class="number">10</span>m;</span><br><span class="line">    client_body_buffer_size   <span class="number">128</span>k;</span><br><span class="line">    proxy_connect_timeout   <span class="number">75</span>;</span><br><span class="line">    proxy_send_timeout   <span class="number">75</span>;</span><br><span class="line">    proxy_<span class="built_in">read</span>_timeout   <span class="number">75</span>;</span><br><span class="line">    proxy_buffer_size   <span class="number">4</span>k;</span><br><span class="line">    proxy_buffers   <span class="number">4</span> <span class="number">32</span>k;</span><br><span class="line">    proxy_busy_buffers_size   <span class="number">64</span>k;</span><br><span class="line">    proxy_temp_file_write_size  <span class="number">64</span>k;</span><br><span class="line">    proxy_temp_path   /usr/<span class="built_in">local</span>/nginx/proxy_temp <span class="number">1</span> <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设定负载均衡后台服务器列表 </span></span><br><span class="line">    upstream  backend  &#123; </span><br><span class="line">        server   <span class="number">192.168</span>.<span class="number">10.100</span>:<span class="number">8080</span> max_fails=<span class="number">2</span> fail_timeout=<span class="number">30</span>s ;  </span><br><span class="line">        server   <span class="number">192.168</span>.<span class="number">10.101</span>:<span class="number">8080</span> max_fails=<span class="number">2</span> fail_timeout=<span class="number">30</span>s ;  </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 很重要的虚拟主机配置</span></span><br><span class="line">    server &#123;</span><br><span class="line">        listen       <span class="number">80</span>;</span><br><span class="line">        server_name  itoatest.example.com;</span><br><span class="line">        root   /apps/oaapp;</span><br><span class="line">        charset utf-<span class="number">8</span>;</span><br><span class="line">        access_<span class="built_in">log</span>  logs/host.access.log  main;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#对 / 所有做负载均衡+反向代理</span></span><br><span class="line">        location / &#123;</span><br><span class="line">            root   /apps/oaapp;</span><br><span class="line">            index  index.jsp index.html index.htm;</span><br><span class="line"></span><br><span class="line">            proxy_pass        http://backend;  </span><br><span class="line">            proxy_redirect off;</span><br><span class="line">            <span class="comment"># 后端的Web服务器可以通过X-Forwarded-For获取用户真实IP</span></span><br><span class="line">            proxy_<span class="built_in">set</span>_header  Host  <span class="variable">$host</span>;</span><br><span class="line">            proxy_<span class="built_in">set</span>_header  X-Real-IP  <span class="variable">$remote_addr</span>;  </span><br><span class="line">            proxy_<span class="built_in">set</span>_header  X-Forwarded-For  <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#静态文件，nginx自己处理，不去backend请求tomcat</span></span><br><span class="line">        location  ~* /download/ &#123;  </span><br><span class="line">            root /apps/oa/fs;  </span><br><span class="line">        &#125;</span><br><span class="line">        location ~ .*\.(gif|jpg|jpeg|bmp|png|ico|txt|js|css)$   </span><br><span class="line">        &#123;   </span><br><span class="line">            root /apps/oaapp;   </span><br><span class="line">            expires      <span class="number">7</span>d; </span><br><span class="line">        &#125;</span><br><span class="line">        location /nginx_status &#123;</span><br><span class="line">            stub_status on;</span><br><span class="line">            access_<span class="built_in">log</span> off;</span><br><span class="line">            allow <span class="number">192.168</span>.<span class="number">10.0</span>/<span class="number">24</span>;</span><br><span class="line">            deny all;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location ~ ^/(WEB-INF)/ &#123;   </span><br><span class="line">            deny all;   </span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        error_page   <span class="number">500</span> <span class="number">502</span> <span class="number">503</span> <span class="number">504</span>  /<span class="number">50</span>x.html;</span><br><span class="line">        location = /<span class="number">50</span>x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">## 其它虚拟主机，server 指令开始</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>6. scp文件拷贝</strong></p>
<p>在主机中要拷贝文件到虚拟机中 </p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[主机]<span class="comment"># scp xxx root<span class="doctag">@h</span>101:~/ </span></span><br><span class="line"><span class="symbol">bash:</span> <span class="symbol">scp:</span> command <span class="keyword">not</span> found </span><br><span class="line">lost connection </span><br><span class="line">[虚机]<span class="comment"># yum install openssh-clients</span></span><br></pre></td></tr></table></figure>
<p><strong>7. http虚拟机yum源</strong></p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># yum install -y httpd</span></span><br></pre></td></tr></table></figure>
<p>将主机上的iso文件拷贝到虚拟机中 </p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[主机]</span><span class="comment"># scp **.iso root@h101:~/ </span></span><br><span class="line"><span class="title">[虚机]</span><span class="comment"># mount -o loop CentOS*.iso /var/www/html</span></span><br></pre></td></tr></table></figure>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi /etc/yum.repos.d/http-local.repo </span></span><br><span class="line"><span class="title">[http-local]</span></span><br><span class="line"><span class="setting">name=<span class="value">http-local-<span class="keyword">on</span></span></span></span><br><span class="line"><span class="setting">baseurl=<span class="value">http://<span class="number">192.168</span>.<span class="number">56.101</span>/CentOS_6.<span class="number">5</span>_Final</span></span></span><br><span class="line"><span class="setting">enabled=<span class="value"><span class="number">1</span></span></span></span><br><span class="line"><span class="setting">gpgcheck=<span class="value"><span class="number">1</span></span></span></span><br><span class="line"><span class="setting">gpgkey=<span class="value">file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-<span class="number">6</span></span></span></span><br></pre></td></tr></table></figure>
<p><strong>8. http主机yum源</strong></p>
<p>上面的方式会将iso文件拷贝到虚拟机中, 占用本来空间就不多的虚拟机.<br>可以不用这种方式, 而是在主机中安装httpd服务器(或者nginx), 在虚拟机中直接能访问也可以.   </p>
<p><strong>9. 虚拟机ftp客户端</strong></p>
<p>◇ 主机中开启vsftpd服务, 虚拟机中安装ftp客户端, </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># yum install ftp </span></span><br><span class="line"><span class="preprocessor"># ftp <span class="number">192.168</span><span class="number">.56</span><span class="number">.1</span> </span></span><br><span class="line">Connected to <span class="number">192.168</span><span class="number">.56</span><span class="number">.1</span> (<span class="number">192.168</span><span class="number">.56</span><span class="number">.1</span>). </span><br><span class="line"><span class="number">220</span> (vsFTPd <span class="number">3.0</span><span class="number">.2</span>) </span><br><span class="line">Name (<span class="number">192.168</span><span class="number">.56</span><span class="number">.1</span>:root): anonymous </span><br><span class="line"><span class="number">331</span> Please specify the password. </span><br><span class="line">Password: </span><br><span class="line"><span class="number">230</span> Login successful. </span><br><span class="line">Remote system type is UNIX. </span><br><span class="line">Using binary mode to transfer files. </span><br><span class="line">ftp&gt;ls      ftp服务器当前位置的文件列表</span><br><span class="line">ftp&gt;!Ls     ftp客户端当前位置的文件列表</span><br></pre></td></tr></table></figure>
<p><strong>10. SVN</strong></p>
<p>1). 查看服务器系统是否已经安装SVN</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@datanode01 svn]<span class="preprocessor"># svn --version</span></span><br><span class="line">svn，版本 <span class="number">1.6</span><span class="number">.11</span> (r934486)   编译于 Apr <span class="number">12</span> <span class="number">2012</span>，<span class="number">11</span>:<span class="number">09</span>:<span class="number">11</span></span><br><span class="line">可使用以下的版本库访问模块: </span><br><span class="line">* ra_neon : 通过 WebDAV 协议使用 neon 访问版本库的模块。</span><br><span class="line">  - 处理“http”方案</span><br><span class="line">  - 处理“https”方案</span><br><span class="line">* ra_svn : 使用 svn 网络协议访问版本库的模块。  - 使用 Cyrus SASL 认证</span><br><span class="line">  - 处理“svn”方案</span><br><span class="line">* ra_local : 访问本地磁盘的版本库模块。</span><br><span class="line">  - 处理“file”方案</span><br></pre></td></tr></table></figure>
<p>说明svn已经安装, 并且支持http访问方式. </p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@datanode01 svn]# whereis httpd</span><br><span class="line">httpd: <span class="regexp">/usr/</span>sbin<span class="regexp">/httpd.event /u</span>sr<span class="regexp">/sbin/</span>httpd <span class="regexp">/usr/</span>sbin<span class="regexp">/httpd.worker /</span>etc<span class="regexp">/httpd /u</span>sr<span class="regexp">/lib64/</span>httpd <span class="regexp">/usr/i</span>nclude<span class="regexp">/httpd /u</span>sr<span class="regexp">/share/m</span>an<span class="regexp">/man8/</span>httpd.<span class="number">8</span>.gz</span><br><span class="line">[root@datanode01 svn]# whereis svn</span><br><span class="line">svn: <span class="regexp">/usr/</span>bin<span class="regexp">/svn /u</span>sr<span class="regexp">/share/m</span>an<span class="regexp">/man1/</span>svn.<span class="number">1</span>.gz</span><br><span class="line">[root@datanode01 svn]# whereis svnserve</span><br><span class="line">svnserve: <span class="regexp">/usr/</span>bin<span class="regexp">/svnserve /u</span>sr<span class="regexp">/share/m</span>an<span class="regexp">/man8/</span>svnserve.<span class="number">8</span>.gz</span><br></pre></td></tr></table></figure>
<p>2). 创建版本仓库  </p>
<p>◆ 新建一个目录用于存储SVN所有文件<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">mkdir</span> /<span class="typedef"><span class="keyword">data</span>/data8/svn</span></span><br></pre></td></tr></table></figure></p>
<p>◆ 新建一个版本仓库<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">svnadmin create <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span><span class="keyword">project</span></span><br></pre></td></tr></table></figure></p>
<p>◆ 修改 vi /data/data8/svn/project/conf/passwd 添加用户  </p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[users]</span>  </span><br><span class="line"><span class="comment"># harry = harryssecret</span></span><br><span class="line"><span class="comment"># sally = sallyssecret</span></span><br><span class="line"><span class="setting">admin = <span class="value">admin123</span></span></span><br><span class="line"><span class="setting">zhengqh = <span class="value">zhengqh</span></span></span><br></pre></td></tr></table></figure>
<p>◆ 修改 vi /data/data8/svn/project/conf/authz 修改用户访问策略  </p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[groups]</span><br><span class="line"><span class="preprocessor"># harry_and_sally = harry,sally</span></span><br><span class="line"><span class="preprocessor"># harry_sally_and_joe = harry,sally,&amp;joe</span></span><br><span class="line">group1 = admin,zhengqh</span><br><span class="line"></span><br><span class="line">[/]</span><br><span class="line">@group1 = rw</span><br></pre></td></tr></table></figure>
<p>◆ 修改 vi /data/data8/svn/project/conf/svnserve.conf文件,让用户和策略配置升效.  </p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[general]  </span><br><span class="line">anon-access = none</span><br><span class="line">auth-access = <span class="keyword">write</span></span><br><span class="line">password-db = <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span><span class="keyword">project</span><span class="regexp">/conf/</span>passwd</span><br><span class="line">authz-db = <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span><span class="keyword">project</span><span class="regexp">/conf/</span>authz</span><br></pre></td></tr></table></figure>
<p>◆ 启动服务器<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">svnserve</span> -d -r /<span class="typedef"><span class="keyword">data</span>/data8/svn</span></span><br></pre></td></tr></table></figure></p>
<p>◆ 测试checkout代码库</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~</span><br><span class="line">svn co <span class="string">svn:</span><span class="comment">//172.17.212.69/project</span></span><br></pre></td></tr></table></figure>
<p>显示如下就表示成功了:</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Authentication <span class="string">realm:</span> &lt;<span class="string">svn:</span><span class="comment">//172.17.212.69:3690&gt; e296f93b-eec2-43dd-92b5-cc10ee55c901</span></span><br><span class="line">Password <span class="keyword">for</span> <span class="string">'root'</span>:</span><br><span class="line">Authentication <span class="string">realm:</span> &lt;<span class="string">svn:</span><span class="comment">//172.17.212.69:3690&gt; e296f93b-eec2-43dd-92b5-cc10ee55c901</span></span><br><span class="line"><span class="string">Username:</span> admin</span><br><span class="line">Password <span class="keyword">for</span> <span class="string">'admin'</span>: ***</span><br></pre></td></tr></table></figure>
<p>3). 配置支持使用http访问</p>
<p>◆ 创建svn帐号或修改密码：</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/usr/<span class="keyword">bin/htpasswd </span>-<span class="keyword">b </span>-c /<span class="preprocessor">data</span>/data8/svn/svn-auth-file admin admin123</span><br><span class="line">/usr/<span class="keyword">bin/htpasswd </span>-<span class="keyword">b </span>/<span class="preprocessor">data</span>/data8/svn/svn-auth-file zhengqh zhengqh</span><br></pre></td></tr></table></figure>
<p>-c表示不存在这个文件时创建它. 注意第一行加-c, 后面添加用户不能加-c, 否则会发生覆盖.</p>
<p>◆ 修改svn用户访问策略 </p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /<span class="type">data</span>/data8/svn/svn-<span class="keyword">access</span>-<span class="keyword">file</span></span><br><span class="line">[project:/]</span><br><span class="line">admin = rw</span><br><span class="line">zhengqh = rw</span><br></pre></td></tr></table></figure>
<p>◆ 修改svn目录权限<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod -R <span class="number">777</span> <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span><span class="keyword">project</span></span><br></pre></td></tr></table></figure></p>
<p>◆ 查看svn和httpd依赖的文件:</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">cd</span> /etc/httpd/modules</span><br><span class="line"># <span class="keyword">ls</span> | <span class="keyword">grep</span> svn</span><br><span class="line">mod_authz_svn.<span class="keyword">so</span></span><br><span class="line">mod_dav_svn.<span class="keyword">so</span></span><br></pre></td></tr></table></figure>
<p>◆ 修改 vi /etc/httpd/conf/httpd.conf 增加</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">LoadModule dav_svn_module     <span class="regexp">/etc/</span>httpd<span class="regexp">/modules/</span>mod_dav_svn.so</span><br><span class="line">LoadModule authz_svn_module   <span class="regexp">/etc/</span>httpd<span class="regexp">/modules/</span>mod_authz_svn.so</span><br><span class="line"></span><br><span class="line">&lt;Location /svn&gt;</span><br><span class="line">    DAV svn</span><br><span class="line">    SVNParentPath <span class="regexp">/data/</span>data8/svn</span><br><span class="line">    AuthType Basic</span><br><span class="line">    AuthName <span class="string">"Subversion repository"</span></span><br><span class="line">    AuthUserFile <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span>svn-auth-file</span><br><span class="line">    Require valid-user</span><br><span class="line">    AuthzSVNAccessFile <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span>svn-access-file</span><br><span class="line">&lt;/Location&gt;</span><br></pre></td></tr></table></figure>
<p>◆ 启动apache httpd服务</p>
<p>下面几个命令只要执行其中一个即可(最后一个/usr/local一般用于自定义安装httpd才使用)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># /usr/sbin/apachectl <span class="operator"><span class="keyword">start</span></span><br><span class="line"># /etc/init.<span class="keyword">d</span>/httpd <span class="keyword">start</span></span><br><span class="line"># /usr/<span class="keyword">local</span>/apache2/<span class="keyword">bin</span>/apachectl <span class="keyword">start</span></span></span><br></pre></td></tr></table></figure>
<p>◆ 重启svn服务</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># ps -ef | grep svn</span></span><br><span class="line">root     <span class="number">41348</span>     <span class="number">1</span>  <span class="number">0</span> <span class="number">11</span>:<span class="number">22</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> svnserve -d -r /data/data8/svn</span><br><span class="line">root     <span class="number">41971</span> <span class="number">40812</span>  <span class="number">0</span> <span class="number">11</span>:<span class="number">39</span> pts/<span class="number">2</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep svn</span><br><span class="line"><span class="preprocessor"># kill -<span class="number">9</span>  <span class="number">41348</span></span></span><br><span class="line"><span class="preprocessor"># svnserve -d -r /data/data8/svn</span></span><br></pre></td></tr></table></figure>
<p>4). 用户维护</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vi <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span>project<span class="regexp">/conf/</span>passwd</span><br><span class="line">添加用户名 = 密码</span><br><span class="line">vi <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span>project<span class="regexp">/conf/</span>authz</span><br><span class="line">添加用户名到group1中</span><br><span class="line">执行命令:   <span class="regexp">/usr/</span>bin<span class="regexp">/htpasswd -b /</span>data<span class="regexp">/data8/</span>svn/svn-auth-file 用户名 密码</span><br><span class="line">添加用户的访问策略</span><br><span class="line">vi <span class="regexp">/data/</span>data8<span class="regexp">/svn/</span>svn-access-file</span><br><span class="line">用户名 = rw</span><br></pre></td></tr></table></figure>
<h2 id="4-_磁盘">4. 磁盘</h2><h3 id="查看文件大小">查看文件大小</h3><p><code>ll -h</code>只能查看文件的大小. 不能查看文件夹占用的大小. </p>
<p>查看某个目录总的大小</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> <span class="keyword">dir</span></span><br><span class="line">du -<span class="keyword">h</span> ./</span><br><span class="line">du -<span class="keyword">sh</span> *</span><br></pre></td></tr></table></figure>
<p>在最后会列出这个文件夹占用的大小.  或者不用cd, 直接du -h dir</p>
<h3 id="扩容操作">扩容操作</h3><p>查看磁盘使用量: <code>df -mh</code><br>根目录/达到了100% :  /dev/mapper/vg_datanode01-LogVol00</p>
<p>查看卷: vgdisplan:  </p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="comment">Volume</span> <span class="comment">group</span> <span class="literal">-</span><span class="literal">-</span><span class="literal">-</span></span><br><span class="line"><span class="comment">VG</span> <span class="comment">Name</span>    <span class="comment">vg_datanode01</span></span><br><span class="line"></span><br><span class="line"><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">Logical</span> <span class="comment">volume</span> <span class="literal">-</span><span class="literal">-</span><span class="literal">-</span></span><br><span class="line"><span class="comment">LV</span> <span class="comment">Path</span>      <span class="comment">/dev/vg_datanode01/LogVol00</span></span><br></pre></td></tr></table></figure>
<p>扩容: </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lvextend -L +<span class="number">10</span>G  <span class="regexp">/dev/</span>mapper/vg_datanode01-LogVol00</span><br><span class="line">resize2fs <span class="regexp">/dev/</span>mapper/vg_datanode01-LogVol00</span><br></pre></td></tr></table></figure>
<h2 id="5-_系统">5. 系统</h2><h3 id="文件数和进程数">文件数和进程数</h3><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ ulimit -a</span><br><span class="line">core file size          <span class="list">(<span class="keyword">blocks</span>, -c)</span> <span class="number">0</span></span><br><span class="line">data seg size           <span class="list">(<span class="keyword">kbytes</span>, -d)</span> unlimited</span><br><span class="line">scheduling priority             <span class="list">(<span class="keyword">-e</span>)</span> <span class="number">0</span></span><br><span class="line">file size               <span class="list">(<span class="keyword">blocks</span>, -f)</span> unlimited</span><br><span class="line">pending signals                 <span class="list">(<span class="keyword">-i</span>)</span> <span class="number">62700</span></span><br><span class="line">max locked memory       <span class="list">(<span class="keyword">kbytes</span>, -l)</span> <span class="number">64</span></span><br><span class="line">max memory size         <span class="list">(<span class="keyword">kbytes</span>, -m)</span> unlimited</span><br><span class="line">open files                      <span class="list">(<span class="keyword">-n</span>)</span> <span class="number">1024</span></span><br><span class="line">pipe size            <span class="list">(<span class="keyword">512</span> bytes, -p)</span> <span class="number">8</span></span><br><span class="line">POSIX message queues     <span class="list">(<span class="keyword">bytes</span>, -q)</span> <span class="number">819200</span></span><br><span class="line">real-time priority              <span class="list">(<span class="keyword">-r</span>)</span> <span class="number">0</span></span><br><span class="line">stack size              <span class="list">(<span class="keyword">kbytes</span>, -s)</span> <span class="number">10240</span></span><br><span class="line">cpu time               <span class="list">(<span class="keyword">seconds</span>, -t)</span> unlimited</span><br><span class="line">max user processes              <span class="list">(<span class="keyword">-u</span>)</span> <span class="number">1024</span></span><br><span class="line">virtual memory          <span class="list">(<span class="keyword">kbytes</span>, -v)</span> unlimited</span><br><span class="line">file locks                      <span class="list">(<span class="keyword">-x</span>)</span> unlimited</span><br><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ ps -ef | grep ETL</span><br><span class="line">midd     <span class="number">14369</span>     <span class="number">1</span>  <span class="number">0</span> Mar29 ?        <span class="number">00</span>:00:00 ETL_ScheduleCenter     </span><br><span class="line">midd     <span class="number">14370</span> <span class="number">14369</span> <span class="number">99</span> Mar29 ?        <span class="number">5</span><span class="number">-06</span>:14:46 ETL_ScheduleServer     </span><br><span class="line">midd     <span class="number">14442</span> <span class="number">14369</span>  <span class="number">6</span> Mar29 ?        <span class="number">02</span>:33:41 ETL_ServerManger       </span><br><span class="line">midd     <span class="number">41892</span> <span class="number">41839</span>  <span class="number">0</span> <span class="number">10</span>:19 pts/3    <span class="number">00</span>:00:00 grep ETL</span><br><span class="line"></span><br><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ lsof -p <span class="number">14369</span> | wc -l</span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ lsof -p <span class="number">14370</span> | wc -l</span><br><span class="line"><span class="number">928</span></span><br><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ lsof -p <span class="number">14442</span> | wc -l</span><br><span class="line"><span class="number">4169</span></span><br><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ cat /proc/sys/fs/file-max</span><br><span class="line"><span class="number">792049</span></span><br><span class="line"></span><br><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ su - </span><br><span class="line"><span class="list">[<span class="keyword">root@datanode01</span> ~]# vi /etc/security/limits.conf</span><br><span class="line">* soft nofile <span class="number">65536</span> * hard nofile <span class="number">65536</span></span><br><span class="line">添加以上, 其中*表示任何用户.  注意要用root用户执行.</span></span></span></span></span></span></span></span></span><br></pre></td></tr></table></figure>
<p>或者使用root用户添加指定用户的文件数和进程数: </p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">echo <span class="string">'########################for ETL 4.1.0'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd soft nofile 65536'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd hard nofile 65536'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd soft nproc 131072'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd hard nproc 131072'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd soft nofile 65536'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd hard nofile 65536'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd soft nproc 131072'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br><span class="line">echo <span class="string">'midd hard nproc 131072'</span> <span class="prompt">&gt;&gt; </span>/etc/security/limits.conf</span><br></pre></td></tr></table></figure>
<p>然后使用midd用户验证ulimit</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="list">[<span class="keyword">midd@datanode01</span> ~]$ ulimit -a</span><br><span class="line">core file size          <span class="list">(<span class="keyword">blocks</span>, -c)</span> <span class="number">0</span></span><br><span class="line">data seg size           <span class="list">(<span class="keyword">kbytes</span>, -d)</span> unlimited</span><br><span class="line">scheduling priority             <span class="list">(<span class="keyword">-e</span>)</span> <span class="number">0</span></span><br><span class="line">file size               <span class="list">(<span class="keyword">blocks</span>, -f)</span> unlimited</span><br><span class="line">pending signals                 <span class="list">(<span class="keyword">-i</span>)</span> <span class="number">62700</span></span><br><span class="line">max locked memory       <span class="list">(<span class="keyword">kbytes</span>, -l)</span> <span class="number">64</span></span><br><span class="line">max memory size         <span class="list">(<span class="keyword">kbytes</span>, -m)</span> unlimited</span><br><span class="line">open files                      <span class="list">(<span class="keyword">-n</span>)</span> <span class="number">65536</span></span><br><span class="line">pipe size            <span class="list">(<span class="keyword">512</span> bytes, -p)</span> <span class="number">8</span></span><br><span class="line">POSIX message queues     <span class="list">(<span class="keyword">bytes</span>, -q)</span> <span class="number">819200</span></span><br><span class="line">real-time priority              <span class="list">(<span class="keyword">-r</span>)</span> <span class="number">0</span></span><br><span class="line">stack size              <span class="list">(<span class="keyword">kbytes</span>, -s)</span> <span class="number">10240</span></span><br><span class="line">cpu time               <span class="list">(<span class="keyword">seconds</span>, -t)</span> unlimited</span><br><span class="line">max user processes              <span class="list">(<span class="keyword">-u</span>)</span> <span class="number">131072</span></span><br><span class="line">virtual memory          <span class="list">(<span class="keyword">kbytes</span>, -v)</span> unlimited</span><br><span class="line">file locks                      <span class="list">(<span class="keyword">-x</span>)</span> unlimited</span></span><br></pre></td></tr></table></figure>
<p>另外方法：<a href="http://gaozzsoft.iteye.com/blog/1824824" target="_blank" rel="external">http://gaozzsoft.iteye.com/blog/1824824</a> </p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.使用ps -ef |grep java   (java代表你程序，查看你程序进程) 查看你的进程ID，记录ID号，假设进程ID为122.使用：lsof -p 12 | wc -<span class="keyword">l</span>    查看当前进程id为12的 文件操作状况    执行该命令出现文件使用情况为 10523.使用命令：ulimit -a   查看每个用户允许打开的最大文件数    发现系统默认的是<span class="keyword">open</span> files (-<span class="keyword">n</span>) 1024，问题就出现在这里。4.然后执行：ulimit -<span class="keyword">n</span> 4096</span><br><span class="line">     将<span class="keyword">open</span> files (-<span class="keyword">n</span>) 1024 设置成<span class="keyword">open</span> files (-<span class="keyword">n</span>) 4096</span><br><span class="line">这样就增大了用户允许打开的最大文件数</span><br></pre></td></tr></table></figure>
<h3 id="内存">内存</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">free</span><br><span class="line">free -<span class="keyword">m</span>   以MB为单位</span><br><span class="line">free -<span class="keyword">g</span>   以GB为单位</span><br><span class="line"></span><br><span class="line">df</span><br><span class="line">df -<span class="keyword">m</span>   以MB</span><br><span class="line">df -<span class="keyword">h</span>   以人类(human)可读的, 即GB</span><br><span class="line"></span><br><span class="line">pstree -p | wc -<span class="keyword">l</span></span><br></pre></td></tr></table></figure>
<h3 id="自动重启(jstat)">自动重启(jstat)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">c=`/usr/install/jdk1.<span class="number">8.0</span>_60/bin/jps -lm | grep CassandraDaemon | awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line">old=`/usr/install/jdk1.<span class="number">8.0</span>_60/bin/jstat -gc <span class="variable">$c</span> |tail -<span class="number">1</span>  |awk <span class="string">'&#123;print $8&#125;'</span>`</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$&#123;old%.*&#125;</span> <span class="operator">-gt</span> <span class="number">8388608</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"error: <span class="variable">$old</span>"</span></span><br><span class="line">  /usr/install/cassandra/bin/nodetool flush</span><br><span class="line">  /usr/install/cassandra/bin/nodetool stopdaemon</span><br><span class="line">  sleep <span class="number">15</span>s</span><br><span class="line">  /usr/install/cassandra/bin/cassandra</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"normal: <span class="variable">$old</span>"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#crontab -e</span></span><br><span class="line"><span class="comment">#*/1 * * * * sh gc_old.sh &gt; gc_old.log 2&gt;&amp;1 &amp;</span></span><br></pre></td></tr></table></figure>
<h3 id="用户和权限">用户和权限</h3><p>添加用户, 设置密码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">useradd <span class="operator">-d</span> /home/postgres postgres </span><br><span class="line">passwd postgres</span><br></pre></td></tr></table></figure>
<p>更改读写权限, 地柜目录使用大写的R. (注意scp时用的是小写的r) </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod <span class="number">755</span> file</span><br><span class="line">chmod -R <span class="number">755</span> folder</span><br></pre></td></tr></table></figure>
<p>更改用户名:组</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">chown</span> <span class="tag">hadoop</span><span class="pseudo">:hadoop</span> <span class="tag">-R</span> <span class="tag">folder</span></span><br></pre></td></tr></table></figure>
<h3 id="定时任务cron">定时任务cron</h3><p>1.编写脚本</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi /usr/lib/zookeeper/bin/cron.day </span></span><br><span class="line"><span class="comment">#bin/sh</span></span><br><span class="line">cd /usr/<span class="class"><span class="keyword">lib</span>/<span class="title">zookeeper</span>/<span class="title">bin</span></span></span><br><span class="line">./zkCleanup.sh /opt/hadoop/zookeeper/version-<span class="number">2</span> <span class="number">5</span></span><br><span class="line">echo <span class="string">'clean up end...'</span></span><br></pre></td></tr></table></figure>
<p>2.更改脚本权限</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># chmod <span class="number">755</span> /usr/lib/zookeeper/bin/cron.day</span></span><br></pre></td></tr></table></figure>
<p>3.定时调度策略</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /etc/cron.d</span></span><br><span class="line"><span class="comment"># vi /etc/cron.d/zk.cron </span></span><br><span class="line"><span class="number">0</span> <span class="number">13</span> * * * root run-parts /usr/<span class="class"><span class="keyword">lib</span>/<span class="title">zookeeper</span>/<span class="title">bin</span>/<span class="title">cron</span>.<span class="title">day</span></span></span><br></pre></td></tr></table></figure>
<p>4.导入调度配置</p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># crontab zk.cron</span></span><br></pre></td></tr></table></figure>
<p>5.查看调度列表</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># crontab -l</span></span><br><span class="line"><span class="number">0</span> <span class="number">13</span> * * * root run-parts /usr/<span class="class"><span class="keyword">lib</span>/<span class="title">zookeeper</span>/<span class="title">bin</span>/<span class="title">cron</span>.<span class="title">day</span></span></span><br></pre></td></tr></table></figure>
<p>6.查看是否调度的日志 </p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tail -f /var/log/cron</span></span><br><span class="line"><span class="constant">Apr</span>  <span class="number">1</span> <span class="number">13</span>:<span class="number">00</span>:<span class="number">01</span> namenode02 <span class="constant">CROND</span>[<span class="number">41859</span>]: (root) <span class="constant">CMD</span> (root run-parts /usr/<span class="class"><span class="keyword">lib</span>/<span class="title">zookeeper</span>/<span class="title">bin</span>/<span class="title">cron</span>.<span class="title">day</span>)</span></span><br></pre></td></tr></table></figure>
<p>2.定时任务 <code>sudo -u admin crontab -e</code>:<br>20 11 <em> </em> * /usr/install/sh/activity.sh &gt;&gt; /home/admin/output/cronlogs/do_activity.log 2&gt;&amp;1</p>
<p>3.创建日志重定向文件:<br>sudu -u admin touch /home/admin/output/cronlogs/do_activity.log</p>
<p>4.查看任务运行日志:<br>$ sudo tail -f /var/log/cron<br>Sep  9 11:20:02 spark047214 CROND[9191]: (admin) CMD (/usr/install/sh/activity.sh &gt;&gt; /home/admin/output/cronlogs/do_activity.log 2&gt;&amp;1)</p>
<p>如果没有使用日志重定向, 则默认定时任务输出到mail中:<br>$ sudo -u admin tail -200f /var/spool/mail/admin</p>
<p>其他知识点: </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># vi /etc/crontab</span></span><br><span class="line">SHELL=/bin/bash</span><br><span class="line">PATH=/sbin:/bin:/usr/sbin:/usr/bin</span><br><span class="line">MAILTO=root</span><br><span class="line">HOME=/</span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># For details see man <span class="number">4</span> crontabs</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># Example of job definition:</span></span><br><span class="line"><span class="preprocessor"># .---------------- minute (<span class="number">0</span> - <span class="number">59</span>)</span></span><br><span class="line"><span class="preprocessor"># |  .------------- hour (<span class="number">0</span> - <span class="number">23</span>)</span></span><br><span class="line"><span class="preprocessor"># |  |  .---------- day of month (<span class="number">1</span> - <span class="number">31</span>)</span></span><br><span class="line"><span class="preprocessor"># |  |  |  .------- month (<span class="number">1</span> - <span class="number">12</span>) OR jan,feb,mar,apr ...</span></span><br><span class="line"><span class="preprocessor"># |  |  |  |  .---- day of week (<span class="number">0</span> - <span class="number">6</span>) (Sunday=<span class="number">0</span> or <span class="number">7</span>) OR sun,mon,tue,wed,thu,fri,sat</span></span><br><span class="line"><span class="preprocessor"># |  |  |  |  |</span></span><br><span class="line"><span class="preprocessor"># *  *  *  *  * user-name command to be executed</span></span><br></pre></td></tr></table></figure>
<h2 id="6-_进程">6. 进程</h2><h3 id="进程和端口">进程和端口</h3><p>根据端口号查询进程名字</p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsof -Pnl +M -i4[i6] <span class="string">| grep 20880</span></span><br></pre></td></tr></table></figure>
<h3 id="top">top</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">按M可以按照Memory排序, 按P按照CPU排序</span><br><span class="line">按u可以选择指定的user, 只显示该用户的进程</span><br><span class="line">top -p $(pidof mongod)    只显示指定的进程</span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                                                                                                      </span><br><span class="line"> <span class="number">4735</span> midd      <span class="number">20</span>   <span class="number">0</span> <span class="number">30.1</span>g <span class="number">748</span>m <span class="number">4944</span> S  <span class="number">2.0</span>  <span class="number">9.5</span> <span class="number">102</span>:<span class="number">39.94</span> mongod</span><br></pre></td></tr></table></figure>
<h3 id="telnet">telnet</h3><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="atom">telnet</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span> <span class="number">80</span></span><br><span class="line"><span class="name">Trying</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>...</span><br><span class="line"><span class="name">Connected</span> <span class="atom">to</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>.</span><br><span class="line"><span class="name">Escape</span> <span class="atom">character</span> <span class="atom">is</span> <span class="string">'^]'</span>.</span><br><span class="line">^]             ⬅️ <span class="name">MAC</span>下同时按下<span class="name">Control</span>和]两个键</span><br><span class="line"><span class="atom">telnet</span>&gt; <span class="atom">quit</span>   ⬅️ 出现这个，键入<span class="atom">quit</span>，成功退出</span><br><span class="line"><span class="name">Connection</span> <span class="atom">closed</span>.</span><br></pre></td></tr></table></figure>
<h3 id="kill">kill</h3><p><strong>killall</strong> 命令可以杀死同一个进程的所有子进程.<br>如果用ps -ef | grep 则要一个一个杀.<br>比如ps -ef | grep ETL 显示一共由三个相关进程   </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[midd@datanode01 bin]$ ps -ef | grep ETL</span><br><span class="line">midd      <span class="number">4812</span> <span class="number">41955</span>  <span class="number">0</span> <span class="number">16</span>:<span class="number">01</span> pts/<span class="number">3</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep ETL</span><br><span class="line">midd     <span class="number">42276</span>     <span class="number">1</span>  <span class="number">0</span> <span class="number">15</span>:<span class="number">52</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> ETL_ScheduleCenter     </span><br><span class="line">midd     <span class="number">42277</span> <span class="number">42276</span> <span class="number">99</span> <span class="number">15</span>:<span class="number">52</span> ?        <span class="number">00</span>:<span class="number">15</span>:<span class="number">35</span> ETL_ScheduleServer     </span><br><span class="line">midd     <span class="number">42345</span> <span class="number">42276</span>  <span class="number">7</span> <span class="number">15</span>:<span class="number">53</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">34</span> ETL_ServerManger</span><br></pre></td></tr></table></figure>
<p>而用killall 只需要一行: <code>killall ScheduleCenter</code></p>
<p>批量杀进程</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">process=<span class="string">"cross-partner"</span></span><br><span class="line">ps aux|<span class="keyword">grep</span> <span class="variable">$process</span>|<span class="keyword">grep</span> -v <span class="keyword">grep</span>|awk <span class="string">'&#123;print $2&#125;'</span>|xargs <span class="keyword">kill</span> -<span class="number">9</span></span><br></pre></td></tr></table></figure>
<p>或者更简单的：（类似ssh-&gt;pssh, kill-&gt;pkill）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kill -<span class="number">9</span> $(pgrep amarok)</span><br><span class="line">pkill -<span class="number">9</span> amarok</span><br></pre></td></tr></table></figure>
<h3 id="screen">screen</h3><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">screen</span><br><span class="line"></span><br><span class="line">dstat -tlrvn <span class="number">10</span></span><br><span class="line">    </span><br><span class="line">Ctrl+a+d  退出screen</span><br><span class="line"></span><br><span class="line">screen -ls 列表</span><br><span class="line">screen -r  恢复</span><br><span class="line"></span><br><span class="line">kill screen: screen -X -S <span class="variable">$session</span> quit</span><br></pre></td></tr></table></figure>
<h3 id="nohup">nohup</h3><p><a href="http://ora12c.blogspot.com/2012/04/how-to-put-scp-in-background.html" target="_blank" rel="external">http://ora12c.blogspot.com/2012/04/how-to-put-scp-in-background.html</a></p>
<p>scp命令需要输入密码, 结合nohup, 而nohup是在后台执行, 因此密码没办法输入.  </p>
<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng<span class="variable">@cass047224</span> ~]<span class="variable">$ </span>nohup scp -l <span class="number">100000</span> -r <span class="number">1447314738524</span> <span class="number">192.168</span>.<span class="number">47.219</span><span class="symbol">:~/snapshot/</span><span class="number">224_1114</span>/ &amp;</span><br><span class="line">[<span class="number">1</span>] <span class="number">16169</span></span><br><span class="line"><span class="symbol">nohup:</span> 忽略输入并把输出追加到<span class="string">"nohup.out"</span></span><br><span class="line">[qihuang.zheng<span class="variable">@cass047224</span> ~]<span class="variable">$ </span>qihuang.zheng<span class="variable">@192</span>.<span class="number">168.47</span>.<span class="number">219</span><span class="string">'s password:</span><br><span class="line"></span><br><span class="line">[1]+  Stopped                 nohup scp -l 100000 -r 1447314738524 192.168.47.219:~/snapshot/224_1114/</span></span><br></pre></td></tr></table></figure>
<p>按这里: <a href="http://unix.stackexchange.com/questions/91065/nohup-sudo-does-not-prompt-for-passwd-and-does-nothing" target="_blank" rel="external">http://unix.stackexchange.com/questions/91065/nohup-sudo-does-not-prompt-for-passwd-and-does-nothing</a><br>和这里: <a href="http://stackoverflow.com/questions/13147861/run-scp-in-background-and-monitor-the-progress" target="_blank" rel="external">http://stackoverflow.com/questions/13147861/run-scp-in-background-and-monitor-the-progress</a><br>不要加&amp;, 可以输入密码, Ctrl+Z暂停任务, bg恢复任务  </p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047224 ~]$ nohup scp -l 100000 -r <span class="number">144731473852</span><span class="number">4 192.168.47</span>.219:~/snapshot/<span class="number">224_1114</span>/</span><br><span class="line">nohup: 忽略输入并把输出追加到"nohup.out"</span><br><span class="line">qihuang.zheng@<span class="number">192.168.47.219</span>'s password:   在这里输入密码, 注意必须等输入密码之后,再暂停任务,还没有出现时,不能暂停!!!</span><br><span class="line">^Z</span><br><span class="line">[2]+  Stopped                 nohup scp -l 100000 -r <span class="number">144731473852</span><span class="number">4 192.168.47</span>.219:~/snapshot/<span class="number">224_1114</span>/</span><br><span class="line">[qihuang.zheng@cass047224 ~]$</span><br><span class="line">[qihuang.zheng@cass047224 ~]$ ps -ef|grep scp</span><br><span class="line">501      <span class="number">16169 11856</span>  0 12:29 pts/0    00:00:00 scp -l 100000 -r <span class="number">144731473852</span><span class="number">4 192.168.47</span>.219:~/snapshot/<span class="number">224_1114</span>/</span><br><span class="line">501      <span class="number">16183 16169</span>  0 12:29 pts/0    00:00:00 /usr/bin/ssh -x -oForwardAgent no -oPermitLocalCommand no -oClearAllForwardings yes <span class="number">192.168.47.219</span> scp -r -t ~/snapshot/<span class="number">224_1114</span>/</span><br><span class="line">501      <span class="number">17492 11856</span>  0 12:33 pts/0    00:00:00 scp -l 100000 -r <span class="number">144731473852</span><span class="number">4 192.168.47</span>.219:~/snapshot/<span class="number">224_1114</span>/</span><br><span class="line">501      <span class="number">17493 17492</span>  0 12:33 pts/0    00:00:00 /usr/bin/ssh -x -oForwardAgent no -oPermitLocalCommand no -oClearAllForwardings yes <span class="number">192.168.47.219</span> scp -r -t ~/snapshot/<span class="number">224_1114</span>/</span><br><span class="line">501      <span class="number">17718 11856</span>  0 12:33 pts/0    00:00:00 grep scp</span><br><span class="line">[qihuang.zheng@cass047224 ~]$ bg</span><br><span class="line">[2]+ nohup scp -l 100000 -r <span class="number">144731473852</span><span class="number">4 192.168.47</span>.219:~/snapshot/<span class="number">224_1114</span>/ &amp;    出现这个表示后台开始运行任务了!!!</span><br><span class="line">[qihuang.zheng@cass047224 ~]$ bg</span><br><span class="line">[1]+ nohup scp -l 100000 -r <span class="number">144731473852</span><span class="number">4 192.168.47</span>.219:~/snapshot/<span class="number">224_1114</span>/ &amp;</span><br></pre></td></tr></table></figure>
<p>如果敲入多次bg, 是不是多次执行?  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@cass047224 ~]$ jobs</span><br><span class="line">[<span class="number">1</span>]+  Stopped                 nohup scp -l <span class="number">100000</span> -r <span class="number">1447314738524</span> <span class="number">192.168</span><span class="number">.47</span><span class="number">.219</span>:~/snapshot/<span class="number">224</span>_1114/</span><br><span class="line">[<span class="number">2</span>]-  Running                 nohup scp -l <span class="number">100000</span> -r <span class="number">1447314738524</span> <span class="number">192.168</span><span class="number">.47</span><span class="number">.219</span>:~/snapshot/<span class="number">224</span>_1114/ &amp;</span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20151114125939707" alt="scp-l"></p>
<p>Linux中&amp;、jobs、fg、bg等命令的使用方法: <a href="http://blog.sina.com.cn/s/blog_673ee2b50100iywr.html" target="_blank" rel="external">http://blog.sina.com.cn/s/blog_673ee2b50100iywr.html</a>  </p>
<h3 id="disown">disown</h3><p>disown 示例1（如果提交命令时已经用“&amp;”将命令放入后台运行，则可以直接使用“disown”）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@pvcent107 build]<span class="preprocessor"># cp -r testLargeFile largeFile &amp;</span></span><br><span class="line">[<span class="number">1</span>] <span class="number">4825</span></span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># jobs</span></span><br><span class="line">[<span class="number">1</span>]+  Running                 cp -i -r testLargeFile largeFile &amp;</span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># disown -h %<span class="number">1</span></span></span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># ps -ef |grep largeFile</span></span><br><span class="line">root      <span class="number">4825</span>   <span class="number">968</span>  <span class="number">1</span> <span class="number">09</span>:<span class="number">46</span> pts/<span class="number">4</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> cp -i -r testLargeFile largeFile</span><br><span class="line">root      <span class="number">4853</span>   <span class="number">968</span>  <span class="number">0</span> <span class="number">09</span>:<span class="number">46</span> pts/<span class="number">4</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep largeFile</span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># logout</span></span><br></pre></td></tr></table></figure>
<p>disown 示例2（如果提交命令时未使用“&amp;”将命令放入后台运行，可使用 CTRL-z 和“bg”将其放入后台，再使用“disown”）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@pvcent107 build]<span class="preprocessor"># cp -r testLargeFile largeFile2</span></span><br><span class="line"></span><br><span class="line">[<span class="number">1</span>]+  Stopped                 cp -i -r testLargeFile largeFile2</span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># bg %<span class="number">1</span></span></span><br><span class="line">[<span class="number">1</span>]+ cp -i -r testLargeFile largeFile2 &amp;</span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># jobs</span></span><br><span class="line">[<span class="number">1</span>]+  Running                 cp -i -r testLargeFile largeFile2 &amp;</span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># disown -h %<span class="number">1</span></span></span><br><span class="line">[root@pvcent107 build]<span class="preprocessor"># ps -ef |grep largeFile2</span></span><br><span class="line">root      <span class="number">5790</span>  <span class="number">5577</span>  <span class="number">1</span> <span class="number">10</span>:<span class="number">04</span> pts/<span class="number">3</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> cp -i -r testLargeFile largeFile2</span><br><span class="line">root      <span class="number">5824</span>  <span class="number">5577</span>  <span class="number">0</span> <span class="number">10</span>:<span class="number">05</span> pts/<span class="number">3</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep largeFile2</span><br><span class="line">[root@pvcent107 build]<span class="preprocessor">#</span></span><br></pre></td></tr></table></figure>
<h2 id="7-_文件">7. 文件</h2><h3 id="查看文件编码格式">查看文件编码格式</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># vi .vimrc</span></span><br><span class="line">:<span class="built_in">set</span> fileencoding</span><br><span class="line">  fileencoding=utf8</span><br><span class="line"><span class="built_in">set</span> fileencodings=ucs-bom,utf-<span class="number">8</span>,cp936,gb18030,big5,latin1</span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># vi XXX.file</span></span><br><span class="line">:<span class="built_in">set</span> fencs?</span><br><span class="line">  fileencodings=ucs-bom,utf-<span class="number">8</span>,cp936,gb18030,big5,latin1</span><br><span class="line">:<span class="built_in">set</span> fenc?</span><br><span class="line">  fileencoding=cp936</span><br><span class="line">:<span class="built_in">set</span> enc?</span><br><span class="line">  encoding=utf-<span class="number">8</span></span><br></pre></td></tr></table></figure>
<h3 id="cat文件乱码">cat文件乱码</h3><p>Windows下生成的纯文本文件，其中文编码为GBK，在Ubuntu下显示为乱码，可以使用iconv命令进行转换：</p>
<figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># iconv -f gbk -t utf8 source_file &gt; target_file</span></span><br><span class="line"><span class="label">iconv:</span> 未知 <span class="number">5</span> 处的非法输入序列</span><br></pre></td></tr></table></figure>
<h3 id="GBK转码实践">GBK转码实践</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="shebang">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$#</span>"</span> != <span class="string">"2"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Usage: `basename <span class="variable">$0</span>` dir filter"</span></span><br><span class="line">  <span class="built_in">exit</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">dir=<span class="variable">$1</span></span><br><span class="line">filter=<span class="variable">$2</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$1</span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> `find <span class="variable">$dir</span> -name <span class="string">"<span class="variable">$2</span>"</span>`; <span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$file</span>"</span></span><br><span class="line">  iconv <span class="operator">-f</span> gbk -t utf8 -o <span class="variable">$file</span> <span class="variable">$file</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>执行方式: 第一个参数是目录, 第二个是文件选择</p>
<figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~<span class="regexp">/ftp/GBK2UTF-8_batch.sh ./</span>  M_EP_PD_AQI*</span><br><span class="line">~<span class="regexp">/ftp/GBK2UTF-8_batch.sh ./</span>  M_EP_PH_AQI*</span><br><span class="line">~<span class="regexp">/ftp/GBK2UTF-8_batch.sh ./</span>  M_METE_CITY_PRED*</span><br><span class="line">~<span class="regexp">/ftp/GBK2UTF-8_batch.sh ./</span>  M_METE_WEATHER_LIVE*</span><br></pre></td></tr></table></figure>
<p>执行最后一个, 文件&gt;32kb, 报错:</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4361</span> 总线错误 (core dumped) iconv -c -f gbk -t utf8 -o <span class="variable">$file</span> <span class="variable">$file</span></span><br></pre></td></tr></table></figure>
<p>解决方式: <a href="http://myotragusbalearicus.wordpress.com/2010/03/10/batch-convert-files-to-utf-8/" target="_blank" rel="external">http://myotragusbalearicus.wordpress.com/2010/03/10/batch-convert-files-to-utf-8/</a><br>还是不行: <a href="http://www.path8.net/tn/archives/3448" target="_blank" rel="external">http://www.path8.net/tn/archives/3448</a><br>使用//IGNORE, 成功!  </p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iconv -f gbk//IGNORE -t utf8//IGNORE <span class="variable">$file</span> -o <span class="variable">$file</span>.tmp</span><br></pre></td></tr></table></figure>
<p>注意原始文件必须是和-f对应,如果原始文件是utf8, 要再次转换成utf8, 也会报错.</p>
<p>GBK2UTF8.sh </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="shebang">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$#</span>"</span> != <span class="string">"2"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Usage: `basename <span class="variable">$0</span>` dir filter"</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"sample: ./GBK2UTF8.sh /home/midd/ftp/fz12345/back/2015-03 fz12345_*.txt"</span></span><br><span class="line">  <span class="built_in">exit</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">dir=<span class="variable">$1</span></span><br><span class="line">filter=<span class="variable">$2</span></span><br><span class="line">tmp=<span class="string">'T'</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> `find <span class="variable">$dir</span> -name <span class="string">"<span class="variable">$2</span>"</span>`; <span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$file</span>"</span></span><br><span class="line">  <span class="comment">#iconv -f gbk -t utf8 -o $file $file</span></span><br><span class="line">  <span class="comment">#Notic, the Source File should not utf8 format. or u 'll get error</span></span><br><span class="line">  iconv <span class="operator">-f</span> gbk//IGNORE -t utf8//IGNORE <span class="variable">$file</span> -o <span class="variable">$tmp</span><span class="variable">$file</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h3 id="wordcount计数">wordcount计数</h3><p>计算文件的行数: <code>wc -l file.txt</code><br>要统计单词数量, 加上w选项. L选项表示最长行的长度. 注意这是一整行.不能按照列计算最长长度. </p>
<p>帮助信息: $ wc –help.  如果不知道一个命令, 最好看看–help</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">用法：wc [选项]... [文件]...</span><br><span class="line">　或：wc [选项]... --files0-from=F</span><br><span class="line">  -<span class="ruby">c, --bytes            print the byte counts</span><br><span class="line"></span>  -<span class="ruby">m, --chars            print the character counts</span><br><span class="line"></span>  -<span class="ruby">l, --lines            print the newline counts</span><br><span class="line"></span>  -<span class="ruby"><span class="constant">L</span>, --max-line-length 显示最长行的长度</span><br><span class="line"></span>  -<span class="ruby">w, --words     显示单词计数</span><br><span class="line"></span>      -<span class="ruby">-help    显示此帮助信息并退出</span><br><span class="line"></span>      -<span class="ruby">-version   显示版本信息并退出</span></span><br></pre></td></tr></table></figure>
<h3 id="grep查找">grep查找</h3><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ps</span> -ef | <span class="keyword">grep</span> ETL  查看进程</span><br><span class="line"><span class="keyword">cat</span> fz12345_original.txt | <span class="keyword">grep</span> FZ15032700599 查找一个文件里的字符串</span><br></pre></td></tr></table></figure>
<p>查找目录下的文件里的内容</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cat</span> foder/*.* | <span class="keyword">grep</span> XXX</span><br><span class="line"><span class="keyword">find</span> /etc/ -name <span class="string">"*"</span> | xargs <span class="keyword">grep</span> XXX</span><br><span class="line"></span><br><span class="line"><span class="keyword">find</span> ./ -name <span class="string">"*"</span> | xargs <span class="keyword">grep</span> <span class="number">8080</span></span><br></pre></td></tr></table></figure>
<p>注意: 第一个cat命令无法用于递归子目录, 第二个命令/etc后面必须跟上/, 而且name是*</p>
<p>grep ‘your-search-word’ . -rn </p>
<h3 id="大文件定位到某一行">大文件定位到某一行</h3><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -n <span class="string">'111,112p'</span> <span class="keyword">file</span>.txt</span><br></pre></td></tr></table></figure>
<p>截取文件：</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2016-05-13</span>T00:00   406465</span><br><span class="line"><span class="number">2016-05-14</span>T00:00   <span class="number">1348308</span></span><br><span class="line"></span><br><span class="line">错误的方式： head -<span class="number">1348308</span> gc.log.0 | tail -406465 &gt; tongdun_cassandra_<span class="number">20160513</span>.log</span><br><span class="line">正确的方式： sed -n '<span class="number">406465,134</span>8308p' gc.log.0 &gt; tongdun_cassandra_<span class="number">20160513</span>.log</span><br></pre></td></tr></table></figure>
<h3 id="find文件名">find文件名</h3><p>在指定目录查找文件名: <code>find ~/repo -name *tmp*</code>  </p>
<p>使用管道, xargs表示递归找到的每个值. 如果是文件, 使用rm. 如果是文件夹, 用rm -rf.<br>递归删除svn文件夹:   <code>find SVNFOLDER -name .svn | xargs rm -rf</code><br>递归删除文件:     <code>find ~/repo -name *tmp* | xargs rm</code>  </p>
<p>find -name ‘<em>0456’ -print<br>cat </em> |grep XXX</p>
<h3 id="文件内容替换">文件内容替换</h3><p>\n替换为,  <code>:%s/\n/,/</code></p>
<p>ORACLE类型转换为hive类型:</p>
<figure class="highlight mojolicious"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"></span><span class="perl"><span class="variable">%s</span>/STRING(.*)/STRING/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/VARCHAR2(.*)/STRING/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/CHAR(.*)/STRING/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/DATE,<span class="regexp">/STRING,/</span></span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/date,<span class="regexp">/STRING,/</span></span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/INTEGER/INT/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/NUMBER(..)/BIGINT/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/NUMBER(.)/INT/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/NUMBER(.*)/DOUBLE/</span><span class="xml"></span><br><span class="line"></span><span class="perl"><span class="variable">%s</span>/.<span class="keyword">not</span> null//</span><span class="xml"></span></span><br></pre></td></tr></table></figure>
<p>find . -name “*” | xargs sed -i -e ‘s%cn.fraudmetrix.pontus%com.spark.connectors%g’</p>
<h3 id="rename批量修改文件名">rename批量修改文件名</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">rename</span> .XLS .xlsx *.XLS   把文件名=.XLS的替换成.xlsx</span><br><span class="line"><span class="keyword">rename</span> \_linux <span class="string">''</span> *.txt     把文件名=_linux的替换成空, 注意_要用转义, 即去掉文件名包含_linux的</span><br><span class="line"><span class="keyword">rename</span> -Unlicensed- <span class="string">''</span> *.xlsx</span><br><span class="line"><span class="keyword">rename</span> fun2 fun *</span><br><span class="line"></span><br><span class="line"><span class="keyword">rename</span> <span class="keyword">data</span> <span class="keyword">md5</span> *   # <span class="keyword">rename</span> 原文件要替换  替换后  要替换的文件</span></span><br></pre></td></tr></table></figure>
<h3 id="ftp文件夹下载">ftp文件夹下载</h3><p>wget ftp://172.17.227.236/ctos_analyze/data/tmp/<em> –ftp-user=ftpd –ftp-password=ftpd123 -r  
</em>必须要有, 最后的-r表示目录下载</p>
<p>wget -r -l 1 <a href="http://www.baidu.com/dir/" target="_blank" rel="external">http://www.baidu.com/dir/</a>   </p>
<p>文件续传:  <code>wget -c xxx.file</code></p>
<p>下载网站的所有文件： wget -r -l 1 <a href="http://atlarge.ewi.tudelft.nl/graphalytics/" target="_blank" rel="external">http://atlarge.ewi.tudelft.nl/graphalytics/</a></p>
<h3 id="文件按行数分割">文件按行数分割</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ wc -l dispatch2012.csv </span><br><span class="line"><span class="number">231272</span> dispatch2012.csv</span><br><span class="line"></span><br><span class="line">$ split -l <span class="number">60000</span> dispatch2012.csv dispatch2012_new.csv </span><br><span class="line">$ ll</span><br><span class="line">-rw-r--r-- <span class="number">1</span> hadoop hadoop <span class="number">27649615</span>  <span class="number">9</span>月 <span class="number">27</span>  <span class="number">2014</span> dispatch2012.csv</span><br><span class="line">-rw-rw-r-- <span class="number">1</span> hadoop hadoop  <span class="number">7115577</span>  <span class="number">4</span>月 <span class="number">14</span> <span class="number">19</span>:<span class="number">08</span> dispatch2012_new.csvaa</span><br><span class="line">-rw-rw-r-- <span class="number">1</span> hadoop hadoop  <span class="number">7188497</span>  <span class="number">4</span>月 <span class="number">14</span> <span class="number">19</span>:<span class="number">08</span> dispatch2012_new.csvab</span><br><span class="line">-rw-rw-r-- <span class="number">1</span> hadoop hadoop  <span class="number">7208496</span>  <span class="number">4</span>月 <span class="number">14</span> <span class="number">19</span>:<span class="number">08</span> dispatch2012_new.csvac</span><br><span class="line">-rw-rw-r-- <span class="number">1</span> hadoop hadoop  <span class="number">6137045</span>  <span class="number">4</span>月 <span class="number">14</span> <span class="number">19</span>:<span class="number">08</span> dispatch2012_new.csvad</span><br><span class="line">$ wc -l dispatch2012_new.csv*</span><br><span class="line">   <span class="number">60000</span> dispatch2012_new.csvaa</span><br><span class="line">   <span class="number">60000</span> dispatch2012_new.csvab</span><br><span class="line">   <span class="number">60000</span> dispatch2012_new.csvac</span><br><span class="line">   <span class="number">51272</span> dispatch2012_new.csvad</span><br><span class="line">  <span class="number">231272</span> 总用量</span><br></pre></td></tr></table></figure>
<h3 id="nc">nc</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">scp复制方式： scp influxdb-<span class="number">0.13</span><span class="number">.0</span>_linux_amd64.tar.gz qihuang.zheng@<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:~/</span><br><span class="line"></span><br><span class="line">用nc需要先在接收端开启一个端口， 然后在发送端把数据发送到接收端的端口</span><br><span class="line"></span><br><span class="line">远程：nc -l <span class="number">1234</span> &gt; influxdb-<span class="number">0.13</span><span class="number">.0</span>_linux_amd64.tar.gz</span><br><span class="line">本地：nc -w  <span class="number">1</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span> <span class="number">1234</span>  &lt; influxdb-<span class="number">0.13</span><span class="number">.0</span>_linux_amd64.tar.gz</span><br><span class="line"></span><br><span class="line">复制文件夹：</span><br><span class="line">scp方式：scp -r influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span> qihuang.zheng@<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:~/</span><br><span class="line"></span><br><span class="line">(注意不要在|之间加空格！默认远程的文件夹和本地的一样)</span><br><span class="line">远程：$ nc -l <span class="number">1234</span>|tar zxvf -</span><br><span class="line">influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/</span><br><span class="line">influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/etc/</span><br><span class="line">influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/usr/</span><br><span class="line">...</span><br><span class="line">influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/etc/influxdb/influxdb.conf</span><br><span class="line"></span><br><span class="line">本地：$ tar -cvzf - influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>|nc <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span> <span class="number">1234</span></span><br><span class="line">a influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span></span><br><span class="line">a influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/etc</span><br><span class="line">a influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/usr</span><br><span class="line">...</span><br><span class="line">a influxdb-<span class="number">0.13</span><span class="number">.0</span>-<span class="number">1</span>/etc/influxdb/influxdb.conf</span><br><span class="line"></span><br><span class="line">tar -cvzf - android_device_session|nc <span class="number">192.168</span><span class="number">.50</span><span class="number">.20</span> <span class="number">1234</span></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nc -l <span class="number">1234</span>|tar xvf -</span><br><span class="line">tar -cvf - android_device_session|nc <span class="number">192.168</span><span class="number">.50</span><span class="number">.20</span> <span class="number">1234</span></span><br></pre></td></tr></table></figure>
<p>nohup 结合 nc报错：  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">gzip: stdin: unexpected <span class="operator"><span class="keyword">end</span> <span class="keyword">of</span> <span class="keyword">file</span></span><br><span class="line">tar: <span class="keyword">Child</span> returned <span class="keyword">status</span> <span class="number">1</span></span><br><span class="line">tar: <span class="keyword">Error</span> <span class="keyword">is</span> <span class="keyword">not</span> recoverable: exiting <span class="keyword">now</span></span><br><span class="line"></span><br><span class="line">[<span class="number">1</span>]+  <span class="keyword">Exit</span> <span class="number">2</span>                  nohup nc -<span class="keyword">l</span> <span class="number">1234</span> | tar zxvf -</span></span><br></pre></td></tr></table></figure>
<p>screen:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">screen -S fp_android_device_session_159 nc -l <span class="number">1234</span>|tar xvf -</span><br><span class="line">screen -S fp_android_device_session_159 tar -cvf - android_device_session|nc <span class="number">192.168</span><span class="number">.50</span><span class="number">.20</span> <span class="number">1234</span></span><br></pre></td></tr></table></figure>
<p>disown:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nc <span class="operator">-l</span> <span class="number">1234</span>|tar xvf - &amp;</span><br><span class="line"><span class="built_in">jobs</span></span><br><span class="line"><span class="built_in">disown</span> -h %<span class="number">1</span></span><br><span class="line"><span class="built_in">jobs</span></span><br><span class="line"><span class="built_in">logout</span></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[admin@spark015010 ~]$ nc -d -l <span class="number">1234</span>|tar xvf - &amp;</span><br><span class="line">[<span class="number">2</span>] <span class="number">13672</span></span><br><span class="line">[admin@spark015010 ~]$ jobs</span><br><span class="line">[<span class="number">1</span>]+  Stopped                 nc -l <span class="number">1234</span> | tar xvf -</span><br><span class="line">[<span class="number">2</span>]-  Running                 nc -l <span class="number">1234</span> | tar xvf - &amp;</span><br><span class="line">[admin@spark015010 ~]$ disown -h %<span class="number">1</span></span><br><span class="line">[admin@spark015010 ~]$ ps -ef |grep <span class="number">1234</span></span><br><span class="line">admin    <span class="number">13671</span>  <span class="number">7993</span>  <span class="number">0</span> <span class="number">13</span>:<span class="number">38</span> pts/<span class="number">0</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> nc -l <span class="number">1234</span></span><br><span class="line">admin    <span class="number">13713</span>  <span class="number">7993</span>  <span class="number">0</span> <span class="number">13</span>:<span class="number">38</span> pts/<span class="number">0</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep <span class="number">1234</span></span><br><span class="line">[admin@spark015010 ~]$ logout</span><br><span class="line">There are stopped jobs.</span><br><span class="line">[admin@spark015010 ~]$ jobs</span><br><span class="line">[<span class="number">1</span>]+  Stopped                 nc -l <span class="number">1234</span> | tar xvf -</span><br><span class="line">[<span class="number">2</span>]-  Running                 nc -l <span class="number">1234</span> | tar xvf - &amp;</span><br><span class="line">[admin@spark015010 ~]$ jobs -p</span><br><span class="line"><span class="number">11590</span></span><br><span class="line"><span class="number">13671</span></span><br></pre></td></tr></table></figure>
<h2 id="8-_VI">8. VI</h2><p>显示行号: :set number<br>复制模式：:set paste<br>打开文件定位到最后一行: vi + file.txt ，或者<strong>G</strong><br>第一行：:0回车<br>从指定行删除到最后一行</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">方法<span class="number">1</span>： </span><br><span class="line">使用<span class="built_in">set</span> number计算当前行和最后一行的差比如<span class="number">100</span></span><br><span class="line">输入<span class="number">100</span>dd</span><br></pre></td></tr></table></figure>
<p>清空文件内容：先跳转到文件最后一行：<strong>G</strong>，<strong>:1,.d</strong>  </p>
<h2 id="9-_Awk/sed">9. Awk/sed</h2><h3 id="列编辑">列编辑</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">replace</span>(<span class="keyword">replace</span>(DISPATCHMEMO,chr(10),''),chr(9),'')</span><br><span class="line"></span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="string">"&lt;"</span> <span class="label">$0</span> <span class="string">"&gt; "</span>&#125;' O_FZ12345_CALLINFO&gt;O_FZ12345_CALLINFO2</span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="string">"select count(*) from "</span> <span class="label">$0</span> <span class="string">" union all \n"</span>&#125;' <span class="keyword">cl</span>&gt;cl2</span><br><span class="line"></span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="string">"replace(replace("</span> <span class="label">$0</span> <span class="string">",chr(10),''),chr(9),'')"</span>&#125;' O_FZ12345_CALLINFO&gt;O_FZ12345_CALLINFO2</span><br><span class="line"></span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="string">"replace(replace("</span> <span class="label">$0</span> <span class="string">",chr(10),''),chr(9),'')"</span>&#125;' c3&gt;c4</span><br><span class="line"></span><br><span class="line">%s/,)/,'')/</span><br><span class="line"></span><br><span class="line">alter <span class="keyword">table</span> owner to etl;</span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="string">"alter table "</span> <span class="label">$0</span> <span class="string">" owner to etl;"</span>&#125;' <span class="keyword">test</span>&gt;test2</span><br></pre></td></tr></table></figure>
<h3 id="列的最大长度">列的最大长度</h3><p>下面2个语句执行的结果不同??<br>打印结果时,用双引号<br>awk ‘{if (length($NF)&gt;maxlength) maxlength=length($NF)} END {print maxlength” “$1” “$2” “$NF}’ fz12345_original.txt   </p>
<p>awk ‘{s[$1] += $2}END{ for(i in s){  print i, s[i] } }’ file1 &gt; file2</p>
<p>awk ‘{s[$1” “$2] += $3}END{ for(i in s){print i, s[i] } }’  file1 &gt; file2</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">找出最后一列的最大长度  </span><br><span class="line">awk '&#123;<span class="keyword">if</span> (<span class="literal">length</span>(<span class="label">$NF</span>)&gt;maxlength) maxlength=<span class="literal">length</span>(<span class="label">$NF</span>)&#125; END &#123;<span class="keyword">print</span> maxlength &#125;' fz12345_original.txt   </span><br><span class="line"></span><br><span class="line">找出最大长度的那一条记录  </span><br><span class="line">awk '<span class="literal">length</span>(<span class="label">$NF</span>)==2016 &#123;<span class="keyword">print</span> <span class="label">$1</span><span class="string">" "</span><span class="label">$2</span><span class="string">" "</span><span class="label">$NF&#125;</span>' fz12345_original.txt  </span><br><span class="line"></span><br><span class="line">最后一列为空  </span><br><span class="line">awk '&#123;<span class="label">$NF</span>=<span class="string">""</span> ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt  | head  </span><br><span class="line"></span><br><span class="line">截取最后一列:  </span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900)&#125;' fz12345_original.txt  </span><br><span class="line"></span><br><span class="line">打印一整行:  </span><br><span class="line">awk '&#123;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt  </span><br><span class="line"></span><br><span class="line">打印第一列和最后一列, 最后一列被截取, 以\t分割  </span><br><span class="line">awk '&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$1</span><span class="string">"\t"</span><span class="label">$NF&#125;</span>' fz12345_original.txt  | head  </span><br><span class="line"></span><br><span class="line">截取最后一列, 并打印整行. 但是分隔符变成空格. 如果原先内容有空格,则无法正确解析  </span><br><span class="line">awk '&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt  | head  </span><br><span class="line"></span><br><span class="line">截取最后一列, 打印整行, 分隔符为\t  </span><br><span class="line">awk 'BEGIN &#123;OFS=<span class="string">"\t"</span>&#125;&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt | head  </span><br><span class="line"></span><br><span class="line">行的字段数不一样. 是因为如果有些字段值为空: 如果是空值,则不会被计算为一列!  </span><br><span class="line">awk '&#123;<span class="keyword">print</span> NF&#125;' fz12345.txt | head  </span><br><span class="line">awk '&#123;<span class="keyword">print</span> NF&#125;' fz12345_original.txt | head  </span><br><span class="line"></span><br><span class="line">输出字段分隔符:  </span><br><span class="line">awk 'BEGIN &#123;OFS=<span class="string">"\t"</span>&#125;&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt &gt; fz12345.txt  </span><br><span class="line">输入字段分隔符,输出字段分隔符:  </span><br><span class="line">awk 'BEGIN &#123;FS=<span class="string">"\t"</span>;OFS=<span class="string">"\t"</span>&#125;&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt &gt; fz12345.txt  </span><br><span class="line">输入字段,输入行,输出字段,输出行分隔符:  </span><br><span class="line">awk 'BEGIN &#123;FS=<span class="string">"\t"</span>;RS=<span class="string">"\n"</span>;OFS=<span class="string">"\t"</span>;ORS=<span class="string">"\n"</span>;&#125;&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' fz12345_original.txt &gt; fz12345.txt  </span><br><span class="line"></span><br><span class="line">awk FS=<span class="string">"\t"</span> '&#123;<span class="label">$NF</span>=<span class="literal">substr</span>(<span class="label">$NF</span>, 0, 900) ;<span class="keyword">print</span> <span class="label">$0&#125;</span>' OFS=<span class="string">"\t"</span> fz12345_original.txt &gt; fz12345.txt  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;http:<span class="comment">//coolshell.cn/articles/9070.html&gt;  </span></span><br><span class="line">awk  -F:'&#123;<span class="keyword">print</span> <span class="label">$1</span>,<span class="label">$3</span>,<span class="label">$6&#125;</span>' OFS=<span class="string">"\t"</span> /etc/passwd  </span><br><span class="line">/etc/passwd文件是以:为分隔符的. 取出第1,3,6列. 以\t分割!</span><br></pre></td></tr></table></figure>
<h3 id="一列转多行">一列转多行</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hadoop@hadoop:~$ echo 'A|B|C|aa,bb|DD' | awk -F\| 'BEGIN&#123;OFS=<span class="string">"|"</span>&#125;&#123;<span class="keyword">split</span>(<span class="label">$4</span>,a,<span class="string">","</span>);<span class="keyword">for</span>(i <span class="keyword">in</span> a)&#123;<span class="label">$4</span>=a[i];<span class="keyword">print</span> <span class="label">$0&#125;</span>&#125;'</span><br><span class="line">A|B|C|aa|DD</span><br><span class="line">A|B|C|bb|DD</span><br><span class="line">hadoop@hadoop:~$ echo 'A|B|C|aa|DD' | awk -F\| 'BEGIN&#123;OFS=<span class="string">"|"</span>&#125;&#123;<span class="keyword">split</span>(<span class="label">$4</span>,a,<span class="string">","</span>);<span class="keyword">for</span>(i <span class="keyword">in</span> a)&#123;<span class="label">$4</span>=a[i];<span class="keyword">print</span> <span class="label">$0&#125;</span>&#125;'</span><br><span class="line">A|B|C|aa|DD</span><br><span class="line"></span><br><span class="line">awk 'BEGIN&#123;FS=<span class="string">"\t"</span>;OFS=<span class="string">"\t"</span>&#125;&#123;<span class="keyword">split</span>(<span class="label">$5</span>,a,<span class="string">","</span>);<span class="keyword">for</span>(i <span class="keyword">in</span> a)&#123;<span class="label">$5</span>=a[i];<span class="keyword">print</span> <span class="label">$0&#125;</span>&#125;' \</span><br><span class="line">fz12345_dispatch_2014.txt &gt; fz12345_dispatch_2014_2.txt</span><br><span class="line"></span><br><span class="line">转换前文件:</span><br><span class="line">2251562 FZ15032600537   福州市信访局    2015-03-26 16:29:50     福州市城乡建设委员会,福州市交通运输委员会       0       1       10      2015-04-10 16:29:50</span><br><span class="line"></span><br><span class="line">转换后文件:</span><br><span class="line">2251562 FZ15032600537   福州市信访局    2015-03-26 16:29:50     福州市城乡建设委员会    0       1       10      2015-04-10 16:29:50</span><br><span class="line">2251562 FZ15032600537   福州市信访局    2015-03-26 16:29:50     福州市交通运输委员会    0       1       10      2015-04-10 16:29:50</span><br></pre></td></tr></table></figure>
<p>去掉文件中的所有双引号: sed -i ‘s/“//g’ dispatch2012.csv  </p>
<h3 id="将^M删除">将^M删除</h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sed -<span class="tag">i</span> <span class="string">'s/"//g'</span> dispatch2012<span class="class">.csv</span>            ×</span><br><span class="line"><span class="tag">tr</span> -d <span class="string">'^M'</span> &lt; dispatch2012_2<span class="class">.csv</span>           ×</span><br><span class="line"></span><br><span class="line">alias dos2unix=<span class="string">"sed -i -e 's/'\"\$(printf '\015')\"'//g' "</span>    √</span><br><span class="line">dos2unix dispatch2012_2<span class="class">.csv</span>             </span><br><span class="line">sed -<span class="tag">i</span> -e <span class="string">'s/'</span>\<span class="string">"\$(printf '\015')\"'//g' dispatch2012_2.csv   报错: bash: 未预期的符号 `(' 附近有语法错误</span></span><br></pre></td></tr></table></figure>
<p>删除第一行: sed -i ‘1d;$d’ dispatch2012_2.csv</p>
<p>第五列因为乱码直接改为空字符串!<br>awk ‘BEGIN {FS=”,”;RS=”\n”;OFS=”\t”;ORS=”\n”;}{$5=””;print $0}’ dispatch2012.csv &gt; dispatch2012_2.csv</p>
<h3 id="指定行前/后插入内容">指定行前/后插入内容</h3><p>i表示在之前匹配Regex之前插入，a表示在之后插入</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -<span class="tag">i</span> <span class="string">'N;/Regex/i\插入的内容'</span> file</span><br><span class="line">sed -<span class="tag">i</span> <span class="string">'N;/Regex/a\插入的内容'</span> file</span><br></pre></td></tr></table></figure>
<p>比如</p>
<figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JVM_OPTS=<span class="string">"$JVM_OPTS -Djava.net.preferIPv4Stack=true"</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># JVM_OPTS=<span class="string">"$JVM_OPTS -Djava.rmi.server.hostname=&lt;public name&gt;"</span></span></span><br></pre></td></tr></table></figure>
<p>要在preferIPv4Stack这一行后面插入RMI</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i '<span class="keyword">N</span>;/preferIPv4Stack/a\JVM_OPTS=<span class="string">"$JVM_OPTS -Djava.rmi.server.hostname=192.168.6.52"</span>' apache-cassandra-2.2.6/<span class="keyword">conf</span>/cassandra-env.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">JVM_OPTS=<span class="string">"<span class="variable">$JVM_OPTS</span> -Djava.net.preferIPv4Stack=true"</span></span><br><span class="line"></span><br><span class="line">JVM_OPTS=<span class="string">"<span class="variable">$JVM_OPTS</span> -Djava.rmi.server.hostname=192.168.6.52"</span></span><br><span class="line"><span class="comment"># JVM_OPTS="$JVM_OPTS -Djava.rmi.server.hostname=&lt;public name&gt;"</span></span><br></pre></td></tr></table></figure>
<h3 id="截取时间段日志文件">截取时间段日志文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">end=`date +<span class="string">"%Y-%m-%d %H"</span>`</span><br><span class="line">start=`date <span class="operator">-d</span> <span class="string">"-1 hour"</span> +<span class="string">"%Y-%m-%d %H"</span>`</span><br><span class="line">gcCount=`sed -n <span class="string">"/<span class="variable">$start</span>:[0-9][0-9]:[0-9][0-9]/,/<span class="variable">$end</span>:[0-9][0-9]:[0-9][0-9]/p"</span> /usr/install/cassandra/logs/system.log | grep <span class="string">'GC in [0-9][0-9][0-9][0-9][0-9]'</span> | wc <span class="operator">-l</span>`</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$start</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$end</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$gcCount</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$gcCount</span> &gt; <span class="number">0</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="comment">#告警</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -n '/2016-06-30 18:[<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]:[<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]/,/2016-06-30 19:[<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]:[<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>]/p' /usr/install/cassandra/logs/system.log | grep 'GC in [<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>][<span class="link_reference">0-9</span>][<span class="link_label">0-9</span>]' | wc -l</span><br></pre></td></tr></table></figure>
<h3 id="统计最大值">统计最大值</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2</span>  url1</span><br><span class="line"><span class="number">8</span>  url3</span><br><span class="line"><span class="number">2</span>  url2</span><br><span class="line"><span class="number">3</span>  url1</span><br><span class="line"><span class="number">4</span>  url3</span><br><span class="line"></span><br><span class="line">awk <span class="string">'&#123;max[$2]=max[$2]&gt;$1?max[$2]:$1;number[$2]++;sum[$2]+=$1&#125;</span><br><span class="line">END&#123;for (i in max) print max[i], sum[i]/number[i],number[i],i&#125;'</span> OFS=<span class="string">"\t"</span> url.txt</span><br><span class="line"></span><br><span class="line">awk <span class="string">'&#123;max[$2]=max[$2]&gt;$1?max[$2]:$1;&#125;END&#123;for (i in max) print max[i],i&#125;'</span> OFS=<span class="string">"\t"</span> url.txt</span><br><span class="line"><span class="number">8</span> url3</span><br><span class="line"><span class="number">3</span> url1</span><br><span class="line"><span class="number">2</span> url2</span><br><span class="line"></span><br><span class="line">awk <span class="string">'&#123;max[$1]=max[$1]&gt;$2?max[$1]:$2;&#125;END&#123;for (i in max) print i,max[i]&#125;'</span> OFS=<span class="string">"\t"</span> url.txt</span><br><span class="line"><span class="comment">#$1是key,map[$1]存的是value，比如$1是url1，$2就是2</span></span><br><span class="line">url1 <span class="number">2</span></span><br><span class="line">url3 <span class="number">8</span></span><br><span class="line">url2 <span class="number">2</span></span><br><span class="line">url1 <span class="number">3</span></span><br><span class="line">url3 <span class="number">4</span></span><br></pre></td></tr></table></figure>
<p>awk ‘BEGIN {max = 0} {if ($1+0 &gt; max+0) max=$1} END {print “Max=”, max}’ data</p>
<p>awk -F ‘,’ ‘BEGIN{sum=0}{sum+=$1;} END {print “sum=”sum”}’</p>
<p>awk ‘BEGIN{sum=0}{sum+=$1;} END {print “sum=”sum”}’</p>
<h2 id="10-_脚本">10. 脚本</h2><h3 id="制作程序启动脚本:">制作程序启动脚本:</h3><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">arg=<span class="variable">$1</span></span><br><span class="line">cmd=<span class="string">''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$arg</span> == <span class="string">'idea'</span> ]; <span class="keyword">then</span></span><br><span class="line">  cmd=<span class="string">'/home/hadoop/tool/idea-IU-139.225.3/bin/idea.sh'</span></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">echo <span class="string">'应用程序启动命令：$cmd'</span></span><br><span class="line">nohup <span class="variable">$cmd</span> &amp;</span><br></pre></td></tr></table></figure>
<h3 id="免密码登录:expect">免密码登录:expect</h3><p><a href="http://wgkgood.blog.51cto.com/1192594/1271543" target="_blank" rel="external">http://wgkgood.blog.51cto.com/1192594/1271543</a><br><a href="http://blog.51yip.com/linux/1462.html" target="_blank" rel="external">http://blog.51yip.com/linux/1462.html</a><br><a href="http://bbs.chinaunix.net/thread-915007-1-1.html" target="_blank" rel="external">http://bbs.chinaunix.net/thread-915007-1-1.html</a><br><a href="http://os.51cto.com/art/200912/167898.htm" target="_blank" rel="external">http://os.51cto.com/art/200912/167898.htm</a> </p>
<p>访问服务器的通用命令是: ssh [-port] user@host. 通常情况下需要输入密码. 可以使用expect交互命令, 使用命令行直接登录.  </p>
<figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/expect</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> timeout <span class="number">30</span></span><br><span class="line">spawn ssh [<span class="keyword">lindex</span> <span class="variable">$argv</span> <span class="number">0</span>]@[<span class="keyword">lindex</span> <span class="variable">$argv</span> <span class="number">1</span>]</span><br><span class="line">expect &#123;</span><br><span class="line">        <span class="string">"(yes/no)?"</span></span><br><span class="line">        &#123;send <span class="string">"yes\n"</span>;exp_continue&#125;</span><br><span class="line">        <span class="string">"password:"</span></span><br><span class="line">        &#123;send <span class="string">"[lindex $argv 2]\n"</span>&#125;</span><br><span class="line">&#125;</span><br><span class="line">interact</span><br></pre></td></tr></table></figure>
<p>使用方式: 指定远程机器的用户名,IP地址,密码,[端口]: <code>./login.exp USERNAME HOST PASS [PORT]</code></p>
<h3 id="免密码登录:ssh">免密码登录:ssh</h3><p>在本机ssh-keygen,并将密钥拷贝到目标机器当前访问用户的<code>~/ssh/authorized_keys</code>下</p>
<figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  ~  ssh qihuang.zheng<span class="macrocall">@JUMP_HOST</span></span><br><span class="line"></span><br><span class="line">每次都要输入用户名<span class="macrocall">@远程服务器地址</span>, 可以把它们写死在一个文件里. </span><br><span class="line">➜  ~  cat jump</span><br><span class="line">qihuang.zheng<span class="macrocall">@JUMP_HOST</span></span><br><span class="line">➜  ~  ssh <span class="string">`cat jump`</span></span><br><span class="line"></span><br><span class="line">或者把ssh qihuang.zheng<span class="macrocall">@JUMP_HOST整个写在一个脚本里并加入到PATH里</span>. 只需要执行脚本即可: sshjump</span><br></pre></td></tr></table></figure>
<h3 id="跳板机访问远程命令:端口映射">跳板机访问远程命令:端口映射</h3><p>👉 将远程ssh端口22映射到本地指定的端口:  <code>ssh -f qihuang.zheng@JUMP_HOST -L 127.0.0.1:2207:192.168.47.207:22 -N</code><br>解释下上面的命令了:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JUMP_HOST         👉跳板机</span><br><span class="line"><span class="number">192.168</span><span class="number">.47</span><span class="number">.207</span>:<span class="number">22</span> 👉远程机器的ssh默认端口</span><br><span class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">2207</span>      👉映射到本机端口</span><br></pre></td></tr></table></figure>
<p>👉 现在远程服务器的ssh端口已经映射到本地的2207端口了,所以可以直接:  <code>ssh -p 2207 qihuang.zheng@localhost</code><br>👉 或者使用调用expect脚本自动登陆的方式:  <code>login.exp qihuang.zheng localhost $pass $port</code></p>
<p>端口映射脚本:  </p>
<figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/expect</span></span><br><span class="line"><span class="keyword">set</span> timeout <span class="number">30</span></span><br><span class="line">spawn ssh -f qihuang.zheng@JUMP_HOST -L <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:[<span class="keyword">lindex</span> <span class="variable">$argv</span> <span class="number">1</span>]:<span class="number">192.168</span><span class="number">.47</span>.[<span class="keyword">lindex</span> <span class="variable">$argv</span> <span class="number">0</span>]:[<span class="keyword">lindex</span> <span class="variable">$argv</span> <span class="number">1</span>] -N</span><br><span class="line">expect &#123;</span><br><span class="line">  <span class="string">"password:"</span></span><br><span class="line">  &#123;send <span class="string">"YOURPASSWORD@\n"</span>&#125;</span><br><span class="line">&#125;</span><br><span class="line">interact</span><br></pre></td></tr></table></figure>
<p>使用方式: <code>map.exp 222 8888</code></p>
<h3 id="命令行时间格式转换(Mac版)">命令行时间格式转换(Mac版)</h3><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">当前时间撮</span><br><span class="line">➜  ~  date +<span class="variable">%s</span></span><br><span class="line"><span class="number">1448811002</span></span><br><span class="line"></span><br><span class="line">格式化当前时间</span><br><span class="line">➜ date <span class="string">"+<span class="variable">%Y</span><span class="variable">%m</span><span class="variable">%d</span><span class="variable">%H</span><span class="variable">%M</span><span class="variable">%S</span>"</span></span><br><span class="line"><span class="number">20150717085930</span></span><br><span class="line">➜ date <span class="string">"+<span class="variable">%Y</span>-<span class="variable">%m</span>-<span class="variable">%d</span> <span class="variable">%H</span>:<span class="variable">%M</span>:<span class="variable">%S</span>"</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">17</span> 09:<span class="number">01</span>:<span class="number">32</span></span><br><span class="line"></span><br><span class="line">指定一个时间的时间撮</span><br><span class="line">➜  ~  date -j -f <span class="string">"<span class="variable">%Y</span>-<span class="variable">%m</span>-<span class="variable">%d</span> <span class="variable">%H</span>:<span class="variable">%M</span>:<span class="variable">%S</span>"</span> <span class="string">"2015-10-21 18:03:00"</span> <span class="string">"+<span class="variable">%s</span>"</span></span><br><span class="line"><span class="number">1445421780</span></span><br><span class="line"></span><br><span class="line">将时间错转换为human格式</span><br><span class="line">➜  ~  date -r <span class="number">1445421780</span></span><br><span class="line"><span class="number">2015</span>年<span class="number">10</span>月<span class="number">21</span>日 星期三 <span class="number">18</span>时<span class="number">03</span>分<span class="number">00</span>秒 CST</span><br></pre></td></tr></table></figure>
<p>上面的是在shell终端的结果, 如果要在bash脚本中获取, 则使用``执行命令</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ echo `date <span class="string">"+%Y-%m-%d %H:%M:%S"</span>`</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">17</span> <span class="number">09</span>:<span class="number">04</span>:<span class="number">23</span></span><br><span class="line">➜ currentDate=`date <span class="string">"+%Y-%m-%d %H:%M:%S"</span>`</span><br><span class="line">➜ echo $currentDate</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">17</span> <span class="number">09</span>:<span class="number">05</span>:<span class="number">38</span></span><br></pre></td></tr></table></figure>
<h3 id="正则表达式">正则表达式</h3><p>日志开始部分</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(\d&#123;<span class="number">4</span>&#125;)-(<span class="number">0</span>\d&#123;<span class="number">1</span>&#125;|<span class="number">1</span>[<span class="number">0</span>-<span class="number">2</span>])-(\d&#123;<span class="number">2</span>&#125;) (\d&#123;<span class="number">2</span>&#125;):(\d&#123;<span class="number">2</span>&#125;):(\d&#123;<span class="number">2</span>&#125;):(\d&#123;<span class="number">3</span>&#125;) (\[main\])</span><br></pre></td></tr></table></figure>
<h2 id="11-_Shell">11. Shell</h2><h3 id="Condition">Condition</h3><p>if条件：</p>
<p>循环：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> `seq <span class="number">0</span> <span class="number">10</span>`; do echo <span class="variable">$i</span>; nodetool cfstats md5s.md5_id<span class="number">_</span><span class="variable">$i</span> | grep memory ;done</span><br></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nums=(<span class="string">'0'</span> <span class="string">'1'</span> <span class="string">'2'</span> <span class="string">'3'</span> <span class="string">'4'</span> <span class="string">'5'</span> <span class="string">'6'</span> <span class="string">'7'</span> <span class="string">'8'</span> <span class="string">'9'</span> <span class="string">'a'</span> <span class="string">'b'</span> <span class="string">'c'</span> <span class="string">'d'</span> <span class="string">'e'</span> <span class="string">'f'</span>)</span><br><span class="line"><span class="keyword">for</span> n1 <span class="keyword">in</span> <span class="variable">$&#123;nums[@]&#125;</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="variable">$n1</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"end."</span></span><br></pre></td></tr></table></figure>
<h3 id="Date">Date</h3><p>Mac:</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">date</span> -r <span class="number">1471190400</span></span><br></pre></td></tr></table></figure>
<p>date -j -f “%Y-%m-%d %H:%M:%S” “2017-6-30 12:00:00” “+%s”</p>
<p>date -j -f “%Y-%m-%d %H:%M:%S” “2016-12-30 00:00:00” “+%s”</p>
<p>Linux</p>
<figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">date</span> +<span class="variable">%s</span></span><br><span class="line"><span class="number">1471254010</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'@'</span></span><br><span class="line"><span class="number">2016</span>年 <span class="number">08</span>月 <span class="number">15</span>日 星期一 <span class="number">17</span>:<span class="number">40</span>:<span class="number">10</span> CST</span><br><span class="line"></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'2016-08-15'</span> +<span class="variable">%s</span></span><br><span class="line"><span class="number">1471190400</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'@1471190400'</span></span><br><span class="line"><span class="number">2016</span>年 <span class="number">08</span>月 <span class="number">15</span>日 星期一 <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> CST</span><br><span class="line"></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'2016-08-15 13:20:24'</span> +<span class="variable">%s</span></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'@1471238424'</span></span><br><span class="line"><span class="number">2016</span>年 <span class="number">08</span>月 <span class="number">15</span>日 星期一 <span class="number">13</span>:<span class="number">20</span>:<span class="number">24</span> CST</span><br><span class="line"></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'yesterday'</span></span><br><span class="line"><span class="number">2016</span>年 <span class="number">08</span>月 <span class="number">15</span>日 星期一 <span class="number">13</span>:<span class="number">20</span>:<span class="number">24</span> CST</span><br><span class="line"></span><br><span class="line"><span class="keyword">date</span> -d <span class="string">'yesterday'</span> +<span class="string">"%Y-%-m-%-d"</span></span><br></pre></td></tr></table></figure>
<p>beg=<code>date -d &#39;2017-05-25 00:00:00&#39; +%s</code><br>end=<code>date -d &#39;2017-05-25 01:10:00&#39; +%s</code><br>cassandra/bin/nodetool compactionhistory |grep model_result| sort -rk 4 |awk ‘{if($4/1000&gt;=$beg &amp;&amp; $4/1000&lt;=$end) print $0}’</p>
<p>cat compaction_history.log|grep model_result| sort -rk 4 |awk ‘{if($4/1000&gt;=$beg &amp;&amp; $4/1000&lt;=$end) print $0}’</p>
<p>cat compaction_history.log|head -50 |tail -30| awk ‘{\<br>$4=strftime(“%Y-%m-%d %H:%M:%S”, substr($4,0,10));\<br>$5=$5/1000/1000; $6=$6/1000/1000;\<br>}1’ | cut -d” “ -f4-</p>
<p><a href="http://blog.csdn.net/jk110333/article/details/8590746" target="_blank" rel="external">http://blog.csdn.net/jk110333/article/details/8590746</a>  </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sh range.sh 20160401 20160405</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#datebeg="20160401"</span></span><br><span class="line"><span class="comment">#dateend="20160405"</span></span><br><span class="line">datebeg=<span class="variable">$1</span></span><br><span class="line">dateend=<span class="variable">$2</span></span><br><span class="line">beg_s=`date <span class="operator">-d</span> <span class="string">"<span class="variable">$datebeg</span>"</span> +%s`</span><br><span class="line">end_s=`date <span class="operator">-d</span> <span class="string">"<span class="variable">$dateend</span>"</span> +%s`</span><br><span class="line"></span><br><span class="line">excludes=<span class="string">"20150101 20150102"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> [ <span class="string">"<span class="variable">$beg_s</span>"</span> -le <span class="string">"<span class="variable">$end_s</span>"</span> ];<span class="keyword">do</span></span><br><span class="line">     day=`date <span class="operator">-d</span> @<span class="variable">$beg_s</span> +<span class="string">"%Y%m%d"</span>`;</span><br><span class="line">     beg_s=$((beg_s+<span class="number">86400</span>));</span><br><span class="line">     flag=<span class="literal">false</span></span><br><span class="line">     <span class="keyword">for</span> item <span class="keyword">in</span> <span class="variable">$excludes</span></span><br><span class="line">     <span class="keyword">do</span></span><br><span class="line">        <span class="keyword">if</span> [ <span class="string">"<span class="variable">$day</span>"</span> == <span class="string">"<span class="variable">$item</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">          <span class="built_in">echo</span> <span class="string">"<span class="variable">$day</span> In the list, skip"</span></span><br><span class="line">          flag=<span class="literal">true</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">     <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">     <span class="keyword">if</span> [ <span class="variable">$flag</span> == <span class="literal">false</span> ]; <span class="keyword">then</span></span><br><span class="line">       <span class="built_in">echo</span> <span class="variable">$day</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#代码中只要关心处理一天的记录,由脚本控制,执行多天</span></span><br><span class="line">        /usr/install/spark-<span class="number">1.6</span>.<span class="number">1</span>-bin-hadoop2.<span class="number">4</span>/bin/spark-submit \</span><br><span class="line">        --conf spark.mesos.role=production --conf spark.cores.max=<span class="number">30</span> --conf spark.executor.memory=<span class="number">10</span>g --conf spark.ui.port=<span class="number">4888</span> \</span><br><span class="line">        --conf spark.cassandra.connection.host=<span class="number">192.168</span>.<span class="number">48.163</span> \</span><br><span class="line">        --class cn.fraudmetrix.vulcan.velocity.VelocityRuleApp \</span><br><span class="line">        spark-cassandra-<span class="number">1.0</span>.<span class="number">1</span>-SNAPSHOT-jar-with-dependencies.jar cass <span class="variable">$day</span> &gt; VelocityRuleApp_<span class="variable">$day</span>.log</span><br><span class="line">     <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h1 id="Develop">Develop</h1><p>jar包解压查看是否有某个文件: jar -tvf abc.jar | grep FileName</p>
<p>jar包运行： java -cp xxx-dependency.jar MainClass args<br>如果是打成fat包，可以这么运行。</p>
<p>但是如果不是fat包，而且依赖了第三方包：java -cp third.jar -jar Run.jar MainClass</p>
<p>jmap -dump:live,format=b,file=<filename>.hprof <pid></pid></filename></p>
<p>jstats -gc <pid></pid></p>
<p>mat命令行：<a href="http://www.techpaste.com/2015/07/how-to-analyse-large-heap-dumps/" target="_blank" rel="external">http://www.techpaste.com/2015/07/how-to-analyse-large-heap-dumps/</a>  </p>
<p><img src="http://img.blog.csdn.net/20161221111359916" alt="mat"></p>
<p><img src="http://img.blog.csdn.net/20161222135615298" alt="mat"></p>
<p>mysql权限：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="string">`pontus`</span> <span class="keyword">DEFAULT</span> <span class="built_in">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">COLLATE</span> utf8_general_ci;</span></span><br><span class="line"><span class="operator"><span class="keyword">create</span> <span class="keyword">user</span> <span class="string">'pontus'</span>@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'pontus'</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">grant</span> all <span class="keyword">on</span> pontus.* <span class="keyword">to</span> <span class="string">'pontus'</span>@<span class="string">'%'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'pontus'</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">grant</span> all <span class="keyword">on</span> pontus.* <span class="keyword">to</span> <span class="string">'pontus'</span>@<span class="string">'dp0653'</span> <span class="keyword">identified</span> <span class="keyword">by</span> <span class="string">'pontus'</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'pontus'</span> <span class="keyword">and</span> host=<span class="string">'dp0653'</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">user</span>,<span class="keyword">password</span>,host <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'pontus'</span>;</span></span><br></pre></td></tr></table></figure>
<h2 id="Build">Build</h2><p>maven: .m2/settings.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">mirrors</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">mirror</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">id</span>&gt;</span>alimaven<span class="tag">&lt;/<span class="title">id</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">name</span>&gt;</span>aliyun maven<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">url</span>&gt;</span>http://maven.aliyun.com/nexus/content/groups/public/<span class="tag">&lt;/<span class="title">url</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="title">mirrorOf</span>&gt;</span>central<span class="tag">&lt;/<span class="title">mirrorOf</span>&gt;</span>        </span><br><span class="line">    <span class="tag">&lt;/<span class="title">mirror</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">mirrors</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>sbt: .sbt/repositories</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[repositories]</span><br><span class="line">local</span><br><span class="line">local-maven: file:///Users/zhengqh/.m2/repository/</span><br><span class="line">maven-releases: http://maven.fraudmetrix.cn/nexus/content/groups/public/</span><br><span class="line">ali-maven: http://maven.aliyun.com/nexus/content/groups/public/</span><br><span class="line"><span class="header">#repox-maven: http://192.168.6.53:8078/</span></span><br><span class="line"><span class="header">#repox-ivy: http://192.168.6.53:8078/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]</span></span><br><span class="line"><span class="header">#osc: http://maven.oschina.net/content/groups/public/</span></span><br><span class="line"><span class="header">#oschina-ivy: http://maven.oschina.net/content/groups/public/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]</span></span><br><span class="line">typesafe: http://repo.typesafe.com/typesafe/ivy-releases/, [<span class="link_label">organization</span>]/[<span class="link_label">module</span>]/(scala<span class="emphasis">_[scalaVersion]/)(sbt_</span>[<span class="link_label">sbtVersion</span>]/)[<span class="link_label">revision</span>]/[<span class="link_label">type</span>]s/[<span class="link_label">artifact</span>](<span class="link_url">-[classifier]</span>).[ext], bootOnly</span><br><span class="line">sonatype-oss-releases</span><br><span class="line">maven-central</span><br><span class="line">sonatype-oss-snapshots</span><br></pre></td></tr></table></figure>
<p>gradle project: build.gradle</p>
<figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml">buildscript </span><span class="expression">&#123;</span><br><span class="line">    <span class="variable">repositories</span> &#123;</span><br><span class="line">        <span class="variable">mavenLocal</span>()</span><br><span class="line">        <span class="variable">maven</span> &#123; <span class="variable">url</span> '<span class="variable">http</span>:/<span class="end-block">/maven.aliyun.com</span><span class="end-block">/nexus</span><span class="end-block">/content</span><span class="end-block">/groups</span><span class="end-block">/public</span>/' &#125;</span><span class="xml"></span><br><span class="line">        jcenter()</span><br><span class="line">    &#125;</span><br><span class="line">    dependencies </span><span class="expression">&#123;</span><br><span class="line">    &#125;</span><span class="xml"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">allprojects </span><span class="expression">&#123;</span><br><span class="line">    <span class="variable">repositories</span> &#123;</span><br><span class="line">        <span class="variable">mavenLocal</span>()</span><br><span class="line">        <span class="variable">maven</span> &#123; <span class="variable">url</span> '<span class="variable">http</span>:/<span class="end-block">/maven.aliyun.com</span><span class="end-block">/nexus</span><span class="end-block">/content</span><span class="end-block">/groups</span><span class="end-block">/public</span>/' &#125;</span><span class="xml"></span><br><span class="line">        jcenter()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure>
<p><a href="https://my.oschina.net/abcfy2/blog/783743" target="_blank" rel="external">https://my.oschina.net/abcfy2/blog/783743</a><br><a href="https://yrom.net/blog/2015/02/07/change-gradle-maven-repo-url/" target="_blank" rel="external">https://yrom.net/blog/2015/02/07/change-gradle-maven-repo-url/</a></p>
<p>gradle global: .gradle/init.gradle</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">allprojects</span>&#123;</span><br><span class="line">    <span class="keyword">repositories</span> &#123;</span><br><span class="line">        <span class="keyword">def</span> REPOSITORY_URL = <span class="string">'http://maven.oschina.net/content/groups/public'</span></span><br><span class="line">        all &#123; ArtifactRepository repo -&gt;</span><br><span class="line">            <span class="keyword">if</span>(repo <span class="keyword">instanceof</span> MavenArtifactRepository)&#123;</span><br><span class="line">                <span class="keyword">def</span> url = repo.url.toString()</span><br><span class="line">                <span class="keyword">if</span> (url.startsWith(<span class="string">'https://repo1.maven.org/maven2'</span>) || url.startsWith(<span class="string">'https://jcenter.bintray.com/'</span>)) &#123;</span><br><span class="line">                    <span class="keyword">project</span>.logger.lifecycle <span class="string">"Repository $&#123;repo.url&#125; replaced by $REPOSITORY_URL."</span></span><br><span class="line">                    remove repo</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        maven &#123;</span><br><span class="line">            url REPOSITORY_URL</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="快捷键">快捷键</h2><h3 id="Mac">Mac</h3><p><strong>👉按键</strong>  </p>
<figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Shift</span> - ⬆️      为什么是向上,因为<span class="keyword">Shift</span>在Ctrl+Alt+<span class="built_in">CMD</span>所有键的上方</span><br></pre></td></tr></table></figure>
<p><strong>👉触摸板</strong><br>右键: 双指单击<br>三指移动: 当前App移动位置<br>四指滑动: 全屏下App切换屏幕<br>两指滑动: Launchpad切换屏幕  </p>
<p><strong>👉箭头</strong><br>FN+⬅️ 一行的开始(Home)和结束位置(End)<br>FN+⬆️ 上一页(Up),下一页(Down)<br>ALT+⬅️ 一个一个单词地移动</p>
<p><strong>终端</strong><br>CMD+⬅️:  标签切换</p>
<p>Control+CMD+A: 使用QQ的截图</p>
<h3 id="IDEA">IDEA</h3><figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">CMD</span>+<span class="number">4</span>  Run Window</span><br><span class="line">Go To Implementation实现方法:   Alt+<span class="built_in">CMD</span>+B</span><br><span class="line">Back and Forword前进后退:       Alt+<span class="built_in">CMD</span>+左右</span><br><span class="line"><span class="built_in">Type</span> Hierarchy类型树:           Ctl+H</span><br><span class="line"><span class="flow">Call</span> Hierarchy调用栈:           Ctl+Alt+H</span><br><span class="line">Into进入方法或类:                <span class="built_in">CMD</span>+单击 或者Fn+F4</span><br><span class="line">全局搜索字符串:                  <span class="built_in">CMD</span>+<span class="keyword">Shift</span>+F</span><br></pre></td></tr></table></figure>
<h3 id="Install">Install</h3><h4 id="GO_sublime">GO sublime</h4><ul>
<li>GOROOT：golang的安装包</li>
<li>GOPATH：golang应用程序的workspace</li>
</ul>
<p>安装go后，goroot会自动设置，而gopath需要自己添加到.bashrc中</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export GOPATH=<span class="variable">$HOME</span>/go</span><br><span class="line">export GOBIN=<span class="variable">$GOPATH</span>/bin</span><br><span class="line">export PATH=<span class="variable">$PATH</span>:<span class="variable">$GOBIN</span></span><br></pre></td></tr></table></figure>
<p>go env查看环境信息</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="setting">GOARCH=<span class="value"><span class="string">"amd64"</span></span></span></span><br><span class="line"><span class="setting">GOBIN=<span class="value"><span class="string">"/Users/zhengqh/go/bin"</span></span></span></span><br><span class="line"><span class="setting">GOEXE=<span class="value"><span class="string">""</span></span></span></span><br><span class="line"><span class="setting">GOHOSTARCH=<span class="value"><span class="string">"amd64"</span></span></span></span><br><span class="line"><span class="setting">GOHOSTOS=<span class="value"><span class="string">"darwin"</span></span></span></span><br><span class="line"><span class="setting">GOOS=<span class="value"><span class="string">"darwin"</span></span></span></span><br><span class="line"><span class="setting">GOPATH=<span class="value"><span class="string">"/Users/zhengqh/go"</span></span></span></span><br><span class="line"><span class="setting">GORACE=<span class="value"><span class="string">""</span></span></span></span><br><span class="line"><span class="setting">GOROOT=<span class="value"><span class="string">"/usr/local/go"</span></span></span></span><br><span class="line"><span class="setting">GOTOOLDIR=<span class="value"><span class="string">"/usr/local/go/pkg/tool/darwin_amd64"</span></span></span></span><br></pre></td></tr></table></figure>
<p>go默认安装到/usr/local/go下，也可以将$GOROOT/bin加入到$PATH下</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export GOROOT=/usr/local/go</span><br><span class="line">export PATH=<span class="variable">$PATH</span>:<span class="variable">$GOBIN</span>:<span class="variable">$GOROOT</span>/bin</span><br></pre></td></tr></table></figure>
<p>查看go的版本</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  <span class="keyword">go</span> <span class="keyword">version</span></span><br><span class="line"><span class="keyword">go</span> <span class="keyword">version</span> go1.<span class="number">8.3</span> darwin/amd64</span><br><span class="line">➜  which <span class="keyword">go</span></span><br><span class="line">/usr/local/<span class="keyword">go</span>/bin/<span class="keyword">go</span></span><br></pre></td></tr></table></figure>
<p>在Sublime中安装GoSublime，然后设置Sublime的环境变了：</p>
<p>Preferences -&gt; package settings -&gt; GoSublime -&gt; Settings - Uesrs</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">env</span>": <span class="value">&#123;</span><br><span class="line">        "<span class="attribute">GOPATH</span>": <span class="value"><span class="string">"/Users/zhengqh/go"</span></span>,</span><br><span class="line">        "<span class="attribute">GOROOT</span>": <span class="value"><span class="string">"/usr/local/go"</span></span><br><span class="line">    </span>&#125;</span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>默认的GoSublime build:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">target</span>": <span class="value"><span class="string">"gs9o_build"</span></span>,</span><br><span class="line">    "<span class="attribute">selector</span>": <span class="value"><span class="string">"source.go"</span></span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>CMD+b后，会在底部弹出终端，需要自己输入go run xxx.go。</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ ~<span class="regexp">/go/src</span><span class="regexp">/test/</span> ] <span class="comment"># 输入go run main.go</span></span><br><span class="line">[ <span class="string">`go run main.go`</span> | <span class="symbol">done:</span> <span class="number">902.434403</span>ms ]</span><br><span class="line">    <span class="constant">GO</span> <span class="constant">GO</span> <span class="constant">GO</span>!!!</span><br><span class="line">    ....</span><br></pre></td></tr></table></figure>
<p>但实际上我希望立即执行，自动编译执行。</p>
<p><strong>第一种尝试</strong></p>
<p>可以在Tools&gt;Build System&gt;New Build System</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">　　"<span class="attribute">cmd</span>": <span class="value">[<span class="string">"go"</span>, <span class="string">"run"</span>, <span class="string">"$file_name"</span>]</span>,</span><br><span class="line">    "<span class="attribute">file_regex</span>": <span class="value"><span class="string">"^[ ]*File \"(...*?)\", line ([0-9]*)"</span></span>,</span><br><span class="line">    "<span class="attribute">working_dir</span>": <span class="value"><span class="string">"$file_path"</span></span>,</span><br><span class="line">    "<span class="attribute">selector</span>": <span class="value"><span class="string">"source.go"</span></span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>点击CMD+b后，不会弹出任何终端，而且提示No Build System</p>
<p><strong>第二种尝试</strong>，New Build System:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">shell_cmd</span>": <span class="value"><span class="string">"go run $file"</span></span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>点击CMD+b后，会编译并运行</p>
<figure class="highlight erlang-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">GO</span> <span class="variable">GO</span> <span class="variable">GO</span><span class="exclamation_mark">!</span><span class="exclamation_mark">!</span><span class="exclamation_mark">!</span></span><br><span class="line">....</span><br><span class="line">[<span class="variable">Finished</span> <span class="function_or_atom">in</span> <span class="number">1.0</span>s]</span><br></pre></td></tr></table></figure>
<p>go的代码不一定要在GOPATH下。下面在kafka-book的clients下新建一个go文件。<br>CMD+B后，会产生一个producer进程。但是CTRL+C虽然会提示CANCEL，但是并不会真正杀掉进程。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  test ps -ef|grep producer</span><br><span class="line">  <span class="number">501</span> <span class="number">24018</span> <span class="number">14631</span>   <span class="number">0</span>  <span class="number">4</span>:<span class="number">22</span>下午 ??         <span class="number">0</span>:<span class="number">00.31</span> go run /Users/zhengqh/Github/kafka-book/clients/src/main/go/producer.go</span><br><span class="line">➜  test ps -ef|grep producer</span><br><span class="line">  <span class="number">501</span> <span class="number">24049</span>     <span class="number">1</span>   <span class="number">0</span>  <span class="number">4</span>:<span class="number">22</span>下午 ??         <span class="number">0</span>:<span class="number">00.01</span> /var/folders/xc/x0b8crk9667ddh1zhfs29_zr0000gn/T/go-build870380723/command-line-arguments/_obj/exe/producer</span><br></pre></td></tr></table></figure>
<h4 id="go_imports">go imports</h4><p>如果没有导入fmt包，直接编译运行报错：</p>
<figure class="highlight typescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># command-line-<span class="built_in">arguments</span></span><br><span class="line">./main.go:<span class="number">4</span>: <span class="literal">undefined</span>: fmt <span class="keyword">in</span> fmt.Println</span><br><span class="line">./main.go:<span class="number">5</span>: <span class="literal">undefined</span>: fmt <span class="keyword">in</span> fmt.Println</span><br><span class="line">[Finished <span class="keyword">in</span> <span class="number">0.5</span>s <span class="keyword">with</span> exit code <span class="number">2</span>]</span><br><span class="line">[shell_cmd: go run /Users/zhengqh/go/src/test/main.go]</span><br><span class="line">[dir: <span class="regexp">/Users/</span>zhengqh/go/src/test]</span><br><span class="line">[path: <span class="regexp">/usr/</span>bin:<span class="regexp">/bin:/u</span>sr/sbin:<span class="regexp">/sbin]</span></span><br></pre></td></tr></table></figure>
<p><a href="https://godoc.org/golang.org/x/tools/cmd/goimports" target="_blank" rel="external">goimports</a>可以在保存的时候自动导入包</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ go get golang.org<span class="regexp">/x/</span>tools<span class="regexp">/cmd/</span>goimports</span><br><span class="line"><span class="keyword">package</span> golang.org<span class="regexp">/x/</span>tools<span class="regexp">/cmd/</span><span class="string">goimports:</span> unrecognized <span class="keyword">import</span> path <span class="string">"golang.org/x/tools/cmd/goimports"</span> </span><br><span class="line">(https <span class="string">fetch:</span> Get <span class="string">https:</span><span class="comment">//golang.org/x/tools/cmd/goimports?go-get=1: dial tcp 220.255.2.153:443: i/o timeout)</span></span><br></pre></td></tr></table></figure>
<h3 id="Sublime">Sublime</h3><p><a href="http://www.jianshu.com/p/3cb5c6f2421c" target="_blank" rel="external">http://www.jianshu.com/p/3cb5c6f2421c</a></p>
<table>
<thead>
<tr>
<th>按键</th>
<th>常用指数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>Shift+CMD+P</td>
<td></td>
<td>然后输入ip 安装PackageControl install    </td>
</tr>
<tr>
<td>Alt+CMD+O</td>
<td></td>
<td>预览MD(OmniMarkupPreview), 在本地浏览器打开  </td>
</tr>
<tr>
<td>CMD+R</td>
<td></td>
<td>Markdown的主题列表, 或者源文件的函数列表  </td>
</tr>
<tr>
<td>CMD+P</td>
<td>⭐️⭐️⭐️⭐️⭐️</td>
<td>查找整个Workspace的文件 </td>
</tr>
<tr>
<td>CMD+P，@</td>
<td>⭐️⭐️⭐️</td>
<td>Markdown的大纲</td>
</tr>
<tr>
<td>Alt+CMD+⬇</td>
<td></td>
<td>go to definition  </td>
</tr>
<tr>
<td>CMD+Shift+T</td>
<td></td>
<td>打开上一个关闭的文件  </td>
</tr>
</tbody>
</table>
<p>GoBuild: CMD+b</p>
<p><strong>Sublime自定义设置(以MD GFM为例)</strong><br>Preferences &gt; Package Settings &gt; Markdown Edit &gt; Markdown GFM Settings - User  </p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">word_wrap</span>": <span class="value"><span class="literal">false</span></span>,</span><br><span class="line">    "<span class="attribute">wrap_width</span>": <span class="value"><span class="number">150</span></span>,</span><br><span class="line">    "<span class="attribute">line_numbers</span>": <span class="value"><span class="literal">true</span></span>,</span><br><span class="line">    "<span class="attribute">highlight_line</span>": <span class="value"><span class="literal">true</span></span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>效果就是打开md文件, 预览时宽度变宽了, 而不再是窄窄的. 而且较长的行不会自动换行 </p>
<p><strong>在一个窗口内显示多个文件夹</strong><br>打开一个文件夹之后, Project &gt; Add Folder to Project &gt; Save Project As .. &gt; 保存在本地<br>下次直接选择keyspace的名称即可打开上次所有的文件夹</p>
<h1 id="Mac_Dailay">Mac Dailay</h1><p>date +%s<br>date -d “2010-07-20 10:25:30” +%s</p>
<h2 id="翻墙">翻墙</h2><p>http版本：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">export http_proxy=http://<span class="number">192.168</span>.<span class="number">6.32</span>:<span class="number">1080</span> </span><br><span class="line">export https_proxy=<span class="variable">$http</span>_proxy  </span><br><span class="line">export ftp_proxy=<span class="variable">$http</span>_proxy  </span><br><span class="line">export rsync_proxy=<span class="variable">$http</span>_proxy  </span><br><span class="line">export no_proxy=<span class="string">"localhost,127.0.0.1,localaddress,.localdomain.com,*.tongdun.cn"</span> </span><br><span class="line"></span><br><span class="line">git config --global https.proxy http://<span class="number">127.0</span>.<span class="number">0</span>.<span class="number">1</span>:<span class="number">8787</span>  </span><br><span class="line">git config --global https.proxy https://<span class="number">127.0</span>.<span class="number">0</span>.<span class="number">1</span>:<span class="number">8787</span>  </span><br><span class="line"></span><br><span class="line">git config --global --unset http.proxy  </span><br><span class="line">git config --global --unset https.proxy </span><br><span class="line"></span><br><span class="line">git config http.proxy</span><br></pre></td></tr></table></figure>
<p>socks5版本：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export ALL_PROXY=<span class="string">"socks5://192.168.6.32:1080"</span></span><br><span class="line"></span><br><span class="line">git config --global http<span class="class">.proxy</span> socks5:<span class="comment">//192.168.6.32:1080</span></span><br><span class="line">git config --global https<span class="class">.proxy</span> socks5:<span class="comment">//192.168.6.32:1080</span></span><br><span class="line"></span><br><span class="line">git config --global http<span class="class">.proxy</span> <span class="string">'socks5://192.168.6.32:1080'</span></span><br><span class="line">git config --global https<span class="class">.proxy</span> <span class="string">'socks5://192.168.6.32:1080'</span></span><br></pre></td></tr></table></figure>
<h1 id="Git">Git</h1><h2 id="QuickStart">QuickStart</h2><table>
<thead>
<tr>
<th>说明</th>
<th>命令</th>
</tr>
</thead>
<tbody>
<tr>
<td>从master<strong>创建并切换</strong>到dev分支</td>
<td><code>git checkout -b dev</code></td>
</tr>
<tr>
<td>相当于创建分支和并切换到分支</td>
<td><code>git branch dev &amp;&amp; git checkout dev</code></td>
</tr>
<tr>
<td>在分支上正常<strong>提交</strong>所有文件</td>
<td><code>git add . &amp;&amp; git commit -m &#39;commit log&#39;</code></td>
</tr>
<tr>
<td><strong>切换</strong>回master分支</td>
<td><code>git checkout master</code></td>
</tr>
<tr>
<td>把dev的<strong>分支合并</strong>到当前分支上</td>
<td><code>git merge dev</code></td>
</tr>
<tr>
<td>删除dev分支</td>
<td><code>git branch -d dev</code></td>
</tr>
<tr>
<td>提交dev分支</td>
<td><code>git push origin dev</code></td>
</tr>
</tbody>
</table>
<h2 id="分支">分支</h2><p><strong>在主分支上创建一个新的分支,修改文件, 添加到版本库,提交,远程推送到分支</strong>  </p>
<p>1.在主分支上创建一个新的分支(-b),并切换(checkout)到这个新的分支上.<br>这里最后还可以跟上一个基础分支名称,表示要从哪个基础分支上创建新的分支,<br>这里因为在master上,所以是以master版本创建一个新的分支(默认在当前分支上创建新的分支).  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  test git:(master) git checkout -<span class="tag">b</span> branch001          创建并切换分支</span><br><span class="line">Switched to <span class="tag">a</span> new branch <span class="string">'branch001'</span></span><br></pre></td></tr></table></figure>
<p>2.在分支上的修改,提交和正常的一样</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  test git:(<span class="keyword">branch001) </span>vi README.md                    修改文件</span><br><span class="line">➜  test git:(<span class="keyword">branch001) </span>✗ git <span class="keyword">add </span>.                     加入版本控制</span><br><span class="line">➜  test git:(<span class="keyword">branch001) </span>✗ git commit -m <span class="string">'branch'</span>        提交</span><br><span class="line">[<span class="keyword">branch001 </span><span class="keyword">b078a04] </span><span class="keyword">branch</span><br><span class="line"></span> <span class="number">1</span> file changed, <span class="number">1</span> insertion(+)</span><br></pre></td></tr></table></figure>
<p>3.提交分支到远程仓库<br>在当前新的分支上执行git push origin 分支名字,表示将当前新的分支push到远程仓库对应的分支上.<br>origin是远程仓库的别名. git push origin master表示将master分支推送到远程的master分支上.<br>注意:不要在master分支上执行git push origin branch001.那样会把master分支推送到远程的branch001分支上.<br>也就是说,本地要push的分支和当前所在的分支有关,而origin后面的分支名称,代表的是远程的某个分支.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> ➜  test git:(branch001) git push origin branch001      远程PUSH</span><br><span class="line">Counting objects: <span class="number">3</span>, done.</span><br><span class="line">Delta compression <span class="keyword">using</span> up to <span class="number">4</span> threads.</span><br><span class="line">Compressing objects: <span class="number">100</span>% (<span class="number">2</span>/<span class="number">2</span>), done.</span><br><span class="line">Writing objects: <span class="number">100</span>% (<span class="number">3</span>/<span class="number">3</span>), <span class="number">287</span> bytes | <span class="number">0</span> bytes/s, done.</span><br><span class="line">Total <span class="number">3</span> (delta <span class="number">0</span>), reused <span class="number">0</span> (delta <span class="number">0</span>)</span><br><span class="line">To https:<span class="comment">//github.com/zqhxuyuan/test.git</span></span><br><span class="line"> * [<span class="keyword">new</span> branch]      branch001 -&gt; branch001</span><br></pre></td></tr></table></figure>
<p>4.在分支下文件被修改,回到主分支,文件没有被修改. 即分支的修改对于主分支不可见.     </p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜  test gi<span class="variable">t:</span>(branch001) <span class="keyword">cat</span> README.md                   分支的内容</span><br><span class="line"># test</span><br><span class="line">branch...</span><br><span class="line"></span><br><span class="line">➜  test gi<span class="variable">t:</span>(branch001) git checkout master             主分支</span><br><span class="line">Switched <span class="keyword">to</span> branch <span class="string">'master'</span></span><br><span class="line">Your branch <span class="keyword">is</span> <span class="keyword">up</span>-<span class="keyword">to</span>-date with <span class="string">'origin/master'</span>.</span><br><span class="line"></span><br><span class="line">➜  test gi<span class="variable">t:</span>(master) <span class="keyword">cat</span> README.md                      主分支不可见分支</span><br><span class="line"># test</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可以把分支的文件合并到主分支上.操作方式是在主分支上merge其他分支: git merge branch001 </p>
</blockquote>
<h2 id="git_flow">git flow</h2><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">master<span class="prompt">&gt;&gt;  </span>git branch develop                            在master分支创建一个develop分支</span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git push -u origin develop                    提交develop分支,虽然这个分支和master分支是一样的,因为还没有做任何修改</span><br><span class="line"></span><br><span class="line">git clone <span class="symbol">ssh:</span>/<span class="regexp">/user@host/path</span><span class="regexp">/to/repo</span>.git              克隆仓库,因为上面提交了develop,所以开发分支也会被拉取到本地</span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git checkout -b develop origin/develop        基于远程仓库的develop分支,切换到develop分支(因为分支已经存在,-b不会在建了)</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git checkout -b some-feature develop          基于develop分支创建一个功能分支,并切换到这个功能分支</span><br><span class="line">feature<span class="prompt">&gt;&gt; </span>git status</span><br><span class="line">feature<span class="prompt">&gt;&gt; </span>git add .</span><br><span class="line">feature<span class="prompt">&gt;&gt; </span>git commit -m <span class="string">'feature'</span></span><br><span class="line">feature<span class="prompt">&gt;&gt; </span>git pull origin develop                       拉取最新的develop分支</span><br><span class="line">feature<span class="prompt">&gt;&gt; </span>git checkout develop                          切换到develop分支</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git merge some-feature                        合并功能分支到develop分支</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git push                                      提交本地的develop分支到远程的develop分支,因为上面设置了-u,所以可以不用手动添加origin develop</span><br><span class="line"></span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git checkout -b release-<span class="number">0</span>.<span class="number">1</span> develop           在最新的develop上创建一个用于新版本发布的分支</span><br><span class="line">release<span class="prompt">&gt;&gt; </span>....                                          在发布版本上的操作和普通的分支一样</span><br><span class="line">release<span class="prompt">&gt;&gt; </span>git checkout master                           切换到master分支</span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git merge release-<span class="number">0</span>.<span class="number">1</span>                         将发布版本的分支合并到本地的master分支</span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git push                                      提交本地的master分支提交到远程的master分支??</span><br><span class="line">master&gt;&gt;  git checkout develop                          切换到develop分支</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git merge release-<span class="number">0</span>.<span class="number">1</span>                         同样将发布版本的分支合并到本地的develop分支</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git push                                      提交到远程develop分支</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git branch -d release-<span class="number">0</span>.<span class="number">1</span>                     删除发布版本这个分支</span><br><span class="line"></span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git checkout -b issue-<span class="comment">#001 master             基于master分支新创建并切换到一个新的hotfix分支</span></span><br><span class="line">hotfix<span class="prompt">&gt;&gt;  </span><span class="comment"># Fix the bug                                 在hotfix分支上正常操作</span></span><br><span class="line">hotfix<span class="prompt">&gt;&gt;  </span>git checkout master                           切换回master分支</span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git merge issue-<span class="comment">#001                          合并hotfix分支的修改到本地master分支</span></span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git push                                      提交到远程master分支</span><br><span class="line"></span><br><span class="line">master<span class="prompt">&gt;&gt;  </span>git checkout develop                          hotfix的修改要同步到master和develop分支上</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git merge issue-<span class="comment">#001</span></span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git push</span><br><span class="line">develop<span class="prompt">&gt;&gt; </span>git branch -d issue-<span class="comment">#001</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>-u选项设置本地分支去跟踪远程对应的分支。 设置好跟踪的分支后，可以使用git push命令省去指定推送分支的参数</p>
</blockquote>
<h2 id="解决冲突">解决冲突</h2><p>场景: 本地修改了pom.xml,和远程已有的pom.xml不一致.  </p>
<p>1.拉取数据时,指出了local changes发生在pom.xml文件. 如果直接提交,会被拒绝的.   </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">➜  td-offline <span class="string">git:</span>(master) ✗ git pull</span><br><span class="line"><span class="string">remote:</span> Counting <span class="string">objects:</span> <span class="number">14</span>, done.</span><br><span class="line"><span class="string">remote:</span> Compressing <span class="string">objects:</span> <span class="number">100</span>% (<span class="number">9</span>/<span class="number">9</span>), done.</span><br><span class="line"><span class="string">remote:</span> Total <span class="number">14</span> (delta <span class="number">5</span>), reused <span class="number">0</span> (delta <span class="number">0</span>)</span><br><span class="line">Unpacking <span class="string">objects:</span> <span class="number">100</span>% (<span class="number">14</span>/<span class="number">14</span>), done.</span><br><span class="line">From gitlab.fraudmetrix.<span class="string">cn:</span>dp/td-offline</span><br><span class="line">   <span class="number">9061</span>c47..ce0b695  master     -&gt; origin/master</span><br><span class="line">Updating <span class="number">9061</span>c47..ce0b695</span><br><span class="line"><span class="string">error:</span> Your local changes to the following files would be overwritten by <span class="string">merge:</span></span><br><span class="line">    pom.xml</span><br><span class="line">Please, commit your changes or stash them before you can merge.</span><br><span class="line">Aborting</span><br><span class="line"></span><br><span class="line">➜  td-offline <span class="string">git:</span>(master) ✗ git add .</span><br><span class="line">➜  td-offline <span class="string">git:</span>(master) ✗ git commit -m <span class="string">'split module out'</span></span><br><span class="line"></span><br><span class="line">➜  td-offline <span class="string">git:</span>(master) git push origin master</span><br><span class="line">To git<span class="annotation">@gitlab</span>.fraudmetrix.<span class="string">cn:</span>dp/td-offline.git</span><br><span class="line"> ! [rejected]        master -&gt; master (non-fast-forward)</span><br><span class="line"><span class="string">error:</span> failed to push some refs to <span class="string">'git@gitlab.fraudmetrix.cn:dp/td-offline.git'</span></span><br><span class="line"><span class="string">hint:</span> Updates were rejected because the tip of your current branch is behind</span><br><span class="line"><span class="string">hint:</span> its remote counterpart. Integrate the remote changes (e.g.</span><br><span class="line"><span class="string">hint:</span> <span class="string">'git pull ...'</span>) before pushing again.</span><br><span class="line"><span class="string">hint:</span> See the <span class="string">'Note about fast-forwards'</span> <span class="keyword">in</span> <span class="string">'git push --help'</span> <span class="keyword">for</span> details.</span><br></pre></td></tr></table></figure>
<p>2.再次pull,会把远程的pom.xml和本地修改后的pom.xml进行尝试合并.  </p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  td-offline git:(master) git pull</span><br><span class="line"><span class="type">Auto</span>-merging pom.xml</span><br><span class="line"><span class="type">CONFLICT</span> (content): <span class="type">Merge</span> conflict <span class="keyword">in</span> pom.xml</span><br><span class="line"><span class="type">Automatic</span> merge failed; fix conflicts <span class="keyword">and</span> then commit the <span class="literal">result</span>.</span><br></pre></td></tr></table></figure>
<p>上面显示合并失败,要自己去修复,然后提交.  </p>
<p>3.修改发生冲突的文件,一般是把自动添加的特殊字符删掉,然后提交.  </p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  td-offline git:(<span class="literal">master</span>) ✗ vi pom.<span class="keyword">xml</span></span><br><span class="line"><span class="title">➜  td-offline</span> git:(<span class="literal">master</span>) ✗ git add .</span><br><span class="line">➜  td-offline git:(<span class="literal">master</span>) ✗ git commit -m 'split module out'</span><br><span class="line">[<span class="keyword">master</span> <span class="title">6f86dcd</span>] split module out</span><br></pre></td></tr></table></figure>
<p>4.最后,成功push到远程分支上. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  td-offline git:(master) git push origin master</span><br><span class="line">Counting objects: <span class="number">67</span>, done.</span><br><span class="line">Delta compression <span class="keyword">using</span> up to <span class="number">4</span> threads.</span><br><span class="line">Compressing objects: <span class="number">100</span>% (<span class="number">35</span>/<span class="number">35</span>), done.</span><br><span class="line">Writing objects: <span class="number">100</span>% (<span class="number">67</span>/<span class="number">67</span>), <span class="number">47.34</span> KiB | <span class="number">0</span> bytes/s, done.</span><br><span class="line">Total <span class="number">67</span> (delta <span class="number">9</span>), reused <span class="number">0</span> (delta <span class="number">0</span>)</span><br><span class="line">To git@gitlab.fraudmetrix.cn:dp/td-offline.git</span><br><span class="line">   ce0b695.<span class="number">.6</span>f86dcd  master -&gt; master</span><br></pre></td></tr></table></figure>
<p>5.尝试拉取,已经是最新的代码了</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  td-offline gi<span class="variable">t:</span>(master) git pull</span><br><span class="line">Already <span class="keyword">up</span>-<span class="keyword">to</span>-date.</span><br></pre></td></tr></table></figure>
<h2 id="忽略文件">忽略文件</h2><p>已经提交的文件，可以从cache中删除</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">➜  riemann-cassandra git:(master) ✗ git <span class="operator"><span class="keyword">commit</span> -<span class="keyword">m</span> <span class="string">'update rieman and cassandra version'</span></span><br><span class="line">[<span class="keyword">master</span> <span class="number">460</span>d78c] <span class="keyword">update</span> rieman <span class="keyword">and</span> cassandra <span class="keyword">version</span></span><br><span class="line"> <span class="number">4</span> files <span class="keyword">changed</span>, <span class="number">118</span> insertions(+), <span class="number">22</span> deletions(-)</span><br><span class="line"> <span class="keyword">create</span> <span class="keyword">mode</span> <span class="number">100644</span> .gitignore</span><br><span class="line"> <span class="keyword">create</span> <span class="keyword">mode</span> <span class="number">100644</span> riemann-cassandra.iml</span><br><span class="line"></span><br><span class="line">➜  riemann-cassandra git:(<span class="keyword">master</span>) git rm  <span class="comment">--cached riemann-cassandra.iml</span></span><br><span class="line">rm <span class="string">'riemann-cassandra.iml'</span></span><br><span class="line"></span><br><span class="line">➜  riemann-cassandra git:(<span class="keyword">master</span>) ✗ git <span class="keyword">status</span></span><br><span class="line"><span class="keyword">On</span> branch <span class="keyword">master</span></span><br><span class="line">Your branch <span class="keyword">is</span> ahead <span class="keyword">of</span> <span class="string">'origin/master'</span> <span class="keyword">by</span> <span class="number">1</span> <span class="keyword">commit</span>.</span><br><span class="line">  (<span class="keyword">use</span> <span class="string">"git push"</span> <span class="keyword">to</span> publish your <span class="keyword">local</span> commits)</span><br><span class="line">Changes <span class="keyword">to</span> be committed:</span><br><span class="line">  (<span class="keyword">use</span> <span class="string">"git reset HEAD &lt;file&gt;..."</span> <span class="keyword">to</span> unstage)</span><br><span class="line"></span><br><span class="line">    deleted:    riemann-cassandra.iml</span><br><span class="line"></span><br><span class="line">➜  riemann-cassandra git:(<span class="keyword">master</span>) ✗ git <span class="keyword">commit</span> -<span class="keyword">m</span> <span class="string">'update rieman and cassandra version'</span></span><br><span class="line">[<span class="keyword">master</span> b56cd65] <span class="keyword">update</span> rieman <span class="keyword">and</span> cassandra <span class="keyword">version</span></span><br><span class="line"> <span class="number">1</span> <span class="keyword">file</span> <span class="keyword">changed</span>, <span class="number">58</span> deletions(-)</span><br><span class="line"> <span class="keyword">delete</span> <span class="keyword">mode</span> <span class="number">100644</span> riemann-cassandra.iml</span></span><br></pre></td></tr></table></figure>
<p>拉取tag：git checkout tag_name</p>
<h2 id="Github的本地fork保持与clone的一致">Github的本地fork保持与clone的一致</h2><p><a href="http://blog.xiayf.cn/2016/01/18/github-fork-pull-request/" target="_blank" rel="external">http://blog.xiayf.cn/2016/01/18/github-fork-pull-request/</a></p>
<p>git remote add kafka <a href="https://github.com/apache/kafka.git">https://github.com/apache/kafka.git</a><br>git pull kafka trunk</p>
<p>git push origin trunk</p>
<p>在本地代码库添加一个新的remote，名为 beego ： git remote add beego <a href="https://github.com/astaxie/beego.git">https://github.com/astaxie/beego.git</a><br>在 develop 分支上执行 git pull beego develop，这会获取 astaxie/beego develop 分支最新的状态，并 merge 到本地代码库的 develop 分支<br>将本地代码库的 develop 分支 push 到 youngsterxyf/beego ：git push origin develop</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;日常工作常用命令（Linux、Mac）&lt;br&gt;
    
    </summary>
    
      <category term="work" scheme="http://github.com/zqhxuyuan/categories/work/"/>
    
    
      <category term="ops" scheme="http://github.com/zqhxuyuan/tags/ops/"/>
    
  </entry>
  
  <entry>
    <title>监控工具集锦</title>
    <link href="http://github.com/zqhxuyuan/2016/12/31/Tools-BigData-Monitor/"/>
    <id>http://github.com/zqhxuyuan/2016/12/31/Tools-BigData-Monitor/</id>
    <published>2016-12-30T16:00:00.000Z</published>
    <updated>2017-04-05T03:19:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>监控工具：ZooKeeper，Kafka，Cassandra…<br><a id="more"></a></p>
<h2 id="ZooKeeper">ZooKeeper</h2><h3 id="node-zk-browser">node-zk-browser</h3><p>依赖条件：nodejs, npm</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/killme2008/node-zk-browser.git</span><br><span class="line">$ <span class="built_in">cd</span> node-zk-browser</span><br><span class="line">$ cat package.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"node-zk-browser"</span>,</span><br><span class="line">  <span class="string">"version"</span>: <span class="string">"0.0.2"</span>,</span><br><span class="line">  <span class="string">"dependencies"</span>: &#123;</span><br><span class="line">    <span class="string">"ejs"</span>: <span class="string">"0.7.2"</span>,</span><br><span class="line">    <span class="string">"express"</span>: <span class="string">"3.2.6"</span>,</span><br><span class="line">    <span class="string">"zookeeper"</span>:<span class="string">"*"</span>,</span><br><span class="line">    <span class="string">"express-namespace"</span>:<span class="string">"0.1.1"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">$ npm install <span class="operator">-d</span></span><br><span class="line">$ node app.js</span><br><span class="line">Express server listening on port <span class="number">3000</span></span><br><span class="line">zk session established, id=<span class="number">154</span>a418d5dd000b</span><br></pre></td></tr></table></figure>
<p><a href="http://localhost:3000" target="_blank" rel="external">http://localhost:3000</a> admin:admin</p>
<p><img src="http://img.blog.csdn.net/20160516095143332" alt="node-zk"></p>
<h3 id="zkui">zkui</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/DeemOpen/zkui.git</span><br><span class="line">$ mvn clean install</span><br><span class="line">$ cp target/zkui-<span class="number">2.0</span>-SNAPSHOT-jar-with-dependencies.jar .</span><br><span class="line">$ java -jar zkui-<span class="number">2.0</span>-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>
<p><a href="http://localhost:9090/" target="_blank" rel="external">http://localhost:9090/</a>  admin:manager</p>
<p><img src="http://img.blog.csdn.net/20160516094813768" alt="zkui"></p>
<h3 id="zk-web">zk-web</h3><p>依赖条件：lein</p>
<figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="char">$ </span>git clone <span class="method">https:</span>//github.com/qiuxiafei/zk-web</span><br><span class="line"><span class="char">$ </span>cd zk-web</span><br><span class="line"><span class="char">$ </span>lein deps</span><br><span class="line"><span class="char">$ </span>lein run</span><br></pre></td></tr></table></figure>
<p><a href="http://localhost:9090/" target="_blank" rel="external">http://localhost:9090/</a> admin:hello</p>
<p><img src="http://img.blog.csdn.net/20160516094831940" alt="zkweb"></p>
<h2 id="Kafka">Kafka</h2><h3 id="trifecta">trifecta</h3><p>下载<a href="https://github.com/ldaniels528/trifecta/releases">https://github.com/ldaniels528/trifecta/releases</a>中的zip包（web ui）， jar是命令行工具（trifecta_cli）。</p>
<p>依赖条件：scalascript</p>
<p><strong>编译并发布scalascript</strong>  </p>
<p>需要先将scalascript发布到本地仓库，默认sbt会发布到ivy下</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">➜  git clone https:<span class="comment">//github.com/ldaniels528/scalascript.git</span></span><br><span class="line">➜  cd scalascript </span><br><span class="line"></span><br><span class="line">修改sbt版本为当前安装的sbt版本，否则如果没有修改，会去下载，尽量使用已有的</span><br><span class="line">➜  cat project/build.properties  </span><br><span class="line">sbt.version=<span class="number">0.13</span><span class="number">.9</span></span><br><span class="line">➜  sbt publish-local  </span><br><span class="line">Getting org.scala-sbt sbt <span class="number">0.13</span><span class="number">.9</span> ...</span><br><span class="line">downloading http:<span class="comment">//repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt/0.13.9/jars/sbt.jar ...</span></span><br><span class="line">    [SUCCESSFUL ] org.scala-sbt<span class="preprocessor">#sbt;<span class="number">0.13</span><span class="number">.9</span>!sbt.jar (<span class="number">5084</span>ms)</span></span><br><span class="line"></span><br><span class="line">使用<span class="number">0.13</span><span class="number">.11</span>就不会下载了</span><br><span class="line">➜  sbt publish-local</span><br><span class="line">[info] Loading global plugins from /Users/zhengqh/.sbt/<span class="number">0.13</span>/plugins</span><br><span class="line">[info] Loading project definition from /Users/zhengqh/Soft/scalascript/project</span><br><span class="line">[info] Packaging /Users/zhengqh/Soft/scalascript/target/scala-<span class="number">2.11</span>/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>-<span class="number">0.2</span><span class="number">.20</span>.jar ...</span><br><span class="line">[info] Done packaging.</span><br><span class="line">[info]  published scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>/<span class="number">0.2</span><span class="number">.20</span>/poms/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>.pom</span><br><span class="line">[info]  published scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>/<span class="number">0.2</span><span class="number">.20</span>/jars/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>.jar</span><br><span class="line">[info]  published scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>/<span class="number">0.2</span><span class="number">.20</span>/srcs/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>-sources.jar</span><br><span class="line">[info]  published scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>/<span class="number">0.2</span><span class="number">.20</span>/docs/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>-javadoc.jar</span><br><span class="line">[info]  published ivy to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/scalascript_sjs0<span class="number">.6</span>_2<span class="number">.11</span>/<span class="number">0.2</span><span class="number">.20</span>/ivys/ivy.xml</span><br><span class="line">[success] Total time: <span class="number">71</span> s, completed <span class="number">2016</span>-<span class="number">5</span>-<span class="number">16</span> <span class="number">10</span>:<span class="number">11</span>:<span class="number">20</span></span><br></pre></td></tr></table></figure>
<p><strong>编译并发布trifecta</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  git clone https:<span class="comment">//github.com/ldaniels528/trifecta.git</span></span><br><span class="line">➜  cd trifecta</span><br><span class="line">➜  sbt publish-local</span><br><span class="line">[info]  published trifecta_cli_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/trifecta_cli_2<span class="number">.11</span>/<span class="number">0.20</span><span class="number">.0</span>/poms/trifecta_cli_2<span class="number">.11</span>.pom</span><br><span class="line">[info]  published trifecta_cli_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/trifecta_cli_2<span class="number">.11</span>/<span class="number">0.20</span><span class="number">.0</span>/jars/trifecta_cli_2<span class="number">.11</span>.jar</span><br><span class="line">[info]  published trifecta_cli_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/trifecta_cli_2<span class="number">.11</span>/<span class="number">0.20</span><span class="number">.0</span>/srcs/trifecta_cli_2<span class="number">.11</span>-sources.jar</span><br><span class="line">[info]  published trifecta_cli_2<span class="number">.11</span> to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/trifecta_cli_2<span class="number">.11</span>/<span class="number">0.20</span><span class="number">.0</span>/docs/trifecta_cli_2<span class="number">.11</span>-javadoc.jar</span><br><span class="line">[info]  published ivy to /Users/zhengqh/.ivy2/local/com.github.ldaniels528/trifecta_cli_2<span class="number">.11</span>/<span class="number">0.20</span><span class="number">.0</span>/ivys/ivy.xml</span><br><span class="line">[success] Total time: <span class="number">75</span> s, completed <span class="number">2016</span>-<span class="number">5</span>-<span class="number">16</span> <span class="number">9</span>:<span class="number">51</span>:<span class="number">09</span></span><br></pre></td></tr></table></figure>
<p>现在已经编译好了trifecta_cli_2.11，这和release的cli.jar是一样的。 不过我们想要编译ui的话：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">➜  sbt "project trifecta_ui" dist</span><br><span class="line">[info] Loading global plugins from /Users/zhengqh/.sbt/0.13/plugins</span><br><span class="line">[info] Loading project definition from /Users/zhengqh/Soft/trifecta/project</span><br><span class="line">[info] <span class="operator"><span class="keyword">Set</span> <span class="keyword">current</span> <span class="keyword">project</span> <span class="keyword">to</span> trifecta_core (<span class="keyword">in</span> <span class="keyword">build</span> <span class="keyword">file</span>:/<span class="keyword">Users</span>/zhengqh/Soft/trifecta/)</span><br><span class="line">[info] <span class="keyword">Set</span> <span class="keyword">current</span> <span class="keyword">project</span> <span class="keyword">to</span> trifecta_cli (<span class="keyword">in</span> <span class="keyword">build</span> <span class="keyword">file</span>:/<span class="keyword">Users</span>/zhengqh/Soft/trifecta/)</span><br><span class="line">[info] <span class="keyword">Set</span> <span class="keyword">current</span> <span class="keyword">project</span> <span class="keyword">to</span> trifecta_ui (<span class="keyword">in</span> <span class="keyword">build</span> <span class="keyword">file</span>:/<span class="keyword">Users</span>/zhengqh/Soft/trifecta/)</span><br><span class="line">...</span><br><span class="line">[info] Done packaging.</span><br><span class="line">[info]</span><br><span class="line">[info] Your <span class="keyword">package</span> <span class="keyword">is</span> ready <span class="keyword">in</span> /<span class="keyword">Users</span>/zhengqh/Soft/trifecta/app-play/target/universal/trifecta_ui-<span class="number">0.20</span><span class="number">.0</span>.zip</span><br><span class="line">[info]</span><br><span class="line">[<span class="keyword">success</span>] Total <span class="keyword">time</span>: <span class="number">109</span> s, completed <span class="number">2016</span>-<span class="number">5</span>-<span class="number">16</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">01</span></span><br><span class="line"></span><br><span class="line">➜  unzip /<span class="keyword">Users</span>/zhengqh/Soft/trifecta/app-play/target/universal/trifecta_ui-<span class="number">0.20</span><span class="number">.0</span>.zip</span><br><span class="line"><span class="keyword">Archive</span>:  /<span class="keyword">Users</span>/zhengqh/Soft/trifecta/app-play/target/universal/trifecta_ui-<span class="number">0.20</span><span class="number">.0</span>.zip</span><br><span class="line">➜  cd trifecta_ui-<span class="number">0.20</span><span class="number">.0</span>/<span class="keyword">bin</span></span><br><span class="line">➜  ./trifecta_ui</span><br><span class="line">[info] <span class="keyword">p</span>.a.<span class="keyword">l</span>.<span class="keyword">c</span>.ActorSystemProvider - <span class="keyword">Starting</span> application <span class="keyword">default</span> Akka <span class="keyword">system</span>: application</span><br><span class="line">[info] application - Application has started</span><br><span class="line">[info] play.api.Play$ - Application started (Prod)</span><br><span class="line">[info] <span class="keyword">p</span>.<span class="keyword">c</span>.s.NettyServer$ - Listening <span class="keyword">for</span> <span class="keyword">HTTP</span> <span class="keyword">on</span> /<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">9000</span></span><br><span class="line">[info] application - Loading Trifecta configuration...</span><br><span class="line">[info] application - <span class="keyword">Starting</span> Zookeeper <span class="keyword">client</span>...</span><br><span class="line">[info] application - created actor com.github.ldaniels528.trifecta.actors.SSEClientHandlingActor</span><br><span class="line">[info] application - Registering <span class="keyword">new</span> SSE <span class="keyword">session</span> <span class="string">'8d82b54dc125406f875c98e9f05e4a4f'</span>...</span><br><span class="line">[info] application - <span class="keyword">GET</span> /api/sse/<span class="keyword">connect</span> ~&gt; <span class="number">200</span> [<span class="number">927.0</span> ms]</span></span><br></pre></td></tr></table></figure>
<p><a href="http://localhost:9000" target="_blank" rel="external">http://localhost:9000</a></p>
<p><img src="http://img.blog.csdn.net/20160516102612129" alt="trifecta"></p>
<h3 id="Kafka_Offset_Monitor(https://github-com/quantifind/KafkaOffsetMonitor)">Kafka Offset Monitor(<a href="https://github.com/quantifind/KafkaOffsetMonitor">https://github.com/quantifind/KafkaOffsetMonitor</a>)</h3><h3 id="Kafka_Monitor()">Kafka Monitor()</h3><h2 id="Cassandra">Cassandra</h2><h3 id="Riemman">Riemman</h3><h4 id="准备工作">准备工作</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rvm install <span class="number">1.9</span><span class="number">.3</span> --with-gcc=clang</span><br><span class="line">rvm <span class="number">1.9</span><span class="number">.3</span> --<span class="keyword">default</span></span><br><span class="line">rvm <span class="built_in">list</span></span><br><span class="line">ruby -v</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget https:<span class="comment">//aphyr.com/riemann/riemann-0.2.11.tar.bz2</span></span><br><span class="line">tar xvfj riemann-<span class="number">0.2</span><span class="number">.11</span>.tar.bz2</span><br><span class="line">cd riemann-<span class="number">0.2</span><span class="number">.11</span></span><br><span class="line"></span><br><span class="line">➜  riemann-<span class="number">0.2</span><span class="number">.11</span> bin/riemann etc/riemann.config</span><br><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">49</span>,<span class="number">069</span>] main - riemann.bin - PID <span class="number">70661</span></span><br><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">49</span>,<span class="number">078</span>] main - riemann.bin - Loading /Users/zhengqh/Soft/riemann-<span class="number">0.2</span><span class="number">.11</span>/etc/riemann.config</span><br><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">49</span>,<span class="number">334</span>] clojure-agent-send-off-pool-<span class="number">1</span> - riemann.transport.websockets - Websockets server <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> <span class="number">5556</span> online</span><br><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">49</span>,<span class="number">629</span>] clojure-agent-send-off-pool-<span class="number">2</span> - riemann.transport.udp - UDP server <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> <span class="number">5555</span> <span class="number">16384</span> -<span class="number">1</span> online</span><br><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">49</span>,<span class="number">656</span>] clojure-agent-send-off-pool-<span class="number">3</span> - riemann.transport.tcp - TCP server <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> <span class="number">5555</span> online</span><br><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">11</span>:<span class="number">50</span>:<span class="number">49</span>,<span class="number">659</span>] main - riemann.core - Hyperspace core online</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">gem install riemann-client riemann-tools riemann-dash</span><br><span class="line"></span><br><span class="line">➜  ~ riemann-dash</span><br><span class="line">DEPRECATION WARNING:</span><br><span class="line">Sass <span class="number">3.5</span> will no longer support Ruby <span class="number">1.9</span><span class="number">.3</span>.</span><br><span class="line">Please upgrade to Ruby <span class="number">2.0</span><span class="number">.0</span> or greater as soon as possible.</span><br><span class="line"></span><br><span class="line">No configuration loaded; <span class="keyword">using</span> defaults.</span><br><span class="line">[<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">12</span>:<span class="number">12</span>:<span class="number">35</span>] INFO  WEBrick <span class="number">1.3</span><span class="number">.1</span></span><br><span class="line">[<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">12</span>:<span class="number">12</span>:<span class="number">35</span>] INFO  ruby <span class="number">1.9</span><span class="number">.3</span> (<span class="number">2014</span>-<span class="number">11</span>-<span class="number">13</span>) [x86_64-darwin14<span class="number">.1</span><span class="number">.1</span>]</span><br><span class="line">== Sinatra (v1<span class="number">.4</span><span class="number">.7</span>) has taken the stage on <span class="number">4567</span> <span class="keyword">for</span> development with backup from WEBrick</span><br><span class="line">[<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">12</span>:<span class="number">12</span>:<span class="number">35</span>] INFO  WEBrick::HTTPServer<span class="preprocessor">#start: pid=<span class="number">20843</span> port=<span class="number">4567</span></span></span><br></pre></td></tr></table></figure>
<p>Mac下command并单击某个区域，这部分会变成深灰色</p>
<p><img src="http://img.blog.csdn.net/20160519131446630" alt="rieman1"></p>
<p>按快捷键e，出现编辑页面</p>
<p><img src="http://img.blog.csdn.net/20160519131501802" alt="rieman2"></p>
<p>点击Apply后，按Esc键，会退出编辑状态，深灰色变为正常颜色</p>
<p><img src="http://img.blog.csdn.net/20160519131513286" alt="rieman3"></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ riemann-health</span><br></pre></td></tr></table></figure>
<p>打包：</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  riemann-cassandra <span class="string">git:</span>(master) ✗ mvn <span class="keyword">package</span></span><br><span class="line">[INFO] Replacing <span class="regexp">/Users/</span>zhengqh<span class="regexp">/Github/</span>riemann-cassandra<span class="regexp">/target/</span>riemann-cassandra-<span class="number">0.0</span><span class="number">.3</span>.jar with </span><br><span class="line"><span class="regexp">/Users/</span>zhengqh<span class="regexp">/Github/</span>riemann-cassandra<span class="regexp">/target/</span>riemann-cassandra-<span class="number">0.0</span><span class="number">.3</span>-shaded.jar</span><br></pre></td></tr></table></figure>
<p>启动客户端：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">java -jar target/riemann-cassandra-<span class="number">0.0</span><span class="number">.3</span>.jar \</span><br><span class="line">    -riemann_host localhost \</span><br><span class="line">    -riemann_port <span class="number">5555</span> \</span><br><span class="line">    -cassandra_host localhost \</span><br><span class="line">    -jmx_port <span class="number">7199</span> \</span><br><span class="line">    -jmx_username null \</span><br><span class="line">    -jmx_password null \</span><br><span class="line">    -interval_seconds <span class="number">5</span></span><br><span class="line"></span><br><span class="line">success start riemann-cassandra-client............@Thu May <span class="number">19</span> <span class="number">14</span>:<span class="number">50</span>:<span class="number">04</span> CST <span class="number">2016</span></span><br><span class="line">^Cclosed riemann-cassandra-client@Thu May <span class="number">19</span> <span class="number">16</span>:<span class="number">14</span>:<span class="number">28</span> CST <span class="number">2016</span></span><br></pre></td></tr></table></figure>
<p>Event的格式：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    <span class="attribute">host</span>: <span class="string">"www1"</span>, </span><br><span class="line">    <span class="attribute">service</span>: <span class="string">"http req"</span>, </span><br><span class="line">    <span class="attribute">metric</span>: <span class="number">2.53</span>, </span><br><span class="line">    <span class="attribute">state</span>: <span class="string">"critical"</span>, </span><br><span class="line">    <span class="attribute">description</span>: <span class="string">"Request took 2.53 seconds."</span>, </span><br><span class="line">    <span class="attribute">tags</span>: [<span class="string">"http"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应的riemann后台日志：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">14</span>:<span class="number">01</span>:<span class="number">46</span>,<span class="number">512</span>] Thread-<span class="number">7</span> - riemann.config - expired </span><br><span class="line">&#123;:host www1, :service http req, :state expired, :time <span class="number">365909426627</span>/<span class="number">250</span>, :ttl <span class="number">60</span>&#125;</span><br></pre></td></tr></table></figure>
<p>riemann-cassandra中的事件格式：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    <span class="attribute">host</span>: <span class="string">"localhost"</span>, </span><br><span class="line">    <span class="attribute">service</span>: <span class="string">"cassandra.db.forseti.velocity_global.max_row_size_ob"</span>, </span><br><span class="line">    <span class="attribute">metric</span>: <span class="number">148</span>,</span><br><span class="line">    <span class="attribute">tags</span>: [<span class="string">"cassandra"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>riemann服务端后台日志：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INFO [<span class="number">2016</span>-<span class="number">05</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">47</span>:<span class="number">54</span>,<span class="number">195</span>] Thread-<span class="number">7</span> - riemann.config - expired </span><br><span class="line">&#123;:host <span class="number">10.57</span><span class="number">.2</span><span class="number">.36</span>, :service cassandra.db.forseti.velocity_partner.mean_row_size_kb, :state expired, :time <span class="number">292729534839</span>/<span class="number">200</span>, :ttl <span class="number">60</span>&#125;</span><br></pre></td></tr></table></figure>
<p>查询所有表的read_latency, query中支持不同的查询</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tagged <span class="string">"cassandra"</span>  <span class="keyword">and</span> service =~ <span class="string">"cassandra.db.forseti.%.read_%"</span></span><br></pre></td></tr></table></figure>
<h2 id="Tracing">Tracing</h2><ul>
<li><a href="http://htrace.incubator.apache.org/" target="_blank" rel="external">http://htrace.incubator.apache.org/</a></li>
<li><a href="https://github.com/naver/pinpoint">https://github.com/naver/pinpoint</a></li>
<li></li>
</ul>
<h3 id="zipkin">zipkin</h3><p>准备环境</p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">alias <span class="variable">npm=</span><span class="string">"npm --registry=https://registry.npm.taobao.org --disturl=https://npm.taobao.org/mirrors/node"</span></span><br><span class="line">yum install npm -y</span><br><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/creationix/nvm.git</span><br><span class="line">source nvm/install.sh</span><br><span class="line"><span class="keyword">node</span><span class="identifier"> </span><span class="title">-v</span> &amp;&amp; npm -v &amp;&amp; nvm -v</span><br><span class="line">nvm install v5.<span class="number">5.0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ tar zxf zipkin-<span class="number">1.40</span><span class="number">.1</span>.tar.gz</span><br><span class="line">$ cd zipkin-<span class="number">1.40</span><span class="number">.1</span></span><br><span class="line">$ bin/query</span><br></pre></td></tr></table></figure>
<p>vi zipkin-ui/webpack.config.js</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">:zipkin-ui:npmBuild</span><br><span class="line"></span><br><span class="line">&gt; zipkin-ui@<span class="number">0.0</span><span class="number">.0</span> build /home/qihuang.zheng/zipkin-<span class="number">1.40</span><span class="number">.1</span>/zipkin-ui</span><br><span class="line">&gt; node node_modules/webpack/bin/webpack.js --bail</span><br><span class="line"></span><br><span class="line">ModuleBuildError: Module build failed: Error: /home/qihuang.zheng/zipkin-<span class="number">1.40</span><span class="number">.1</span>/zipkin-ui/node_modules/node-sass/vendor/linux-x64-<span class="number">47</span>/binding.node: ELF load command past end of file</span><br><span class="line">npm ERR! Linux <span class="number">3.10</span><span class="number">.96</span>-<span class="number">1.</span>el6.elrepo.x86_64</span><br><span class="line">npm ERR! argv <span class="string">"/home/qihuang.zheng/zipkin-1.40.1/zipkin-ui/build/nodejs/node-v5.5.0-linux-x64/bin/node"</span> <span class="string">"/home/qihuang.zheng/zipkin-1.40.1/zipkin-ui/build/nodejs/node-v5.5.0-linux-x64/lib/node_modules/npm/bin/npm-cli.js"</span> <span class="string">"run-script"</span> <span class="string">"build"</span></span><br><span class="line">npm ERR! node v5<span class="number">.5</span><span class="number">.0</span></span><br><span class="line">npm ERR! npm  v3<span class="number">.3</span><span class="number">.12</span></span><br><span class="line">npm ERR! code ELIFECYCLE</span><br><span class="line">npm ERR! zipkin-ui@<span class="number">0.0</span><span class="number">.0</span> build: `node node_modules/webpack/bin/webpack.js --bail`</span><br><span class="line">npm ERR! Exit status <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="cat">cat</h3><h3 id="HTrace">HTrace</h3><h2 id="Operation">Operation</h2><h3 id="FrameGraph">FrameGraph</h3><p>git clone <a href="https://github.com/brendangregg/FlameGraph">https://github.com/brendangregg/FlameGraph</a><br>cd FlameGraph<br>sudo perf record –call-graph dwarf -p $(pgrep scylla)<br>sudo perf script | <code>pwd</code>/stackcollapse-perf.pl | <code>pwd</code>/flamegraph.pl &gt; flame.svg</p>
<p><a href="http://blog.csdn.net/justlinux2010/article/details/11520829" target="_blank" rel="external">http://blog.csdn.net/justlinux2010/article/details/11520829</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;监控工具：ZooKeeper，Kafka，Cassandra…&lt;br&gt;
    
    </summary>
    
      <category term="work" scheme="http://github.com/zqhxuyuan/categories/work/"/>
    
    
      <category term="ops" scheme="http://github.com/zqhxuyuan/tags/ops/"/>
    
  </entry>
  
  <entry>
    <title>译：Kafka事件驱动和流处理</title>
    <link href="http://github.com/zqhxuyuan/2016/11/18/Kafka-CQRS-Streams/"/>
    <id>http://github.com/zqhxuyuan/2016/11/18/Kafka-CQRS-Streams/</id>
    <published>2016-11-17T16:00:00.000Z</published>
    <updated>2016-10-29T03:50:13.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/" target="_blank" rel="external">http://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/</a><br><a id="more"></a></p>
<p>Event sourcing as an application architecture pattern is rising in popularity. Event sourcing involves modeling the state changes made by applications as an immutable sequence or “log” of events. Instead of modifying the state of the application in-place, event sourcing involves storing the event that triggers the state change in an immutable log and modeling the state changes as responses to the events in the log. We previously wrote about event sourcing, Apache Kafka and how they are related. In this post, I explore these ideas further and show how stream processing and, in particular, Kafka Streams helps to put Event sourcing and CQRS into practice.</p>
<blockquote>
<p>事件驱动（Event sourcing）作为应用程序架构的一种模式现在越来越流行了。事件驱动包括了将应用程序产生的状态改变数据建模成不可变的序列、或者说是事件的日志。它不是直接就地修改应用程序的状态，事件驱动会将触发状态改变的事件存储到一个不可变的日志，在日志中将状态改变建模成针对发生事件的响应。在<a href="http://www.confluent.io/blog/making-sense-of-stream-processing/" target="_blank" rel="external">之前的文章</a>中，我们阐述了事件驱动和Kafka的关系。本篇文章，我们会更进一步探索这些思想，并且会展示流处理，更准确的说是Kafka Streams，怎么帮助我们将事件驱动和CQRS运用在实际中。  </p>
</blockquote>
<p>Let’s take an example. Consider a Facebook-like social networking app (albeit a completely hypothetical one) that updates the profiles database when a user updates their Facebook profile. There are several applications that need to be notified when a user updates their profile — the search application so the user’s profile can be reindexed to be searchable on the changed attribute; the newsfeed application so the user’s connections can find out about the profile update; the data warehouse ETL application to load the latest profile data into the central data warehouse that powers various analytical queries and so on.</p>
<blockquote>
<p>举例类似于Facebook这样的社交网络应用，当用户在Facebook的设置页面更新自己的信息时会更新profiles数据库。有很多应用因为和个人信息相关，在用户更新他们的设置时，这些应用都需要被通知到。比如通知搜索应用，用户的设置可以被重新索引，被改变的属性就可以被搜索出来；通知新闻订阅应用，这样用户的连接就可以找出被更新过的设置；数据仓库应用，加载最新的设置数据到中央仓库，用于不同维护的数据查询和分析等等。</p>
</blockquote>
<p><img src="http://www.confluent.io/wp-content/uploads/2016/09/Event-sourced-based-architecture.jpeg" alt=""><br>基于事件驱动的架构</p>
<p>Event sourcing involves changing the profile web app to model the profile update as an event — something important that happened — and write it to a central log, like a Kafka topic. In this state of the world, all the applications that need to respond to the profile update event, merely subscribe to the Kafka topic and create the respective materialized views – be it a write to cache, index the event in Elasticsearch or simply compute an in-memory aggregate. The profile web app itself also subscribes to the same Kafka topic and writes the update to the profiles database.</p>
<blockquote>
<p>事件驱动会将profile应用程序的profile更新作为一个事件，写入到中央日志系统中，比如一个Kafka的主题。在这个充满状态的世界中，所有需要对profile更新事件作出响应的，所做的工作仅仅是订阅到Kafka的主题，并创建各自的物化视图（消费者读取数据，产生各自的数据），比如将其作为缓存的以写入、Elasticsearch的一个索引事件、或者只是内存中的简单聚合计算。当然profile应用程序本身也要订阅相同的Kafka主题（实际上不是订阅，而是生产，这里的意思是生产者和消费者使用相同的主题，否则生产者生产的消息没有被任何消费者订阅，就没有什么意义了。生产者这里是web应用程序产生的更新事件，消费者是下方各种数据源），并且会将更新写入到profiles数据库。</p>
</blockquote>
<h2 id="事件驱动：一些利弊">事件驱动：一些利弊</h2><p>There are several advantages to modeling applications to use event sourcing — It provides a complete log of every state change ever made to an object; so troubleshooting is easier. By expressing the user intent as an ordered log of immutable events, event sourcing gives the business an audit and compliance(合规性审计) log which also has the added benefit of providing data provenance(起源). It enables resilient(弹性) applications; rolling back applications amounts to rewinding(绕回) the event log and reprocessing data. It has better performance characteristics; writes and reads can be scaled independently. It enables a loosely coupled(松耦合) application architecture; one that makes it easier to move towards a microservices-based architecture. But most importantly:</p>
<p><strong>Event sourcing enables building a forward-compatible(向前兼容) application architecture — the ability to add more applications in the future that need to process the same event but create a different materialized view.</strong></p>
<blockquote>
<p>将应用程序建模成事件驱动有很多优点：它提供了针对一个对象的每个状态改变的完整日志，所以排查问题非常简单。</p>
</blockquote>
<p>For the upsides mentioned above, there are some downsides as well. Event sourcing has a higher learning curve(曲线); it is a new and unfamiliar programming model. The event log might involve more work to query it as it requires converting the events into the required materialized state suitable to query.</p>
<p>That was a quick introduction to event sourcing and some tradeoffs. This article is not meant to go into details of event sourcing or advocate for it’s usage. You can read more about event sourcing and various tradeoffs <a href="http://martinfowler.com/eaaDev/EventSourcing.html" target="_blank" rel="external">here</a>.</p>
<p>除了上面提到的一些优点，当然它也有缺点。事件驱动有更高的学习曲线，它是一种崭新的、不熟悉的编程模型。</p>
<h2 id="Kafka作为事件驱动的支柱">Kafka作为事件驱动的支柱</h2><h2 id="事件驱动和CQRS">事件驱动和CQRS</h2><h2 id="CQRS和Kafka_Streams">CQRS和Kafka Streams</h2><h3 id="方案1：应用程序状态存储到外部存储">方案1：应用程序状态存储到外部存储</h3><h3 id="方案2：应用程序状态存储到Kafka_Streams的本地状态">方案2：应用程序状态存储到Kafka Streams的本地状态</h3><h2 id="Kafka_Streams的交互式查询">Kafka Streams的交互式查询</h2><h2 id="交互式查询的用例">交互式查询的用例</h2><h2 id="使用Kafka作为事件驱动、CQRS">使用Kafka作为事件驱动、CQRS</h2><h3 id="零售店示例">零售店示例</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/&quot;&gt;http://www.confluent.io/blog/event-sourcing-cqrs-stream-processing-apache-kafka-whats-connection/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>PlayFramework快速入门</title>
    <link href="http://github.com/zqhxuyuan/2016/11/11/Play-Quickstart/"/>
    <id>http://github.com/zqhxuyuan/2016/11/11/Play-Quickstart/</id>
    <published>2016-11-10T16:00:00.000Z</published>
    <updated>2017-09-01T03:09:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>Scala PlayFramework（2.4）入门<br>示例程序：<a href="https://github.com/zqhxuyuan/first-player">https://github.com/zqhxuyuan/first-player</a></p>
<!-- MarkdownTOC -->
<ul>
<li>Run! Run!! Run!!!</li>
<li>Hello World!<ul>
<li>路由和Controller</li>
<li>页面和渲染<ul>
<li>模板编译</li>
</ul>
</li>
</ul>
</li>
<li>Products Example<ul>
<li>implicit</li>
</ul>
</li>
<li>Spark集成<ul>
<li>play-spark-module</li>
<li>Spark Launcher<ul>
<li>mesos</li>
<li>问题</li>
</ul>
</li>
<li>Spark Cassandra</li>
<li>预览</li>
<li>Actor</li>
<li>Quartz<ul>
<li>Job依赖注入</li>
<li>Quartz作业的自动重启</li>
</ul>
</li>
<li>Kafka</li>
</ul>
</li>
<li>附录<ul>
<li>sbt私服</li>
<li>Play MultiProject</li>
<li>Run &amp; Debug With IDEA<ul>
<li><ol>
<li>Play App（❌）</li>
</ol>
</li>
<li><ol>
<li>Sbt Task（✅）</li>
</ol>
</li>
<li><ol>
<li>jvm-debug（✅）</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /MarkdownTOC -->
<a id="more"></a>
<h1 id="Run!_Run!!_Run!!!">Run! Run!! Run!!!</h1><p><strong>安装activator</strong>  </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">brew install typesafe-activator</span><br><span class="line">activator new first-player</span><br><span class="line"><span class="built_in">cd</span> first-player</span><br><span class="line">activator run</span><br></pre></td></tr></table></figure>
<p><strong>导入IntelliJ IDEA</strong>  </p>
<ol>
<li>install IDEA professional</li>
<li>install PlayFramework support</li>
<li>import SBT project</li>
<li>right click controllers.Products, Run Play2 App in idea</li>
</ol>
<h1 id="Hello_World!">Hello World!</h1><p><code>activator new first-player</code>生成的目录结构如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">➜  first-player tree -L <span class="number">2</span></span><br><span class="line">.</span><br><span class="line">├── activator                   使用activator new才有该文件</span><br><span class="line">├── activator-launch-<span class="number">1.3</span>.<span class="number">4</span><span class="class">.jar</span></span><br><span class="line">├── app </span><br><span class="line">│   ├── controllers             控制器</span><br><span class="line">│   ├── models                  模型</span><br><span class="line">│   └── views                   视图（Web页面）</span><br><span class="line">├── build<span class="class">.sbt</span></span><br><span class="line">├── conf</span><br><span class="line">│   ├── application<span class="class">.conf</span>        配置文件</span><br><span class="line">│   └── routes                  路由配置</span><br><span class="line">├── project</span><br><span class="line">│   ├── build<span class="class">.properties</span></span><br><span class="line">│   ├── plugins<span class="class">.sbt</span>             插件配置</span><br><span class="line">├── public                      样式和脚本</span><br><span class="line">│   ├── images</span><br><span class="line">│   ├── javascripts</span><br><span class="line">│   └── stylesheets</span><br></pre></td></tr></table></figure>
<h2 id="路由和Controller">路由和Controller</h2><p>路由配置：</p>
<figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># http://localhost:9000</span></span><br><span class="line"><span class="keyword">GET</span>     /                           controllers.HomeController.index</span><br><span class="line"></span><br><span class="line"><span class="preprocessor"># http://localhost:9000/hello?n=Play!</span></span><br><span class="line"><span class="keyword">GET</span>     /hello                      controllers.HomeController.hello(n: <span class="built_in">String</span>)</span><br></pre></td></tr></table></figure>
<p>对应的HomeController方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> javax.inject._</span><br><span class="line"><span class="keyword">import</span> models.<span class="type">Product</span></span><br><span class="line"><span class="keyword">import</span> play.api._</span><br><span class="line"><span class="keyword">import</span> play.api.mvc._</span><br><span class="line"></span><br><span class="line"><span class="annotation">@Singleton</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HomeController</span> <span class="title">@Inject</span>(</span>) <span class="keyword">extends</span> <span class="type">Controller</span> &#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">index</span> =</span> <span class="type">Action</span> &#123;</span><br><span class="line">    <span class="type">Ok</span>(views.html.index(<span class="string">"Your new application is ready."</span>))  <span class="comment">//①</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hello</span>(</span>name: <span class="type">String</span>) = <span class="type">Action</span> &#123;</span><br><span class="line">    <span class="comment">//Ok("Hello " + name)</span></span><br><span class="line">    <span class="type">Ok</span>(views.html.hello(name))  <span class="comment">//②</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>①：index方法的Response响应会返回一个Html页面，指向<code>app/views/index.scala.html</code>页面<br>②：hello方法的响应返回<code>app/views/hello.scala.html</code>页面，只不过这个页面会渲染一个参数</p>
<blockquote>
<p>注：如果Controller的Action方法是Ok(“字符串”)则返回的是一个字符串。<br>2.2版本中Controller可以用object，在2.4需要用class，以及@Inject和@Singleton</p>
</blockquote>
<p>本节知识点：</p>
<ol>
<li>路由配置到控制器的映射</li>
<li>Action的Response返回值如何指向页面</li>
<li>Action的参数如何传递给页面</li>
</ol>
<h2 id="页面和渲染">页面和渲染</h2><p>views是Play的html页面，也可以在该页面下新建子目录。在IntelliJ中，在views下创建子目录，<br>也是创建一个Package，，因为views和controllers的等级相同。  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜  app tree views</span><br><span class="line">views</span><br><span class="line">├── hello<span class="class">.scala</span><span class="class">.html</span></span><br><span class="line">├── index<span class="class">.scala</span><span class="class">.html</span></span><br><span class="line">├── main<span class="class">.scala</span><span class="class">.html</span></span><br><span class="line">├── products</span><br><span class="line">│   ├── <span class="tag">details</span><span class="class">.scala</span><span class="class">.html</span></span><br><span class="line">│   ├── edit<span class="class">.scala</span><span class="class">.html</span></span><br><span class="line">│   ├── list<span class="class">.scala</span><span class="class">.html</span></span><br><span class="line">│   └── main<span class="class">.scala</span><span class="class">.html</span></span><br></pre></td></tr></table></figure>
<p>app/views/index.scala.html：index方法中的”Your new application is ready.”<br>会传递到index.scala.html作为第一行的message参数。  </p>
<p>@main调用的是同一个路径的main.scala.html模板页面，也可以用@views.html.main绝对路径。<br>Play的scala.html页面实际上是模板，可以用Scala代码的方式来调用。<br>所以@main方法的第一参数是字符串（标题），第二个参数是Html对象用来表示page body。<br>@play20.welcome是Play内置的一个方法（绝对路径是@views.html.play20）</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">@(message: String)</span><br><span class="line"></span><br><span class="line">@main("Welcome to Play") &#123;</span><br><span class="line">    @play20.welcome(message, style = "Scala")</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的@main方法会调用同路径的main.scala.html模板页面（Layout嵌套）。<br>app/views/main.scala.html：第一行有两个参数，而且是参数列表的方式，而不是两个参数放在一起。  </p>
<p>index页面的”Welcome to Play”作为title参数，第二个参数则作为content参数，正好对应了Html类型。</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@(title: String)(content: Html)</span><br><span class="line"></span><br><span class="line"><span class="doctype">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">html</span> <span class="attribute">lang</span>=<span class="value">"en"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">title</span>&gt;</span>@title<span class="tag">&lt;/<span class="title">title</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">link</span> <span class="attribute">rel</span>=<span class="value">"stylesheet"</span> <span class="attribute">media</span>=<span class="value">"screen"</span> <span class="attribute">href</span>=<span class="value">"@routes.Assets.versioned("</span><span class="value">stylesheets</span>/<span class="attribute">main.css</span>")"&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">link</span> <span class="attribute">rel</span>=<span class="value">"shortcut icon"</span> <span class="attribute">type</span>=<span class="value">"image/png"</span> <span class="attribute">href</span>=<span class="value">"@routes.Assets.versioned("</span><span class="value">images</span>/<span class="attribute">favicon.png</span>")"&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">body</span>&gt;</span></span><br><span class="line">        @content</span><br><span class="line">    <span class="tag">&lt;/<span class="title">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>两个参数放在一起的话，类似：@(title: String, content: Html)。对应的调用方式也要更改为：</p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">@main</span>(<span class="string">"Welcome to Play"</span>, &#123;</span><br><span class="line">    <span class="variable">@play20</span>.<span class="function">welcome</span>(message, style = <span class="string">"Scala"</span>)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>本节知识点：</p>
<ol>
<li>views子目录的创建，以及在控制器中如何访问到指定子目录下的页面</li>
<li>页面的参数定义和使用（@标记后面的都是Scala代码），参数列表</li>
<li>嵌套模板的使用，对应的Scala方法</li>
<li>缺省的模板调用路径是和当前页面相同路径，绝对路径是<code>@views.html.</code></li>
</ol>
<h3 id="模板编译">模板编译</h3><p>views下的每个页面都会被编译成Scala的模板类，下面右图views.html实际上就和控制器的调用是类似的。  </p>
<blockquote>
<p>如果是其他格式，比如json、xml，则用views.json或者views.xml，可见不总是views.html。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161111154235071" alt="play vies"></p>
<p>HomeController的idnex方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span> =</span> <span class="type">Action</span> &#123;</span><br><span class="line">  <span class="type">Ok</span>(views.html.index(<span class="string">"Your new application is ready."</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编译后的文件路径：target/scala-2.11/twirl/main/views.html/index.template.scala<br>index是一个object，对应index类的apply方法，接收一个参数，参数名称为message。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">index_Scope0</span> &#123;</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">index</span>  &#123;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>message: <span class="type">String</span>):play.twirl.api.<span class="type">HtmlFormat</span>.<span class="type">Appendable</span> = &#123;...&#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">render</span>(</span>message:<span class="type">String</span>): play.twirl.api.<span class="type">HtmlFormat</span>.<span class="type">Appendable</span> = apply(message)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span>:</span>((<span class="type">String</span>) =&gt; play.twirl.api.<span class="type">HtmlFormat</span>.<span class="type">Appendable</span>) = (message) =&gt; apply(message)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ref</span>:</span> <span class="keyword">this</span>.<span class="keyword">type</span> = <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">index</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">index_Scope0</span>.<span class="title">index</span></span></span><br></pre></td></tr></table></figure>
<p>index.scala.html调用@main方法对应的views/main.scala.html</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="at_rule">@<span class="keyword">(message:</span> String)</span><br><span class="line">@<span class="function">main</span>(<span class="string">"Welcome to Play"</span>) </span>&#123;</span><br><span class="line">    <span class="at_rule">@<span class="keyword">play20.welcome(message,</span> style = <span class="string">"Scala"</span>)</span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure>
<p>main方法对应的编译文件：target/scala-2.11/twirl/main/views.html/main.template.scala<br>main类的apply方法有两个参数，分别是(title: String)(content: Html)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">main_Scope0</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">main</span> &#123;</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>title: <span class="type">String</span>)(content: <span class="type">Html</span>):play.twirl.api.<span class="type">HtmlFormat</span>.<span class="type">Appendable</span> = &#123; ... &#125;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">render</span>(</span>title:<span class="type">String</span>,content:<span class="type">Html</span>): play.twirl.api.<span class="type">HtmlFormat</span>.<span class="type">Appendable</span> = apply(title)(content)</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">f</span>:</span>((<span class="type">String</span>) =&gt; (<span class="type">Html</span>) =&gt; play.twirl.api.<span class="type">HtmlFormat</span>.<span class="type">Appendable</span>) = (title) =&gt; (content) =&gt; apply(title)(content)</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">ref</span>:</span> <span class="keyword">this</span>.<span class="keyword">type</span> = <span class="keyword">this</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">main</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">main_Scope0</span>.<span class="title">main</span></span></span><br></pre></td></tr></table></figure>
<h1 id="Products_Example">Products Example</h1><p>路由配置：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">POST    /products                      controllers<span class="class">.Products</span><span class="class">.save</span></span><br><span class="line">GET     /products/new                  controllers<span class="class">.Products</span><span class="class">.newProduct</span></span><br><span class="line">GET     /products                      controllers<span class="class">.Products</span><span class="class">.products</span></span><br><span class="line">GET     /products/:ean                 controllers<span class="class">.Products</span><span class="class">.show</span>(ean: Long)</span><br></pre></td></tr></table></figure>
<p>Products控制器，为了支持国际化，添加I18nSupport接口。注意每个Action都添加了<code>implicit request =&gt;</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="annotation">@Singleton</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Products</span> <span class="title">@Inject</span>(</span>) (<span class="keyword">val</span> messagesApi: <span class="type">MessagesApi</span>) <span class="keyword">extends</span> <span class="type">Controller</span> <span class="keyword">with</span> <span class="type">I18nSupport</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">products</span> =</span> <span class="type">Action</span> &#123; <span class="keyword">implicit</span> request =&gt;</span><br><span class="line">    <span class="keyword">val</span> products = <span class="type">Product</span>.findAll</span><br><span class="line">    <span class="type">Ok</span>(views.html.products.list(products))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">show</span>(</span>ean: <span class="type">Long</span>) = <span class="type">Action</span> &#123; <span class="keyword">implicit</span> request =&gt;</span><br><span class="line">    <span class="type">Product</span>.findByEan(ean).map &#123; product =&gt;</span><br><span class="line">      <span class="type">Ok</span>(views.html.products.details(product))</span><br><span class="line">    &#125;.getOrElse(<span class="type">NotFound</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>views/products/list.scala.html</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">@(productList: List[Product])(implicit lang: Messages)</span><br><span class="line"></span><br><span class="line">@products.main(Messages("application.name")) &#123;</span><br><span class="line">    &lt;dl class="products"&gt;</span><br><span class="line">    @for(product &lt;- productList) &#123;</span><br><span class="line">        &lt;dt&gt;&lt;a href="@controllers.routes.Products.show(product.ean)"&gt;@product.ean&lt;/a&gt;&lt;/dt&gt;</span><br><span class="line">        &lt;dt&gt;@product.name&lt;/dt&gt;</span><br><span class="line">        &lt;dd&gt;@product.description&lt;/dd&gt;</span><br><span class="line">    &#125;</span><br><span class="line">    &lt;/dl&gt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意第一行的参数<code>productList</code>不能命名为：<code>products</code>，否则第二行的<code>@products.main</code>会报错，<br>它会认为<code>products</code>来自于第一行的参数，但这个参数并没有<code>main</code>方法（List没有main方法）。而实际上<code>@products.main</code><br>代表的是要调用<code>@views.html.products.main</code>对应的<code>views/products/main.scala.html</code>页面。<br>注意：<code>productList</code>参数名称不需要和<code>Products</code>控制器的<code>products</code>方法的<code>products</code>变量相同，只要类型相同即可，所以可以是任意的名称。</p>
</blockquote>
<p>这里还有一个隐式的Messages，用来做国际化的支持。</p>
<p>反向路由：在product的en上超链接：”@controllers.routes.Products.show(product.ean)”，也可以省略掉controllers：”@routes.Products.show(product.ean)”。<br>在从路由配置到控制器时，Play会帮我们创建一个控制器到路由的反向路由配置。比如这里页面要访问”Products.show”方法。  </p>
<p><img src="http://img.blog.csdn.net/20161111154733495" alt="reverse route"></p>
<p>本节知识点：</p>
<ol>
<li>国际化的支持，以及implicit隐式参数的使用</li>
<li>参数名称不能和嵌套模板的子目录名称一样（比如参数productList，子目录是products，两者不能相同）</li>
<li>在页面中访问控制器方法，使用反向路由</li>
</ol>
<h2 id="implicit">implicit</h2><p>正常的Action调用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//normal invoke, must pass Cart Object</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">catalog10</span>(</span>) = <span class="type">Action</span> &#123; request =&gt;</span><br><span class="line">  <span class="keyword">val</span> products = <span class="type">ProductDAO</span>.list</span><br><span class="line">  <span class="type">Ok</span>(views.html.shop.catalog10(products, cart(request)))</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cart</span>(</span>request: <span class="type">RequestHeader</span>) = &#123;</span><br><span class="line">  <span class="type">Cart</span>.demoCart()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应的页面：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@(products: Seq[Product3], cart: Cart)</span><br><span class="line"></span><br><span class="line">@shop.cart10("")(cart) &#123;</span><br><span class="line">    <span class="tag">&lt;<span class="title">h2</span>&gt;</span>Catalog<span class="tag">&lt;/<span class="title">h2</span>&gt;</span></span><br><span class="line">    @views.html.shop.productList(products)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用implicit，不需要传递Cat对象，不过request需要使用implicit，同时还要提供一个能够生成Cart对象的方法或者trait接口</p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//<span class="type">implicit</span> invoke, no need to <span class="keyword">pass</span> Cart</span><br><span class="line">def catalog11() = <span class="keyword">Action</span> &#123; <span class="type">implicit</span> request =&gt;</span><br><span class="line">  val products = ProductDAO.list</span><br><span class="line">  //too many arguments for method apply: (content: C)(<span class="type">implicit</span> writeable: play.api.http.Writeable[C])play.api.mvc.Result <span class="type">in</span> <span class="keyword">class</span> <span class="keyword">Status</span></span><br><span class="line">  //Ok(views.html.shop.catalog11(products), cart(request))</span><br><span class="line"></span><br><span class="line">  //如果没有定义<span class="type">implicit</span>的方法,是不会直接使用上面的cart方法的</span><br><span class="line">  //could not find <span class="type">implicit</span> <span class="keyword">value</span> for <span class="type">parameter</span> cart: models.Cart</span><br><span class="line">  Ok(views.html.shop.catalog11(products))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//方法名称任意,只要定义成<span class="type">implicit</span>即可</span><br><span class="line">//注意:如果去掉返回类型: models.Cart,也会报错说找不到models.Cart,所以要显示声明返回类型</span><br><span class="line"><span class="type">implicit</span> def cartImplicit(<span class="type">implicit</span> request: RequestHeader): models.Cart = &#123;</span><br><span class="line">  Cart.demoCart()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应的页面：可以看到参数中cart变成implicit，调用时，不需要传递cart了。</p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@(products: Seq[Product3])(<span class="type">implicit</span> cart: Cart)</span><br><span class="line"></span><br><span class="line">@shop.cart11(<span class="string">""</span>) &#123;</span><br><span class="line">    &lt;h2&gt;Catalog&lt;/h2&gt;</span><br><span class="line">    @views.html.shop.productList(products)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果没有implicit方法没有显示指定类型，也会报错：</p>
<p><img src="http://img.blog.csdn.net/20161111170901722" alt="implicit 1"></p>
<p>We’ve moved the Cart parameter to a second parameter list and made it implicit,<br>so we can apply this template and omit the second parameter list if an implicit Cart is available on the calling side. </p>
<p>Now we’ve declared the cart method as implicit. In addition, we’ve declared the RequestHeader parameter of<br>both our action and the cart method as implicit. If we now call the views.html.shop.catalog template and<br>omit the Cart parameter, the Scala compiler will look for an implicit Cart in scope. It’ll find the cart method,<br>which requires a RequestHeader parameter that’s also declared as implicit, but that’s also available.<br>We can make our newly created cart method reusable, by moving it into a trait. Then We can<br>now mix this trait into every controller where we need access to our implicit Cart.</p>
<p>我们定义了Action方法的RequestHeader（implicit request）以及cart方法（implicit def cartImplicit）都是implicit。<br>那么调用catalog模板（views.html.shop.catalog11(products)）时就可以省略Cart参数，Scala的编译器会在范围内寻找隐式的Cart对象。<br>它会找到cart方法（implicit def cartImplicit）。为了让隐式的Cart方法可重用，可以定义在一个接口中，这样其他控制器都可以使用这个方法</p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">trait WithCart &#123;</span><br><span class="line">  <span class="type">implicit</span> def cart(<span class="type">implicit</span> request: RequestHeader) = &#123;</span><br><span class="line">    // Get cart from session</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20161111170829019" alt="implicit cart"></p>
<p>虽然使用implicit有诸多好处，但是在需要添加其他implicit参数时，还要在页面中依次添加对应的(implicit)声明。解决办法是用一个对象来表示所有的隐式参数<br>然后在页面中使用这个隐式的对象即可，当需要添加隐式参数时，往对象中添加即可，页面中因为引用的是对象，所以自动继承了这个对象的所有隐式参数。</p>
<p>It’s often necessary to pass multiple values from your controller into your main template. Even with implicit parameters,<br>it would be a hassle to have to add another one each time, because you’d still have to add the implicit parameter to all<br>of the template definitions. One straightforward solution is to create a single class that contains all the objects you<br>need in your template, and pass an instance of that. If you want to add a value to it, you only need to adapt the template<br>where you use it, and the method that constructs it.  </p>
<p>It’s common to pass the RequestHeader or Request to templates, as we’ll see in section 6.7.2.<br>Play provides a WrappedRequest class, which wraps a Request and implements the interface itself as well,<br>so it’s usable as if it were a regular Request. But by extending WrappedRequest, you can add other fields:</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">case class UserDataRequest[<span class="link_label">A</span>](<span class="link_url">val user: User, val cart: Cart, request: Request[A]</span>) extends WrappedRequest(request)</span><br></pre></td></tr></table></figure>
<p>If you pass an instance of this UserDataRequest to your template, you have a refer- ence to the Request, User, and Cart.</p>
<h1 id="Spark集成">Spark集成</h1><p>Play和Spark使用的jackson依赖包有冲突，如果没有显示指定jackson，报错：</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">play.api.http.HttpErrorHandlerExceptions$<span class="variable">$anon</span><span class="variable">$1</span>: Execution exception[[RuntimeException: </span><br><span class="line">java.lang.VerifyError: class com.fasterxml.jackson.module.scala.ser.ScalaIteratorSerializer overrides final method withResolved.</span><br><span class="line">(Lcom/fasterxml/jackson/databind/BeanProperty;Lcom/fasterxml/jackson/databind/jsontype/TypeSerializer;</span><br><span class="line">  Lcom/fasterxml/jackson/databind/JsonSerializer;)Lcom/fasterxml/jackson/databind/ser/std/AsArraySerializerBase;]]</span><br><span class="line">  at play.api.http.HttpErrorHandlerExceptions$.throwableToUsefulException(HttpErrorHandler.scala:<span class="number">280</span>)</span><br><span class="line">  at play.api.http.DefaultHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">206</span>)</span><br><span class="line">  at play.api.GlobalSettings<span class="variable">$class</span>.onError(GlobalSettings.scala:<span class="number">160</span>)</span><br><span class="line">  at Global$.onError(Global.scala:<span class="number">11</span>)</span><br><span class="line">  at play.api.http.GlobalSettingsHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">98</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">99</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">344</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">343</span>)</span><br><span class="line">  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<p><a href="https://groups.google.com/forum/#!topic/play-framework/LIf_Ughidcc" target="_blank" rel="external">https://groups.google.com/forum/#!topic/play-framework/LIf_Ughidcc</a><br><a href="https://github.com/FasterXML/jackson-module-scala/issues/214">https://github.com/FasterXML/jackson-module-scala/issues/214</a><br><a href="http://stackoverflow.com/questions/33815396/spark-com-fasterxml-jackson-module-error" target="_blank" rel="external">http://stackoverflow.com/questions/33815396/spark-com-fasterxml-jackson-module-error</a>  </p>
<p>没有指定jackson时，jackson-module-scala和jackson-databind的不一致：</p>
<p><img src="http://img.blog.csdn.net/20161118100439795" alt="play-spark-jackson1"></p>
<p>在build.sbt手动添加<code>jackson-module-scala</code>的依赖，版本和Play的一致。  </p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">libraryDependencies</span> += <span class="string">"com.fasterxml.jackson.module"</span> % <span class="string">"jackson-module-scala_2.11"</span> % <span class="string">"2.7.6"</span></span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20161118100453598" alt="play-spark-jackson2"></p>
<p>如果Actor版本不对，执行Spark时可能报错：比如引入了libraryDependencies += “com.github.dnvriend” %% “akka-persistence-jdbc” % “2.6.8”<br>是的Actor版本从2.4.10升级到2.4.12</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">play.api.http.HttpErrorHandlerExceptions$<span class="variable">$anon</span><span class="variable">$1</span>: Execution exception[[RuntimeException: java.lang.NoSuchMethodError: akka.actor.LocalActorRefProvider.log()Lakka/event/LoggingAdapter;]]</span><br><span class="line">  at play.api.http.HttpErrorHandlerExceptions$.throwableToUsefulException(HttpErrorHandler.scala:<span class="number">293</span>)</span><br><span class="line">  at play.api.http.DefaultHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">220</span>)</span><br><span class="line">  at play.api.GlobalSettings<span class="variable">$class</span>.onError(GlobalSettings.scala:<span class="number">160</span>)</span><br><span class="line">  at Global$.onError(Global.scala:<span class="number">11</span>)</span><br><span class="line">  at play.api.http.GlobalSettingsHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">99</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">344</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">343</span>)</span><br><span class="line">  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
<h2 id="play-spark-module">play-spark-module</h2><p><a href="https://github.com/JoaoVasques/play-spark-module">https://github.com/JoaoVasques/play-spark-module</a>  </p>
<p>修改SparkController的timeout时间为30s，访问：<a href="http://localhost:9000/spark/count" target="_blank" rel="external">http://localhost:9000/spark/count</a>  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[warn] o<span class="class">.a</span><span class="class">.h</span><span class="class">.u</span><span class="class">.NativeCodeLoader</span> - Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line">[info] <span class="tag">p</span><span class="class">.m</span><span class="class">.i</span><span class="class">.j</span><span class="class">.p</span><span class="class">.s</span><span class="class">.w</span><span class="class">.SparkJobWorker</span> - Starting job <span class="number">1561847669</span> future</span><br><span class="line"><span class="number">3.216</span></span><br><span class="line">[info] <span class="tag">p</span><span class="class">.m</span><span class="class">.i</span><span class="class">.j</span><span class="class">.p</span><span class="class">.s</span><span class="class">.w</span><span class="class">.SparkJobWorker</span> - Job is done. Shutting down worker</span><br></pre></td></tr></table></figure>
<h2 id="Spark_Launcher">Spark Launcher</h2><p>Package org.apache.spark.launcher：Library for launching Spark applications.<br>This library allows applications to launch Spark programmatically. There’s only one entry point to the library - the SparkLauncher class.</p>
<p>The SparkLauncher.startApplication( org.apache.spark.launcher.SparkAppHandle.Listener…) can be used to start Spark and provide a handle to monitor and control the running application:</p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"><span class="keyword">import</span> org.apache.spark.launcher.SparkAppHandle;</span></span><br><span class="line"><span class="preprocessor"><span class="keyword">import</span> org.apache.spark.launcher.SparkLauncher;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyLauncher</span> </span>&#123;</span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(String[] args) throws Exception &#123;</span><br><span class="line">   SparkAppHandle handle = <span class="keyword">new</span> SparkLauncher()</span><br><span class="line">     .setAppResource(<span class="string">"/my/app.jar"</span>)</span><br><span class="line">     .setMainClass(<span class="string">"my.spark.app.Main"</span>)</span><br><span class="line">     .setMaster(<span class="string">"local"</span>)</span><br><span class="line">     .setConf(SparkLauncher.DRIVER_MEMORY, <span class="string">"2g"</span>)</span><br><span class="line">     .startApplication();</span><br><span class="line">   <span class="comment">// Use handle API to monitor / control application.</span></span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>It’s also possible to launch a raw child process, using the SparkLauncher.launch() method:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.launcher.SparkLauncher;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyLauncher</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">   Process spark = <span class="keyword">new</span> SparkLauncher()</span><br><span class="line">     .setAppResource(<span class="string">"/my/app.jar"</span>)</span><br><span class="line">     .setMainClass(<span class="string">"my.spark.app.Main"</span>)</span><br><span class="line">     .setMaster(<span class="string">"local"</span>)</span><br><span class="line">     .setConf(SparkLauncher.DRIVER_MEMORY, <span class="string">"2g"</span>)</span><br><span class="line">     .launch();</span><br><span class="line">   spark.waitFor();</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>This method requires the calling code to manually manage the child process, including its output streams (to avoid possible deadlocks). It’s recommended that SparkLauncher.startApplication( org.apache.spark.launcher.SparkAppHandle.Listener…) be used instead.</p>
<h3 id="mesos">mesos</h3><p>代码中指定本地文件（不推荐用法）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /Users/zhengqh/Github/scala/simple-app</span><br><span class="line">mvn clean package</span><br><span class="line">~/Soft/spark-<span class="number">1.6</span><span class="number">.2</span>-bin-hadoop2<span class="number">.6</span>/bin/spark-submit --<span class="keyword">class</span> <span class="string">"SimpleApp"</span> --master local\[<span class="number">2</span>\] \</span><br><span class="line">target/scala-<span class="number">2.10</span>/simple-project_2<span class="number">.10</span>-<span class="number">1.0</span>.jar</span><br></pre></td></tr></table></figure>
<p>命令行指定file路径（也可以指定文件类型，比如本地或者hdfs，推荐用法）</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~/Soft/spark-<span class="number">1.6</span><span class="number">.2</span>-bin-hadoop2<span class="number">.6</span>/bin/spark-submit --<span class="keyword">class</span> <span class="string">"SimpleApp"</span> --master local\[<span class="number">2</span>\] \</span><br><span class="line">--conf spark.app.logfile=<span class="string">"file:///Users/zhengqh/Github/scala/simple-app/README.md"</span> \</span><br><span class="line">target/scala-<span class="number">2.10</span>/simple-project_2<span class="number">.10</span>-<span class="number">1.0</span>.jar</span><br></pre></td></tr></table></figure>
<p>如果本地环境变量有HADOOP_CONF_DIR，需要注释掉，否则会使用hdfs，报错：  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread "main" java.net.ConnectException: <span class="operator"><span class="keyword">Call</span> <span class="keyword">From</span> zqhmac/<span class="number">10.57</span><span class="number">.2</span><span class="number">.31</span> <span class="keyword">to</span> localhost:<span class="number">9000</span> </span><br><span class="line"><span class="keyword">failed</span> <span class="keyword">on</span> <span class="keyword">connection</span> <span class="keyword">exception</span>: <span class="keyword">java</span>.net.ConnectException: <span class="keyword">Connection</span> refused;</span> For more details see: </span><br><span class="line">http://wiki.apache.org/hadoop/ConnectionRefused</span><br></pre></td></tr></table></figure>
<p>命令行也可以指定其他运行方式，比如mesos，所以不推荐在代码中写死setMaster，除非开发时设置为setMaster(“local[*]”)</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~/Soft/spark-<span class="number">1.6</span><span class="number">.2</span>-bin-hadoop2<span class="number">.6</span>/bin/spark-submit --<span class="keyword">class</span> <span class="string">"SimpleApp"</span> --master <span class="string">"mesos://192.168.6.52:5050"</span> \</span><br><span class="line">--conf spark.app.logfile=<span class="string">"file:///Users/zhengqh/Github/scala/simple-app/README.md"</span> \</span><br><span class="line">target/scala-<span class="number">2.10</span>/simple-project_2<span class="number">.10</span>-<span class="number">1.0</span>.jar</span><br></pre></td></tr></table></figure>
<p>在本地运行时报错：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Failed to <span class="operator"><span class="keyword">load</span> <span class="keyword">native</span> Mesos <span class="keyword">library</span> <span class="keyword">from</span> /<span class="keyword">Users</span>/zhengqh/<span class="keyword">Library</span>/<span class="keyword">Java</span>/Extensions:/<span class="keyword">Library</span>/<span class="keyword">Java</span>/Extensions:</span><br><span class="line">/Network/<span class="keyword">Library</span>/<span class="keyword">Java</span>/Extensions:/<span class="keyword">System</span>/<span class="keyword">Library</span>/<span class="keyword">Java</span>/Extensions:/usr/lib/<span class="keyword">java</span>:.</span><br><span class="line"><span class="keyword">Exception</span> <span class="keyword">in</span> <span class="keyword">thread</span> <span class="string">"main"</span> <span class="keyword">java</span>.lang.UnsatisfiedLinkError: <span class="keyword">no</span> mesos <span class="keyword">in</span> <span class="keyword">java</span>.<span class="keyword">library</span>.<span class="keyword">path</span></span></span><br></pre></td></tr></table></figure>
<p>所以需要在本地安装有mesos环境，Mac安装mesos可以参考<a href="https://mesosphere.com/blog/2014/07/07/installing-mesos-on-your-mac-with-homebrew/" target="_blank" rel="external">https://mesosphere.com/blog/2014/07/07/installing-mesos-on-your-mac-with-homebrew/</a>  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brew <span class="operator"><span class="keyword">install</span> mesos</span><br><span class="line">/usr/<span class="keyword">local</span>/sbin/mesos-<span class="keyword">master</span> <span class="comment">--registry=in_memory --ip=127.0.0.1</span></span></span><br></pre></td></tr></table></figure>
<p>环境变量添加mesos:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For Linux</span></span><br><span class="line">$ <span class="built_in">export</span> MESOS_NATIVE_JAVA_LIBRARY=<span class="string">'/usr/local/lib/libmesos.so'</span></span><br><span class="line"><span class="comment"># For OSX</span></span><br><span class="line">$ <span class="built_in">export</span> MESOS_NATIVE_JAVA_LIBRARY=<span class="string">'/usr/local/lib/libmesos.dylib'</span></span><br></pre></td></tr></table></figure>
<p>本地安装好mesos,设置好mesos库，指定远程mesos：mesos://192.168.6.52:5050</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.</span><br><span class="line"><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span><span class="keyword">*</span></span><br><span class="line">I1121 15:07:33.008949 778174464 sched.cpp:157] Version: 0.22.1</span><br><span class="line">I1121 15:07:33.016233 651489280 sched.cpp:254] New master detected at master<span class="comment">@192.168.6.52:5050</span></span><br></pre></td></tr></table></figure>
<p>但是，执行到上面后就不动了，可能是本地的mesos和远程mesos版本不对：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">~/Soft/spark-<span class="number">1.6</span>.<span class="number">2</span>-bin-hadoop2.<span class="number">6</span>/bin/spark-submit --class <span class="string">"SimpleApp"</span> --master mesos:<span class="comment">//zk://192.168.6.55:2181,192.168.6.56:2181,192.168.6.57:2181/mesos \</span></span><br><span class="line">--conf spark<span class="class">.app</span><span class="class">.logfile</span>=<span class="string">"file:///Users/zhengqh/Github/scala/simple-app/README.md"</span> \</span><br><span class="line">target/scala-<span class="number">2.10</span>/simple-project_2.<span class="number">10</span>-<span class="number">1.0</span><span class="class">.jar</span></span><br><span class="line"></span><br><span class="line">I1121 <span class="number">15</span>:<span class="number">34</span>:<span class="number">42.195302</span> <span class="number">655667200</span> group<span class="class">.cpp</span>:<span class="number">313</span>] Group process (<span class="function"><span class="title">group</span><span class="params">(<span class="number">1</span>)</span></span>@<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">61225</span>) connected to ZooKeeper</span><br><span class="line">I1121 <span class="number">15</span>:<span class="number">34</span>:<span class="number">42.195796</span> <span class="number">655667200</span> group<span class="class">.cpp</span>:<span class="number">790</span>] Syncing group operations: queue size (joins, cancels, datas) = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">I1121 <span class="number">15</span>:<span class="number">34</span>:<span class="number">42.196069</span> <span class="number">655667200</span> group<span class="class">.cpp</span>:<span class="number">385</span>] Trying to create path <span class="string">'/mesos'</span> <span class="keyword">in</span> ZooKeeper</span><br><span class="line">I1121 <span class="number">15</span>:<span class="number">34</span>:<span class="number">42.208238</span> <span class="number">662253568</span> detector<span class="class">.cpp</span>:<span class="number">138</span>] Detected <span class="tag">a</span> new leader: (id=<span class="string">'62'</span>)</span><br><span class="line">I1121 <span class="number">15</span>:<span class="number">34</span>:<span class="number">42.208575</span> <span class="number">655667200</span> group<span class="class">.cpp</span>:<span class="number">659</span>] Trying to get <span class="string">'/mesos/json.info_0000000062'</span> <span class="keyword">in</span> ZooKeeper</span><br><span class="line">Failed to detect <span class="tag">a</span> master: Failed to parse data of unknown <span class="tag">label</span> <span class="string">'json.info</span></span><br></pre></td></tr></table></figure>
<p>如果指定本地mesos（要先在本地启动mesos-local）：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">~/Soft/spark-<span class="number">1.6</span><span class="number">.2</span>-bin-hadoop2<span class="number">.6</span>/bin/spark-submit --<span class="keyword">class</span> <span class="string">"SimpleApp"</span> --master <span class="string">"mesos://localhost:5050"</span> \</span><br><span class="line">--conf spark.app.logfile=<span class="string">"file:///Users/zhengqh/Github/scala/simple-app/README.md"</span> \</span><br><span class="line">target/scala-<span class="number">2.10</span>/simple-project_2<span class="number">.10</span>-<span class="number">1.0</span>.jar</span><br><span class="line"></span><br><span class="line">I1121 <span class="number">15</span>:<span class="number">11</span>:<span class="number">07.611460</span> <span class="number">515149824</span> sched.cpp:<span class="number">448</span>] Framework registered with <span class="number">20161121</span>-<span class="number">150645</span>-<span class="number">16777343</span>-<span class="number">5050</span>-<span class="number">14247</span>-<span class="number">0000</span> 出现Framework registered，实际上就说明连接本地mesos成功了</span><br><span class="line"><span class="number">16</span>/<span class="number">11</span>/<span class="number">21</span> <span class="number">15</span>:<span class="number">11</span>:<span class="number">07</span> INFO mesos.CoarseMesosSchedulerBackend: Registered as framework ID <span class="number">20161121</span>-<span class="number">150645</span>-<span class="number">16777343</span>-<span class="number">5050</span>-<span class="number">14247</span>-<span class="number">0000</span></span><br><span class="line"><span class="number">16</span>/<span class="number">11</span>/<span class="number">21</span> <span class="number">15</span>:<span class="number">11</span>:<span class="number">08</span> INFO mesos.CoarseMesosSchedulerBackend: SchedulerBackend is ready <span class="keyword">for</span> scheduling beginning after reached minRegisteredResourcesRatio: <span class="number">0.0</span></span><br><span class="line"><span class="number">16</span>/<span class="number">11</span>/<span class="number">21</span> <span class="number">15</span>:<span class="number">11</span>:<span class="number">08</span> INFO mesos.CoarseMesosSchedulerBackend: Mesos task <span class="number">0</span> is now TASK_RUNNING</span><br><span class="line"><span class="number">16</span>/<span class="number">11</span>/<span class="number">21</span> <span class="number">15</span>:<span class="number">11</span>:<span class="number">12</span> INFO spark.SparkContext: Starting job: count at SimpleApp.scala:<span class="number">13</span></span><br><span class="line">Lines with a: <span class="number">1</span>, Lines with b: <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>或者将jar包上传到具有mesos环境的客户端节点，因为远程spark-mesos已经集成好了，所以可以不写–master</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/usr/install/spark/bin/spark-submit --<span class="keyword">class</span> <span class="string">"SimpleApp"</span> --master <span class="string">"mesos://192.168.6.52:5050"</span> \</span><br><span class="line">--conf spark.app.logfile=<span class="string">"/user/qihuang.zheng/hello.txt"</span> \</span><br><span class="line">simple-project_2<span class="number">.10</span>-<span class="number">1.0</span>.jar</span><br></pre></td></tr></table></figure>
<h3 id="问题">问题</h3><p>远程运行任务，并指定proxy-user</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">sudo -u admin /usr/install/spark/bin/spark-submit \</span><br><span class="line">--master mesos:<span class="comment">//zk://192.168.6.55:2181,192.168.6.56:2181,192.168.6.57:2181/mesos \</span></span><br><span class="line">--class cn<span class="class">.fraudmetrix</span><span class="class">.pontus</span><span class="class">.demo</span><span class="class">.SimpleApp</span> \</span><br><span class="line">--proxy-user qihuang<span class="class">.zheng</span> \</span><br><span class="line">--conf spark<span class="class">.app</span><span class="class">.logfile</span>=<span class="string">"/user/qihuang.zheng/hello.txt"</span> \</span><br><span class="line">/usr/install/pontus/pontus-spark-<span class="number">1.0</span>.<span class="number">0</span>-SNAPSHOT-fat<span class="class">.jar</span></span><br><span class="line"></span><br><span class="line">sudo -u admin /usr/install/spark/bin/spark-submit \</span><br><span class="line">--master <span class="string">"mesos://zk://192.168.6.55:2181,192.168.6.56:2181,192.168.6.57:2181/mesos"</span> \</span><br><span class="line">--proxy-user qihuang<span class="class">.zheng</span> \</span><br><span class="line">--class cn<span class="class">.fraudmetrix</span><span class="class">.pontus</span><span class="class">.cassandra</span><span class="class">.PontusCassandra2HDFSJobHandler</span> \</span><br><span class="line">--conf spark<span class="class">.app</span><span class="class">.logfile</span>=<span class="string">"/usr/install/spark/README.md"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.destTable</span>=<span class="string">""</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.tableId</span>=<span class="string">""</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.destType</span>=<span class="string">"HDFS"</span> \</span><br><span class="line">--conf spark<span class="class">.cores</span><span class="class">.max</span>=<span class="string">"10"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.execution</span><span class="class">.id</span>=<span class="string">"3"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.destUsername</span>=<span class="string">""</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.creator</span>=<span class="string">"qihuang.zheng"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.sourceUri</span>=<span class="string">"192.168.6.53/keyspace1"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.sourcePassword</span>=<span class="string">""</span> \</span><br><span class="line">--conf spark<span class="class">.mesos</span><span class="class">.role</span>=<span class="string">"test"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.sourceTable</span>=<span class="string">"standard1"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.sourceType</span>=<span class="string">"Cassandra"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.readMode</span>=<span class="string">"1"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.tableTs</span>=<span class="string">""</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.destUri</span>=<span class="string">"/user/tongdun/pontus/standard1"</span> \</span><br><span class="line">--conf spark<span class="class">.executor</span><span class="class">.memory</span>=<span class="string">"4g"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.name</span>=<span class="string">"cassandra-hdfs"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.destPassword</span>=<span class="string">""</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.id</span>=<span class="string">"2"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.writeMode</span>=<span class="string">"1"</span> \</span><br><span class="line">--conf spark<span class="class">.pontus</span><span class="class">.sourceUsername</span>=<span class="string">""</span> \</span><br><span class="line">/usr/install/pontus/pontus-spark-<span class="number">1.0</span>.<span class="number">0</span>-SNAPSHOT-fat.jar</span><br></pre></td></tr></table></figure>
<p>mesos kill framework, but won’t kill spark driver</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">curl</span> -XPOST <span class="url">http://192.168.6.52:5050/master/teardown</span> -d <span class="string">'frameworkId=475189a7-dcde-4859-9af8-6fc2e63be94e-0556'</span></span><br></pre></td></tr></table></figure>
<p>手动设置spark.app.id无效：</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/usr/install/spark/bin/spark-submit \</span></span><br><span class="line">-<span class="ruby">-master <span class="symbol">mesos:</span>/<span class="regexp">/zk:/</span><span class="regexp">/192.168.6.55:2181,192.168.6.56:2181,192.168.6.57:2181/mesos</span> \</span><br><span class="line"></span>-<span class="ruby">-<span class="class"><span class="keyword">class</span> <span class="title">cn</span>.<span class="title">fraudmetrix</span>.<span class="title">pontus</span>.<span class="title">demo</span>.<span class="title">SimpleApp</span> \</span></span><br><span class="line"></span>-<span class="ruby">-conf <span class="string">"spark.app.id=SimpleApp"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.app.logfile=<span class="string">"/user/qihuang.zheng/hello.txt"</span> \</span><br><span class="line"></span><span class="comment">/usr/install/pontus/pontus-spark-1.0.0-SNAPSHOT-fat.jar</span></span><br></pre></td></tr></table></figure>
<p>序列化不适配？？ spark_2.10和2.11的版本问题？？</p>
<p>play_2.5.9不支持scal_2.10: <a href="https://www.playframework.com/documentation/2.5.x/Migration25" target="_blank" rel="external">https://www.playframework.com/documentation/2.5.x/Migration25</a><br>Play 2.3 and 2.4 supported both Scala 2.10 and 2.11. Play 2.5 has dropped support for Scala 2.10 and now only supports Scala 2.11.  </p>
<p>Jira: <a href="https://issues.apache.org/jira/browse/SPARK-13956" target="_blank" rel="external">https://issues.apache.org/jira/browse/SPARK-13956</a><br>说的是driver版本和executor版本不一致导致的。</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">I1209 20:58:27.190430  3039 sched.cpp:703] Framework registered with 475189a7-dcde-4859-9af8-6fc2e63be94e-0423</span><br><span class="line">[info] p.<span class="keyword">m</span>.i.j.p.s.w.SparkJobWorker - Starting job -1651747420 future</span><br><span class="line">[Stage 0:&gt;                                                          (0 + 0) / 2][<span class="keyword">error</span>] o.a.s.<span class="keyword">n</span>.s.TransportRequestHandler - <span class="keyword">Error</span> <span class="keyword">while</span> invoking RpcHandler#receive() <span class="keyword">on</span> RPC id 8897300371223004861</span><br><span class="line">java.io.InvalidClassException: org.apache.spark.rpc.netty.RequestMessage; <span class="keyword">local</span> <span class="keyword">class</span> incompatible: stream classdesc serialVersionUID = -2221986757032131007, <span class="keyword">local</span> <span class="keyword">class</span> serialVersionUID = -5447855329526097695</span><br><span class="line">  at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)</span><br><span class="line">  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1623)</span><br><span class="line">  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)</span><br><span class="line">  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)</span><br><span class="line">  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)</span><br><span class="line">  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)</span><br><span class="line">  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.<span class="keyword">scala</span>:76)</span><br><span class="line">  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.<span class="keyword">scala</span>:109)</span><br><span class="line">  at org.apache.spark.rpc.netty.NettyRpcEnv$<span class="label">$anonfun</span><span class="label">$deserialize</span><span class="label">$1</span>$<span class="label">$anonfun</span><span class="label">$apply</span><span class="label">$1</span>.apply(NettyRpcEnv.<span class="keyword">scala</span>:258)</span><br><span class="line">  at <span class="keyword">scala</span>.util.DynamicVariable.withValue(DynamicVariable.<span class="keyword">scala</span>:58)</span><br><span class="line">[<span class="keyword">error</span>] o.a.s.<span class="keyword">n</span>.s.TransportRequestHandler - <span class="keyword">Error</span> <span class="keyword">while</span> invoking RpcHandler#receive() <span class="keyword">on</span> RPC id 9207561725902043069</span><br><span class="line">java.io.InvalidClassException: org.apache.spark.rpc.netty.RequestMessage; <span class="keyword">local</span> <span class="keyword">class</span> incompatible: stream classdesc serialVersionUID = -2221986757032131007, <span class="keyword">local</span> <span class="keyword">class</span> serialVersionUID = -5447855329526097695</span><br><span class="line">  at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)</span><br><span class="line">  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1623)</span><br><span class="line">  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)</span><br><span class="line">  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)</span><br><span class="line">  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)</span><br><span class="line">  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)</span><br><span class="line">  at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.<span class="keyword">scala</span>:76)</span><br><span class="line">  at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.<span class="keyword">scala</span>:109)</span><br><span class="line">  at org.apache.spark.rpc.netty.NettyRpcEnv$<span class="label">$anonfun</span><span class="label">$deserialize</span><span class="label">$1</span>$<span class="label">$anonfun</span><span class="label">$apply</span><span class="label">$1</span>.apply(NettyRpcEnv.<span class="keyword">scala</span>:258)</span><br><span class="line">  at <span class="keyword">scala</span>.util.DynamicVariable.withValue(DynamicVariable.<span class="keyword">scala</span>:58)</span><br><span class="line">[warn] o.a.s.s.TaskSchedulerImpl - Initial job has not accepted any resources; check your <span class="keyword">cluster</span> UI to ensure that workers are registered and have sufficient resources</span><br><span class="line">[<span class="keyword">error</span>] application -</span><br><span class="line"></span><br><span class="line">! @72b33a0kb - Internal server <span class="keyword">error</span>, <span class="keyword">for</span> (GET) [/spark] -&gt;</span><br><span class="line"></span><br><span class="line">play.api.UnexpectedException: Unexpected exception[TimeoutException: Futures timed <span class="keyword">out</span> after [30 seconds]]</span><br><span class="line">  at play.api.http.HttpErrorHandlerExceptions$.throwableToUsefulException(HttpErrorHandler.<span class="keyword">scala</span>:276)</span><br><span class="line">  at play.api.http.DefaultHttpErrorHandler.onServerError(HttpErrorHandler.<span class="keyword">scala</span>:206)</span><br><span class="line">  at play.api.GlobalSettings<span class="label">$class</span>.onError(GlobalSettings.<span class="keyword">scala</span>:160)</span><br><span class="line">  at util.<span class="keyword">Global</span>$.onError(<span class="keyword">Global</span>.<span class="keyword">scala</span>:22)</span><br><span class="line">  at play.api.http.GlobalSettingsHttpErrorHandler.onServerError(HttpErrorHandler.<span class="keyword">scala</span>:98)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="label">$anonfun</span><span class="label">$2</span>$<span class="label">$anonfun</span><span class="label">$apply</span><span class="label">$1</span>.applyOrElse(PlayRequestHandler.<span class="keyword">scala</span>:100)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="label">$anonfun</span><span class="label">$2</span>$<span class="label">$anonfun</span><span class="label">$apply</span><span class="label">$1</span>.applyOrElse(PlayRequestHandler.<span class="keyword">scala</span>:99)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.Future$<span class="label">$anonfun</span><span class="label">$recoverWith</span><span class="label">$1</span>.apply(Future.<span class="keyword">scala</span>:344)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.Future$<span class="label">$anonfun</span><span class="label">$recoverWith</span><span class="label">$1</span>.apply(Future.<span class="keyword">scala</span>:343)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.impl.CallbackRunnable.<span class="keyword">run</span>(Promise.<span class="keyword">scala</span>:32)</span><br><span class="line">Caused <span class="keyword">by</span>: java.util.concurrent.TimeoutException: Futures timed <span class="keyword">out</span> after [30 seconds]</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.impl.Promise<span class="label">$DefaultPromise</span>.ready(Promise.<span class="keyword">scala</span>:219)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.impl.Promise<span class="label">$DefaultPromise</span>.result(Promise.<span class="keyword">scala</span>:223)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.Await$<span class="label">$anonfun</span><span class="label">$result</span><span class="label">$1</span>.apply(package.<span class="keyword">scala</span>:190)</span><br><span class="line">  at akka.dispatch.MonitorableThreadFactory<span class="label">$AkkaForkJoinWorkerThread</span>$<span class="label">$anon</span><span class="label">$3</span>.block(ThreadPoolBuilder.<span class="keyword">scala</span>:167)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinPool.managedBlock(ForkJoinPool.java:3640)</span><br><span class="line">  at akka.dispatch.MonitorableThreadFactory<span class="label">$AkkaForkJoinWorkerThread</span>.blockOn(ThreadPoolBuilder.<span class="keyword">scala</span>:165)</span><br><span class="line">  at akka.dispatch.BatchingExecutor<span class="label">$BlockableBatch</span>.blockOn(BatchingExecutor.<span class="keyword">scala</span>:106)</span><br><span class="line">  at <span class="keyword">scala</span>.concurrent.Await$.result(package.<span class="keyword">scala</span>:190)</span><br><span class="line">  at controllers.SparkController$<span class="label">$anonfun</span><span class="label">$sparkJob</span><span class="label">$1</span>.apply(SparkController.<span class="keyword">scala</span>:56)</span><br><span class="line">  at controllers.SparkController$<span class="label">$anonfun</span><span class="label">$sparkJob</span><span class="label">$1</span>.apply(SparkController.<span class="keyword">scala</span>:42)</span><br><span class="line">[warn] o.a.s.s.TaskSchedulerImpl - Initial job has not accepted any resources; check your <span class="keyword">cluster</span> UI to ensure that workers are registered and have sufficient resources</span><br></pre></td></tr></table></figure>
<p>把spark-core打包方式改为provided是不行的。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java<span class="class">.lang</span><span class="class">.NoClassDefFoundError</span>: org/apache/spark/Logging</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.defineClass1</span>(Native Method)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.defineClass</span>(ClassLoader<span class="class">.java</span>:<span class="number">760</span>)</span><br><span class="line">  at java<span class="class">.security</span><span class="class">.SecureClassLoader</span><span class="class">.defineClass</span>(SecureClassLoader<span class="class">.java</span>:<span class="number">142</span>)</span><br><span class="line">  at java<span class="class">.net</span><span class="class">.URLClassLoader</span><span class="class">.defineClass</span>(URLClassLoader<span class="class">.java</span>:<span class="number">467</span>)</span><br><span class="line">  at java<span class="class">.net</span><span class="class">.URLClassLoader</span><span class="class">.access</span>$<span class="number">100</span>(URLClassLoader<span class="class">.java</span>:<span class="number">73</span>)</span><br><span class="line">  at java<span class="class">.net</span><span class="class">.URLClassLoader</span>$<span class="number">1</span>.<span class="function"><span class="title">run</span><span class="params">(URLClassLoader.java:<span class="number">368</span>)</span></span></span><br><span class="line">  at java<span class="class">.net</span><span class="class">.URLClassLoader</span>$<span class="number">1</span>.<span class="function"><span class="title">run</span><span class="params">(URLClassLoader.java:<span class="number">362</span>)</span></span></span><br><span class="line">  at java<span class="class">.security</span><span class="class">.AccessController</span><span class="class">.doPrivileged</span>(Native Method)</span><br><span class="line">  at java<span class="class">.net</span><span class="class">.URLClassLoader</span><span class="class">.findClass</span>(URLClassLoader<span class="class">.java</span>:<span class="number">361</span>)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.loadClass</span>(ClassLoader<span class="class">.java</span>:<span class="number">424</span>)</span><br><span class="line">  at sun<span class="class">.misc</span><span class="class">.Launcher</span><span class="variable">$AppClassLoader</span>.<span class="function"><span class="title">loadClass</span><span class="params">(Launcher.java:<span class="number">331</span>)</span></span></span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.loadClass</span>(ClassLoader<span class="class">.java</span>:<span class="number">357</span>)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.Class</span><span class="class">.getDeclaredConstructors0</span>(Native Method)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.Class</span><span class="class">.privateGetDeclaredConstructors</span>(Class<span class="class">.java</span>:<span class="number">2671</span>)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.Class</span><span class="class">.getDeclaredConstructors</span>(Class<span class="class">.java</span>:<span class="number">2020</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.spi</span><span class="class">.InjectionPoint</span><span class="class">.forConstructorOf</span>(InjectionPoint<span class="class">.java</span>:<span class="number">245</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorBindingImpl</span><span class="class">.create</span>(ConstructorBindingImpl<span class="class">.java</span>:<span class="number">99</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.createUninitializedBinding</span>(InjectorImpl<span class="class">.java</span>:<span class="number">658</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.createJustInTimeBinding</span>(InjectorImpl<span class="class">.java</span>:<span class="number">882</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.createJustInTimeBindingRecursive</span>(InjectorImpl<span class="class">.java</span>:<span class="number">805</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.getJustInTimeBinding</span>(InjectorImpl<span class="class">.java</span>:<span class="number">282</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.getBindingOrThrow</span>(InjectorImpl<span class="class">.java</span>:<span class="number">214</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.getProviderOrThrow</span>(InjectorImpl<span class="class">.java</span>:<span class="number">1006</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.getProvider</span>(InjectorImpl<span class="class">.java</span>:<span class="number">1038</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.getProvider</span>(InjectorImpl<span class="class">.java</span>:<span class="number">1001</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.getInstance</span>(InjectorImpl<span class="class">.java</span>:<span class="number">1051</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.guice</span><span class="class">.GuiceInjector</span><span class="class">.instanceOf</span>(GuiceInjectorBuilder<span class="class">.scala</span>:<span class="number">405</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.RoutesProvider</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>.<span class="function"><span class="title">apply</span><span class="params">(BuiltinModule.scala:<span class="number">82</span>)</span></span></span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.RoutesProvider</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>.<span class="function"><span class="title">apply</span><span class="params">(BuiltinModule.scala:<span class="number">82</span>)</span></span></span><br><span class="line">  at scala<span class="class">.Option</span><span class="class">.fold</span>(Option<span class="class">.scala</span>:<span class="number">158</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.RoutesProvider</span><span class="class">.get</span><span class="variable">$lzycompute</span>(BuiltinModule<span class="class">.scala</span>:<span class="number">82</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.RoutesProvider</span><span class="class">.get</span>(BuiltinModule<span class="class">.scala</span>:<span class="number">78</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.RoutesProvider</span><span class="class">.get</span>(BuiltinModule<span class="class">.scala</span>:<span class="number">77</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ProviderInternalFactory</span><span class="class">.provision</span>(ProviderInternalFactory<span class="class">.java</span>:<span class="number">81</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.BoundProviderFactory</span><span class="class">.provision</span>(BoundProviderFactory<span class="class">.java</span>:<span class="number">72</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ProviderInternalFactory</span><span class="class">.circularGet</span>(ProviderInternalFactory<span class="class">.java</span>:<span class="number">61</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.BoundProviderFactory</span><span class="class">.get</span>(BoundProviderFactory<span class="class">.java</span>:<span class="number">62</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.SingleParameterInjector</span><span class="class">.inject</span>(SingleParameterInjector<span class="class">.java</span>:<span class="number">38</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.SingleParameterInjector</span><span class="class">.getAll</span>(SingleParameterInjector<span class="class">.java</span>:<span class="number">62</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorInjector</span><span class="class">.provision</span>(ConstructorInjector<span class="class">.java</span>:<span class="number">104</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorInjector</span><span class="class">.construct</span>(ConstructorInjector<span class="class">.java</span>:<span class="number">85</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorBindingImpl</span><span class="variable">$Factory</span>.<span class="function"><span class="title">get</span><span class="params">(ConstructorBindingImpl.java:<span class="number">267</span>)</span></span></span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.FactoryProxy</span><span class="class">.get</span>(FactoryProxy<span class="class">.java</span>:<span class="number">56</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.SingleParameterInjector</span><span class="class">.inject</span>(SingleParameterInjector<span class="class">.java</span>:<span class="number">38</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.SingleParameterInjector</span><span class="class">.getAll</span>(SingleParameterInjector<span class="class">.java</span>:<span class="number">62</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorInjector</span><span class="class">.provision</span>(ConstructorInjector<span class="class">.java</span>:<span class="number">104</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorInjector</span><span class="class">.construct</span>(ConstructorInjector<span class="class">.java</span>:<span class="number">85</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ConstructorBindingImpl</span><span class="variable">$Factory</span>.<span class="function"><span class="title">get</span><span class="params">(ConstructorBindingImpl.java:<span class="number">267</span>)</span></span></span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ProviderToInternalFactoryAdapter</span>$<span class="number">1</span>.<span class="function"><span class="title">call</span><span class="params">(ProviderToInternalFactoryAdapter.java:<span class="number">46</span>)</span></span></span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.callInContext</span>(InjectorImpl<span class="class">.java</span>:<span class="number">1103</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.ProviderToInternalFactoryAdapter</span><span class="class">.get</span>(ProviderToInternalFactoryAdapter<span class="class">.java</span>:<span class="number">40</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.SingletonScope</span>$<span class="number">1</span>.<span class="function"><span class="title">get</span><span class="params">(SingletonScope.java:<span class="number">145</span>)</span></span></span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InternalFactoryToProviderAdapter</span><span class="class">.get</span>(InternalFactoryToProviderAdapter<span class="class">.java</span>:<span class="number">41</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.FactoryProxy</span><span class="class">.get</span>(FactoryProxy<span class="class">.java</span>:<span class="number">56</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InternalInjectorCreator</span>$<span class="number">1</span>.<span class="function"><span class="title">call</span><span class="params">(InternalInjectorCreator.java:<span class="number">205</span>)</span></span></span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InternalInjectorCreator</span>$<span class="number">1</span>.<span class="function"><span class="title">call</span><span class="params">(InternalInjectorCreator.java:<span class="number">199</span>)</span></span></span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InjectorImpl</span><span class="class">.callInContext</span>(InjectorImpl<span class="class">.java</span>:<span class="number">1092</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InternalInjectorCreator</span><span class="class">.loadEagerSingletons</span>(InternalInjectorCreator<span class="class">.java</span>:<span class="number">199</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InternalInjectorCreator</span><span class="class">.injectDynamically</span>(InternalInjectorCreator<span class="class">.java</span>:<span class="number">180</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.internal</span><span class="class">.InternalInjectorCreator</span><span class="class">.build</span>(InternalInjectorCreator<span class="class">.java</span>:<span class="number">110</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.Guice</span><span class="class">.createInjector</span>(Guice<span class="class">.java</span>:<span class="number">96</span>)</span><br><span class="line">  at com<span class="class">.google</span><span class="class">.inject</span><span class="class">.Guice</span><span class="class">.createInjector</span>(Guice<span class="class">.java</span>:<span class="number">84</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.guice</span><span class="class">.GuiceBuilder</span><span class="class">.injector</span>(GuiceInjectorBuilder<span class="class">.scala</span>:<span class="number">181</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.guice</span><span class="class">.GuiceApplicationBuilder</span><span class="class">.build</span>(GuiceApplicationBuilder<span class="class">.scala</span>:<span class="number">123</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.inject</span><span class="class">.guice</span><span class="class">.GuiceApplicationLoader</span><span class="class">.load</span>(GuiceApplicationLoader<span class="class">.scala</span>:<span class="number">21</span>)</span><br><span class="line">  at play<span class="class">.core</span><span class="class">.server</span><span class="class">.ProdServerStart</span>$.<span class="function"><span class="title">start</span><span class="params">(ProdServerStart.scala:<span class="number">47</span>)</span></span></span><br><span class="line">  at play<span class="class">.core</span><span class="class">.server</span><span class="class">.ProdServerStart</span>$.<span class="function"><span class="title">main</span><span class="params">(ProdServerStart.scala:<span class="number">22</span>)</span></span></span><br><span class="line">  at play<span class="class">.core</span><span class="class">.server</span><span class="class">.ProdServerStart</span><span class="class">.main</span>(ProdServerStart.scala)</span><br><span class="line">Caused by: java<span class="class">.lang</span><span class="class">.ClassNotFoundException</span>: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.Logging</span></span><br><span class="line">  at java<span class="class">.net</span><span class="class">.URLClassLoader</span><span class="class">.findClass</span>(URLClassLoader<span class="class">.java</span>:<span class="number">381</span>)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.loadClass</span>(ClassLoader<span class="class">.java</span>:<span class="number">424</span>)</span><br><span class="line">  at sun<span class="class">.misc</span><span class="class">.Launcher</span><span class="variable">$AppClassLoader</span>.<span class="function"><span class="title">loadClass</span><span class="params">(Launcher.java:<span class="number">331</span>)</span></span></span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.ClassLoader</span><span class="class">.loadClass</span>(ClassLoader<span class="class">.java</span>:<span class="number">357</span>)</span><br><span class="line">  ... <span class="number">68</span> more</span><br></pre></td></tr></table></figure>
<p>在pontus-web中直接运行Spark作业</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">! @<span class="number">72</span>ck129fb - Internal server error, <span class="keyword">for</span> (GET) [/preview/HDFS/activity] -&gt;</span><br><span class="line"></span><br><span class="line">play.api.UnexpectedException: Unexpected exception[RuntimeException: java.lang.NoSuchMethodError: akka.actor.LocalActorRefProvider.log()Lakka/event/LoggingAdapter;]</span><br><span class="line">  at play.api.http.HttpErrorHandlerExceptions$.throwableToUsefulException(HttpErrorHandler.scala:<span class="number">289</span>)</span><br><span class="line">  at play.api.http.DefaultHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">220</span>)</span><br><span class="line">  at play.api.GlobalSettings<span class="variable">$class</span>.onError(GlobalSettings.scala:<span class="number">160</span>)</span><br><span class="line">  at util.Global$.onError(Global.scala:<span class="number">20</span>)</span><br><span class="line">  at play.api.http.GlobalSettingsHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">99</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">344</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">343</span>)</span><br><span class="line">  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:<span class="number">32</span>)</span><br><span class="line">Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodError: akka.actor.LocalActorRefProvider.log()Lakka/event/LoggingAdapter;</span><br><span class="line">  at play.api.mvc.ActionBuilder$<span class="variable">$anon</span><span class="variable">$2</span>.apply(Action.scala:<span class="number">463</span>)</span><br><span class="line">  at play.api.mvc.Action$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$5</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$6</span>.apply(Action.scala:<span class="number">112</span>)</span><br><span class="line">  at play.api.mvc.Action$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$5</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$6</span>.apply(Action.scala:<span class="number">112</span>)</span><br><span class="line">  at play.utils.Threads$.withContextClassLoader(Threads.scala:<span class="number">21</span>)</span><br><span class="line">  at play.api.mvc.Action$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$5</span>.apply(Action.scala:<span class="number">111</span>)</span><br><span class="line">  at play.api.mvc.Action$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$5</span>.apply(Action.scala:<span class="number">110</span>)</span><br><span class="line">  at scala.Option.<span class="keyword">map</span>(Option.scala:<span class="number">146</span>)</span><br><span class="line">  at play.api.mvc.Action$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>.apply(Action.scala:<span class="number">110</span>)</span><br><span class="line">  at play.api.mvc.Action$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>.apply(Action.scala:<span class="number">103</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$flatMap</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">251</span>)</span><br><span class="line">Caused by: java.lang.NoSuchMethodError: akka.actor.LocalActorRefProvider.log()Lakka/event/LoggingAdapter;</span><br><span class="line">  at akka.remote.RemoteActorRefProvider.&lt;init&gt;(RemoteActorRefProvider.scala:<span class="number">128</span>)</span><br><span class="line">  at sun.reflect.NativeConstructorAccessorImpl.newInstance<span class="number">0</span>(Native Method)</span><br><span class="line">  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:<span class="number">62</span>)</span><br><span class="line">  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:<span class="number">45</span>)</span><br><span class="line">  at java.lang.reflect.Constructor.newInstance(Constructor.java:<span class="number">422</span>)</span><br><span class="line">  at akka.actor.ReflectiveDynamicAccess$<span class="variable">$anonfun</span><span class="variable">$createInstanceFor</span><span class="variable">$2</span>.apply(ReflectiveDynamicAccess.scala:<span class="number">32</span>)</span><br><span class="line">  at scala.util.Try$.apply(Try.scala:<span class="number">192</span>)</span><br><span class="line">  at akka.actor.ReflectiveDynamicAccess.createInstanceFor(ReflectiveDynamicAccess.scala:<span class="number">27</span>)</span><br><span class="line">  at akka.actor.ReflectiveDynamicAccess$<span class="variable">$anonfun</span><span class="variable">$createInstanceFor</span><span class="variable">$3</span>.apply(ReflectiveDynamicAccess.scala:<span class="number">38</span>)</span><br><span class="line">  at akka.actor.ReflectiveDynamicAccess$<span class="variable">$anonfun</span><span class="variable">$createInstanceFor</span><span class="variable">$3</span>.apply(ReflectiveDynamicAccess.scala:<span class="number">38</span>)</span><br></pre></td></tr></table></figure>
<p><a href="http://stackoverflow.com/questions/40883978/migration-to-play-2-5-leads-to-this-error-nosuchmethoderror-akka-actor-locala" target="_blank" rel="external">http://stackoverflow.com/questions/40883978/migration-to-play-2-5-leads-to-this-error-nosuchmethoderror-akka-actor-locala</a>  </p>
<p>监听器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ssc.sparkContext.addSparkListener(new SparkListener()&#123;</span><br><span class="line">  override <span class="function"><span class="keyword">def</span> <span class="title">onJobEnd</span><span class="params">(jobEnd: SparkListenerJobEnd)</span>:</span> Unit = &#123;&#125;</span><br><span class="line">  override <span class="function"><span class="keyword">def</span> <span class="title">onApplicationEnd</span><span class="params">(applicationEnd: SparkListenerApplicationEnd)</span>:</span> Unit = &#123;</span><br><span class="line">    println(<span class="string">"PontusKafka2HDFSJobHandler onApplicationEnd...."</span>)</span><br><span class="line">    PontusExecutionDao.updateStatus(conf)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>使用命令行测试，当关闭程序后，会打印onApplicationEnd事件。在mesos上使用kill driver直接杀死应用程序，也会调用监听器的方法</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@716: Client environment:host.name=dp0653</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@723: Client environment:os.name=Linux</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@724: Client environment:os.arch=<span class="number">3.10.97-1</span>.el6.elrepo.x86_64</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@725: Client environment:os.version=#1 SMP Sat Feb <span class="number">20 11:55:29</span> EST 2016</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@733: Client environment:user.name=qihuang.zheng</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@741: Client environment:user.home=/home/qihuang.zheng</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@log_env@753: Client environment:user.dir=/home/qihuang.zheng</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">4:41791(0</span>x<span class="number">2b1d28803</span>700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=<span class="number">192.168.6.55</span>:<span class="number">2181,192.168</span>.<span class="number">6.56:2181</span>,<span class="number">192.168.6.57</span>:2181 sessionTimeout=10000 watcher=<span class="number">0x2b1d00</span>0e1645 sessionId=0 sessionPasswd=&lt;null&gt; context=<span class="number">0x2b1d44</span>000960 flags=0</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,53</span><span class="number">5:41791(0</span>x<span class="number">2b1d2950</span>a700):ZOO_INFO@check_events@1703: initiated connection to server [<span class="number">192.168.6.57</span>:2181]</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.536334</span> 42031 sched.cpp:222] Version: 0.28.2</span><br><span class="line"><span class="number">2016-12-13</span> <span class="number">11:34:45,55</span><span class="number">5:41791(0</span>x<span class="number">2b1d2950</span>a700):ZOO_INFO@check_events@1750: session establishment complete on server [<span class="number">192.168.6.57</span>:2181], sessionId=<span class="number">0x3589e76</span>ef2f82e0, negotiated timeout=10000</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.556421</span> 42015 group.cpp:349] Group process (group(1)@<span class="number">192.168.6.53</span>:45534) connected to ZooKeeper</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.556823</span> 42015 group.cpp:831] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.556903</span> 42015 group.cpp:427] Trying to create path '/mesos' in ZooKeeper</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.561131 42</span>008 detector.cpp:152] Detected a new leader: (id='70')</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.561971</span> 42011 group.cpp:700] Trying to get '/mesos/json.info_<span class="number">0000000070</span>' in ZooKeeper</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.565317</span> 42023 detector.cpp:479] <span class="keyword">A</span> new leading master (UPID=master@<span class="number">192.168.6.52</span>:5050) is detected</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.565773</span> 42026 sched.cpp:326] New master detected at master@<span class="number">192.168.6.52</span>:5050</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.567448</span> 42026 sched.cpp:336] No credentials provided. Attempting to register without authentication</span><br><span class="line">I<span class="number">1213 11:34</span>:<span class="number">45.574049</span> 42026 sched.cpp:703] Framework registered with <span class="number">475189a7</span>-dcde-4859-9af8-6fc2e63be94e-0558</span><br><span class="line"></span><br><span class="line">^CPontusKafka2HDFSJobHandler onApplicationEnd....</span><br><span class="line">I<span class="number">1213 11:42</span>:<span class="number">21.526136</span>  4384 sched.cpp:1911] Asked to stop the driver</span><br><span class="line">I<span class="number">1213 11:42</span>:<span class="number">21.526562</span> 42020 sched.cpp:1143] Stopping framework '<span class="number">475189a7</span>-dcde-4859-9af8-6fc2e63be94e-0558'</span><br></pre></td></tr></table></figure>
<h2 id="Spark_Cassandra">Spark Cassandra</h2><p>IDE中直接运行，spark依赖包不能为provided，否则找不到Spark的相关包，运行正常</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-core_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-sql_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-hive_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>使用maven-assembly-plugin插件打包运行，spark环境使用本地安装，scope=provide</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-core_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-sql_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-hive_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>打包，并用完整的依赖包执行spark-submit</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mvn clean <span class="keyword">package</span></span><br><span class="line"></span><br><span class="line"><span class="regexp">/Users/</span>zhengqh<span class="regexp">/Soft/</span>spark-<span class="number">1.6</span>.<span class="number">2</span>-bin-hadoop2.<span class="number">6</span><span class="regexp">/bin/</span>spark-submit \</span><br><span class="line">--master <span class="string">"local[2]"</span> \</span><br><span class="line">--<span class="keyword">class</span> cn.fraudmetrix.pontus.demo.CassandraReadWrite \</span><br><span class="line"><span class="regexp">/Users/</span>zhengqh<span class="regexp">/Github/</span>vulcan<span class="regexp">/pontus-spark/</span>target<span class="regexp">/pontus-spark-1.0.0-SNAPSHOT-jar-with-dependencies.jar</span></span><br></pre></td></tr></table></figure>
<p>报错guava版本不对，即使手动加上guava版本也报错：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com<span class="class">.google</span><span class="class">.guava</span>&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;guava&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;<span class="number">19.0</span>&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java<span class="class">.lang</span><span class="class">.ExceptionInInitializerError</span></span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.DefaultConnectionFactory</span>$.<span class="function"><span class="title">clusterBuilder</span><span class="params">(CassandraConnectionFactory.scala:<span class="number">35</span>)</span></span></span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.DefaultConnectionFactory</span>$.<span class="function"><span class="title">createCluster</span><span class="params">(CassandraConnectionFactory.scala:<span class="number">87</span>)</span></span></span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.CassandraConnector</span>$.com<span class="variable">$datastax</span><span class="variable">$spark</span><span class="variable">$connector</span><span class="variable">$cql</span><span class="variable">$CassandraConnector</span>$<span class="variable">$createSession</span>(CassandraConnector<span class="class">.scala</span>:<span class="number">153</span>)</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.CassandraConnector</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>.<span class="function"><span class="title">apply</span><span class="params">(CassandraConnector.scala:<span class="number">148</span>)</span></span></span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.CassandraConnector</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>.<span class="function"><span class="title">apply</span><span class="params">(CassandraConnector.scala:<span class="number">148</span>)</span></span></span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.RefCountedCache</span><span class="class">.createNewValueAndKeys</span>(RefCountedCache<span class="class">.scala</span>:<span class="number">31</span>)</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.RefCountedCache</span><span class="class">.acquire</span>(RefCountedCache<span class="class">.scala</span>:<span class="number">56</span>)</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.CassandraConnector</span><span class="class">.openSession</span>(CassandraConnector<span class="class">.scala</span>:<span class="number">81</span>)</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.spark</span><span class="class">.connector</span><span class="class">.cql</span><span class="class">.CassandraConnector</span><span class="class">.withSessionDo</span>(CassandraConnector<span class="class">.scala</span>:<span class="number">109</span>)</span><br><span class="line">  at cn<span class="class">.fraudmetrix</span><span class="class">.pontus</span><span class="class">.demo</span><span class="class">.CassandraReadWrite</span>$.cn<span class="variable">$fraudmetrix</span><span class="variable">$pontus</span><span class="variable">$demo</span><span class="variable">$CassandraReadWrite</span>$<span class="variable">$executeCommand</span>(CassandraReadWrite<span class="class">.scala</span>:<span class="number">45</span>)</span><br><span class="line">  at cn<span class="class">.fraudmetrix</span><span class="class">.pontus</span><span class="class">.demo</span><span class="class">.CassandraReadWrite</span><span class="variable">$delayedInit</span><span class="variable">$body</span>.<span class="function"><span class="title">apply</span><span class="params">(CassandraReadWrite.scala:<span class="number">53</span>)</span></span></span><br><span class="line">  at scala.Function0<span class="variable">$class</span>.apply<span class="variable">$mcV</span><span class="variable">$sp</span>(Function0<span class="class">.scala</span>:<span class="number">40</span>)</span><br><span class="line">  at scala<span class="class">.runtime</span><span class="class">.AbstractFunction0</span><span class="class">.apply</span><span class="variable">$mcV</span><span class="variable">$sp</span>(AbstractFunction0<span class="class">.scala</span>:<span class="number">12</span>)</span><br><span class="line">  at scala.App$<span class="variable">$anonfun</span><span class="variable">$main</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(App.scala:<span class="number">71</span>)</span></span></span><br><span class="line">  at scala.App$<span class="variable">$anonfun</span><span class="variable">$main</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(App.scala:<span class="number">71</span>)</span></span></span><br><span class="line">  at scala<span class="class">.collection</span><span class="class">.immutable</span><span class="class">.List</span><span class="class">.foreach</span>(List<span class="class">.scala</span>:<span class="number">318</span>)</span><br><span class="line">  at scala<span class="class">.collection</span><span class="class">.generic</span><span class="class">.TraversableForwarder</span><span class="variable">$class</span>.<span class="function"><span class="title">foreach</span><span class="params">(TraversableForwarder.scala:<span class="number">32</span>)</span></span></span><br><span class="line">  at scala.App<span class="variable">$class</span>.<span class="function"><span class="title">main</span><span class="params">(App.scala:<span class="number">71</span>)</span></span></span><br><span class="line">  at cn<span class="class">.fraudmetrix</span><span class="class">.pontus</span><span class="class">.demo</span><span class="class">.CassandraReadWrite</span>$.<span class="function"><span class="title">main</span><span class="params">(CassandraReadWrite.scala:<span class="number">36</span>)</span></span></span><br><span class="line">  at cn<span class="class">.fraudmetrix</span><span class="class">.pontus</span><span class="class">.demo</span><span class="class">.CassandraReadWrite</span><span class="class">.main</span>(CassandraReadWrite.scala)</span><br><span class="line">  at sun<span class="class">.reflect</span><span class="class">.NativeMethodAccessorImpl</span><span class="class">.invoke0</span>(Native Method)</span><br><span class="line">  at sun<span class="class">.reflect</span><span class="class">.NativeMethodAccessorImpl</span><span class="class">.invoke</span>(NativeMethodAccessorImpl<span class="class">.java</span>:<span class="number">62</span>)</span><br><span class="line">  at sun<span class="class">.reflect</span><span class="class">.DelegatingMethodAccessorImpl</span><span class="class">.invoke</span>(DelegatingMethodAccessorImpl<span class="class">.java</span>:<span class="number">43</span>)</span><br><span class="line">  at java<span class="class">.lang</span><span class="class">.reflect</span><span class="class">.Method</span><span class="class">.invoke</span>(Method<span class="class">.java</span>:<span class="number">497</span>)</span><br><span class="line">  at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.org<span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$deploy</span><span class="variable">$SparkSubmit</span>$<span class="variable">$runMain</span>(SparkSubmit<span class="class">.scala</span>:<span class="number">731</span>)</span><br><span class="line">  at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.doRunMain$<span class="number">1</span>(SparkSubmit<span class="class">.scala</span>:<span class="number">181</span>)</span><br><span class="line">  at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.<span class="function"><span class="title">submit</span><span class="params">(SparkSubmit.scala:<span class="number">206</span>)</span></span></span><br><span class="line">  at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.<span class="function"><span class="title">main</span><span class="params">(SparkSubmit.scala:<span class="number">121</span>)</span></span></span><br><span class="line">  at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span><span class="class">.main</span>(SparkSubmit.scala)</span><br><span class="line">Caused by: java<span class="class">.lang</span><span class="class">.IllegalStateException</span>: Detected Guava issue <span class="hexcolor">#163</span>5 which indicates that <span class="tag">a</span> version of Guava less than <span class="number">16.01</span> is <span class="keyword">in</span> use.  This introduces codec resolution issues and potentially other incompatibility issues <span class="keyword">in</span> the driver.  Please upgrade to Guava <span class="number">16.01</span> or later.</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.driver</span><span class="class">.core</span><span class="class">.SanityChecks</span><span class="class">.checkGuava</span>(SanityChecks<span class="class">.java</span>:<span class="number">62</span>)</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.driver</span><span class="class">.core</span><span class="class">.SanityChecks</span><span class="class">.check</span>(SanityChecks<span class="class">.java</span>:<span class="number">36</span>)</span><br><span class="line">  at com<span class="class">.datastax</span><span class="class">.driver</span><span class="class">.core</span><span class="class">.Cluster</span>.&lt;clinit&gt;(Cluster<span class="class">.java</span>:<span class="number">67</span>)</span><br><span class="line">  ... <span class="number">29</span> more</span><br></pre></td></tr></table></figure>
<p>参考：<a href="http://stackoverflow.com/questions/36877897/detected-guava-issue-1635-which-indicates-that-a-version-of-guava-less-than-16" target="_blank" rel="external">http://stackoverflow.com/questions/36877897/detected-guava-issue-1635-which-indicates-that-a-version-of-guava-less-than-16</a><br>添加maven-shade-plugin插件，重新打包，除了其他包，还会生成fat包，并用fat包运行，最后正常</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ll target</span><br><span class="line">-rw-r--r--  <span class="number">1</span> zhengqh  staff    <span class="number">14</span>M <span class="number">11</span> <span class="number">24</span> <span class="number">19</span>:<span class="number">17</span> pontus-spark-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT-fat.jar</span><br><span class="line">-rw-r--r--  <span class="number">1</span> zhengqh  staff    <span class="number">19</span>M <span class="number">11</span> <span class="number">24</span> <span class="number">19</span>:<span class="number">15</span> pontus-spark-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line">-rw-r--r--  <span class="number">1</span> zhengqh  staff    <span class="number">14</span>K <span class="number">11</span> <span class="number">24</span> <span class="number">19</span>:<span class="number">15</span> pontus-spark-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT-sources.jar</span><br><span class="line">-rw-r--r--  <span class="number">1</span> zhengqh  staff    <span class="number">94</span>K <span class="number">11</span> <span class="number">24</span> <span class="number">19</span>:<span class="number">15</span> pontus-spark-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT.jar</span><br><span class="line"></span><br><span class="line">/Users/zhengqh/Soft/spark-<span class="number">1.6</span><span class="number">.2</span>-bin-hadoop2<span class="number">.6</span>/bin/spark-submit \</span><br><span class="line">--master <span class="string">"local[2]"</span> \</span><br><span class="line">--<span class="keyword">class</span> cn.fraudmetrix.pontus.demo.CassandraReadWrite \</span><br><span class="line">/Users/zhengqh/Github/vulcan/pontus-spark/target/pontus-spark-<span class="number">1.0</span><span class="number">.0</span>-SNAPSHOT-fat.jar</span><br></pre></td></tr></table></figure>
<p>测试读取Cassandra写入到HDFS是否正确（需要启动本地dfs和Cassandra）</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/Users/zhengqh/Soft/spark-1.6.2-bin-hadoop2.6/bin/spark-submit \</span></span><br><span class="line">-<span class="ruby">-master <span class="string">"local[2]"</span> \</span><br><span class="line"></span>-<span class="ruby">-<span class="class"><span class="keyword">class</span> <span class="title">cn</span>.<span class="title">fraudmetrix</span>.<span class="title">pontus</span>.<span class="title">cassandra</span>.<span class="title">PontusCassandra2HDFSJobHandler</span> \</span></span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.destTable=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.tableId=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.destType=<span class="string">"HDFS"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.destUsername=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.creator=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.sourceUri=<span class="string">"localhost/demo"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.sourcePassword=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.sourceTable=<span class="string">"sales"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.sourceType=<span class="string">"Cassandra"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.readMode=<span class="string">"1"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.tableTs=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.destUri=<span class="string">"hdfs://localhost:9000/test"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.name=<span class="string">"cassandra-hdfs"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.destPassword=<span class="string">""</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.id=<span class="string">"1"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.writeMode=<span class="string">"1"</span> \</span><br><span class="line"></span>-<span class="ruby">-conf spark.pontus.sourceUsername=<span class="string">""</span> \</span><br><span class="line"></span><span class="comment">/Users/zhengqh/Github/vulcan/pontus-spark/target/pontus-spark-1.0.0-SNAPSHOT-fat.jar</span></span><br><span class="line"></span><br><span class="line">16/11/24 19:19:16 INFO DAGScheduler: Job 0 finished: show at PontusCassandra2HDFSJobHandler.scala:42, took 4.044479 s</span><br><span class="line">+---------+--------+-----+</span><br><span class="line">|   center|products|total|</span><br><span class="line">+---------+--------+-----+</span><br><span class="line">|  Sevilla|      28| 3200|</span><br><span class="line">| Valencia|      23| 3300|</span><br><span class="line">|   Bilbao|      25| 2500|</span><br><span class="line">|   Madrid|      51| 7700|</span><br><span class="line">|Barcelona|      47| 6400|</span><br><span class="line">+---------+--------+-----+</span><br></pre></td></tr></table></figure>
<p>上面的Spark作业会将Parquet文件写入HDFS的根路径，可以用spark-shell测试读取parquet文件。</p>
<p>Spark作业执行页面：</p>
<p><img src="http://img.blog.csdn.net/20161125100527204" alt="pontus-spark"></p>
<p>Pontus作业执行成功页面：</p>
<p><img src="http://img.blog.csdn.net/20161125100553908" alt="pontus-job"></p>
<p>HDFS追加作业：</p>
<p><img src="http://img.blog.csdn.net/20161129174641219" alt="hdfs"></p>
<p>空值问题：解决办法：Option</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">play.api.http.HttpErrorHandlerExceptions$<span class="variable">$anon</span><span class="variable">$1</span>: Execution exception[[RuntimeException: ColumnName(PONTUS_EXECUTION.LOGOUT,Some(LOGOUT))]]</span><br><span class="line">  at play.api.http.HttpErrorHandlerExceptions$.throwableToUsefulException(HttpErrorHandler.scala:<span class="number">293</span>)</span><br><span class="line">  at play.api.http.DefaultHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">220</span>)</span><br><span class="line">  at play.api.GlobalSettings<span class="variable">$class</span>.onError(GlobalSettings.scala:<span class="number">160</span>)</span><br><span class="line">  at util.Global$.onError(Global.scala:<span class="number">8</span>)</span><br><span class="line">  at play.api.http.GlobalSettingsHttpErrorHandler.onServerError(HttpErrorHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">100</span>)</span><br><span class="line">  at play.core.server.netty.PlayRequestHandler$<span class="variable">$anonfun</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$1</span>.applyOrElse(PlayRequestHandler.scala:<span class="number">99</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">344</span>)</span><br><span class="line">  at scala.concurrent.Future$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span><span class="variable">$1</span>.apply(Future.scala:<span class="number">343</span>)</span><br><span class="line">  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:<span class="number">32</span>)</span><br><span class="line">Caused by: java.lang.RuntimeException: ColumnName(PONTUS_EXECUTION.LOGOUT,Some(LOGOUT))</span><br><span class="line">  at scala.sys.package$.error(package.scala:<span class="number">27</span>)</span><br><span class="line">  at anorm.SqlRequestError<span class="variable">$class</span>.toFailure(Anorm.scala:<span class="number">20</span>)</span><br><span class="line">  at anorm.UnexpectedNullableFound.toFailure(Anorm.scala:<span class="number">37</span>)</span><br><span class="line">  at anorm.Sql$<span class="variable">$anonfun</span><span class="variable">$asTry</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$7</span>.apply(Anorm.scala:<span class="number">303</span>)</span><br><span class="line">  at anorm.Sql$<span class="variable">$anonfun</span><span class="variable">$asTry</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$7</span>.apply(Anorm.scala:<span class="number">303</span>)</span><br><span class="line">  at anorm.SqlResult<span class="variable">$class</span>.fold(SqlResult.scala:<span class="number">23</span>)</span><br><span class="line">  at anorm.Error.fold(SqlResult.scala:<span class="number">31</span>)</span><br><span class="line">  at anorm.Sql$<span class="variable">$anonfun</span><span class="variable">$asTry</span><span class="variable">$2</span>.apply(Anorm.scala:<span class="number">303</span>)</span><br><span class="line">  at anorm.Sql$<span class="variable">$anonfun</span><span class="variable">$asTry</span><span class="variable">$2</span>.apply(Anorm.scala:<span class="number">303</span>)</span><br><span class="line">  at scala.util.Success.flatMap(Try.scala:<span class="number">231</span>)</span><br></pre></td></tr></table></figure>
<h2 id="预览">预览</h2><p><a href="http://192.168.6.53:9000/preview/HDFS/%2Fuser%2Ftongdun%2Fpontus%2Fantifraud%2Ffeedback%2Fyear=2016%2Fmonth=12%2Fday=15" target="_blank" rel="external">http://192.168.6.53:9000/preview/HDFS/%2Fuser%2Ftongdun%2Fpontus%2Fantifraud%2Ffeedback%2Fyear=2016%2Fmonth=12%2Fday=15</a></p>
<p><a href="http://192.168.6.53:9000/preview/HDFS/hdfs:%2F%2F192.168.6.52:9000%2Fuser%2Ftongdun%2Fpontus%2Fantifraud%2Ffeedback%2Fyear=2016%2Fmonth=12%2Fday=15" target="_blank" rel="external">http://192.168.6.53:9000/preview/HDFS/hdfs:%2F%2F192.168.6.52:9000%2Fuser%2Ftongdun%2Fpontus%2Fantifraud%2Ffeedback%2Fyear=2016%2Fmonth=12%2Fday=15</a></p>
<p><a href="http://192.168.6.53:9000/preview/HDFS/hdfs:%2F%2Ftdfs%2Fuser%2Ftongdun%2Fpontus%2Fantifraud%2Ffeedback%2Fyear=2016%2Fmonth=12%2Fday=15" target="_blank" rel="external">http://192.168.6.53:9000/preview/HDFS/hdfs:%2F%2Ftdfs%2Fuser%2Ftongdun%2Fpontus%2Fantifraud%2Ffeedback%2Fyear=2016%2Fmonth=12%2Fday=15</a></p>
<h2 id="Actor">Actor</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SparkJobActor <span class="function"><span class="title">preStart</span><span class="params">()</span></span>.............</span><br><span class="line"><span class="function"><span class="title">PontusSchedule</span><span class="params">(<span class="number">1</span>,<span class="number">1</span>,qihuang.zheng,<span class="number">2016</span>-<span class="number">11</span>-<span class="number">29</span> <span class="number">21</span>:<span class="number">10</span>:<span class="number">00.0</span>,<span class="number">5</span>d)</span></span></span><br><span class="line">[warn] o<span class="class">.a</span><span class="class">.h</span><span class="class">.u</span><span class="class">.NativeCodeLoader</span> - Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line">Global <span class="function"><span class="title">onStart</span><span class="params">()</span></span>.............</span><br><span class="line">mode:demo</span><br><span class="line"><span class="function"><span class="title">PontusSchedule</span><span class="params">(<span class="number">1</span>,<span class="number">1</span>,qihuang.zheng,<span class="number">2016</span>-<span class="number">11</span>-<span class="number">29</span> <span class="number">21</span>:<span class="number">10</span>:<span class="number">00.0</span>,<span class="number">5</span>d)</span></span></span><br><span class="line">[info] play<span class="class">.api</span><span class="class">.Play</span> - Application started (Dev)</span><br></pre></td></tr></table></figure>
<h2 id="Quartz">Quartz</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.JobBuilder</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.TriggerBuilder</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.SimpleScheduleBuilder</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.CronScheduleBuilder</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.CalendarIntervalScheduleBuilder</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.JobKey</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.TriggerKey</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.DateBuilder</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.impl</span><span class="class">.matchers</span><span class="class">.KeyMatcher</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.impl</span><span class="class">.matchers</span><span class="class">.GroupMatcher</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.impl</span><span class="class">.matchers</span><span class="class">.AndMatcher</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.impl</span><span class="class">.matchers</span><span class="class">.OrMatcher</span><span class="class">._</span></span><br><span class="line"><span class="tag">import</span> <span class="tag">org</span><span class="class">.quartz</span><span class="class">.impl</span><span class="class">.matchers</span><span class="class">.EverythingMatcher</span><span class="class">._</span></span><br></pre></td></tr></table></figure>
<h3 id="Job依赖注入">Job依赖注入</h3><p>Job注入其他实例报错无法实例化，那么如何注入DAO对象？</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">PontusSparkJob</span> @<span class="type">Inject</span><span class="container">()</span><span class="container">(<span class="title">configuration</span>: <span class="type">Configuration</span>,</span><br><span class="line">                               <span class="title">executionRepository</span>: <span class="type">ExecutionRepository</span>,</span><br><span class="line">                               <span class="title">scheduleRepository</span>: <span class="type">ScheduleRepository</span>,</span><br><span class="line">                               <span class="title">jobRepository</span>: <span class="type">JobRepository</span></span><br><span class="line">                              )</span> extends <span class="type">Job</span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[error] o.q.c.<span class="type">ErrorLogger</span> - <span class="type">An</span> error occured instantiating job to be executed. job= '<span class="type">Cassandra</span>.pontusjob_1'</span><br><span class="line">org.quartz.<span class="type">SchedulerException</span>: <span class="type">Problem</span> instantiating <span class="keyword">class</span> 'schedule.<span class="type">PontusSparkJob'</span></span><br><span class="line">  at org.quartz.simpl.<span class="type">SimpleJobFactory</span>.newJob<span class="container">(<span class="type">SimpleJobFactory</span>.<span class="title">java</span>:58)</span></span><br><span class="line">  at org.quartz.simpl.<span class="type">PropertySettingJobFactory</span>.newJob<span class="container">(<span class="type">PropertySettingJobFactory</span>.<span class="title">java</span>:69)</span></span><br><span class="line">  at org.quartz.core.<span class="type">JobRunShell</span>.initialize<span class="container">(<span class="type">JobRunShell</span>.<span class="title">java</span>:127)</span></span><br><span class="line">  at org.quartz.core.<span class="type">QuartzSchedulerThread</span>.run<span class="container">(<span class="type">QuartzSchedulerThread</span>.<span class="title">java</span>:375)</span></span><br><span class="line"><span class="type">Caused</span> by: java.lang.<span class="type">InstantiationException</span>: schedule.<span class="type">PontusSparkJob</span></span><br><span class="line">  at java.lang.<span class="type">Class</span>.newInstance<span class="container">(<span class="type">Class</span>.<span class="title">java</span>:427)</span></span><br><span class="line">  at org.quartz.simpl.<span class="type">SimpleJobFactory</span>.newJob<span class="container">(<span class="type">SimpleJobFactory</span>.<span class="title">java</span>:56)</span></span><br><span class="line">  ... 3 common frames omitted</span><br><span class="line"><span class="type">Caused</span> by: java.lang.<span class="type">NoSuchMethodException</span>: schedule.<span class="type">PontusSparkJob</span>.&lt;init&gt;<span class="container">()</span></span><br><span class="line">  at java.lang.<span class="type">Class</span>.getConstructor0<span class="container">(<span class="type">Class</span>.<span class="title">java</span>:3082)</span></span><br><span class="line">  at java.lang.<span class="type">Class</span>.newInstance<span class="container">(<span class="type">Class</span>.<span class="title">java</span>:412)</span></span><br><span class="line">  ... 4 common frames omitted</span></span><br></pre></td></tr></table></figure>
<p>解决办法：启动时通过Global注册Application实例，在Job中通过Scheduler获取</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Global</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">GlobalSettings</span> &#123;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span>(</span>application: play.api.<span class="type">Application</span>) &#123;</span><br><span class="line">    schedule.<span class="type">PontusQuartzScheduler</span>.scheduler.start()</span><br><span class="line">    schedule.<span class="type">PontusQuartzScheduler</span>.scheduler.getContext.put(<span class="string">"configuration"</span>, application.configuration)</span><br><span class="line">    schedule.<span class="type">PontusQuartzScheduler</span>.scheduler.getContext.put(<span class="string">"application"</span>, application)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PontusSparkJob</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">Job</span> &#123;</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span>(</span>context: <span class="type">JobExecutionContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> application = context.getScheduler.getContext.get(<span class="string">"application"</span>).asInstanceOf[<span class="type">Application</span>]</span><br><span class="line">    <span class="keyword">val</span> configuration = context.getScheduler.getContext.get(<span class="string">"configuration"</span>).asInstanceOf[<span class="type">Configuration</span>]</span><br><span class="line">    <span class="keyword">val</span> jobRepository = application.injector.instanceOf(classOf[<span class="type">JobRepository</span>])</span><br><span class="line">    <span class="keyword">val</span> executionRepository = application.injector.instanceOf(classOf[<span class="type">ExecutionRepository</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jobId = context.getMergedJobDataMap.getString(<span class="string">"jobId"</span>).toLong</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>TODO：自定义JobFactory</p>
<p>mesos任务报错，页面会显示申请不到资源：</p>
<figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">I1205 <span class="number">15</span>:<span class="number">15</span>:<span class="number">20.217545</span> <span class="number">19517</span> exec.cpp:<span class="number">143</span>] Version: <span class="number">0.28</span>.<span class="number">2</span></span><br><span class="line">I1205 <span class="number">15</span>:<span class="number">15</span>:<span class="number">20.243422</span> <span class="number">19521</span> exec.cpp:<span class="number">217</span>] Executor registered <span class="keyword">on</span> slave <span class="number">475189</span>a7-dcde-<span class="number">4859</span>-<span class="number">9</span>af8-<span class="number">6</span>fc2e63be94e-S0</span><br><span class="line">Unrecognized VM option <span class="string">'UseCompressedStrings'</span></span><br><span class="line">Error: Could <span class="keyword">not</span> <span class="keyword">create</span> the Java <span class="keyword">Virtual</span> Machine.</span><br><span class="line">Error: A fatal exception <span class="keyword">has</span> occurred. Program will <span class="keyword">exit</span>.</span><br></pre></td></tr></table></figure>
<p>去掉spark中executor的jvm配置</p>
<h3 id="Quartz作业的自动重启">Quartz作业的自动重启</h3><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">创建时：</span><br><span class="line"></span><br><span class="line">mysql&gt; select <span class="keyword">*</span> from QRTZ_TRIGGERS;</span><br><span class="line">+-----------------+--------------+---------------+-------------+-----------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">|<span class="string"> SCHED_NAME      </span>|<span class="string"> TRIGGER_NAME </span>|<span class="string"> TRIGGER_GROUP </span>|<span class="string"> JOB_NAME    </span>|<span class="string"> JOB_GROUP </span>|<span class="string"> DESCRIPTION </span>|<span class="string"> NEXT_FIRE_TIME </span>|<span class="string"> PREV_FIRE_TIME </span>|<span class="string"> PRIORITY </span>|<span class="string"> TRIGGER_STATE </span>|<span class="string"> TRIGGER_TYPE </span>|<span class="string"> START_TIME    </span>|<span class="string"> END_TIME </span>|<span class="string"> CALENDAR_NAME </span>|<span class="string"> MISFIRE_INSTR </span>|<span class="string"> JOB_DATA </span>|</span><br><span class="line">+-----------------+--------------+---------------+-------------+-----------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">|<span class="string"> QuartzScheduler </span>|<span class="string"> trigger2_1   </span>|<span class="string"> Cassandra     </span>|<span class="string"> pontusjob_1 </span>|<span class="string"> Cassandra </span>|<span class="string"> NULL        </span>|<span class="string">  1480523100000 </span>|<span class="string">  1480519500000 </span>|<span class="string">        5 </span>|<span class="string"> WAITING       </span>|<span class="string"> SIMPLE       </span>|<span class="string"> 1480515900000 </span>|<span class="string">        0 </span>|<span class="string"> NULL          </span>|<span class="string">             1 </span>|<span class="string">          </span>|</span><br><span class="line">+-----------------+--------------+---------------+-------------+-----------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">1 row in set (10.33 sec)</span><br><span class="line"></span><br><span class="line">创建时间（START_TIME）：         1480515900000 - 2016年11月30日 星期三 22时25分00秒 CST</span><br><span class="line">第一次执行时间（PREV_FIRE_TIME）：1480519500000 - 2016年11月30日 星期三 23时25分00秒 CST</span><br><span class="line">下一次执行时间（NEXT_FIRE_TIME）：1480523100000 - 2016年12月 1日 星期四 00时25分00秒 CST</span><br><span class="line"></span><br><span class="line">停止服务器</span><br><span class="line"></span><br><span class="line">第二天早上重启，重启后如果没有访问页面，仍然不会调度，但是只要登录，就会开始调度</span><br><span class="line"></span><br><span class="line">mysql&gt; select <span class="keyword">*</span> from QRTZ_TRIGGERS;</span><br><span class="line">+-----------------+--------------+---------------+-------------+-----------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">|<span class="string"> SCHED_NAME      </span>|<span class="string"> TRIGGER_NAME </span>|<span class="string"> TRIGGER_GROUP </span>|<span class="string"> JOB_NAME    </span>|<span class="string"> JOB_GROUP </span>|<span class="string"> DESCRIPTION </span>|<span class="string"> NEXT_FIRE_TIME </span>|<span class="string"> PREV_FIRE_TIME </span>|<span class="string"> PRIORITY </span>|<span class="string"> TRIGGER_STATE </span>|<span class="string"> TRIGGER_TYPE </span>|<span class="string"> START_TIME    </span>|<span class="string"> END_TIME </span>|<span class="string"> CALENDAR_NAME </span>|<span class="string"> MISFIRE_INSTR </span>|<span class="string"> JOB_DATA </span>|</span><br><span class="line">+-----------------+--------------+---------------+-------------+-----------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">|<span class="string"> QuartzScheduler </span>|<span class="string"> trigger2_1   </span>|<span class="string"> Cassandra     </span>|<span class="string"> pontusjob_1 </span>|<span class="string"> Cassandra </span>|<span class="string"> NULL        </span>|<span class="string">  1480561432130 </span>|<span class="string">  1480557832130 </span>|<span class="string">        5 </span>|<span class="string"> WAITING       </span>|<span class="string"> SIMPLE       </span>|<span class="string"> 1480557832130 </span>|<span class="string">        0 </span>|<span class="string"> NULL          </span>|<span class="string">             1 </span>|<span class="string">          </span>|</span><br><span class="line">+-----------------+--------------+---------------+-------------+-----------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">1 row in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">START_TIME：     1480557832130 - 2016年12月 1日 星期四 10时03分52秒 CST</span><br><span class="line">PREV_FIRE_TIME： 1480557832130 - 2016年12月 1日 星期四 10时03分52秒 CST</span><br><span class="line">NEXT_FIRE_TIME： 1480561432130 - 2016年12月 1日 星期四 11时03分52秒 CST</span><br></pre></td></tr></table></figure>
<p>相同Job不允许同时有两个Trigger在运行。解决办法：可以设置不同的JobId。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">! @<span class="number">729</span>o8hm27 - Internal server error, <span class="keyword">for</span> (GET) [/job/execute/Cassandra/HDFS/<span class="number">2</span>] -&gt;</span><br><span class="line"></span><br><span class="line">play<span class="class">.api</span><span class="class">.UnexpectedException</span>: Unexpected exception[ObjectAlreadyExistsException: Unable to store Job : <span class="string">'Cassandra-HDFS.pontusjob_2'</span>, because one already exists with this identification.]</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.http</span><span class="class">.HttpErrorHandlerExceptions</span>$.<span class="function"><span class="title">throwableToUsefulException</span><span class="params">(HttpErrorHandler.scala:<span class="number">289</span>)</span></span></span><br><span class="line">  at play<span class="class">.api</span><span class="class">.http</span><span class="class">.DefaultHttpErrorHandler</span><span class="class">.onServerError</span>(HttpErrorHandler<span class="class">.scala</span>:<span class="number">220</span>)</span><br><span class="line">  at play<span class="class">.api</span><span class="class">.GlobalSettings</span><span class="variable">$class</span>.<span class="function"><span class="title">onError</span><span class="params">(GlobalSettings.scala:<span class="number">160</span>)</span></span></span><br><span class="line">  at util.Global$.<span class="function"><span class="title">onError</span><span class="params">(Global.scala:<span class="number">23</span>)</span></span></span><br><span class="line">  at play<span class="class">.api</span><span class="class">.http</span><span class="class">.GlobalSettingsHttpErrorHandler</span><span class="class">.onServerError</span>(HttpErrorHandler<span class="class">.scala</span>:<span class="number">100</span>)</span><br><span class="line">  at play<span class="class">.core</span><span class="class">.server</span><span class="class">.netty</span><span class="class">.PlayRequestHandler</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span>$<span class="number">1</span>.<span class="function"><span class="title">applyOrElse</span><span class="params">(PlayRequestHandler.scala:<span class="number">100</span>)</span></span></span><br><span class="line">  at play<span class="class">.core</span><span class="class">.server</span><span class="class">.netty</span><span class="class">.PlayRequestHandler</span>$<span class="variable">$anonfun</span>$<span class="number">2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span>$<span class="number">1</span>.<span class="function"><span class="title">applyOrElse</span><span class="params">(PlayRequestHandler.scala:<span class="number">99</span>)</span></span></span><br><span class="line">  at scala<span class="class">.concurrent</span><span class="class">.Future</span>$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(Future.scala:<span class="number">344</span>)</span></span></span><br><span class="line">  at scala<span class="class">.concurrent</span><span class="class">.Future</span>$<span class="variable">$anonfun</span><span class="variable">$recoverWith</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(Future.scala:<span class="number">343</span>)</span></span></span><br><span class="line">  at scala<span class="class">.concurrent</span><span class="class">.impl</span><span class="class">.CallbackRunnable</span><span class="class">.run</span>(Promise<span class="class">.scala</span>:<span class="number">32</span>)</span><br><span class="line">Caused by: org<span class="class">.quartz</span><span class="class">.ObjectAlreadyExistsException</span>: Unable to store Job : <span class="string">'Cassandra-HDFS.pontusjob_2'</span>, because one already exists with this identification.</span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreSupport</span><span class="class">.storeJob</span>(JobStoreSupport<span class="class">.java</span>:<span class="number">1108</span>)</span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreSupport</span>$<span class="number">2</span>.<span class="function"><span class="title">executeVoid</span><span class="params">(JobStoreSupport.java:<span class="number">1062</span>)</span></span></span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreSupport</span><span class="variable">$VoidTransactionCallback</span>.<span class="function"><span class="title">execute</span><span class="params">(JobStoreSupport.java:<span class="number">3715</span>)</span></span></span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreSupport</span><span class="variable">$VoidTransactionCallback</span>.<span class="function"><span class="title">execute</span><span class="params">(JobStoreSupport.java:<span class="number">3713</span>)</span></span></span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreSupport</span><span class="class">.executeInNonManagedTXLock</span>(JobStoreSupport<span class="class">.java</span>:<span class="number">3799</span>)</span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreTX</span><span class="class">.executeInLock</span>(JobStoreTX<span class="class">.java</span>:<span class="number">93</span>)</span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.jdbcjobstore</span><span class="class">.JobStoreSupport</span><span class="class">.storeJobAndTrigger</span>(JobStoreSupport<span class="class">.java</span>:<span class="number">1058</span>)</span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.core</span><span class="class">.QuartzScheduler</span><span class="class">.scheduleJob</span>(QuartzScheduler<span class="class">.java</span>:<span class="number">886</span>)</span><br><span class="line">  at org<span class="class">.quartz</span><span class="class">.impl</span><span class="class">.StdScheduler</span><span class="class">.scheduleJob</span>(StdScheduler<span class="class">.java</span>:<span class="number">249</span>)</span><br><span class="line">  at controllers.PontusJobController$<span class="variable">$anonfun</span><span class="variable">$execute</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(PontusJobController.scala:<span class="number">72</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>spark streaming一次性作业：</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select <span class="keyword">*</span> from QRTZ_TRIGGERS;</span><br><span class="line">+-----------------+--------------+----------------+--------------+----------------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">|<span class="string"> SCHED_NAME      </span>|<span class="string"> TRIGGER_NAME </span>|<span class="string"> TRIGGER_GROUP  </span>|<span class="string"> JOB_NAME     </span>|<span class="string"> JOB_GROUP      </span>|<span class="string"> DESCRIPTION </span>|<span class="string"> NEXT_FIRE_TIME </span>|<span class="string"> PREV_FIRE_TIME </span>|<span class="string"> PRIORITY </span>|<span class="string"> TRIGGER_STATE </span>|<span class="string"> TRIGGER_TYPE </span>|<span class="string"> START_TIME    </span>|<span class="string"> END_TIME </span>|<span class="string"> CALENDAR_NAME </span>|<span class="string"> MISFIRE_INSTR </span>|<span class="string"> JOB_DATA </span>|</span><br><span class="line">+-----------------+--------------+----------------+--------------+----------------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">|<span class="string"> QuartzScheduler </span>|<span class="string"> trigger1_2   </span>|<span class="string"> Kafka-HDFS     </span>|<span class="string"> pontusjob1_2 </span>|<span class="string"> Kafka-HDFS     </span>|<span class="string"> NULL        </span>|<span class="string">             -1 </span>|<span class="string">  1481165690342 </span>|<span class="string">        5 </span>|<span class="string"> COMPLETE      </span>|<span class="string"> SIMPLE       </span>|<span class="string"> 1481165690342 </span>|<span class="string">        0 </span>|<span class="string"> NULL          </span>|<span class="string">             0 </span>|<span class="string">          </span>|</span><br><span class="line">|<span class="string"> QuartzScheduler </span>|<span class="string"> trigger2_1   </span>|<span class="string"> Cassandra-HDFS </span>|<span class="string"> pontusjob2_1 </span>|<span class="string"> Cassandra-HDFS </span>|<span class="string"> NULL        </span>|<span class="string">  1481165913700 </span>|<span class="string">  1481165433700 </span>|<span class="string">        5 </span>|<span class="string"> WAITING       </span>|<span class="string"> SIMPLE       </span>|<span class="string"> 1481103993700 </span>|<span class="string">        0 </span>|<span class="string"> NULL          </span>|<span class="string">             1 </span>|<span class="string">          </span>|</span><br><span class="line">+-----------------+--------------+----------------+--------------+----------------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span><br><span class="line">2 rows in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; select <span class="keyword">*</span> from QRTZ_SIMPLE_TRIGGERS;</span><br><span class="line">+-----------------+--------------+----------------+--------------+-----------------+-----------------+</span><br><span class="line">|<span class="string"> SCHED_NAME      </span>|<span class="string"> TRIGGER_NAME </span>|<span class="string"> TRIGGER_GROUP  </span>|<span class="string"> REPEAT_COUNT </span>|<span class="string"> REPEAT_INTERVAL </span>|<span class="string"> TIMES_TRIGGERED </span>|</span><br><span class="line">+-----------------+--------------+----------------+--------------+-----------------+-----------------+</span><br><span class="line">|<span class="string"> QuartzScheduler </span>|<span class="string"> trigger1_2   </span>|<span class="string"> Kafka-HDFS     </span>|<span class="string">            0 </span>|<span class="string">               0 </span>|<span class="string">               1 </span>|</span><br><span class="line">|<span class="string"> QuartzScheduler </span>|<span class="string"> trigger2_1   </span>|<span class="string"> Cassandra-HDFS </span>|<span class="string">           -1 </span>|<span class="string">          480000 </span>|<span class="string">             129 </span>|</span><br><span class="line">+-----------------+--------------+----------------+--------------+-----------------+-----------------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>
<p>过了会儿，一次性的任务信息会从Quartz删除，但是实际上spark streaming还在运行！<br>这时，如果再次点击执行，虽然triggerKey不存在，但是也不应该允许再次执行！否则就会存在两个spark streaming程序！<br>还有一个问题：由于spark streaming不会结束，所以无法看到日志，状态也永远是正在执行。</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="header">mysql&gt; select * from QRTZ_SIMPLE_TRIGGERS;</span><br><span class="line">+-----------------+--------------+----------------+--------------+-----------------+-----------------+</span></span><br><span class="line"><span class="header">| SCHED_NAME      | TRIGGER_NAME | TRIGGER_GROUP  | REPEAT_COUNT | REPEAT_INTERVAL | TIMES_TRIGGERED |</span><br><span class="line">+-----------------+--------------+----------------+--------------+-----------------+-----------------+</span></span><br><span class="line"><span class="header">| QuartzScheduler | trigger2_1   | Cassandra-HDFS |           -1 |          480000 |             129 |</span><br><span class="line">+-----------------+--------------+----------------+--------------+-----------------+-----------------+</span></span><br><span class="line">1 row in set (0.01 sec)</span><br><span class="line"></span><br><span class="line"><span class="header">mysql&gt; select * from QRTZ_TRIGGERS;</span><br><span class="line">+-----------------+--------------+----------------+--------------+----------------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span></span><br><span class="line"><span class="header">| SCHED_NAME      | TRIGGER_NAME | TRIGGER_GROUP  | JOB_NAME     | JOB_GROUP      | DESCRIPTION | NEXT_FIRE_TIME | PREV_FIRE_TIME | PRIORITY | TRIGGER_STATE | TRIGGER_TYPE | START_TIME    | END_TIME | CALENDAR_NAME | MISFIRE_INSTR | JOB_DATA |</span><br><span class="line">+-----------------+--------------+----------------+--------------+----------------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span></span><br><span class="line"><span class="header">| QuartzScheduler | trigger2_1   | Cassandra-HDFS | pontusjob2_1 | Cassandra-HDFS | NULL        |  1481165913700 |  1481165433700 |        5 | WAITING       | SIMPLE       | 1481103993700 |        0 | NULL          |             1 |          |</span><br><span class="line">+-----------------+--------------+----------------+--------------+----------------+-------------+----------------+----------------+----------+---------------+--------------+---------------+----------+---------------+---------------+----------+</span></span><br><span class="line">1 row in set (0.01 sec)</span><br></pre></td></tr></table></figure>
<h2 id="Kafka">Kafka</h2><p>创建测试主题</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --create --zookeeper <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">2181</span> --replication-factor <span class="number">1</span> --partitions <span class="number">1</span> --topic graylog_nginx</span><br><span class="line">bin/kafka-console-producer.sh --broker-<span class="built_in">list</span> <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">9092</span> --topic graylog_nginx</span><br><span class="line">bin/kafka-console-consumer.sh --zookeeper <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">2181</span> --topic graylog_nginx --from-beginning</span><br></pre></td></tr></table></figure>
<p>准备json数据</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">1</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Betty"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"bsmithrl@simplemachines.org"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Eláteia"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Greece"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"9.19.204.44"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">2</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Anna"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"alewisrm@canalblog.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Shangjing"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"China"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"14.207.119.126"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">3</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"David"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"dgarrettrn@japanpost.jp"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Tsarychanka"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Ukraine"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"111.252.63.159"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">4</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Heather"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"hgilbertro@skype.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Koilás"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Greece"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"29.57.181.250"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">5</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Diane"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"ddanielsrp@statcounter.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Mapiripán"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Colombia"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"19.205.181.99"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">6</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Philip"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"pfullerrq@reuters.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"El Cairo"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Colombia"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"210.248.121.194"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">7</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Maria"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"mfordrr@shop-pro.jp"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Karabash"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Russia"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"224.21.41.52"</span></span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">8</span></span>,"<span class="attribute">username</span>":<span class="value"><span class="string">"Bety"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"bsmithrl@simplemachines.org"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Eláteia"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Greece"</span></span>,"<span class="attribute">ipAddress</span>":<span class="value"><span class="string">"9.19.204.44"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">9</span></span>,"<span class="attribute">username</span>":<span class="value"><span class="string">"Ana"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"alewisrm@canalblog.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Shangjing"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"China"</span></span>,"<span class="attribute">ipAddress</span>":<span class="value"><span class="string">"14.207.119.126"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">10</span></span>,"<span class="attribute">username</span>":<span class="value"><span class="string">"David"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"dgarrettrn@japanpost.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Tsarychanka"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Ukraine"</span></span>,"<span class="attribute">ipAddress</span>":<span class="value"><span class="string">"111.252.63.159"</span></span>&#125;</span><br><span class="line">&#123;"<span class="attribute">id</span>":<span class="value"><span class="number">11</span></span>,"<span class="attribute">name</span>":<span class="value"><span class="string">"Calis"</span></span>,"<span class="attribute">email</span>":<span class="value"><span class="string">"carlis@google.com"</span></span>,"<span class="attribute">city</span>":<span class="value"><span class="string">"Koran"</span></span>,"<span class="attribute">country</span>":<span class="value"><span class="string">"Greece"</span></span>,"<span class="attribute">ip</span>":<span class="value"><span class="string">"29.57.181.250"</span></span>&#125;</span><br></pre></td></tr></table></figure>
<h1 id="附录">附录</h1><h2 id="sbt私服">sbt私服</h2><p>安装repox：<a href="https://github.com/Centaur/repox/wiki">https://github.com/Centaur/repox/wiki</a><br>可以在本机开发环境编译jar包，再上次到服务器（因为服务器上可能没有编译所需的node环境）。  </p>
<p>启动repox，然后打开：<a href="http://192.168.6.53:8078/admin/admin.html#/upstreams" target="_blank" rel="external">http://192.168.6.53:8078/admin/admin.html#/upstreams</a>，密码：zhimakaimen</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">nohup</span> <span class="tag">java</span> <span class="tag">-Xmx512m</span> <span class="tag">-jar</span> <span class="tag">repox-assembly-0</span><span class="class">.1-SNAPSHOT</span><span class="class">.jar</span> &amp;</span><br></pre></td></tr></table></figure>
<p>在<code>~/.sbt/repositories</code>添加两行配置：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">repox-maven: http://192.168.6.53:8078/</span><br><span class="line">repox-ivy: http://192.168.6.53:8078/, [<span class="link_label">organization</span>]/[<span class="link_label">module</span>]/(scala<span class="emphasis">_[scalaVersion]/)(sbt_</span>[<span class="link_label">sbtVersion</span>]/)[<span class="link_label">revision</span>]/[<span class="link_label">type</span>]s/[<span class="link_label">artifact</span>](<span class="link_url">-[classifier]</span>).[ext]</span><br></pre></td></tr></table></figure>
<p>完整的repositories文件：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[repositories]</span><br><span class="line">local</span><br><span class="line">local-maven: file:///Users/zhengqh/.m2/repository/</span><br><span class="line">repox-maven: http://192.168.6.53:8078/</span><br><span class="line">repox-ivy: http://192.168.6.53:8078/, [<span class="link_label">organization</span>]/[<span class="link_label">module</span>]/(scala<span class="emphasis">_[scalaVersion]/)(sbt_</span>[<span class="link_label">sbtVersion</span>]/)[<span class="link_label">revision</span>]/[<span class="link_label">type</span>]s/[<span class="link_label">artifact</span>](<span class="link_url">-[classifier]</span>).[ext]</span><br><span class="line"><span class="header">#osc: http://maven.oschina.net/content/groups/public/</span></span><br><span class="line"><span class="header">#oschina-ivy: http://maven.oschina.net/content/groups/public/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]</span></span><br><span class="line">typesafe: http://repo.typesafe.com/typesafe/ivy-releases/, [<span class="link_label">organization</span>]/[<span class="link_label">module</span>]/(scala<span class="emphasis">_[scalaVersion]/)(sbt_</span>[<span class="link_label">sbtVersion</span>]/)[<span class="link_label">revision</span>]/[<span class="link_label">type</span>]s/[<span class="link_label">artifact</span>](<span class="link_url">-[classifier]</span>).[ext], bootOnly</span><br><span class="line">sonatype-oss-releases</span><br><span class="line">maven-central</span><br><span class="line">sonatype-oss-snapshots</span><br></pre></td></tr></table></figure>
<p>使用activator运行play时，创建activatorconfig.txt文件，添加以下配置：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat ~/.activator/activatorconfig<span class="class">.txt</span></span><br><span class="line">-Dsbt<span class="class">.override</span><span class="class">.build</span><span class="class">.repos</span>=true</span><br></pre></td></tr></table></figure>
<p>命令行运行activator，如果出现<code>downloading http://192.168.6.53:8078</code>，表示私服搭建成功。<br>如果本地不存在jar包，则私服会自动下载，然后再下载到本地。  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">➜  first-player git:(master) ✗ ./activator run</span><br><span class="line">[info] Loading global plugins from /Users/zhengqh/.sbt/<span class="number">0.13</span>/plugins</span><br><span class="line">[info] Loading project definition from /Users/zhengqh/Github/first-player/project</span><br><span class="line">[info] Updating &#123;file:/Users/zhengqh/Github/first-player/project/&#125;first-player-build...</span><br><span class="line">[info] Resolving org.fusesource.jansi<span class="preprocessor">#jansi;<span class="number">1.4</span> ...</span></span><br><span class="line">[info] downloading http:<span class="comment">//192.168.6.53:8078/com.typesafe.play/sbt-plugin/scala_2.10/sbt_0.13/2.5.10/jars/sbt-plugin.jar ...</span></span><br><span class="line">[info]  [SUCCESSFUL ] com.typesafe.play<span class="preprocessor">#sbt-plugin;<span class="number">2.5</span><span class="number">.10</span>!sbt-plugin.jar (<span class="number">7242</span>ms)</span></span><br><span class="line">[info] downloading http:<span class="comment">//192.168.6.53:8078/com/typesafe/play/sbt-routes-compiler_2.10/2.5.10/sbt-routes-compiler_2.10-2.5.10.jar ...</span></span><br><span class="line">[info]  [SUCCESSFUL ] com.typesafe.play<span class="preprocessor">#sbt-routes-compiler_2<span class="number">.10</span>;<span class="number">2.5</span><span class="number">.10</span>!sbt-routes-compiler_2<span class="number">.10</span>.jar (<span class="number">9753</span>ms)</span></span><br><span class="line">[info] downloading http:<span class="comment">//192.168.6.53:8078/com/typesafe/play/sbt-run-support_2.10/2.5.10/sbt-run-support_2.10-2.5.10.jar ...</span></span><br><span class="line">[info]  [SUCCESSFUL ] com.typesafe.play<span class="preprocessor">#sbt-run-support_2<span class="number">.10</span>;<span class="number">2.5</span><span class="number">.10</span>!sbt-run-support_2<span class="number">.10</span>.jar (<span class="number">6686</span>ms)</span></span><br><span class="line">[info] downloading http:<span class="comment">//192.168.6.53:8078/com/typesafe/play/build-link/2.5.10/build-link-2.5.10.jar ...</span></span><br><span class="line">[info]  [SUCCESSFUL ] com.typesafe.play<span class="preprocessor">#build-link;<span class="number">2.5</span><span class="number">.10</span>!build-link.jar (<span class="number">6589</span>ms)</span></span><br><span class="line">[info] downloading http:<span class="comment">//192.168.6.53:8078/com/typesafe/play/play-exceptions/2.5.10/play-exceptions-2.5.10.jar ...</span></span><br><span class="line">[info]  [SUCCESSFUL ] com.typesafe.play<span class="preprocessor">#play-exceptions;<span class="number">2.5</span><span class="number">.10</span>!play-exceptions.jar (<span class="number">2358</span>ms)</span></span><br><span class="line">[info] Done updating.</span><br></pre></td></tr></table></figure>
<p>使用sbt运行时（sbt run），需要更改sbt的选项，比如下面是mac环境使用brew安装sbt配置</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cat /usr/<span class="built_in">local</span>/bin/sbt</span><br><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line"><span class="built_in">export</span> SBT_OPTS=<span class="string">"-Dsbt.override.build.repos=true"</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="operator">-f</span> <span class="string">"<span class="variable">$HOME</span>/.sbtconfig"</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"Use of ~/.sbtconfig is deprecated, please migrate global settings to /usr/local/etc/sbtopts"</span> &gt;&amp;<span class="number">2</span></span><br><span class="line">  . <span class="string">"<span class="variable">$HOME</span>/.sbtconfig"</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">exec</span> <span class="string">"/usr/local/Cellar/sbt/0.13.8/libexec/sbt"</span> <span class="string">"<span class="variable">$@</span>"</span></span><br></pre></td></tr></table></figure>
<p>IDEA开发环境，还需要配置：</p>
<p><img src="http://img.blog.csdn.net/20170512105246182" alt="3"></p>
<h2 id="Play_MultiProject">Play MultiProject</h2><p>参考：<a href="https://github.com/aaronp/multi-project">https://github.com/aaronp/multi-project</a></p>
<p>增加play后，</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">java.lang.ClassCastException: org.slf4j.helpers.NOPLoggerFactory cannot be <span class="keyword">cast</span> <span class="keyword">to</span> ch.qos.logback.classic.LoggerContext</span><br><span class="line">  at play.api.libs.logback.LogbackLoggerConfigurator.configure(LogbackLoggerConfigurator.scala:<span class="number">80</span>)</span><br><span class="line">  at play.api.libs.logback.LogbackLoggerConfigurator.init(LogbackLoggerConfigurator.scala:<span class="number">26</span>)</span><br><span class="line">  at play.core.server.DevServerStart$<span class="variable">$anonfun</span><span class="variable">$mainDev</span><span class="variable">$1</span>.apply(DevServerStart.scala:<span class="number">94</span>)</span><br><span class="line">  at play.core.server.DevServerStart$<span class="variable">$anonfun</span><span class="variable">$mainDev</span><span class="variable">$1</span>.apply(DevServerStart.scala:<span class="number">65</span>)</span><br><span class="line">  at play.utils.Threads$.withContextClassLoader(Threads.scala:<span class="number">21</span>)</span><br><span class="line">  at play.core.server.DevServerStart$.mainDev(DevServerStart.scala:<span class="number">64</span>)</span><br><span class="line">  at play.core.server.DevServerStart$.mainDevHttpMode(DevServerStart.scala:<span class="number">54</span>)</span><br><span class="line">  at play.core.server.DevServerStart.mainDevHttpMode(DevServerStart.scala)</span><br><span class="line">  at sun.reflect.NativeMethodAccessorImpl.invoke<span class="number">0</span>(Native Method)</span><br><span class="line">  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class="number">62</span>)</span><br><span class="line">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</span><br><span class="line">  at java.lang.reflect.Method.invoke(Method.java:<span class="number">497</span>)</span><br><span class="line">  at play.runsupport.Reloader$.startDevMode(Reloader.scala:<span class="number">207</span>)</span><br><span class="line">  at play.sbt.run.PlayRun$<span class="variable">$anonfun</span><span class="variable">$playRunTask</span><span class="variable">$1</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$3</span>.devModeServer<span class="variable">$lzycompute</span><span class="variable">$1</span>(PlayRun.scala:<span class="number">73</span>)</span><br><span class="line">  at play.sbt.run.PlayRun$<span class="variable">$anonfun</span><span class="variable">$playRunTask</span><span class="variable">$1</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$3</span>.play<span class="variable">$sbt</span><span class="variable">$run</span><span class="variable">$PlayRun</span>$<span class="variable">$anonfun</span>$<span class="variable">$anonfun</span>$<span class="variable">$anonfun</span>$<span class="variable">$devModeServer</span><span class="variable">$1</span>(PlayRun.scala:<span class="number">73</span>)</span><br><span class="line">  at play.sbt.run.PlayRun$<span class="variable">$anonfun</span><span class="variable">$playRunTask</span><span class="variable">$1</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$3</span>.apply(PlayRun.scala:<span class="number">99</span>)</span><br><span class="line">  at play.sbt.run.PlayRun$<span class="variable">$anonfun</span><span class="variable">$playRunTask</span><span class="variable">$1</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$3</span>.apply(PlayRun.scala:<span class="number">52</span>)</span><br><span class="line">  at scala.Function1$<span class="variable">$anonfun</span><span class="variable">$compose</span><span class="variable">$1</span>.apply(Function1.scala:<span class="number">47</span>)</span><br><span class="line">[trace] Stack trace suppressed: run last web/compile:run <span class="keyword">for</span> the full output.</span><br><span class="line">[error] (web/compile:run) java.lang.reflect.InvocationTargetException</span><br><span class="line">[error] Total time: <span class="number">37</span> s, completed <span class="number">2017</span>-<span class="number">3</span>-<span class="number">8</span> <span class="number">20</span>:<span class="number">28</span>:<span class="number">39</span></span><br></pre></td></tr></table></figure>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">java.lang.ClassCastException: org.slf4j.impl.Log4jLoggerFactory cannot be <span class="keyword">cast</span> <span class="keyword">to</span> ch.qos.logback.classic.LoggerContext</span><br><span class="line">  at play.api.libs.logback.LogbackLoggerConfigurator.configure(LogbackLoggerConfigurator.scala:<span class="number">80</span>)</span><br><span class="line">  at play.api.libs.logback.LogbackLoggerConfigurator.init(LogbackLoggerConfigurator.scala:<span class="number">26</span>)</span><br><span class="line">  at play.core.server.DevServerStart$<span class="variable">$anonfun</span><span class="variable">$mainDev</span><span class="variable">$1</span>.apply(DevServerStart.scala:<span class="number">94</span>)</span><br><span class="line">  at play.core.server.DevServerStart$<span class="variable">$anonfun</span><span class="variable">$mainDev</span><span class="variable">$1</span>.apply(DevServerStart.scala:<span class="number">65</span>)</span><br><span class="line">  at play.utils.Threads$.withContextClassLoader(Threads.scala:<span class="number">21</span>)</span><br><span class="line">  at play.core.server.DevServerStart$.mainDev(DevServerStart.scala:<span class="number">64</span>)</span><br><span class="line">  at play.core.server.DevServerStart$.mainDevHttpMode(DevServerStart.scala:<span class="number">54</span>)</span><br><span class="line">  at play.core.server.DevServerStart.mainDevHttpMode(DevServerStart.scala)</span><br><span class="line">  at sun.reflect.NativeMethodAccessorImpl.invoke<span class="number">0</span>(Native Method)</span><br><span class="line">  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:<span class="number">62</span>)</span><br><span class="line">  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</span><br><span class="line">  at java.lang.reflect.Method.invoke(Method.java:<span class="number">497</span>)</span><br><span class="line">  at play.runsupport.Reloader$.startDevMode(Reloader.scala:<span class="number">234</span>)</span><br><span class="line">  at play.sbt.run.PlayRun$<span class="variable">$anonfun</span><span class="variable">$playRunTask</span><span class="variable">$1</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$3</span>.devModeServer<span class="variable">$lzycompute</span><span class="variable">$1</span>(PlayRun.scala:<span class="number">74</span>)</span><br><span class="line">  at play.sbt.run.PlayRun$<span class="variable">$anonfun</span><span class="variable">$playRunTask</span><span class="variable">$1</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$3</span>.play<span class="variable">$sbt</span><span class="variable">$run</span><span class="variable">$PlayRun</span>$<span class="variable">$anonfun</span>$<span class="variable">$anonfun</span>$<span class="variable">$anonfun</span>$<span class="variable">$devModeServer</span><span class="variable">$1</span>(PlayRun.scala:<span class="number">74</span>)</span><br><span class="line">  at play.sbt.run.PlayRun$<span class="variable">$anonfun</span><span class="variable">$playRunTask</span><span class="variable">$1</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$3</span>.apply(PlayRun.scala:<span class="number">100</span>)</span><br><span class="line">  at play.sbt.run.PlayRun$<span class="variable">$anonfun</span><span class="variable">$playRunTask</span><span class="variable">$1</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$2</span>$<span class="variable">$anonfun</span><span class="variable">$apply</span><span class="variable">$3</span>.apply(PlayRun.scala:<span class="number">53</span>)</span><br><span class="line">  at scala.Function1$<span class="variable">$anonfun</span><span class="variable">$compose</span><span class="variable">$1</span>.apply(Function1.scala:<span class="number">47</span>)</span><br><span class="line">[trace] Stack trace suppressed: run last web/compile:run <span class="keyword">for</span> the full output.</span><br><span class="line">[error] (web/compile:run) java.lang.reflect.InvocationTargetException</span><br><span class="line">[error] Total time: <span class="number">3</span> s, completed <span class="number">2017</span>-<span class="number">3</span>-<span class="number">8</span> <span class="number">20</span>:<span class="number">16</span>:<span class="number">56</span></span><br></pre></td></tr></table></figure>
<p>解决方式：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val webExcludeDependencies = Seq(</span><br><span class="line">  <span class="function"><span class="title">SbtExclusionRule</span><span class="params">(<span class="string">"org.slf4j"</span> ,<span class="string">"slf4j-simple"</span>)</span></span>,</span><br><span class="line">  <span class="function"><span class="title">SbtExclusionRule</span><span class="params">(<span class="string">"org.slf4j"</span>, <span class="string">"slf4j-jdk12"</span>)</span></span> ,</span><br><span class="line">  <span class="function"><span class="title">SbtExclusionRule</span><span class="params">(<span class="string">"org.slf4j"</span>, <span class="string">"slf4j-log4j12"</span>)</span></span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="Run_&amp;_Debug_With_IDEA">Run &amp; Debug With IDEA</h2><p>参考：<a href="http://stackoverflow.com/questions/24218341/how-to-run-play-framework-2-x-in-debug-mode-in-intellij-idea" target="_blank" rel="external">http://stackoverflow.com/questions/24218341/how-to-run-play-framework-2-x-in-debug-mode-in-intellij-idea</a><br><a href="http://stackoverflow.com/questions/22195364/debugging-sbt-project-with-play-in-intellij-idea" target="_blank" rel="external">http://stackoverflow.com/questions/22195364/debugging-sbt-project-with-play-in-intellij-idea</a><br><a href="http://stackoverflow.com/questions/4150776/debugging-scala-code-with-simple-build-tool-sbt-and-intellij" target="_blank" rel="external">http://stackoverflow.com/questions/4150776/debugging-scala-code-with-simple-build-tool-sbt-and-intellij</a></p>
<p>命令行运行： <code>sbt web/run -Dconfig.resource=local/application.zqh.conf -Dhttp.port=9091</code></p>
<h3 id="1-_Play_App（❌）">1. Play App（❌）</h3><p>IDEA中运行Play App，或者直接在Controller上右键点击Run Play 2 App</p>
<p><img src="http://img.blog.csdn.net/20170512103522643" alt="1"></p>
<p>sbt多模块无法定位到web</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[info] Loading project definition <span class="keyword">from</span> /Users/zhengqh/Github/pontus/project</span><br><span class="line">[info] Updating &#123;<span class="type">file</span>:/Users/zhengqh/Github/pontus/project/&#125;pontus-build...</span><br><span class="line">[info] Compiling <span class="number">3</span> Scala sources <span class="keyword">to</span> /Users/zhengqh/Github/pontus/project/target/scala-<span class="number">2.10</span>/sbt-<span class="number">0.13</span>/classes...</span><br><span class="line">[info] Set current project <span class="keyword">to</span> pontus-root (<span class="keyword">in</span> build <span class="type">file</span>:/Users/zhengqh/Github/pontus/)</span><br><span class="line">[warn] compile:<span class="command">run</span>::javaOptions will be ignored, compile:<span class="command">run</span>::fork <span class="keyword">is</span> <span class="keyword">set</span> <span class="keyword">to</span> <span class="constant">false</span></span><br><span class="line">java.lang.RuntimeException: No main <span class="type">class</span> detected.</span><br><span class="line">  <span class="keyword">at</span> scala.sys.package$.<span class="keyword">error</span>(package.scala:<span class="number">27</span>)</span><br><span class="line">[trace] Stack trace suppressed: <span class="command">run</span> '<span class="keyword">last</span> root/compile:<span class="command">run</span>' <span class="keyword">for</span> <span class="keyword">the</span> full output.</span><br><span class="line">[<span class="keyword">error</span>] (root/compile:<span class="command">run</span>) No main <span class="type">class</span> detected.</span><br><span class="line">[<span class="keyword">error</span>] Total <span class="property">time</span>: <span class="number">0</span> s, completed <span class="number">2017</span>-<span class="number">5</span>-<span class="number">12</span> <span class="number">10</span>:<span class="number">35</span>:<span class="number">01</span></span><br><span class="line">Disconnected <span class="keyword">from</span> <span class="keyword">the</span> target VM, address: '<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">62036</span>', transport: 'socket'</span><br></pre></td></tr></table></figure>
<p>只有Play工程，应该是可以的</p>
<h3 id="2-_Sbt_Task（✅）">2. Sbt Task（✅）</h3><p>IDEA中运行Sbt Task，注意-D参数必须添加到VM arguments，不能放在Tasks中</p>
<p><img src="http://img.blog.csdn.net/20170512103539643" alt="2"></p>
<h3 id="3-_jvm-debug（✅）">3. jvm-debug（✅）</h3><p>Play工程的命令行执行方式：sbt -jvm-debug 9999 “run 9091”</p>
<p>Sbt多模块的命令行执行方式：sbt -jvm-debug 9999  web/run -Dconfig.resource=local/application.zqh.conf -Dhttp.port=9091</p>
<p>先在命令行启动sbt，然后在IDEA中配置Remote</p>
<p><img src="http://img.blog.csdn.net/20170512104754468" alt="3"></p>
<p>用启动Remote的debug，设置断点，可以进入断点，控制器会打印</p>
<p>Connected to the target VM, address: ‘localhost:9999’, transport: ‘socket’</p>
<p>访问：<a href="http://localhost:9091/api/v1/job" target="_blank" rel="external">http://localhost:9091/api/v1/job</a> ，进入DEBUG模式</p>
<p><img src="http://img.blog.csdn.net/20170512105802005" alt="4"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scala PlayFramework（2.4）入门&lt;br&gt;示例程序：&lt;a href=&quot;https://github.com/zqhxuyuan/first-player&quot;&gt;https://github.com/zqhxuyuan/first-player&lt;/a&gt;&lt;/p&gt;
&lt;!-- MarkdownTOC --&gt;
&lt;ul&gt;
&lt;li&gt;Run! Run!! Run!!!&lt;/li&gt;
&lt;li&gt;Hello World!&lt;ul&gt;
&lt;li&gt;路由和Controller&lt;/li&gt;
&lt;li&gt;页面和渲染&lt;ul&gt;
&lt;li&gt;模板编译&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Products Example&lt;ul&gt;
&lt;li&gt;implicit&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Spark集成&lt;ul&gt;
&lt;li&gt;play-spark-module&lt;/li&gt;
&lt;li&gt;Spark Launcher&lt;ul&gt;
&lt;li&gt;mesos&lt;/li&gt;
&lt;li&gt;问题&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Spark Cassandra&lt;/li&gt;
&lt;li&gt;预览&lt;/li&gt;
&lt;li&gt;Actor&lt;/li&gt;
&lt;li&gt;Quartz&lt;ul&gt;
&lt;li&gt;Job依赖注入&lt;/li&gt;
&lt;li&gt;Quartz作业的自动重启&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Kafka&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;附录&lt;ul&gt;
&lt;li&gt;sbt私服&lt;/li&gt;
&lt;li&gt;Play MultiProject&lt;/li&gt;
&lt;li&gt;Run &amp;amp; Debug With IDEA&lt;ul&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;Play App（❌）&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;Sbt Task（✅）&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;jvm-debug（✅）&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- /MarkdownTOC --&gt;
    
    </summary>
    
      <category term="scala" scheme="http://github.com/zqhxuyuan/categories/scala/"/>
    
    
      <category term="scala" scheme="http://github.com/zqhxuyuan/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>ETL Tools</title>
    <link href="http://github.com/zqhxuyuan/2016/11/11/Data-Transform/"/>
    <id>http://github.com/zqhxuyuan/2016/11/11/Data-Transform/</id>
    <published>2016-11-10T16:00:00.000Z</published>
    <updated>2017-08-29T06:33:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>ETL Tools（Sqoop…）<br><a id="more"></a></p>
<h2 id="Sqoop">Sqoop</h2><h3 id="Sqoop1">Sqoop1</h3><ol>
<li>单张表、全部表、查询条件、Direct方式</li>
<li>RDBMS导入到HDFS、从HDFS导出到RDBMS</li>
<li>增量（增量方式、检查列、上一次的值）</li>
<li>Job用来支持增量和定时（重复执行）</li>
<li>Evaluation（DDL/DML），RDBMS的客户端工具而已</li>
</ol>
<p>增量查询示例（how to identify new data）：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sqoop import \</span><br><span class="line">--connect jdbc:mysql://localhost/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--table emp \</span><br><span class="line">--m <span class="number">1</span> \</span><br><span class="line">--incremental append \</span><br><span class="line">--check-column id \</span><br><span class="line">-last value <span class="number">1205</span></span><br></pre></td></tr></table></figure>
<p>数据不会被更新：append<br>数据会被更新：lastmodified（check-column是一个时间列，表示记录的更新时间）  </p>
<p>定期执行增量任务，推荐采用作业（会自动存储上一次的值）。创建作业，执行作业：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ sqoop job --create myjob \</span><br><span class="line">--import \</span><br><span class="line">--connect jdbc:mysql://localhost/db \</span><br><span class="line">--username root \</span><br><span class="line">--table employee --m <span class="number">1</span></span><br><span class="line"></span><br><span class="line">$ sqoop job --show myjob</span><br><span class="line">Job: myjob </span><br><span class="line"> Tool: import Options:</span><br><span class="line"> ---------------------------- </span><br><span class="line"> direct.import = <span class="literal">true</span></span><br><span class="line"> codegen.input.delimiters.record = <span class="number">0</span></span><br><span class="line"> hdfs.append.dir = <span class="literal">false</span> </span><br><span class="line"> db.table = employee</span><br><span class="line"> ...</span><br><span class="line"> incremental.last.value = <span class="number">1206</span></span><br><span class="line"> ...</span><br><span class="line"></span><br><span class="line">$ sqoop job --exec myjob</span><br></pre></td></tr></table></figure>
<h3 id="Sqoop2">Sqoop2</h3><h2 id="DataX"><a href="https://github.com/alibaba/DataX/wiki/Quick-Start">DataX</a></h2><p>快速入门示例：</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="list">[<span class="keyword">qihuang.zheng@dp0653</span> datax]$ bin/datax.py job/job.json</span><br><span class="line">DataX <span class="list">(<span class="keyword">DATAX-OPENSOURCE-3.0</span>)</span>, From Alibaba !</span><br><span class="line">Copyright <span class="list">(<span class="keyword">C</span>)</span> <span class="number">2010</span><span class="number">-2016</span>, Alibaba Group. All Rights Reserved.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.162 <span class="list">[<span class="keyword">main</span>] INFO  Engine -</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"content"</span>:<span class="list">[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"reader"</span>:&#123;</span><br><span class="line">                <span class="string">"name"</span>:<span class="string">"streamreader"</span>,</span><br><span class="line">                <span class="string">"parameter"</span>:&#123;</span><br><span class="line">                    <span class="string">"column"</span>:<span class="list">[</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="string">"type"</span>:<span class="string">"string"</span>,</span><br><span class="line">                            <span class="string">"value"</span>:<span class="string">"DataX"</span></span><br><span class="line">                        &#125;,</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="string">"type"</span>:<span class="string">"long"</span>,</span><br><span class="line">                            <span class="string">"value"</span>:19890604</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="string">"type"</span>:<span class="string">"date"</span>,</span><br><span class="line">                            <span class="string">"value"</span>:<span class="string">"1989-06-04 00:00:00"</span></span><br><span class="line">                        &#125;,</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="string">"type"</span>:<span class="string">"bool"</span>,</span><br><span class="line">                            <span class="string">"value"</span>:true</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="string">"type"</span>:<span class="string">"bytes"</span>,</span><br><span class="line">                            <span class="string">"value"</span>:<span class="string">"test"</span></span><br><span class="line">                        &#125;</span><br><span class="line">                    ],</span><br><span class="line">                    <span class="string">"sliceRecordCount"</span>:100000</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">"writer"</span>:&#123;</span><br><span class="line">                <span class="string">"name"</span>:<span class="string">"streamwriter"</span>,</span><br><span class="line">                <span class="string">"parameter"</span>:&#123;</span><br><span class="line">                    <span class="string">"encoding"</span>:<span class="string">"UTF-8"</span>,</span><br><span class="line">                    <span class="string">"print"</span>:false</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"setting"</span>:&#123;</span><br><span class="line">        <span class="string">"errorLimit"</span>:&#123;</span><br><span class="line">            <span class="string">"percentage"</span>:0.02,</span><br><span class="line">            <span class="string">"record"</span>:0</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"speed"</span>:&#123;</span><br><span class="line">            <span class="string">"byte"</span>:10485760</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.194 <span class="list">[<span class="keyword">main</span>] WARN  Engine - prioriy set to <span class="number">0</span>, because NumberFormatException, the value is: null</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.196 <span class="list">[<span class="keyword">main</span>] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.196 <span class="list">[<span class="keyword">main</span>] INFO  JobContainer - DataX jobContainer starts job.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.198 <span class="list">[<span class="keyword">main</span>] INFO  JobContainer - Set jobId = <span class="number">0</span></span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.215 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - jobContainer starts to do prepare ...</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.215 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - DataX Reader.Job <span class="list">[<span class="keyword">streamreader</span>] do prepare work .</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.216 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - DataX Writer.Job <span class="list">[<span class="keyword">streamwriter</span>] do prepare work .</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.216 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - jobContainer starts to do split ...</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.217 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - Job set Max-Byte-Speed to <span class="number">10485760</span> bytes.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.217 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - DataX Reader.Job <span class="list">[<span class="keyword">streamreader</span>] splits to <span class="list">[<span class="keyword">1</span>] tasks.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.218 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - DataX Writer.Job <span class="list">[<span class="keyword">streamwriter</span>] splits to <span class="list">[<span class="keyword">1</span>] tasks.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.238 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - jobContainer starts to do schedule ...</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.242 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - Scheduler starts <span class="list">[<span class="keyword">1</span>] taskGroups.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.244 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - Running by standalone Mode.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.252 <span class="list">[<span class="keyword">taskGroup-0</span>] INFO  TaskGroupContainer - taskGroupId=<span class="list">[<span class="keyword">0</span>] start <span class="list">[<span class="keyword">1</span>] channels for <span class="list">[<span class="keyword">1</span>] tasks.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.256 <span class="list">[<span class="keyword">taskGroup-0</span>] INFO  Channel - Channel set byte_speed_limit to <span class="number">-1</span>, No bps activated.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.256 <span class="list">[<span class="keyword">taskGroup-0</span>] INFO  Channel - Channel set record_speed_limit to <span class="number">-1</span>, No tps activated.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.267 <span class="list">[<span class="keyword">taskGroup-0</span>] INFO  TaskGroupContainer - taskGroup<span class="list">[<span class="keyword">0</span>] taskId<span class="list">[<span class="keyword">0</span>] attemptCount<span class="list">[<span class="keyword">1</span>] is started</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.568 <span class="list">[<span class="keyword">taskGroup-0</span>] INFO  TaskGroupContainer - taskGroup<span class="list">[<span class="keyword">0</span>] taskId<span class="list">[<span class="keyword">0</span>] is successed, used<span class="list">[<span class="keyword">302</span>]ms</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29.569 <span class="list">[<span class="keyword">taskGroup-0</span>] INFO  TaskGroupContainer - taskGroup<span class="list">[<span class="keyword">0</span>] completed it<span class="variable">'s</span> tasks.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:39.263 <span class="list">[<span class="keyword">job-0</span>] INFO  StandAloneJobContainerCommunicator - Total <span class="number">100000</span> records, <span class="number">2600000</span> bytes | Speed <span class="number">253.91</span>KB/s, <span class="number">10000</span> records/s | Error <span class="number">0</span> records, <span class="number">0</span> bytes |  All Task WaitWriterTime <span class="number">0.016</span>s |  All Task WaitReaderTime <span class="number">0.029</span>s | Percentage <span class="number">100.00</span>%</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:39.264 <span class="list">[<span class="keyword">job-0</span>] INFO  AbstractScheduler - Scheduler accomplished all tasks.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:39.265 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - DataX Writer.Job <span class="list">[<span class="keyword">streamwriter</span>] do post work.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:39.265 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - DataX Reader.Job <span class="list">[<span class="keyword">streamreader</span>] do post work.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:39.265 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - DataX jobId <span class="list">[<span class="keyword">0</span>] completed successfully.</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:39.267 <span class="list">[<span class="keyword">job-0</span>] INFO  HookInvoker - No hook invoked, because base dir not exists or is a file: /home/qihuang.zheng/datax/hook</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:39.270 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer -</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:39.270 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer - PerfTrace not enable!</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:39.271 <span class="list">[<span class="keyword">job-0</span>] INFO  StandAloneJobContainerCommunicator - Total <span class="number">100000</span> records, <span class="number">2600000</span> bytes | Speed <span class="number">253.91</span>KB/s, <span class="number">10000</span> records/s | Error <span class="number">0</span> records, <span class="number">0</span> bytes |  All Task WaitWriterTime <span class="number">0.016</span>s |  All Task WaitReaderTime <span class="number">0.029</span>s | Percentage <span class="number">100.00</span>%</span><br><span class="line"><span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:39.273 <span class="list">[<span class="keyword">job-0</span>] INFO  JobContainer -</span><br><span class="line">任务启动时刻                    : <span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:29</span><br><span class="line">任务结束时刻                    : <span class="number">2016</span><span class="number">-10</span><span class="number">-27</span> <span class="number">14</span>:46:39</span><br><span class="line">任务总计耗时                    :                 <span class="number">10</span>s</span><br><span class="line">任务平均流量                    :          <span class="number">253.91</span>KB/s</span><br><span class="line">记录写入速度                    :          <span class="number">10000</span>rec/s</span><br><span class="line">读出记录总数                    :              <span class="number">100000</span></span><br><span class="line">读写失败总数                    :                   <span class="number">0</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><br></pre></td></tr></table></figure>
<h3 id="实验：MySQL-&gt;HDFS">实验：MySQL-&gt;HDFS</h3><p>下面的实验中DataX部署在本机，MySQL也是本机，HDFS是测试环境。  </p>
<ol>
<li>查看模板：</li>
</ol>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">$ bin/datax.py -r mysqlreader -w hdfswriter</span><br><span class="line">Please refer <span class="keyword">to</span> the mysqlreader document: https://github.com/alibaba/DataX/blob/master/mysqlreader/doc/mysqlreader.md</span><br><span class="line">Please refer <span class="keyword">to</span> the hdfswriter document: https://github.com/alibaba/DataX/blob/master/hdfswriter/doc/hdfswriter.md</span><br><span class="line">Please save the following configuration <span class="keyword">as</span> a json file and  use python &#123;DATAX_HOME&#125;/bin/datax.py &#123;JSON_FILE_NAME&#125;.json <span class="keyword">to</span> run the job.</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"job"</span>: &#123;</span><br><span class="line">        <span class="string">"content"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"reader"</span>: &#123;</span><br><span class="line">                    <span class="string">"name"</span>: <span class="string">"mysqlreader"</span>,</span><br><span class="line">                    <span class="string">"parameter"</span>: &#123;</span><br><span class="line">                        <span class="string">"column"</span>: [],</span><br><span class="line">                        <span class="string">"connection"</span>: [</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="string">"jdbcUrl"</span>: [],</span><br><span class="line">                                <span class="string">"table"</span>: []</span><br><span class="line">                            &#125;</span><br><span class="line">                        ],</span><br><span class="line">                        <span class="string">"password"</span>: <span class="string">""</span>,</span><br><span class="line">                        <span class="string">"username"</span>: <span class="string">""</span>,</span><br><span class="line">                        <span class="string">"where"</span>: <span class="string">""</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">"writer"</span>: &#123;</span><br><span class="line">                    <span class="string">"name"</span>: <span class="string">"hdfswriter"</span>,</span><br><span class="line">                    <span class="string">"parameter"</span>: &#123;</span><br><span class="line">                        <span class="string">"column"</span>: [],</span><br><span class="line">                        <span class="string">"compress"</span>: <span class="string">""</span>,</span><br><span class="line">                        <span class="string">"defaultFS"</span>: <span class="string">""</span>,</span><br><span class="line">                        <span class="string">"fieldDelimiter"</span>: <span class="string">""</span>,</span><br><span class="line">                        <span class="string">"fileName"</span>: <span class="string">""</span>,</span><br><span class="line">                        <span class="string">"fileType"</span>: <span class="string">""</span>,</span><br><span class="line">                        <span class="string">"path"</span>: <span class="string">""</span>,</span><br><span class="line">                        <span class="string">"writeMode"</span>: <span class="string">""</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="string">"setting"</span>: &#123;</span><br><span class="line">            <span class="string">"speed"</span>: &#123;</span><br><span class="line">                <span class="string">"channel"</span>: <span class="string">""</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>准备MySQL表和Hive表</li>
</ol>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#mysql</span><br><span class="line">CREATE TABLE <span class="code">`task`</span> (</span><br><span class="line"><span class="code">  `id` bigint(20) NOT NULL,</span></span><br><span class="line"><span class="code">  `name` varchar(50) NOT NULL,</span></span><br><span class="line"><span class="code">  `done` tinyint(1) DEFAULT NULL,</span></span><br><span class="line"><span class="code">  PRIMARY KEY (`id`)</span></span><br><span class="line">);</span><br><span class="line">插入一些MySQL数据</span><br><span class="line"><span class="header">mysql&gt; select * from a;</span><br><span class="line">+------+-------+</span></span><br><span class="line"><span class="header">| id   | price |</span><br><span class="line">+------+-------+</span></span><br><span class="line">|    1 |    15 |</span><br><span class="line">|    2 |    25 |</span><br><span class="line">|    3 |    10 |</span><br><span class="line">|    4 |    45 |</span><br><span class="line">|    5 |    10 |</span><br><span class="line"><span class="header">|    6 |    10 |</span><br><span class="line">+------+-------+</span></span><br><span class="line">6 rows in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">#hive</span><br><span class="line">create EXTERNAL table text<span class="emphasis">_table(</span><br><span class="line">  id INT,</span><br><span class="line">  price INT</span><br><span class="line">)</span><br><span class="line">row format delimited</span><br><span class="line">fields terminated by "\t"</span><br><span class="line">STORED AS TEXTFILE</span><br><span class="line">location '/user/qihuang.zheng/text_</span>table';</span><br></pre></td></tr></table></figure>
<ol>
<li>测试环境的一些注意点：</li>
</ol>
<p>1) 修改权限</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/usr/i</span>nstall<span class="regexp">/hadoop/</span>bin<span class="regexp">/hadoop fs -mkdir /u</span>ser<span class="regexp">/qihuang.zheng/</span>text_table</span><br><span class="line"><span class="regexp">/usr/i</span>nstall<span class="regexp">/hadoop/</span>bin<span class="regexp">/hadoop fs -chmod 777 /u</span>ser<span class="regexp">/qihuang.zheng/</span></span><br></pre></td></tr></table></figure>
<p>2) hdfs://tdfs修改为主机端口，因为datax在本机运行，不认识tdfs。</p>
<p>3) 模板给出的都必须填写，不允许有空字符串，比如MySQL的用户名密码，channel数量等</p>
<ol>
<li>job/mysql2hdfs.job</li>
</ol>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "<span class="attribute">job</span>": <span class="value">&#123;</span><br><span class="line">        "<span class="attribute">content</span>": <span class="value">[</span><br><span class="line">            &#123;</span><br><span class="line">                "<span class="attribute">reader</span>": <span class="value">&#123;</span><br><span class="line">                    "<span class="attribute">name</span>": <span class="value"><span class="string">"mysqlreader"</span></span>,</span><br><span class="line">                    "<span class="attribute">parameter</span>": <span class="value">&#123;</span><br><span class="line">                        "<span class="attribute">column</span>": <span class="value">[<span class="string">"id"</span>,<span class="string">"price"</span>]</span>,</span><br><span class="line">                        "<span class="attribute">connection</span>": <span class="value">[</span><br><span class="line">                            &#123;</span><br><span class="line">                                "<span class="attribute">jdbcUrl</span>": <span class="value">[<span class="string">"jdbc:mysql://127.0.0.1:3306/test"</span>]</span>,</span><br><span class="line">                                "<span class="attribute">table</span>": <span class="value">[<span class="string">"a"</span>]</span><br><span class="line">                            </span>&#125;</span><br><span class="line">                        ]</span>,</span><br><span class="line">                        "<span class="attribute">password</span>": <span class="value"><span class="string">"root"</span></span>,</span><br><span class="line">                        "<span class="attribute">username</span>": <span class="value"><span class="string">"root"</span></span>,</span><br><span class="line">                        "<span class="attribute">where</span>": <span class="value"><span class="string">""</span></span><br><span class="line">                    </span>&#125;</span><br><span class="line">                </span>&#125;</span>,</span><br><span class="line">                "<span class="attribute">writer</span>": <span class="value">&#123;</span><br><span class="line">                    "<span class="attribute">name</span>": <span class="value"><span class="string">"hdfswriter"</span></span>,</span><br><span class="line">                    "<span class="attribute">parameter</span>": <span class="value">&#123;</span><br><span class="line">                        "<span class="attribute">column</span>": <span class="value">[</span><br><span class="line">                            &#123;</span><br><span class="line">                                "<span class="attribute">name</span>": <span class="value"><span class="string">"id"</span></span>,</span><br><span class="line">                                "<span class="attribute">type</span>": <span class="value"><span class="string">"INT"</span></span><br><span class="line">                            </span>&#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                "<span class="attribute">name</span>": <span class="value"><span class="string">"price"</span></span>,</span><br><span class="line">                                "<span class="attribute">type</span>": <span class="value"><span class="string">"INT"</span></span><br><span class="line">                            </span>&#125;</span><br><span class="line">                        ]</span>,</span><br><span class="line">                        "<span class="attribute">compress</span>": <span class="value"><span class="string">"GZIP"</span></span>,</span><br><span class="line">                        "<span class="attribute">defaultFS</span>": <span class="value"><span class="string">"hdfs://192.168.6.52:9000"</span></span>,</span><br><span class="line">                        "<span class="attribute">fieldDelimiter</span>": <span class="value"><span class="string">"\t"</span></span>,</span><br><span class="line">                        "<span class="attribute">fileName</span>": <span class="value"><span class="string">"text"</span></span>,</span><br><span class="line">                        "<span class="attribute">fileType</span>": <span class="value"><span class="string">"text"</span></span>,</span><br><span class="line">                        "<span class="attribute">path</span>": <span class="value"><span class="string">"/user/qihuang.zheng/text_table"</span></span>,</span><br><span class="line">                        "<span class="attribute">writeMode</span>": <span class="value"><span class="string">"append"</span></span><br><span class="line">                    </span>&#125;</span><br><span class="line">                </span>&#125;</span><br><span class="line">            </span>&#125;</span><br><span class="line">        ]</span>,</span><br><span class="line">        "<span class="attribute">setting</span>": <span class="value">&#123;</span><br><span class="line">            "<span class="attribute">speed</span>": <span class="value">&#123;</span><br><span class="line">                "<span class="attribute">channel</span>": <span class="value"><span class="string">"2"</span></span><br><span class="line">            </span>&#125;</span><br><span class="line">        </span>&#125;</span><br><span class="line">    </span>&#125;</span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li>执行任务</li>
</ol>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">➜  datax bin/datax.py job/mysql2hdfs.json</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:07.45</span>4 [main] WARN  Engine - prioriy set to 0, because NumberFormatException, the value is: null</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:07.46</span>1 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:07.46</span>2 [main] INFO  JobContainer - DataX jobContainer starts job.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:07.46</span>5 [main] INFO  JobContainer - Set jobId = 0</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:08.27</span>4 [job-0] INFO  OriginalConfPretreatmentUtil - Available jdbcUrl:jdbc:mysql://<span class="number">127.0.0.1</span>:3306/test?yearIsDateType=false&amp;zeroDateTimeBehavior=convertToNull&amp;tinyInt1isBit=false&amp;rewriteBatchedStatements=true.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:08.31</span>2 [job-0] INFO  OriginalConfPretreatmentUtil - table:[a] has columns:[id,price].</span><br><span class="line">十月 27, <span class="number">2016 4:26</span>:10 下午 org.apache.hadoop.util.NativeCodeLoader &lt;clinit&gt;</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:12.66</span>5 [job-0] INFO  JobContainer - jobContainer starts to do prepare ...</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:12.66</span>6 [job-0] INFO  JobContainer - DataX Reader.Job [mysqlreader] do prepare work .</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:12.66</span>7 [job-0] INFO  JobContainer - DataX Writer.Job [hdfswriter] do prepare work .</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.00</span>8 [job-0] INFO  HdfsWriter$Job - 由于您配置了writeMode append, 写入前不做清理工作, [/user/qihuang.zheng/text_table] 目录下写入相应文件名前缀  [text] 的文件</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.00</span>8 [job-0] INFO  JobContainer - jobContainer starts to do split ...</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.00</span>9 [job-0] INFO  JobContainer - Job set Channel-Number to 2 channels.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.02</span>1 [job-0] INFO  JobContainer - DataX Reader.Job [mysqlreader] splits to [1] tasks.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.02</span>2 [job-0] INFO  HdfsWriter$Job - begin do split...</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.03</span>1 [job-0] INFO  HdfsWriter$Job - splited write file name:[hdfs://<span class="number">192.168.6.52</span>:9000/user/qihuang.zheng/text_table__8a0c1ab7_af4b_4228_8ff<span class="number">0_36e138f2</span>aa31/text__ec1afe8c_<span class="number">5286_4372</span>_afb<span class="number">8_91d99223</span>c61d]</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.03</span>1 [job-0] INFO  HdfsWriter$Job - end do split.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.03</span>1 [job-0] INFO  JobContainer - DataX Writer.Job [hdfswriter] splits to [1] tasks.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.07</span>0 [job-0] INFO  JobContainer - jobContainer starts to do schedule ...</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.08</span>1 [job-0] INFO  JobContainer - Scheduler starts [1] taskGroups.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.09</span>3 [job-0] INFO  JobContainer - Running by standalone Mode.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.133</span> [taskGroup-0] INFO  TaskGroupContainer - taskGroupId=[0] start [1] channels for [1] tasks.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.157</span> [taskGroup-0] INFO  Channel - Channel set byte_speed_limit to -1, No bps activated.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.157</span> [taskGroup-0] INFO  Channel - Channel set record_speed_limit to -1, No tps activated.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.197</span> [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] taskId[0] attemptCount[1] is started</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.219</span> [0-0-0-reader] INFO  CommonRdbmsReader$Task - Begin to read record by Sql: [select id,price from a</span><br><span class="line">] jdbcUrl:[jdbc:mysql://<span class="number">127.0.0.1</span>:3306/test?yearIsDateType=false&amp;zeroDateTimeBehavior=convertToNull&amp;tinyInt1isBit=false&amp;rewriteBatchedStatements=true].</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.30</span>4 [0-0-0-writer] INFO  HdfsWriter$Task - begin do write...</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.30</span>5 [0-0-0-writer] INFO  HdfsWriter$Task - write to file : [hdfs://<span class="number">192.168.6.52</span>:9000/user/qihuang.zheng/text_table__8a0c1ab7_af4b_4228_8ff<span class="number">0_36e138f2</span>aa31/text__ec1afe8c_<span class="number">5286_4372</span>_afb<span class="number">8_91d99223</span>c61d]</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.31</span>4 [0-0-0-reader] INFO  CommonRdbmsReader$Task - Finished read record by Sql: [select id,price from a</span><br><span class="line">] jdbcUrl:[jdbc:mysql://<span class="number">127.0.0.1</span>:3306/test?yearIsDateType=false&amp;zeroDateTimeBehavior=convertToNull&amp;tinyInt1isBit=false&amp;rewriteBatchedStatements=true].</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:13.91</span>6 [0-0-0-writer] INFO  HdfsWriter$Task - end do write</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:14.01</span>3 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] taskId[0] is successed, used[823]ms</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:14.01</span>4 [taskGroup-0] INFO  TaskGroupContainer - taskGroup[0] completed it's tasks.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.185</span> [job-0] INFO  StandAloneJobContainerCommunicator - Total 6 records, 18 bytes | Speed 1B/s, 0 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.185</span> [job-0] INFO  AbstractScheduler - Scheduler accomplished all tasks.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.186</span> [job-0] INFO  JobContainer - DataX Writer.Job [hdfswriter] do post work.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.187</span> [job-0] INFO  HdfsWriter$Job - start rename file [hdfs://<span class="number">192.168.6.52</span>:9000/user/qihuang.zheng/text_table__8a0c1ab7_af4b_4228_8ff<span class="number">0_36e138f2</span>aa31/text__ec1afe8c_<span class="number">5286_4372</span>_afb<span class="number">8_91d99223</span>c61d.gz] to file [hdfs://<span class="number">192.168.6.52</span>:9000/user/qihuang.zheng/text_table/text__ec1afe8c_<span class="number">5286_4372</span>_afb<span class="number">8_91d99223</span>c61d.gz].</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.213</span> [job-0] INFO  HdfsWriter$Job - finish rename file [hdfs://<span class="number">192.168.6.52</span>:9000/user/qihuang.zheng/text_table__8a0c1ab7_af4b_4228_8ff<span class="number">0_36e138f2</span>aa31/text__ec1afe8c_<span class="number">5286_4372</span>_afb<span class="number">8_91d99223</span>c61d.gz] to file [hdfs://<span class="number">192.168.6.52</span>:9000/user/qihuang.zheng/text_table/text__ec1afe8c_<span class="number">5286_4372</span>_afb<span class="number">8_91d99223</span>c61d.gz].</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.214</span> [job-0] INFO  HdfsWriter$Job - start delete tmp dir [hdfs://<span class="number">192.168.6.52</span>:9000/user/qihuang.zheng/text_table__8a0c1ab7_af4b_4228_8ff<span class="number">0_36e138f2</span>aa31] .</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.239</span> [job-0] INFO  HdfsWriter$Job - finish delete tmp dir [hdfs://<span class="number">192.168.6.52</span>:9000/user/qihuang.zheng/text_table__8a0c1ab7_af4b_4228_8ff<span class="number">0_36e138f2</span>aa31] .</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.240</span> [job-0] INFO  JobContainer - DataX Reader.Job [mysqlreader] do post work.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.241</span> [job-0] INFO  JobContainer - DataX jobId [0] completed successfully.</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.244</span> [job-0] INFO  HookInvoker - No hook invoked, because base dir not exists or is a file: /Users/zhengqh/Soft/datax/hook</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.35</span>3 [job-0] INFO  JobContainer -</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.35</span>3 [job-0] INFO  JobContainer - PerfTrace not enable!</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.35</span>5 [job-0] INFO  StandAloneJobContainerCommunicator - Total 6 records, 18 bytes | Speed 1B/s, 0 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%</span><br><span class="line"><span class="number">2016-10-27</span> <span class="number">16:26:23.35</span>7 [job-0] INFO  JobContainer -</span><br><span class="line">任务启动时刻                    : <span class="number">2016-10-27</span> 16:26:07</span><br><span class="line">任务结束时刻                    : <span class="number">2016-10-27</span> 16:26:23</span><br><span class="line">任务总计耗时                    :                 15s</span><br><span class="line">任务平均流量                    :                1B/s</span><br><span class="line">记录写入速度                    :              0rec/s</span><br><span class="line">读出记录总数                    :                   6</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure>
<ol>
<li>验证数据</li>
</ol>
<p>查询HIVE表</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from text_table;</span><br><span class="line">OK</span><br><span class="line"><span class="number">1</span>   <span class="number">15</span></span><br><span class="line"><span class="number">2</span>   <span class="number">25</span></span><br><span class="line"><span class="number">3</span>   <span class="number">10</span></span><br><span class="line"><span class="number">4</span>   <span class="number">45</span></span><br><span class="line"><span class="number">5</span>   <span class="number">10</span></span><br><span class="line"><span class="number">6</span>   <span class="number">10</span></span><br><span class="line">Time taken: <span class="number">0.305</span> seconds, Fetched: <span class="number">6</span> row(s)</span><br></pre></td></tr></table></figure>
<p>查询HDFS</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 ~]$ hadoop fs -ls /user/qihuang.zheng/text_table</span><br><span class="line">Found <span class="number">1</span> items</span><br><span class="line"><span class="number">2016</span>-<span class="number">10</span>-<span class="number">27</span> <span class="number">16</span>:<span class="number">26</span> /user/qihuang.zheng/text_table/text__ec1afe8c_5286_4372_afb8_91d99223c61d.gz</span><br><span class="line">[qihuang.zheng@dp0653 ~]$ hadoop fs -text /user/qihuang.zheng/text_table/text*</span><br><span class="line"><span class="number">1</span>   <span class="number">15</span></span><br><span class="line"><span class="number">2</span>   <span class="number">25</span></span><br><span class="line"><span class="number">3</span>   <span class="number">10</span></span><br><span class="line"><span class="number">4</span>   <span class="number">45</span></span><br><span class="line"><span class="number">5</span>   <span class="number">10</span></span><br><span class="line"><span class="number">6</span>   <span class="number">10</span></span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ETL Tools（Sqoop…）&lt;br&gt;
    
    </summary>
    
      <category term="etl" scheme="http://github.com/zqhxuyuan/categories/etl/"/>
    
    
      <category term="etl" scheme="http://github.com/zqhxuyuan/tags/etl/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Streams 博客</title>
    <link href="http://github.com/zqhxuyuan/2016/11/06/Kafka-Streams-blog/"/>
    <id>http://github.com/zqhxuyuan/2016/11/06/Kafka-Streams-blog/</id>
    <published>2016-11-05T16:00:00.000Z</published>
    <updated>2016-11-06T14:44:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka Streams流处理 英文博客翻译<br><a id="more"></a></p>
<h1 id="Kafka_Process_API">Kafka Process API</h1><p><a href="http://codingjunkie.net/kafka-processor-part1/" target="_blank" rel="external">http://codingjunkie.net/kafka-processor-part1/</a></p>
<p>The aim of the Processor API is to introduce a client to enable processing data consumed from Kafka and writing the results back into Kafka. There are two components of the processor client:</p>
<ol>
<li>A “lower-level” processor that providea API’s for data-processing, composable processing and local state storage.</li>
<li>A “higher-level” stream DSL that would cover most processor implementation needs.</li>
</ol>
<p>Potential Use Cases For the Processor API</p>
<ol>
<li>There is a need for notification/alerts on singular values as they are processed. In other words the business requirements are such that you don’t need to establish patterns or examine the value(s) in context with other data being processed. For example you want immediate notification that a fraudulent credit card has been used.</li>
<li>You filter your data when running analytics. Filtering out a medium to large percentage of data ideally should be re-partitioned to avoid data-skew issues. Partitioning is an expensive operation, so by filtering out what data is delivered to your analytics cluster, you can save the filter-repartition step.</li>
<li>You want to run analytics on only a portion of your source data, while delivering the entirety of you data to another store.</li>
</ol>
<h1 id="KStream_API">KStream API</h1><h1 id="使用Kafka_Streams在用户活动事件流上做分布式实时join和聚合">使用Kafka Streams在用户活动事件流上做分布式实时join和聚合</h1><p><a href="https://www.confluent.io/blog/distributed-real-time-joins-and-aggregations-on-user-activity-events-using-kafka-streams/" target="_blank" rel="external">https://www.confluent.io/blog/distributed-real-time-joins-and-aggregations-on-user-activity-events-using-kafka-streams/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kafka Streams流处理 英文博客翻译&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka Streams中文翻译</title>
    <link href="http://github.com/zqhxuyuan/2016/11/02/Kafka-Streams-cn/"/>
    <id>http://github.com/zqhxuyuan/2016/11/02/Kafka-Streams-cn/</id>
    <published>2016-11-01T16:00:00.000Z</published>
    <updated>2017-01-02T04:03:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>Confluent Kafka Streams Documentation 中文翻译：  <a href="http://docs.confluent.io/3.0.1/streams/introduction.html" target="_blank" rel="external">http://docs.confluent.io/3.0.1/streams/introduction.html</a><br><a id="more"></a></p>
<h1 id="介绍">介绍</h1><h2 id="Kafka_Streams">Kafka Streams</h2><p>Kafka Streams, a component of open source Apache Kafka, is a powerful, easy-to-use library for building highly scalable, fault-tolerant, distributed stream processing applications on top of Apache Kafka. It builds upon important concepts for stream processing such as properly distinguishing between event-time and processing-time, handling of late-arriving data, and efficient management of application state.</p>
<blockquote>
<p>Kafka Streams是构建在Apache Kafka的一个组件，它是一个功能强大的、对于构建高可用、故障容错、分布式流处理应用程序都很容易使用的库。它构建在流处理的重要概念之上，比如正确地区分事件时间（event-time）和处理时间（process-time），处理延时数据，高效的应用程序状态管理。  </p>
</blockquote>
<p>One of the mantras(祷文) of Kafka Streams is to “Build apps, not clusters!”, which means to bring stream processing out of the Big Data niche into the world of mainstream application development. Using the Kafka Streams library you can implement standard Java applications to solve your stream processing needs – whether at small or at large scale – and then run these applications on client machines at the perimeter(边界) of your Kafka cluster. Deployment-wise you are free to chose from any technology that can deploy Java applications, including but not limited to Puppet, Chef, Ansible, Docker, Mesos, YARN, Kubernetes, and so on. This lightweight and integrative(综合) approach of Kafka Streams is in stark(完全、突出) contrast(对比) to other stream processing tools that require you to install and operate separate stream processing clusters and similar heavy-weight infrastructure that come with their own special set of rules on how to use and interact with them.</p>
<blockquote>
<p>Kafka Streams的一个思想是“构建应用程序，不要集群”，这意味着将流处理从大数据生态圈中解放出来，而专注于主流的应用程序开发。使用Kafka Streams客户端库，你可以用标准的Java应用程序（main方法）来解决你的流处理需求（不管是小规模还是大规模的数据），然后可以在你的Kafka集群之外的客户端机器执行这些应用程序。你可以选择任何可以部署Java应用的技术来部署Kafka Streams，包括但不限于Puppet、Chef、Ansible、Docker、Mesos、YARN、Kubernetes等等。Kafka Streams的轻量级以及综合能力使得它和其他流处理工具形成了鲜明的对比，后者需要你单独安装并维护一个流处理集群，需要依赖重量级的基础架构设施。</p>
</blockquote>
<p>The following list highlights several key capabilities and aspects of Kafka Streams that make it a compelling(引人注目) choice for use cases such as stream processing applications, event-driven systems, continuous queries and transformations, reactive applications, and microservices.</p>
<blockquote>
<p>下面列出了Kafka Streams的几个重要的功能，对于这些用例都是个吸引人的选择：流处理应用程序、事件驱动系统、持续查询和转换、响应式应用程序、微服务。</p>
</blockquote>
<p><strong>Powerful</strong>功能强大</p>
<ul>
<li>Highly scalable, elastic, fault-tolerant 高可用、可扩展性、故障容错</li>
<li>Stateful and stateless processing 有状态和无状态的处理</li>
<li>Event-time processing with windowing, joins, aggregations 针对事件的窗口函数、联合操作、聚合操作</li>
</ul>
<p><strong>Lightweight</strong>轻量级</p>
<ul>
<li>No dedicated cluster required 不需要专用的集群</li>
<li>No external dependencies 不需要外部的依赖</li>
<li>“It’s a library, not a framework.” 它是一个客户端库，不是一个框架</li>
</ul>
<p><strong>Fully integrated</strong>完全完整的</p>
<ul>
<li>100% compatible with Kafka 0.10.0.x 和Kafka完全兼容</li>
<li>Easy to integrate into existing applications 和已有应用程序容易集成</li>
<li>No artificial rules for deploying applications 对部署方式没有严格的规则限制</li>
</ul>
<p><strong>Real-time</strong>实时的</p>
<ul>
<li>Millisecond processing latency 微秒级别的处理延迟</li>
<li>Does not micro-batch messages 不是micro-batch处理</li>
<li>Windowing with out-of-order data 对无序数据的窗口操作</li>
<li>Allows for arrival of late data 允许延迟的数据</li>
</ul>
<h2 id="A_closer_look">A closer look</h2><p>Before we dive into the details such as the concepts and architecture of Kafka Streams or getting our feet wet by following the Kafka Streams quickstart guide, let us provide more context to the previous list of capabilities.</p>
<p>在深入研究Kafka Streams的概念和架构细节之前，你应该先看下<a href="http://docs.confluent.io/3.0.1/streams/quickstart.html#streams-quickstart" target="_blank" rel="external">快速指南</a>，现在我们为上面的那些特点提供更多的上下文信息（背景知识）。</p>
<p>1.Stream Processing Made Simple: Designed as a lightweight library in Apache Kafka, much like the Kafka producer and consumer client libraries. You can easily embed and integrate Kafka Streams into your own applications, which is a significant departure from framework-based stream processing tools that dictate many requirements upon you such as how you must package and “submit” processing jobs to their cluster.</p>
<blockquote>
<p>流处理更加简单：被设计为一个轻量级的库，就像Kafka的生产者和消费者客户端库一样。你可以很方便地将Kafka Streams集成到你自己的应用程序中，这是和其他基于框架的流处理工具的主要区别，它们对你会有很多要求，比如你必须打包然后把流处理作业提交到集群上执行。</p>
</blockquote>
<p>Has no external dependencies on systems other than Apache Kafka and can be used in any Java application. Read: You do not need to deploy and operate a separate cluster for your stream processing needs. Your Operations and Info Sec teams, among others, will surely be happy to hear this.</p>
<blockquote>
<p>在应用程序中除了Apache Kafka之外没有别的依赖：你不需要为你的流处理需求部署或维护一个单独的集群。你们的运维和安全团队肯定听到这个信息肯定很happy吧。</p>
</blockquote>
<p>2.Leverages Kafka as its internal messaging layer instead of (re)implementing a custom messaging layer like many other stream processing tools. Notably, Kafka Streams uses Kafka’s partitioning model to horizontally scale processing while maintaining strong ordering guarantees. This ensures high performance, scalability, and operational simplicity for production environments. A key benefit of this design decision is that you do not have to understand and tune two different messaging layers – one for moving data streams at scale (Kafka) plus a separate one for your stream processing tool. Similarly, any performance and reliability improvements of Kafka will automatically be available to Kafka Streams, too, thus tapping into the momentum of Kafka’s strong developer community.</p>
<blockquote>
<p>利用Kafka作为它的内部消息层而不像其他流处理工具一样重新造轮子。特别是，Kafka Streams使用Kafka的分区模型在维护强一致性的同时也具备了线性的处理能力，这种设计的优点是：你不需要理解或者调整两种消息模型（一种是线性地移动数据流，另外一种是流处理的消息）。同样，任何针对Kafka的性能和可靠性的提升，Kafka Streams都会自动具备，这也促使了Kafka开发者社区的动力。</p>
</blockquote>
<p>3.Is agnostic(不可知论) to resource management and configuration tools, so it integrates much more seamlessly(无缝) into the existing development, packaging, deployment, and operational practices of your organization. You are free to use your favorite tools such as Java application servers, Puppet, Ansible, Mesos, YARN, Docker – or even to run your application manually on a single machine for proof-of-concept scenarios.</p>
<blockquote>
<p>Kafka Streams不需要依赖资源管理和配置工具，所以它可以和已有的开发环境、打包、部署等工具无缝集成。可以运行在Java应用服务器，甚至在单机环境下做原型验证（POC）。</p>
</blockquote>
<p>4.Supports fault-tolerant local state, which enables very fast and efficient stateful operations like joins and windowed aggregations. Local state is replicated to Kafka so that, in case of a machine failure, another machine can automatically restore the local state and resume the processing from the point of failure.</p>
<blockquote>
<p>支持本地状态的故障容错，这使得有状态的操作（比如联合、窗口聚合）更快速和高效。由于本地状态本身通过Kafka进行复制，所以当一个机器宕机时，其他机器可以自动恢复本地状态，并且从故障出错的那个点继续处理。</p>
</blockquote>
<p>5.Employs one-record-at-a-time processing to achieve low processing latency, which is crucial(重要，决定性) for a variety of use cases such as fraud detection. This makes Kafka Streams different from micro-batch based stream processing tools.</p>
<blockquote>
<p>一次处理一条记录的流处理模型，所以处理延迟很低，对于像欺诈检测等场景来说非常重要。这也是Kafka Streams有别于基于micro-batch的流处理工具的区别。</p>
</blockquote>
<p>Furthermore, Kafka Streams has a strong focus on usability(可用性) and a great developer experience. It offers all the necessary stream processing primitives to allow applications to read data from Kafka as streams, process the data, and then either write the resulting data back to Kafka or send the final output to an external system. Developers can choose between a high-level DSL with commonly used operations like filter, map, join, as well as a low-level API for developers who need maximum control and flexibility.</p>
<blockquote>
<p>另外，Kafka Streams对开发者是易用和友好的。它提供了所有必要的流处理算子，允许应用程序将从Kafka读取出来的数据作为一个流，然后处理数据，最后可以将处理结果写回到Kafka或者发送给外部系统。开发者可以使用高级DSL（提供很多常用的操作比如filter、map、join）或者低级API两种方式（当需要更好地控制和灵活性时）。</p>
</blockquote>
<p>Finally, Kafka Streams helps with scaling developers, too – yes, the human side – because it has a low barrier(接线) to entry and a smooth path to scale from development to production: You can quickly write and run a small-scale proof-of-concept on a single machine because you don’t need to install or understand a distributed stream processing cluster; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently(透明地) handles the load balancing of multiple instances of the same application by leveraging Kafka’s parallelism model.</p>
<blockquote>
<p>最后，Kafka Streams对于开发者也是扩展的。是的，从程序员的视角来看的话，它从开发环境到生产环境几乎没有界线：你可以在一台机器上运行一个很小批量的POC，因为你不需要安装或者理解一个分布式的流处理集群是怎么样（不需要知道程序在分布式环境下会有什么不同）；在多台机器上时，你只需要多运行几个应用程序实例就可以扩展到大规模的生产负载（生产环境下负载很高，只需多启动几个新的实例）。Kafka Streams会利用Kafka的并行模型透明底在相同应用程序多个实例之间处理负载均衡。</p>
</blockquote>
<p>In summary, Kafka Streams is a compelling choice for building stream processing applications. Give it a try and run your first Hello World Streams application! The next sections in this documentation will get you started.</p>
<blockquote>
<p>总之Kafka Streams对于构建流处理应用程序是一个非常不错的选择。快来运行一个<a href="http://docs.confluent.io/3.0.1/streams/quickstart.html#streams-quickstart" target="_blank" rel="external">Hello World的流处理应用吧</a>。</p>
</blockquote>
<h1 id="快速开始">快速开始</h1><h2 id="本节目标">本节目标</h2><p>The goal of this quickstart guide is to provide you with a first hands-on look at Kafka Streams. We will demonstrate how to run your first Java application that uses the Kafka Streams library by showcasing a simple end-to-end data pipeline powered by Kafka.</p>
<p>It is worth noting that this quickstart will only scratch the surface of Kafka Streams. More details are provided in the remainder of the Kafka Streams documentation, and we will include pointers throughout the quickstart to give you directions.</p>
<blockquote>
<p>本节的目标是让你亲自看看Kafka Streams是如何实现的。我们会向你展示使用Kafka完成的一个端到端的数据流管道，以及运行你的第一个使用Kafka库的Java应用程序。注意这里仅仅会涉及到Kafka Streams的表层，后续的部分会深入一些细节。</p>
</blockquote>
<h2 id="我们要做什么">我们要做什么</h2><p>下面是使用Java8实现的WordCount示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Serializers/deserializers (serde) for String and Long types</span></span><br><span class="line"><span class="keyword">final</span> Serde&lt;String&gt; stringSerde = Serdes.String();</span><br><span class="line"><span class="keyword">final</span> Serde&lt;Long&gt; longSerde = Serdes.Long();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Construct a `KStream` from the input topic ""streams-file-input", where message values</span></span><br><span class="line"><span class="comment">// represent lines of text (for the sake of this example, we ignore whatever may be stored in the message keys).</span></span><br><span class="line">KStream&lt;String, String&gt; textLines = builder.stream(stringSerde, stringSerde, <span class="string">"streams-file-input"</span>);</span><br><span class="line"></span><br><span class="line">KStream&lt;String, Long&gt; wordCounts = textLines</span><br><span class="line">    <span class="comment">// Split each text line, by whitespace, into words.  The text lines are the message</span></span><br><span class="line">    <span class="comment">// values, i.e. we can ignore whatever data is in the message keys and thus invoke</span></span><br><span class="line">    <span class="comment">// `flatMapValues` instead of the more generic `flatMap`.</span></span><br><span class="line">    .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split(<span class="string">"\\W+"</span>)))</span><br><span class="line">    <span class="comment">// We will subsequently invoke `countByKey` to count the occurrences of words, so we use</span></span><br><span class="line">    <span class="comment">// `map` to ensure the words are available as message keys, too.</span></span><br><span class="line">    .map((key, value) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(value, value))</span><br><span class="line">    <span class="comment">// Count the occurrences of each word (message key).</span></span><br><span class="line">    <span class="comment">// This will change the stream type from `KStream&lt;String, String&gt;` to</span></span><br><span class="line">    <span class="comment">// `KTable&lt;String, Long&gt;` (word -&gt; count), hence we must provide serdes for `String` and `Long`.</span></span><br><span class="line">    .countByKey(stringSerde, <span class="string">"Counts"</span>)</span><br><span class="line">    <span class="comment">// Convert the `KTable&lt;String, Long&gt;` into a `KStream&lt;String, Long&gt;`.</span></span><br><span class="line">    .toStream();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Write the `KStream&lt;String, Long&gt;` to the output topic.</span></span><br><span class="line">wordCounts.to(stringSerde, longSerde, <span class="string">"streams-wordcount-output"</span>);</span><br></pre></td></tr></table></figure>
<p>然后，我们会执行如下步骤来完成第一个流应用程序：</p>
<ol>
<li>在一台机器上启动一个Kafka集群</li>
<li>使用Kafka内置的控制台生产者模拟往一个Kafka主题中写入一些示例数据</li>
<li>使用Kafka Streams库处理输入的数据，处理程序就是上面的wordcount示例</li>
<li>使用Kafka内置的控制台消费者检查应用程序的输出</li>
<li>停止Kafka集群</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">wget http://packages.confluent.io/archive/3.0.1/confluent-3.0.1-2.11.zip</span><br><span class="line">unzip confluent-3.0.1-2.11.zip</span><br><span class="line">cd confluent-3.0.1/</span><br><span class="line">bin/zookeeper-server-<span class="operator"><span class="keyword">start</span> ./etc/kafka/zookeeper.properties</span><br><span class="line"><span class="keyword">bin</span>/kafka-<span class="keyword">server</span>-<span class="keyword">start</span> ./etc/kafka/<span class="keyword">server</span>.properties</span><br><span class="line"></span><br><span class="line"><span class="keyword">bin</span>/kafka-topics <span class="comment">--create \</span></span><br><span class="line">          <span class="comment">--zookeeper localhost:2181 \</span></span><br><span class="line">          <span class="comment">--replication-factor 1 \</span></span><br><span class="line">          <span class="comment">--partitions 1 \</span></span><br><span class="line">          <span class="comment">--topic streams-file-input</span></span><br><span class="line"></span><br><span class="line">echo -<span class="keyword">e</span> <span class="string">"all streams lead to kafka\nhello kafka streams\njoin kafka summit"</span> &gt; /tmp/<span class="keyword">file</span>-<span class="keyword">input</span>.txt</span><br><span class="line"></span><br><span class="line">cat /tmp/<span class="keyword">file</span>-<span class="keyword">input</span>.txt | ./<span class="keyword">bin</span>/kafka-console-producer <span class="comment">--broker-list localhost:9092 --topic streams-file-input</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">bin</span>/kafka-run-<span class="keyword">class</span> org.apache.kafka.streams.examples.wordcount.WordCountDemo</span></span><br></pre></td></tr></table></figure>
<p>Similar to the bounded variant, it is a stateful algorithm that tracks and updates the counts of words. However, since it must assume potentially unbounded input data, it will periodically output its current state and results while continuing to process more data because it cannot know when it has processed “all” the input data. This is a typical difference between the class of algorithms that operate on unbounded streams of data and, say, batch processing algorithms such as Hadoop MapReduce. It will be easier to understand this difference once we inspect the actual output data later on.</p>
<blockquote>
<p>WordCount程序会计算输入单词出现次数的直方图，和之前看到的其他应用程序在有界数据集上不同的是，本例是在一个无限的、无界的数据流上操作。和有界操作相同的是，它也是一个有状态的算法（跟踪和更新单词的次数）。不过，由于它必须假设无限的输入数据，它会定时地输出当前状态和结果，并且持续地处理更多的数据，因为它不知道什么时候它已经处理完了所有的输入数据。这和在有界流数据上的算法是不同的比如Hadoop的MapReduce。在我们检查了实际的输出结果后，你就会更加容易地理解这里的不同点。</p>
</blockquote>
<p>The WordCount demo application will read from the input topic streams-file-input, perform the computations of the WordCount algorithm on the input data, and continuously write its current results to the output topic streams-wordcount-output (the names of its input and output topics are hardcoded). The demo will run for a few seconds and then, unlike typical stream processing applications, terminate automatically.</p>
<blockquote>
<p>这个WordCount示例会从Kafka的输入主题“streams-file-input”中读取数据，在输入数据上执行WorldCount算法，并且持续地将当前结果写入到输出主题“streams-wordcount-output”。不过和其他流处理程序不同的是，这里为了实验，仅仅运行几秒钟后就会退出，通常实际运行的流应用程序是永远不会停止的。</p>
</blockquote>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer --zookeeper localhost:<span class="number">2181</span> \</span><br><span class="line">          --topic streams-wordcount-output \</span><br><span class="line">          --from-beginning \</span><br><span class="line">          --formatter kafka<span class="class">.tools</span><span class="class">.DefaultMessageFormatter</span> \</span><br><span class="line">          --property print.key=true \</span><br><span class="line">          --property key.deserializer=org<span class="class">.apache</span><span class="class">.kafka</span><span class="class">.common</span><span class="class">.serialization</span><span class="class">.StringDeserializer</span> \</span><br><span class="line">          --property value.deserializer=org<span class="class">.apache</span><span class="class">.kafka</span><span class="class">.common</span><span class="class">.serialization</span><span class="class">.LongDeserializer</span></span><br></pre></td></tr></table></figure>
<p>打印信息如下，这里第一列是Kafka消息的键（字符串格式），第二列是消息的值（Long类型）。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">all     <span class="number">1</span></span><br><span class="line">streams <span class="number">1</span></span><br><span class="line">lead    <span class="number">1</span></span><br><span class="line">to      <span class="number">1</span></span><br><span class="line">kafka   <span class="number">1</span></span><br><span class="line">hello   <span class="number">1</span></span><br><span class="line">kafka   <span class="number">2</span></span><br><span class="line">streams <span class="number">2</span></span><br><span class="line">join    <span class="number">1</span></span><br><span class="line">kafka   <span class="number">3</span></span><br><span class="line">summit  <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>等等，输出结果看起来有点奇怪，为什么有重复的条目比如”streams”出现了两次，”kafka”出现了三次，难道不应该是下面这样的吗：  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#为什么不是这样，你可能会有疑问</span></span><br><span class="line">all     <span class="number">1</span></span><br><span class="line">lead    <span class="number">1</span></span><br><span class="line">to      <span class="number">1</span></span><br><span class="line">hello   <span class="number">1</span></span><br><span class="line">streams <span class="number">2</span></span><br><span class="line">join    <span class="number">1</span></span><br><span class="line">kafka   <span class="number">3</span></span><br><span class="line">summit  <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>The explanation is that the output of the WordCount application is actually a continuous stream of updates, where each data record (i.e. each line in the original output above) is an updated count of a single word aka record key such as “kafka”. For multiple records with the same key, each later record is an update of the previous one.</p>
<blockquote>
<p>合理的解释是：WordCount应用程序的输出实际上是一个持续不断的”更新流”，每条数据记录（上面示例中每一行的输出结果）都是一个单词（记录的key，比如”kafka”）的更新次数。对于相同key的多条记录，后面的记录都是对前面记录的更新。</p>
</blockquote>
<p>The two diagrams below illustrate what is essentially(本质) happening behind the scenes. The first column shows the evolution of the current state of the KTable<string, long=""> that is counting word occurrences for countByKey. The second column shows the change records that result from state updates to the KTable and that eventually, once converted to a KStream</string,></p>
<blockquote>
<p>下面的两幅图展示了发生在背后的本质，第一列表示<code>KTable&lt;String, Long&gt;</code>的当前状态的进化，通过<code>countByKey</code>计算单词的出现次数。第二列的结果显示了从状态改变到KTable的变更记录，最终被转换为一个KStream。</p>
</blockquote>
<p>First the text line “all streams lead to kafka” is being processed. The KTable is being built up as each new word results in a new table entry (highlighted with a green background), and a corresponding change record is sent to the downstream KStream.</p>
<p>When the second text line “hello kafka streams” is processed, we observe, for the first time, that existing entries in the KTable are being updated (here: for the words “kafka” and for “streams”). And again, change records are being sent to the KStream.</p>
<blockquote>
<p>当第一次处理文本行“all streams lead to kafka”时，KTable会在每个表的条目中构建一个新的单词结果（绿色高亮），并且<strong>把对应的变更记录发送给下游的KStream</strong>。<br>当处理第二个文本行“hello kafka streams”时，我们注意到，和第一次不同的是，存在于KTable的条目会被更新（比如这里的”kafka”和”streams”），并且同样的，变更记录也会被发送到KStream。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161103092411029" alt="10-1 ktable kstream"></p>
<p>And so on (we skip the illustration of how the third line is being processed). This explains why the output topic has the contents we showed above, because it contains the full record of changes, i.e. the information shown in the second column for KStream above:</p>
<blockquote>
<p>第三行的处理也是类似的，这里就不再累述。这就解释了为什么上面输出的主题内容是我们看到的那样，因为它包含了所有完整的变更记录，即上面第二列KStream的内容：</p>
</blockquote>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">all     <span class="number">1</span></span><br><span class="line">streams <span class="number">1</span></span><br><span class="line">lead    <span class="number">1</span></span><br><span class="line"><span class="built_in">to</span>      <span class="number">1</span></span><br><span class="line">kafka   <span class="number">1</span>  &lt;- <span class="keyword">first</span> <span class="built_in">line</span> <span class="function"><span class="keyword">end</span></span><br><span class="line"><span class="title">hello</span>   <span class="title">1</span></span></span><br><span class="line">kafka   <span class="number">2</span></span><br><span class="line">streams <span class="number">2</span>  &lt;- <span class="keyword">second</span> <span class="built_in">line</span> <span class="function"><span class="keyword">end</span></span><br><span class="line"><span class="title">join</span>    <span class="title">1</span></span></span><br><span class="line">kafka   <span class="number">3</span></span><br><span class="line">summit  <span class="number">1</span>  &lt;- <span class="keyword">third</span> <span class="built_in">line</span> <span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>为什么不输出KTable，这是因为KTable每次处理一条记录，都会发送变更记录给下游的KStream，即KTable每次处理一条记录，产生一条变更记录。而KTable本身是有状态的，可以看到在处理第一个单词时，KTable有一条记录，在处理第二个不同的单词时，KTable有两条记录，这个状态是一直保存的，如果说把KTable作为输出，那么就会有重复的问题，比如下面这样的输出肯定不是我们希望看到的：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">all     <span class="number">1</span>  &lt;-处理第一个单词后的KTable</span><br><span class="line">all     <span class="number">1</span></span><br><span class="line">streams <span class="number">1</span>  &lt;-处理第二个单词后的KTable</span><br><span class="line">all     <span class="number">1</span></span><br><span class="line">streams <span class="number">1</span></span><br><span class="line">lead    <span class="number">1</span>  &lt;-处理第三个单词后的KTable</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>Looking beyond the scope of this concrete example, what Kafka Streams is doing here is to leverage the duality(二元性，对偶) between a table and a changelog stream (here: table = the KTable, changelog stream = the downstream KStream): you can publish every change of the table to a stream, and if you consume the entire changelog stream from beginning to end, you can reconstruct the contents of the table.</p>
<blockquote>
<p>Kafka Streams这里做的工作利用了一张表和一个变更流的二元性（表指的是KTable，变更流指的是下游的KStream）：你可以将表的每个变更记录发布给一个流，如果你从整个变更流的最开始消费到最后，你就可以重新构造出表的内容。</p>
</blockquote>
<h1 id="概念">概念</h1><h2 id="Kafka_101">Kafka 101</h2><p>Kafka Streams is, by deliberate(深思熟虑) design, tightly integrated with Apache Kafka: it uses Kafka as its internal messaging layer. As such it is important to familiarize yourself with the key concepts of Kafka, too, notably the sections 1. Getting Started and 4. Design in the Kafka documentation. In particular you should understand:</p>
<p>Kafka Streams是经过深思熟虑的设计，它和Apache Kafka仅仅地集成：它使用Kafka作为内部的消息层。所以理解Kafka的关键概念非常重要，如果不熟悉，可以看Kafka的文档。</p>
<ul>
<li>The who’s who: Kafka distinguishes producers, consumers, and brokers. In short, producers publish data to Kafka brokers, and consumers read published data from Kafka brokers. Producers and consumers are totally decoupled. A Kafka cluster consists of one or more brokers.</li>
<li>The data: Data is stored in topics. The topic is the most important abstraction provided by Kafka: it is a category or feed name to which data is published by producers. Every topic in Kafka is split into one or more partitions, which are replicated across Kafka brokers for fault tolerance.</li>
<li>Parallelism: Partitions of Kafka topics, and especially their number for a given topic, are also the main factor that determines the parallelism of Kafka with regards to reading and writing data. Because of their tight integration the parallelism of Kafka Streams is heavily influenced by and depending on Kafka’s parallelism.</li>
</ul>
<ol>
<li>Kafka分成生产者、消费者、Brokers。生产者发布数据给Kafka的Brokers，消费者从Kafka的Brokers读取发布过的数据。生产者和消费者完全解耦。一个Kafka集群包括一个或多个Broekrs节点。</li>
<li>数据以主题的形式存储。主题是Kafka提供的最重要的一个抽象：它是生产者发布数据的一种分类（相同类型的消息应该发布到相同的主题）。每个主题会分成一个或多个分区，并且为了故障容错，每个分区都会在Kafka的Brokers中进行复制。</li>
<li>Kafka主题的分区数量决定了读取或写入数据的并行度。因为Kafka Streams和Kafka结合的很紧，所以Kafka Streams也依赖于Kafka的并行度。</li>
</ol>
<h2 id="流、流处理、拓扑、算子">流、流处理、拓扑、算子</h2><p>A stream is the most important abstraction provided by Kafka Streams: it represents an unbounded, continuously updating data set, where unbounded means “of unknown or of unlimited size”. A stream is an ordered, replayable, and fault-tolerant sequence of immutable data records, where a data record is defined as a key-value pair.</p>
<blockquote>
<p>流是Kafka Streams提供的最重要的抽象：它代表了一个无界的、持续更新的数据集。流是一个有序的、可重放的、容错的不可变数据记录序列，其中每个数据记录被定义成一个key-value键值对</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161103101744946" alt="stream-record">  </p>
<p>A stream processing application is any program that makes use of the Kafka Streams library. In practice, this means it is probably “your” Java application. It may define its computational logic through one or more processor topologies (see next section).</p>
<blockquote>
<p>流处理应用程序是任何使用了Kafka Streams库进行开发的应用程序，它会通过一个或多个处理拓扑定义计算逻辑。</p>
</blockquote>
<p>A processor topology or simply topology defines the computational logic of the data processing that needs to be performed by a stream processing application. A topology is a graph of stream processors (nodes) that are connected by streams (edges). Developers can define topologies either via the low-level Processor API or via the Kafka Streams DSL, which builds on top of the former.</p>
<p>处理拓扑或者叫拓扑定义了流处理应用程序对数据处理的计算逻辑。拓扑是一张由流处理算子和相连接的流组成的DAG图，其中算子是图的节点，流是图的边。开发者可以通过低级的Processor API或者高级的Kafka Streams DSL定义拓扑，其中后者实际上是构建在前者之上的。</p>
<p>A stream processor is a node in the processor topology(as shown in the diagram of section Processor Topology). It represents a processing step in a topology, i.e. it is used to transform data in streams. Standard operations such as map, filter, join, and aggregations are examples of stream processors that are available in Kafka Streams out of the box. A stream processor receives one input record at a time from its upstream processors in the topology, applies its operation to it, and may subsequently produce one or more output records to its downstream processors.</p>
<blockquote>
<p>流算子是处理拓扑中的节点，它代表了在拓扑中的处理步骤，比如转换算子会在流中转换数据。标准的算子包括map/filter/join/aggregation，这些都是流算子的示例，并且内置在Kafka Streams中开箱即用。一个流算子从它在拓扑中的上游算子一次接收一条输入记录，将操作运用到记录，并且可能会产生一条或多条输出记录给下游的算子。Kafka Streams提供了两种方式来定义算子：</p>
</blockquote>
<ul>
<li>The Kafka Streams DSL provides the most common data transformation operations such as map and filter so you don’t have to implement these stream processors from scratch.</li>
<li>The low-level Processor API allows developers to define and connect custom processors as well as to interact with state stores.</li>
</ul>
<ol>
<li>Kafka Streams DSL提供了最通用的数据转换操作，比如map、filter，这样你不需要自己实现这些算子</li>
<li>低级的Processor API，允许开发者定义和连接定制的算子，并且还可以和状态存储交互</li>
</ol>
<h2 id="时间">时间</h2><p>A critical aspect in stream processing is the the notion of time, and how it is modeled and integrated. For example, some operations such as Windowing are defined based on time boundaries.</p>
<blockquote>
<p>流处理的一个重要概念是时间，如何对时间进行建模和整合非常重要，因为有些操作比如窗口函数会基于时间的边界来定义。有几种类型的时间表示方式：</p>
</blockquote>
<ul>
<li>Event-time: The point in time when an event or data record occurred, i.e. was originally created “by the source”. Achieving event-time semantics typically requires embedding timestamps in the data records at the time a data record is being produced. Example: If the event is a geo-location change reported by a GPS sensor in a car, then the associated event-time would be the time when the GPS sensor captured the location change.</li>
<li>Processing-time: The point in time when the event or data record happens to be processed by the stream processing application, i.e. when the record is being consumed. The processing-time may be milliseconds, hours, or days etc. later than the original event-time. Example: Imagine an analytics application that reads and processes the geo-location data reported from car sensors to present it to a fleet management dashboard. Here, processing-time in the analytics application might be milliseconds or seconds (e.g. for real-time pipelines based on Apache Kafka and Kafka Streams) or hours (e.g. for batch pipelines based on Apache Hadoop or Apache Spark) after event-time.</li>
<li>Ingestion-time: The point in time when an event or data record is stored in a topic partition by a Kafka broker. Ingestion-time is similar to event-time, as a timestamp gets embedded in the data record itself. The difference is, that the timestamp is generated when the record is appended to the target topic by the Kafka broker, not when the record is created “at the source”. Ingestion-time may approximate event-time reasonably well if we assume that the time difference between creation of the record and its ingestion into Kafka is sufficiently small, where “sufficiently” depends on the specific use case. Thus, ingestion-time may be a reasonable alternative for use cases where event-time semantics are not possible, e.g. because the data producers don’t embed timestamps (e.g. older versions of Kafka’s Java producer client) or the producer cannot assign timestamps directly (e.g., it does not have access to a local clock).</li>
</ul>
<ol>
<li>事件时间：事件或数据记录发生的时间点，它是由事件源创建的。实现事件时间语义通常需要在记录中有内置的时间撮字段，表示这条记录在什么时候产生。比如一条事件是车辆传感器报告的地理位置变更，那么对应的事件时间表示GPS传感器捕获位置变更的时间点。</li>
<li>处理时间：事件被流处理应用程序处理的时间点，比如就被消费的时候，处理时间会比原始的事件时间要晚。举例一个分析应用程序读取并处理车辆上传的地理位置，并且呈现到一个dashboard上。这里分析程序的处理时间可能比事件的时间晚几毫米、几秒、甚至几个小时。</li>
<li>摄取时间：事件存储到Kafka Brokers的主题分区中的时间点。摄取时间和事件时间类似，它也是作为数据记录本身的一个内置字段，不同的是<strong>摄取时间是在追加到Kafka中时自动生成的，而不是数据源创建的时间</strong>。如果我们假设记录的创建时间和摄取到Kafka的时间间隔足够短的话，可以认为摄取时间近似于事件时间，当然足够短这个时间跟具体的用例有关。什么场景下采用摄取时间比较合理呢？比如数据源没有内置的事件时间（比如旧版本的Java生产者客户端在消息中不会带有时间撮，新版本则有），或者说生产者不能直接分配时间撮（无法获取到本地时钟）。</li>
</ol>
<p>The choice between event-time and ingestion-time is actually done through the configuration of Kafka (not Kafka Streams): From Kafka 0.10.x onwards, timestamps are automatically embedded into Kafka messages. Depending on Kafka’s configuration these timestamps represent event-time or ingestion-time. The respective Kafka configuration setting can be specified on the broker level or per topic. The default timestamp extractor in Kafka Streams will retrieve these embedded timestamps as-is. Hence, the effective time semantics of your application depend on the effective Kafka configuration for these embedded timestamps. See :ref:Developer Guide <timestamp extractor=""> for detailed information.</timestamp></p>
<blockquote>
<p>选择事件时间还是摄取时间，是通过Kafka的配置文件（不是Kafka Streams的配置），在0.10版本之后，时间撮会自动内嵌到Kafka的消息中。根据Kafka的配置，时间撮可以指定为事件时间还是摄取时间，这个配置可以设置到Broker级别，也可以是每个Topic。默认的Kafka Streams时间撮抽取方式会取出内置的时间撮字段。所以应用程序的有效时间语义依赖于Kafka的内置时间撮。</p>
</blockquote>
<p>Kafka Streams assigns a timestamp to every data record via so-called timestamp extractors. These per-record timestamps describe the progress of a stream with regards to time (although records may be out-of-order within the stream) and are leveraged by time-dependent operations such as joins. They are also used to synchronize multiple input streams within the same application.</p>
<blockquote>
<p>Kafka Streams会通过时间撮抽取器把一个时间撮分配给每条记录。每条记录的时间撮描述了一条流关于时间的进度（尽管流中的记录可能没有顺序），这个时间撮会被时间相关的操作比如join所使用。同时它们也会被用来在同一个应用程序中多个输入流的同步。</p>
</blockquote>
<p>Concrete implementations of timestamp extractors may retrieve or compute timestamps based on the actual contents of data records such as an embedded timestamp field to provide event-time or ingestion-time semantics, or use any other approach such as returning the current wall-clock time at the time of processing, thereby yielding processing-time semantics to stream processing applications. Developers can thus enforce(执行，强制) different notions/semantics of time depending on their business needs.</p>
<blockquote>
<p>具体的时间撮抽取器实现，基于数据记录实际内容的时间撮，可能会读取或者计算，比如（数据记录中）提供的事件时间或者摄取时间语义的时间撮字段，或者使用其他的方式，比如返回当前的处理时间，即流处理应用程序的处理时间语义。开发者可以根据他们的业务需求使用不同的时间语义。</p>
</blockquote>
<p>Be aware that ingestion-time in Kafka Streams is used slightly different as in other stream processing systems. Ingestion-time could mean the time when a record is fetched by a stream processing application’s source operator. In Kafka Streams, ingestion-time refers to the time when a record was appended to a Kafka topic partition.</p>
<blockquote>
<p>注意Kafka Streams的摄取时间可能和其他流处理系统的使用方式有点不同。摄取时间可以表示为被流处理的源算子获取的时间点。而在Kafka Streams中，摄取时间指的是当一条记录被追加到Kafka主题分区的那个时间点（即Producer写入分区的时间）。</p>
</blockquote>
<h2 id="有状态的流处理">有状态的流处理</h2><p>Some stream processing applications don’t require state, which means the processing of a message is independent from the processing of all other messages. If you only need to transform one message at a time, or filter out messages based on some condition, the topology defined in your stream processing application can be simple.</p>
<blockquote>
<p>有些流处理应用程序并不需要状态，这意味着一条消息的处理和其他所有消息的处理都是独立的。如果你只需要在一个时间点转换一条消息，或者基于某些条件对消息进行过滤，你的流计算应用层序的拓扑可以非常简单。</p>
</blockquote>
<p>However, being able to maintain state opens up many possibilities for sophisticated(复杂) stream processing applications: you can join input streams, or group and aggregate data records. Many such stateful operators are provided by the Kafka Streams DSL.</p>
<blockquote>
<p>不过，对于复杂的流处理应用程序，为了能够维护状态，会有很多可能性：联合不同的输入流，对数据记录进行分组和聚合。Kafka Streams的DSL提供了很多有状态的操作算子。</p>
</blockquote>
<h2 id="Streams和Tables的二元性">Streams和Tables的二元性</h2><p>Before we discuss concepts such as aggregations in Kafka Streams we must first introduce tables, and most importantly the relationship between tables and streams: the so-called stream-table duality. Essentially, this duality means that a stream can be viewed as a table, and vice versa. Kafka’s log compaction feature, for example, exploits(功绩，利用，开发) this duality.</p>
<p>在介绍Kafka Streams的概念之前（比如聚合），我们必须先介绍tables，以及tables和streams的关系（所谓的stream-table二元性）。从本质上来说，二元性意味着一个流可以被看做是一张表，反过来也是成立的。Kafka的日志压缩特性，可以实现这样的二元转换。一张表，简单来说就是一系列的键值对，或者被叫做字典、关联数组。</p>
<p><img src="http://img.blog.csdn.net/20161103130945109" alt="k-table"></p>
<p>stream-table二元性描述了两者的紧密关系：</p>
<ul>
<li>Stream as Table: A stream can be considered a changelog of a table, where each data record in the stream captures a state change of the table. A stream is thus a table in disguise(假装), and it can be easily turned into a “real” table by replaying the changelog from beginning to end to reconstruct the table. Similarly, in a more general analogy(类比), aggregating data records in a stream – such as computing the total number of pageviews by user from a stream of pageview events – will return a table (here with the key and the value being the user and its corresponding pageview count, respectively).</li>
<li>Table as Stream: A table can be considered a snapshot, at a point in time, of the latest value for each key in a stream (a stream’s data records are key-value pairs). A table is thus a stream in disguise, and it can be easily turned into a “real” stream by iterating over each key-value entry in the table.</li>
</ul>
<ol>
<li>将流作为表：一个流可以被认为是一张表的变更记录(changelog)，流中的每条数据记录捕获了表的每个变更状态。一个流可以<em>假装</em>是一张表，也可以通过从头到尾重放变更日志来重构表，很容易地变成<em>真正</em>的表。同样地，在流中聚合记录（比如从PV事件流中计算用户的PV数）会返回一张表（这里键值分别是用户，以及对应的PV数）。</li>
<li>将表作为流：一张表可以认为是在某个时间点的一份快照，是流的每个key对应的最近的值。一张表因此也可以假装是一个流，也可以通过迭代表中所有的键值条目转换成一个真实的流。</li>
</ol>
<p>Let’s illustrate this with an example. Imagine a table that tracks the total number of pageviews by user (first column of diagram below). Over time, whenever a new pageview event is processed, the state of the table is updated accordingly. Here, the state changes between different points in time – and different revisions(修正) of the table – can be represented as a changelog stream (second column).</p>
<p>Interestingly, because of the stream-table duality, the same stream can be used to reconstruct the original table (third column):</p>
<p>举例，一张表会跟踪用户的PV总数（左图第一列），当一条新的pageview事件被处理的时候，表的状态会相应地被更新。这里不同时间点的状态变更（针对表的不同修改），可以作为一个变更日志流（左图第二列）。有趣的是，由于stream-table的二元性，相同的流可以被用来构造出原始的表（右图第三列）。</p>
<p><img src="http://img.blog.csdn.net/20161103131309314" alt="k stream table durable"></p>
<p>The same mechanism(机制) is used, for example, to replicate databases via change data capture (CDC) and, within Kafka Streams, to replicate its so-called state stores across machines for fault-tolerance. The stream-table duality is such an important concept that Kafka Streams models it explicitly(明确地) via the KStream and KTable interfaces, which we describe in the next sections.</p>
<blockquote>
<p>这种类似的机制也被用在其他系统中，比如通过CDC复制数据库。在Kafka Streams中，为了容错处理，会将它的状态存储复制到多台机器上。stream-table的二元性是很重要的概念，Kafka Streams通过KStream和KTable接口对它们进行建模。</p>
</blockquote>
<h2 id="KStream（记录流_record_stream）">KStream（记录流 record stream）</h2><p>A KStream is an abstraction of a record stream, where each data record represents a self-contained datum(基准，资料) in the unbounded data set. Using the table analogy(类比), data records in a record stream are always interpreted as “inserts” – think: append-only ledger(分类) – because no record replaces an existing row with the same key. Examples are a credit card transaction, a page view event, or a server log entry.</p>
<blockquote>
<p>一个KStream是对记录流的抽象，每条数据记录能够表示在无限数据集中自包含的数据。用传统数据库中的表这个概念来类比，记录流中的数据可以理解为“插入”（只有追加），因为不会有记录会替换已有的相同key的行。比如信用卡交易、访问时间、服务端日志条目。举例有两条记录发送到流中：  </p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">"alice"</span>, <span class="number">1</span>) --&gt; (<span class="string">"alice"</span>, <span class="number">3</span>)  <span class="comment">//这两条记录依次发送到流中</span></span><br></pre></td></tr></table></figure>
<p>If your stream processing application were to sum the values per user, it would return 4 for alice. Why? Because the second data record would not be considered an update of the previous record. Compare this behavior of KStream to KTable below, which would return 3 for alice.</p>
<blockquote>
<p>如果你的流处理应用程序是为每个用户求和（记录的value含义不是很明确，但是我们只是要对value值求和），那么alice用户的返回结果是4。因为第二条记录不会被认为是对前一条记录的更新（第一条记录和第二条记录是同时存在的）。如果将其和下面的KTable对比，KTable中alice用户的返回结果是3。</p>
</blockquote>
<h2 id="KTable（变更流_changelog_stream）">KTable（变更流 changelog stream）</h2><p>A KTable is an abstraction of a changelog stream, where each data record represents an update. More precisely, the value in a data record is considered to be an update of the last value for the same record key, if any (if a corresponding key doesn’t exist yet, the update will be considered a create). Using the table analogy, a data record in a changelog stream is interpreted as an update because any existing row with the same key is overwritten.</p>
<blockquote>
<p>一个KTable是对变更日志流的抽象，每条数据记录代表的是一个更新。更准确的说，数据记录中的值被认为是对已有相同记录的key的值更新（如果存在key则更新，如果key不存在，更新操作会被认为是创建）。用传统数据库中的表这个概念来类比，变更流中的数据可以理解为“更新”，因为任何已经存在相同key的行都会被覆盖。</p>
</blockquote>
<p>If your stream processing application were to sum the values per user, it would return 3 for alice. Why? Because the second data record would be considered an update of the previous record. Compare this behavior of KTable with the illustration for KStream above, which would return 4 for alice.</p>
<blockquote>
<p>还是以上面的两条记录发送到流中为例，如果也是为每个用户求和，那么alice用户的返回结果是3。因为第二条记录会被认为是对前一条记录的更新（那么第一条记录实际上就不存在了）。如果将其和上面的KStream对比，KStream中alice用户的返回结果是4。</p>
</blockquote>
<p>Effects of Kafka’s log compaction: Another way of thinking about KStream and KTable is as follows: If you were to store a KTable into a Kafka topic, you’d probably want to enable Kafka’s log compaction feature, e.g. to save storage space.</p>
<blockquote>
<p>理解KStream和KTable的另外一种思路是：如果将KTable存储到Kafka主题中，你应该开启Kafka的日志压缩功能。</p>
</blockquote>
<p>However, it would not be safe to enable log compaction in the case of a KStream because, as soon as log compaction would begin purging older data records of the same key, it would break the semantics of the data. To pick up the illustration example again, you’d suddenly get a 3 for alice instead of a 4 because log compaction would have removed the (“alice”, 1) data record. Hence log compaction is perfectly safe for a KTable (changelog stream) but it is a mistake for a KStream (record stream).</p>
<blockquote>
<p>如果是KStream，开启日志压缩不是一个安全的做法，因为日志压缩会清除相同key的不同数据，这会破坏数据的语义。举例，你可能会突然看到用户alice的结果为3而不是4，因为日志压缩会删除(“alice”, 1)这条记录。所以日志压缩对于KTable是安全的，而对KSteram则是错误的用法。</p>
</blockquote>
<p>We have already seen an example of a changelog stream in the section Duality of Streams and Tables. Another example are change data capture (CDC) records in the changelog of a relational database, representing which row in a database table was inserted, updated, or deleted.</p>
<blockquote>
<p>在stream-table二元性中，我们已经看到了一个变更日志流的示例。另一个示例是关系型数据库中的CDC变更日志，表示数据库中哪一行执行了插入，更新、删除动作。</p>
</blockquote>
<p>KTable also provides an ability to look up current values of data records by keys. This table-lookup functionality is available through join operations (see also Joining Streams in the Developer Guide).</p>
<blockquote>
<p>KTable也支持根据记录的key查询当前的value，这种特性会在join操作时使用。</p>
</blockquote>
<h2 id="窗口操作">窗口操作</h2><p>A stream processor may need to divide data records into time buckets, i.e. to window the stream by time. This is usually needed for for join and aggregation operations, etc. Windowed stream buckets can be maintained in the processor’s local state.</p>
<blockquote>
<p>一个流算子可能需要将数据记录分成多个时间段，比如对流按照时间做成一个个窗口。通常在联合和聚合操作时需要这么做。在算子的本地状态中会维护窗口流。</p>
</blockquote>
<p>Windowing operations are available in the Kafka Streams DSL, where users can specify a retention period for the window. This allows Kafka Streams to retain old window buckets for a period of time in order to wait for the late arrival of records whose timestamps fall within the window interval. If a record arrives after the retention period has passed, the record cannot be processed and is dropped.</p>
<blockquote>
<p>窗口算子在Kafka Stream DSL中可以使用，用户可以指定窗口的保留时间。这样允许Kafka Streams会在一段时间内保留旧的窗口段，目的是等待迟来的记录，这些记录的时间撮落在窗口间隔内（虽然不一定是当前窗口，但可能是旧的窗口，如果没有保留旧窗口的话，迟来的记录就会被直接丢弃了，因为当前窗口不能存放旧记录）。如果一条记录在保留时间过去之后才到达，这条记录就不会被处理，只能被丢弃了。</p>
</blockquote>
<p>Late-arriving records are always possible in real-time data streams. However, it depends on the effective time semantics how late records are handled. Using processing-time, the semantics are “when the data is being processed”, which means that the notion of late records is not applicable as, by definition, no record can be late. Hence, late-arriving records only really can be considered as such (i.e. as arriving “late”) for event-time or ingestion-time semantics. In both cases, Kafka Streams is able to properly handle late-arriving records.</p>
<blockquote>
<p>迟到的记录在实时数据流中总是可能发生的。不过，它取决于记录到底有多晚才被处理的有效时间语义。使用处理时间，语义是“当数据正在被处理”，这就意味着迟来的记录是不适合的，也就是说不会有记录迟到的。所以迟到的记录只能针对事件时间或者摄取时间这两种语义。这两种情况下，Kafka Streams都可以很好地处理迟到的记录。</p>
</blockquote>
<h2 id="联合操作">联合操作</h2><p>A join operation merges two streams based on the keys of their data records, and yields a new stream. A join over record streams usually needs to be performed on a windowing basis because otherwise the number of records that must be maintained for performing the join may grow indefinitely.</p>
<blockquote>
<p>联合操作会合并两个流，基于他们的数据记录的keys，并产生一个新的流。在记录流上的join操作通常需要在窗口的基础上执行，否则为了执行join而需要维护的记录数量会无限膨胀（在无限的记录集上无法做join操作，因为你不知道什么时候结束，就无法join，联合操作必须是在有限的记录集上，而窗口正好是有限的记录集）。</p>
</blockquote>
<p>The join operations available in the Kafka Streams DSL differ based on which kinds of streams are being joined (e.g. KStream-KStream join versus KStream-KTable join).</p>
<blockquote>
<p>Kafka Streams DSL中的join操作跟流的类型有关，比如KStream-KStream进行join，或者KStream-KTable进行join。</p>
</blockquote>
<h2 id="聚合操作">聚合操作</h2><p>An aggregation operation takes one input stream, and yields a new stream by combining multiple input records into a single output record. Examples of aggregations are computing counts or sum. An aggregation over record streams usually needs to be performed on a windowing basis because otherwise the number of records that must be maintained for performing the aggregation may grow indefinitely.</p>
<blockquote>
<p>一个聚合操作会接受一个输入流，然后通过合并多条输入记录产生一个新的流，最终生成一个单一的输出记录，聚合算子的示例比如计算次数或求和。和join操作一样，聚合操作也需要在窗口的基础上执行。</p>
</blockquote>
<p>In the Kafka Streams DSL, an input stream of an aggregation can be a KStream or a KTable, but the output stream will always be a KTable. This allows Kafka Streams to update an aggregate value upon the late arrival of further records after the value was produced and emitted. When such late arrival happens, the aggregating KStream or KTable simply emits a new aggregate value. Because the output is a KTable, the new value is considered to overwrite the old value with the same key in subsequent processing steps.</p>
<blockquote>
<p>在Kafka Streams DSL中，<strong>聚合操作的输入流可以是一个KStream或者是一个KTable，但是输出流只能是一个KTable</strong>。这就允许Kafka Streams在value被产生并发送出去之后，即使迟到的记录到来时，也可以更新聚合结果（第一次产生的结果是在当前窗口，然后把结果发送出去，第二次产生的结果已经不在当前窗口，它属于旧的窗口，也会更新对应的聚合结果，然后再把最新的结果发送出去）。当这样的迟到记录到来时，聚合的KStream或者KTable仅仅简单地发送新的聚合结果。由于输出是一个KTable，相同key下，在后续的处理步骤中，新的值会覆盖旧的值。</p>
</blockquote>
<h1 id="架构">架构</h1><p>Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers. Below is Logical view of a Kafka Streams application that contains multiple stream threads, each of which in turn containing multiple stream tasks.</p>
<p>Kafka Streams是构建在Kafka生产者和消费者的库，并且利用了Kafka本身的特性提供了数据的并行、分布式协调、容错，简化了应用程序的开发。下图是Kafka Streams应用程序的逻辑视图，包括了多个流线程，每个线程包括多个流任务。</p>
<p><img src="http://img.blog.csdn.net/20161103180445363" alt="kstream arch overview"></p>
<h2 id="拓扑">拓扑</h2><p>A processor topology or simply topology defines the stream processing computational logic for your application, i.e., how input data is transformed into output data. A topology is a graph of stream processors (nodes) that are connected by streams (edges). There are two special processors in the topology:</p>
<blockquote>
<p>拓扑定义了流处理应用程序的计算逻辑，比如输入数据怎么转换成输出数据。拓扑是由流处理算子和相连的流组成的一张图。在拓扑中有两种特殊类型的流算子：</p>
</blockquote>
<ul>
<li>Source Processor: A source processor is a special type of stream processor that does not have any upstream processors. It produces an input stream to its topology from one or multiple Kafka topics by consuming records from these topics and forward them to its down-stream processors.</li>
<li>Sink Processor: A sink processor is a special type of stream processor that does not have down-stream processors. It sends any received records from its up-stream processors to a specified Kafka topic.</li>
</ul>
<ol>
<li>源算子：没有任何上游算子，它从一个或多个Kafka主题中消费记录，然后产生一个到拓扑的输入流，并且转发到下游的算子</li>
<li>目标算子：没有任何的下游算子，它会把从上游算子接收到的任何记录，发送给指定的Kafka主题</li>
</ol>
<p><img src="http://img.blog.csdn.net/20161103180512627" alt="kstream topo"></p>
<p>A stream processing application – i.e., your application – may define one or more such topologies, though typically it defines only one. Developers can define topologies either via the low-level Processor API or via the Kafka Streams DSL, which builds on top of the former.</p>
<p>A processor topology is merely a logical abstraction for your stream processing code. At runtime, the logical topology is instantiated and replicated inside the application for parallel processing (see Parallelism Model).</p>
<p>一个流处理应用程序可以定义一个或多个拓扑，尽管通常你只会定义一个。开发者可以通过低级的Processor API或者高级的DSL方式定义拓扑。一个拓扑仅仅是流处理代码的逻辑抽象，在运行时，逻辑拓扑会被实例化，并且在应用程序中进行复制以获得并行处理的能力。</p>
<h2 id="并行模型">并行模型</h2><h3 id="Stream_Partitions_and_Tasks">Stream Partitions and Tasks</h3><p>Kafka Streams uses the concepts of partitions and tasks as logical units of its parallelism model. There are close links between Kafka Streams and Kafka in the context of parallelism:</p>
<p>Kafka Streams使用分区和任务的概念作为它的并行模型的逻辑单元。Kafka Streams和Kafka在并行这个上下文上有紧密的联系：  </p>
<ul>
<li>Each stream partition is a totally ordered sequence of data records and maps to a Kafka topic partition.</li>
<li>A data record in the stream maps to a Kafka message from that topic.</li>
<li>The keys of data records determine the partitioning of data in both Kafka and Kafka Streams, i.e., how data is routed to specific partitions within topics.</li>
</ul>
<ol>
<li>每个分区流完全是一个有序的数据记录序列，映射到Kafka的主题分区</li>
<li>流中的一条数据记录，对应了Kafka主题中的一条消息</li>
<li>数据记录的Key决定了它在Kafka和Kafka Streams中的分区方式，比如数据怎么路由到主题的指定分区</li>
</ol>
<p>An application’s processor topology is scaled by breaking it into multiple tasks. More specifically, Kafka Streams creates a fixed number of tasks based on the input stream partitions for the application, with each task assigned a list of partitions from the input streams (i.e., Kafka topics). The assignment of partitions to tasks never changes so that each task is a fixed unit of parallelism of the application. Tasks can then instantiate their own processor topology based on the assigned partitions; they also maintain a buffer for each of its assigned partitions and process messages one-at-a-time from these record buffers. As a result stream tasks can be processed independently and in parallel without manual intervention.</p>
<blockquote>
<p>应用程序的处理拓扑会被分成多个任务来进行扩展。更具体来说，Kafka Streams会基于输入流的分区创建固定数量的任务，每个任务会从输入流（Kafka的主题）分配到多个分区。每个任务分配的分区永远不会改变，这样每个任务作为应用程序固定的并行单元。任务可以基于分配给它们的分区实例化它们自己的处理拓扑；它们也会为每个分配的分区维护一个缓冲区，并且从这些记录的缓冲区中一次只处理一条消息。这样的好处是所有的流任务都各自独立地并行处理，并不需要人工干预。</p>
</blockquote>
<p>Sub-topologies aka topology sub-graphs: If there are multiple processor topologies specified in a Kafka Streams application, each task will only instantiate one of the topologies for processing. In addition, a single processor topology may be decomposed(分离分解) into independent sub-topologies (sub-graphs) as long as sub-topologies are not connected by any streams in the topology; here, each task may instantiate only one such sub-topology for processing. This further scales out the computational workload to multiple tasks.</p>
<blockquote>
<p>子拓扑或者叫拓扑子图：如果在Kafka Streams应用程序中指定了多个处理拓扑，每个任务只会实例化其中的一个拓扑并处理。另外，一个拓扑也可能分成多个独立的子拓扑，只要子拓扑不和拓扑中的任何流存在连接。这里每个任务可能只会实例化一个子拓扑并处理。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161103181154902" alt="kstream task"></p>
<p>It is important to understand that Kafka Streams is not a resource manager, but a library that “runs” anywhere its stream processing application runs. Multiple instances of the application are executed either on the same machine, or spread across multiple machines and tasks can be distributed automatically by the library to those running application instances. The assignment of partitions to tasks never changes; if an application instance fails, all its assigned tasks will be restarted on other instances and continue to consume from the same stream partitions.</p>
<blockquote>
<p>注意Kafka Streams不是一个资源管理器，而是一个可以在和流处理应用程序一起运行在任何地方的客户端库。你可以在一台机器上运行应用程序的多个实例；或者分散在多台机器上，任务就会自动分布式地运行这些应用程序实例。注意分配给任务的分区永远不会改变（和Kafka消费者有点不同，消费者分配的分区是可以改变的）；如果一个应用程序的实例失败了，它的所有任务会在其他实例上重新启动，并且从相同的流分区继续消费。总结下：一个流处理应用程序实例（进程）有多个Task，每个Task分配多个固定的分区，如果进程挂了，其上的所有Task都会在其他进程上执行。而不会说把分区重新分配给剩下的Task。由于Task的分区固定，实际上Task的数量也是固定的，Task会分布式地在多个进程上执行。</p>
</blockquote>
<h3 id="Threading_Model">Threading Model</h3><p>Kafka Streams allows the user to configure the number of threads that the library can use to parallelize processing within an application instance. Each thread can execute one or more tasks with their processor topologies independently.</p>
<blockquote>
<p>Kafka Streams允许用户配置线程的数量，这样Kafka Streams库可以用来决定在一个应用程序实例中的处理并行粒度。每个线程可以执行一个或多个任务，它们的拓扑也都是独立的。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161103181203678" alt="kstream thread"></p>
<p>Starting more stream threads or more instances of the application merely amounts to replicating the topology and having it process a different subset of Kafka partitions, effectively parallelizing processing. It is worth noting that there is no shared state amongst the threads, so no inter-thread coordination is necessary. This makes it very simple to run topologies in parallel across the application instances and threads. The assignment of Kafka topic partitions amongst the various stream threads is transparently handled by Kafka Streams leveraging Kafka’s server-side coordination functionality.</p>
<blockquote>
<p>开启更多的流线程或者更多的应用程序实例仅仅相当于复制拓扑，并且处理不同的Kafka分区子集，有效地并行化处理。注意线程之间不会共享状态，所以不需要内部的线程进行协调。这使得在多个应用程序或者线程之间并行地运行拓扑变得非常简单。不同线程分配到的Kafka主题分区会被Kafka Streams透明地处理，利用的是Kafka服务端的协调者特性。</p>
</blockquote>
<p>As we described above, scaling your stream processing application with Kafka Streams is easy: you merely need to start additional instances of your application, and Kafka Streams takes care of distributing partitions amongst tasks that run in the application instances. You can start as many threads of the application as there are input Kafka topic partitions so that, across all running instances of an application, every thread (or rather, the tasks it runs) has at least one input partition to process.</p>
<blockquote>
<p>正如上面所描述的，使用Kafka Streams扩展你的流处理应用程序非常简单：你只需要为你的应用程序启动额外的实例，然后Kafka Streams就会自动帮你将分区分布在任务之间，任务会运行在应用程序实例中。你可以启动和Kafka的输入主题分区相同数量的应用程序线程，这样在一个应用程序的所有运行实例中，每个线程（更精确地说，是运行的任务）至少都会处理一个输入分区。</p>
</blockquote>
<h3 id="Example">Example</h3><p>To understand the parallelism model that Kafka Streams offers, let’s walk through an example.</p>
<p>Imagine a Kafka Streams application that consumes from two topics, A and B, with each having 3 partitions. If we now start the application on a single machine with the number of threads configured to 2, we end up with two stream threads instance1-thread1 and instance1-thread2. Kafka Streams will break this topology by default into three tasks because the maximum number of partitions across the input topics A and B is max(3, 3) == 3, and then distribute the six input topic partitions evenly across these three tasks; in this case, each task will consume from one partition of each input topic, for a total of two input partitions per task. Finally, these three tasks will be spread evenly – to the extent this is possible – across the two available threads, which in this example means that the first thread will run 2 tasks (consuming from 4 partitions) and the second thread will run 1 task (consuming from 2 partitions).</p>
<p>为了理解Kafka Streams提供的并行度模型，我们来看一个示例。假设有一个Kafka Streams应用程序会消费两个主题：A和B，每个主题都有3个分区。如果我们在一台机器上启动了一个应用程序，配置的线程数量为2，最终我们会有两个流线程：instance1-thread1和instance1-thread2。Kafka Streams会默认将拓扑分成三个任务，因为所有输入主题A和B的最大分区数是max(3,3)=3，然后会将6个输入分区平均分配到这三个任务上。这种情况下，每个任务都会消费每个输入主题的一个分区，即每个任务分配到了总共两个分区。最后，这三个任务会被均匀地分散到两个可用的线程中，这里因为有两个线程，这就意味着第一个线程会运行两个任务（消费了4个分区），第二个线程会运行一个任务（消费了2个分区）。</p>
<p>Now imagine we want to scale out this application later on, perhaps because the data volume has increased significantly. We decide to start running the same application but with only a single thread on another, different machine. A new thread instance2-thread1 will be created, and input partitions will be re-assigned similar to:</p>
<p>现在假设我们要扩展应用程序，可能是因为数据量增长的很明显。我们决定在其他机器上运行相同的应用程序，不过只配置了一个线程。那么一个新的线程instance2-thread1就会被创建，输入分区会被重新分配成下面右图那样。</p>
<p><img src="http://img.blog.csdn.net/20161103181630440" alt="kstream example"></p>
<p>When the re-assignment occurs, some partitions – and hence their corresponding tasks including any local state stores – will be “migrated” from the existing threads to the newly added threads (here: from instance1-thread1 on the first machine to instance2-thread1 on the second machine). As a result, Kafka Streams has effectively rebalanced the workload among instances of the application at the granularity of Kafka topic partitions.</p>
<blockquote>
<p>当重新分配发生时，一些分区，以及它们对应的任务，包括本地存储的状态，都会从已有的线程迁移到新添加的线程。比如这里第一台机器的instance1-thread1线程会迁移到第二台机器的instance2-thread1线程。最终，Kafka Streamsh会在所有应用程序实例中有效地平衡负载，而且是以Kafka主题分区的粒度进行负载均衡。</p>
</blockquote>
<p>What if we wanted to add even more instances of the same application? We can do so until a certain point, which is when the number of running instances is equal to the number of available input partitions to read from. At this point, before it would make sense to start further application instances, we would first need to increase the number of partitions for topics A and B; otherwise, we would over-provision the application, ending up with idle instances that are waiting for partitions to be assigned to them, which may never happen.</p>
<p>如果想要添加相同应用程序的更多实例呢？我们可以像上面那样做，直到运行实例的数量等于读取的可用分区数量（所有主题）。在这之后，如果想要启动更多的应用程序实例变得有意义，我们需要先为主题A和B增加分区；否则会存在空闲的应用程序实例，它们会等待有可用的分区分配给它们，但这<strong>可能</strong>永远都不会发生（虽然应用程序的实例比分区多，导致有些应用程序实例是空闲的，但是如果有应用程序挂掉了，那些空闲的应用程序就有可能分配到分区，而不再空闲。就像Kafka的消费者一样，如果消费者数量比分区数要多，空闲的消费者也会得不到分区，但如果有消费者挂掉了，空闲的消费者也是有机会得到分区的。不过我们无法保证空闲的应用程序实例或者消费者就一定有机会得到分区）。</p>
<h2 id="状态">状态</h2><p>Kafka Streams provides so-called state stores, which can be used by stream processing applications to store and query data, which is an important capability when implementing stateful operations. Every task in Kafka Streams embeds one or more state stores that can be accessed via APIs to store and query data required for processing. These state stores can either be a RocksDB database, an in-memory hash map, or another convenient data structure. Kafka Streams offers fault-tolerance and automatic recovery for local state stores.</p>
<p>Kafka Streams提供了所谓的状态存储，可以被流处理应用程序用来存储和查询数据，这在实现有状态的操作时是一个非常重要的功能。Kafka Streams中的每个任务内置了一个或多个状态存储，并且可以在流处理时通过API的方式存储或者查询状态存储中的数据。这些状态存储可以是RocksDB数据库、内存的hash map、或者其他的数据结构。Kafka Streams为本地状态存储提供了容错和自动恢复机制。</p>
<p><img src="http://img.blog.csdn.net/20161104091711617" alt="kstream state"></p>
<h2 id="容错">容错</h2><p>Kafka Streams builds on fault-tolerance capabilities integrated natively within Kafka. Kafka partitions are highly available and replicated; so when stream data is persisted to Kafka it is available even if the application fails and needs to re-process it. Tasks in Kafka Streams leverage the fault-tolerance capability offered by the Kafka consumer client to handle failures. If a task runs on a machine that fails, Kafka Streams automatically restarts the task in one of the remaining running instances of the application.</p>
<blockquote>
<p>Kafka Streams的容错能力基于原生的Kafka，Kafka的分区是高可用和复制的；所以当流数据持久化到Kafka中，即使应用程序失败了或者需要重新处理，数据也还是可用的。Kafka Streams的任务利用了Kafka消费者客户端提供的容错机制来处理故障。如果运行在一台机器上的一个任务失败了，Kafka Streams会在剩余的应用程序实例选择一个自动重启任务。</p>
</blockquote>
<p>In addition, Kafka Streams makes sure that the local state stores are robust to failures, too. It follows a similar approach as Apache Samza and, for each state store, maintains a replicated changelog Kafka topic in which it tracks any state updates. These changelog topics are partitioned as well so that each local state store instance, and hence the task accessing the store, has its own dedicated changelog topic partition. Log compaction is enabled on the changelog topics so that old data can be purged safely to prevent the topics from growing indefinitely. If tasks run on a machine that fails and are restarted on another machine, Kafka Streams guarantees to restore their associated state stores to the content before the failure by replaying the corresponding changelog topics prior to resuming the processing on the newly started tasks. As a result, failure handling is completely transparent to the end user.</p>
<blockquote>
<p>另外，Kafka Streams确保了本地状态的存储对于故障是鲁棒性的。它采用了和Apache Samza类似的方法，每个状态存储，都维护了具有复制的变更日志（Kafka主题），变更日志（changelog）会跟踪每次状态的更新。这些变更日志主题（change topic）会进行分区，每个本地状态存储的实例（local state store instance），都可以被任务获取，任务都有自己专属的变更日志分区（changelog topic partition）。在变更日志主题上会开启日志压缩，来安全地删除旧数据，防止旧数据无限膨胀。如果在一台机器上的任务运行失败，会在其他机器上重新启动，Kafka Streams可以保证恢复故障发生之前相关的状态存储。这是通过在新启动的任务上恢复处理之前，重放对应的变更日志主题来做到的。最终，故障处理对终端用户而言是透明的。</p>
</blockquote>
<p>Optimization: In order to minimize the time for the state restoration and hence the cost of task (re)initialization, users can configure their applications to have shadow copies of local states. When a task migration happens, Kafka Streams then attempts to assign a task to where a standby replica exists in order to minimize the task initialization cost. See setting num.standby.replicas at Optional configuration parameters in the Developer Guide.</p>
<blockquote>
<p>优化点：为了最小化恢复状态的时间以及任务重新初始化的代价，用户可以为应用程序配置一个本地状态的shadow副本。当一个任务迁移发生时，Kafka Streams会尝试将任务分配到备用副本所在的节点，以尽可能最小化任务初始化的代价。</p>
</blockquote>
<h2 id="流处理的保证">流处理的保证</h2><p>Kafka Streams currently supports at-least-once processing guarantees in the presence of failure. This means that if your stream processing application fails, no data records are lost and fail to be processed, but some data records may be re-read and therefore re-processed.</p>
<p>It depends on the specific use case whether at-least-once processing guarantees are acceptable or whether you may need exactly-once processing.</p>
<blockquote>
<p>Kafka Streams目前支持在错误场景下至少一次的处理语义。这意味着如果你的流处理应用程序失败了，数据不会丢失，也不会被漏掉处理，但是有些数据可能会被重复读取，并被重复处理。根据不同的用例，用户自己决定是否可以接受至少处理一次的保证，还是需要正好一次的处理。</p>
</blockquote>
<p>For many processing use cases, at-least-once processing turns out to be perfectly acceptable: Generally, as long as the effect of processing a data record is idempotent, it is safe for the same data record to be processed more than once. Also, some use cases can tolerate processing data records more than once even if the processing is not idempotent. For example, imagine you are counting hits by IP address to auto-generate blacklists that help with mitigating DDoS attacks against your infrastructure; here, some overcounting is tolerable because hits from malicious IP addresses involved(涉及) in an attack(攻击) will vastly(极大地) outnumber hits from benign(良性的) IP addresses anyway.</p>
<blockquote>
<p>对于很多处理场景，至少一次的处理被证明是可接受的：通常而言，只要处理一条记录的影响是幂等的，那么多次处理同一条记录就是安全的。同时，有些用例也允许容忍多次处理，即使处理的影响不是幂等的。比如，想象下你要根据IP地址计算命中次数，来生成帮你你与DDOS攻击的黑名单；这里，（重复处理导致）过高的计数也是允许的，因为来自恶意IP地址的计数参与的攻击相比良性的IP地址数量上会更多。</p>
</blockquote>
<p>In general however, for non-idempotent operations such as counting, at-least-once processing guarantees may yield incorrect results. If a Kafka Streams application fails and restarts, it may double-count some data records that were processed shortly before the failure. We are planning to address this limitation and will support stronger guarantees and exactly-once processing semantics in a future release of Kafka Streams.</p>
<blockquote>
<p>不过非幂等操作比如计数，在至少一次的处理语义下有可能得到错误的结果。如果流应用程序失败或重启，那么在错误发生前一小段时间内，相同的记录可能会被重复计数。我们正在考虑解决这种限制，并且尝试支持更强的消息处理保证。</p>
</blockquote>
<h2 id="流控">流控</h2><p>Kafka Streams regulates(控制) the progress of streams by the timestamps of data records by attempting to synchronize all source streams in terms of time. By default, Kafka Streams will provide your application with event-time processing semantics. This is important especially when an application is processing multiple streams (i.e., Kafka topics) with a large amount of historical data. For example, a user may want to re-process past data in case the business logic of an application was changed significantly, e.g. to fix a bug in an analytics algorithm. Now it is easy to retrieve a large amount of past data from Kafka; however, without proper flow control, the processing of the data across topic partitions may become out-of-sync and produce incorrect results.</p>
<blockquote>
<p>Kafka Streams通过数据记录的时间撮控制流的进度，它会尝试根据时间来同步所有数据源产生的流。默认Kafka Streams为应用程序提供事件时间的处理语义。对于应用程序处理多个具有大量历史数据的流这种场景是特别重要的。举例应用程序的业务逻辑变化很显著时，用户可能想要重新处理过去的数据，比如在一个分析型的算法中修复一个错误。现在，我们可以很容易地从Kafka中接收大量的历史数据，不过如果没有做恰当的流控，在Kafka主题分区之间的数据处理可能变得不同步，并且产生错误的结果。</p>
</blockquote>
<p>As mentioned in the Concepts section, each data record in Kafka Streams is associated with a timestamp. Based on the timestamps of the records in its stream record buffer, stream tasks determine the next assigned partition to process among all its input streams. However, Kafka Streams does not reorder records within a single stream for processing since reordering would break the delivery semantics of Kafka and make it difficult to recover in the face of failure. This flow control is of course best-effort(尽最大努力) because it is not always possible to strictly enforce execution order across streams by record timestamp; in fact, in order to enforce strict execution ordering, one must either wait until the system has received all the records from all streams (which may be quite infeasible in practice) or inject additional information about timestamp boundaries or heuristic estimates(启发式的预估) such as MillWheel’s watermarks.</p>
<blockquote>
<p>Kafka Streams中的每条数据记录都关联了一个时间撮。基于<strong>流记录缓冲区</strong>（stream record buffer）中每条记录的时间撮，流任务会在所有输入流中决定下一个需要处理的分配分区。不过Kafka Streams不会在处理一个单一的流时对记录重新排序，因为重新排序会破坏Kafka的消息传递语义，并且在故障发生时不容易恢复（消息的顺序）。这种流控当然会尽最大努力，因为在流中并不可能总是按照记录的时间撮严格限制执行的顺序。实际上，如果要实现严格的执行顺序，一个流要么需要等待，直到系统（流处理应用程序）从所有的流中接收到所有的记录（这在实际中是不可实行的），或者注入关于时间边界的额外信息，或者使用类似MillWheel的水位概念做一些启发式的预估。</p>
</blockquote>
<h2 id="背压">背压</h2><p>Kafka Streams does not use a backpressure mechanism because it does not need one. Using a depth-first processing strategy, each record consumed from Kafka will go through the whole processor (sub-)topology for processing and for (possibly) being written back to Kafka before the next record will be processed. As a result, no records are being buffered in-memory between two connected stream processors. Also, Kafka Streams leverages Kafka’s consumer client behind the scenes, which works with a pull-based messaging model that allows downstream processors to control the pace(步伐) at which incoming data records are being read.</p>
<blockquote>
<p>Kafka Streams不适用背压机制，因为它并不需要。使用深入优先的处理策略，从Kafka中消费的每条记录在处理时会流经整个处理拓扑，并且有可能会在处理下一条记录之前回写到Kafka。结果就是：在两个链接的流处理算子中不会有记录被缓存在内存中。同时Kafka Streams利用了Kafka中基于消息拉取模型的消费者客户端，允许下游处理算子控制读取的输入数据的消费速度。</p>
</blockquote>
<p>The same applies to the case of a processor topology that contains multiple independent sub-topologies, which will be processed independently from each other (cf. Parallelism Model). For example, the following code defines a topology with two independent sub-topologies:</p>
<blockquote>
<p>同样的方式也运用在包含多个独立子拓扑的处理拓扑，每个子拓扑都会各自独立地处理，比如下面的代码定义了一个拓扑，具有两个独立的子拓扑：</p>
</blockquote>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">strea<span class="title">m1.</span>to<span class="comment">("my-topic")</span>;</span><br><span class="line">strea<span class="title">m2</span> = builder.stream<span class="comment">("my-topic")</span>;</span><br></pre></td></tr></table></figure>
<p>Any data exchange between sub-topologies will happen through Kafka, i.e. there is no direct data exchange (in the example above, data would be exchanged through the topic “my-topic”). For this reason there is no need for a backpressure mechanism in this scenario, too.</p>
<p>在子拓扑中的任何数据交换都会经过Kafka，在上面的示例中，并没有直接的数据交换，而是通过”my-topic”进行数据交换。基于这些原因，这种场景下也不需要一个背压机制。</p>
<h1 id="开发者指南">开发者指南</h1><h2 id="Kafka_Streams配置">Kafka Streams配置</h2><p>Kafka Streams的配置通过一个StreamsConfig实例完成。其中下面三个是必须要有的配置项：</p>
<ol>
<li>application.id：流处理应用程序的编号，在Kafka集群中必须是唯一的</li>
<li>bootstrap.servers：建立和Kafka集群的初始连接</li>
<li>zookeeper.connect：管理Kafka主题的ZooKeeper</li>
</ol>
<p>每个流处理应用程序的编号必须是唯一的，相同的应用程序编号会给应用程序所有的实例，编号作为资源隔离的标识，用在下面几个地方</p>
<ol>
<li>作为默认的Kafka生产者、消费者的client.id前缀</li>
<li>作为Kafka消费者的group.id，会用来协调工作</li>
<li>作为状态目录（state.dir）的子目录名称</li>
<li>作为内部Kafka主题名称的前缀</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"></span><br><span class="line">Properties settings = <span class="keyword">new</span> Properties();</span><br><span class="line"><span class="comment">// Set a few key parameters</span></span><br><span class="line">settings.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"my-first-streams-application"</span>);</span><br><span class="line">settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"kafka-broker1:9092"</span>);</span><br><span class="line">settings.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, <span class="string">"zookeeper1:2181"</span>);</span><br><span class="line"><span class="comment">// Any further settings</span></span><br><span class="line">settings.put(... , ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an instance of StreamsConfig from the Properties instance</span></span><br><span class="line">StreamsConfig config = <span class="keyword">new</span> StreamsConfig(settings);</span><br></pre></td></tr></table></figure>
<p><strong>Ser-/Deserialization (key.serde, value.serde)</strong>: Serialization and deserialization in Kafka Streams happens whenever data needs to be materialized, i.e.,:</p>
<ul>
<li>Whenever data is read from or written to a Kafka topic (e.g., via the KStreamBuilder#stream() and KStream#to() methods).</li>
<li>Whenever data is read from or written to a state store.</li>
</ul>
<p>当数据需要物化时，在Kafka Streams中会发生序列化和反序列化：  </p>
<ol>
<li>当从Kafka主题中读取数据，或者写入数据到Kafka主题（比如通过KStreamBuilder.stream()或者KStream.to()方法）</li>
<li>当从状态存储中读取数据，或者写入数据到状态存储</li>
</ol>
<p><strong>Number of Standby Replicas (num.standby.replicas)</strong>: This specifies the number of standby replicas. Standby replicas are shadow copies of local state stores. Kafka Streams attempts to create the specified number of replicas and keep them up to date as long as there are enough instances running. Standby replicas are used to minimize the latency of task failover. A task that was previously running on a failed instance is preferred(优先) to restart on an instance that has standby replicas so that the local state store restoration process from its changelog can be minimized. Details about how Kafka Streams makes use of the standby replicas to minimize the cost of resuming tasks on failover can be found in the State section.</p>
<p>指定备用副本的数量，备用副本是本地状态存储的shadow复制。Kafka Streams会尝试创建指定数量的副本，并且使这些副本一直保持最新的状态，只要有足够的实例在运行的话。备用副本会在任务发生故障切换时最小化延迟。运行在失败实例上的任务会优先在含有备用副本的实例上重启任务，这样可以最小化从变更日志中恢复本地的状态存储。</p>
<p><strong>Number of Stream Threads (num.stream.threads)</strong>: This specifies the number of stream threads in an instance of the Kafka Streams application. The stream processing code runs in these threads. Details about Kafka Streams threading model can be found in section Threading Model.</p>
<p>指定一个Kafka Streams应用程序实例的流线程数量。流处理代码运行在这些线程上。</p>
<p><strong>Replication Factor of Internal Topics (replication.factor)</strong>: This specifies the replication factor of internal topics that Kafka Streams creates when local states are used or a stream is repartitioned for aggregation. Replication is important for fault tolerance. Without replication even a single broker failure may prevent progress of the stream processing application. It is recommended to use a similar replication factor as source topics.</p>
<p>指定内部主题的副本因子，在使用本地状态或者流在聚合需要重新分区时，Kafka Streams会创建内部主题。副本对于故障容错非常重要。如果没有副本机制，即使一个Broker挂掉后，也会阻止流处理应用程序的正常进行。推荐设置为和源主题相同的副本因子。</p>
<p><strong>State Directory (state.dir)</strong>: Kafka Streams persists local states under the state directory. Each application has a subdirectory on its hosting machine, whose name is the application id, directly under the state directory. The state stores associated with the application are created under this subdirectory.</p>
<p>Kafka Streams会在状态目录下持久化本地状态。每个应用程序在它的物理机的状态目录下都有一个子目录，名称是应用程序的编号。和应用程序关联的状态存储都会在这个子目录下创建。</p>
<p><strong>Timestamp Extractor (timestamp.extractor)</strong>: A timestamp extractor extracts a timestamp from an instance of ConsumerRecord. Timestamps are used to control the progress of streams.</p>
<p>The default extractor is ConsumerRecordTimestampExtractor. This extractor retrieves built-in timestamps that are automatically embedded into Kafka messages by the Kafka producer client (introduced in Kafka 0.10.0.0, see KIP-32: Add timestamps to Kafka message). Depending on the setting of Kafka’s log.message.timestamp.type parameter, this extractor will provide you with:</p>
<ul>
<li>event-time processing semantics if log.message.timestamp.type is set to CreateTime aka “producer time” (which is the default). This represents the time when the Kafka producer sent the original message.</li>
<li>ingestion-time processing semantics if log.message.timestamp.type is set to LogAppendTime aka “broker time”. This represents the time when the Kafka broker received the original message.</li>
</ul>
<p>从一个ConsumerRecord实例解析出时间撮的解析器，时间撮会用来控制流的进度。默认的时间撮解析器是<code>ConsumerRecordTimestampExtractor</code>，这个解析器会获取被自动嵌入到Kafka消息中的内置时间撮（KIP-32：生产者产生消息时，会嵌入一个时间段到消息中）。根据<code>log.message.timestamp.type</code>的设置，有两种类型的解析器：  </p>
<ol>
<li>设置类型为<code>CreateTime</code>，即事件时间的处理语义。也是Producer的时间，作为默认值。表示Kafka生产者发送原始消息的时间点</li>
<li>设置类型为<code>LogAppendTime</code>，即摄取时间的处理语义。也是Broker的时间。表示Kafka Broker接收原始消息的时间点</li>
</ol>
<p>Another built-in extractor is WallclockTimestampExtractor. This extractor does not actually “extract” a timestamp from the consumed record but rather returns the current time in milliseconds from the system clock, which effectively means Streams will operate on the basis(基础) of the so-called processing-time of events.</p>
<p>You can also provide your own timestamp extractors, for instance to retrieve timestamps embedded in the payload of messages. Here is an example of a custom TimestampExtractor implementation:</p>
<p>另一个内置的解析器是<code>WallclockTimestampExtractor</code>，这个解析器并不会从消费记录中解析出一个时间撮，而是返回当前的系统时钟。你也可以提供自定义的时间撮解析器，比如从消息的内容（payload）中获取时间撮（通常是数据源自带的时间，而不是摄取时间），下面是一个自定义的TimestampExtractor实现类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.TimestampExtractor;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Extracts the embedded timestamp of a record (giving you "event time" semantics).</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyEventTimeExtractor</span> <span class="keyword">implements</span> <span class="title">TimestampExtractor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extract</span><span class="params">(ConsumerRecord&lt;Object, Object&gt; record)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// `Foo` is your own custom class, which we assume has a method that returns</span></span><br><span class="line">    <span class="comment">// the embedded timestamp (in milliseconds).</span></span><br><span class="line">    Foo myPojo = (Foo) record.value();</span><br><span class="line">    <span class="keyword">if</span> (myPojo != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> myPojo.getTimestampInMillis();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Kafka allows `null` as message value.  How to handle such message values</span></span><br><span class="line">      <span class="comment">// depends on your use case.  In this example, we decide to fallback to</span></span><br><span class="line">      <span class="comment">// wall-clock time (= processing-time).</span></span><br><span class="line">      <span class="keyword">return</span> System.currentTimeMillis();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You would then define the custom timestamp extractor in your Streams configuration as follows:<br>然后你需要在Streams配置中指定自定义的时间撮解析器（就像自定义序列化和反序列化器一样，都需要在配置文件中明确指定）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"></span><br><span class="line">Properties settings = <span class="keyword">new</span> Properties();</span><br><span class="line">settings.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MyEventTimeExtractor.class.getName());</span><br></pre></td></tr></table></figure>
<p><strong>Partition Grouper (partition.grouper)</strong>: A partition grouper is used to create a list of stream tasks given the partitions of source topics, where each created task is assigned with a group of source topic partitions. The default implementation provided by Kafka Streams is DefaultPartitionGrouper, which assigns each task with at most one partition for each of the source topic partitions; therefore, the generated number of tasks is equal to the largest number of partitions among the input topics. Usually an application does not need to customize the partition grouper.</p>
<p>分区分组用来在给定源主题的分区下创建流任务列表，每个创建的任务都会分配一组源主题的分区。默认的实现是<code>DefaultPartitionGrouper</code>，每个任务<strong>至多</strong>分配到每个源主题分区的一个分区。因此生成的任务数量等于输入主题中的最大分区数量（假设主题A有3个分区，主题B有4个分区，任务数量就等于max(3,4)=4）。通常应用程序不需要自定义分区分组方式。</p>
<p>Apart from Kafka Streams’ own configuration parameters (see previous sections) you can also specify parameters for the Kafka consumers and producers that are used internally, depending on the needs of your application. Similar to the Streams settings you define any such consumer and/or producer settings via StreamsConfig:</p>
<p>除了Kafka Streams自己的配置，你也可以根据你自己应用程序的需求设置内部的Kafka消费者和生产者的配置。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Properties streamsSettings = <span class="keyword">new</span> Properties();</span><br><span class="line"><span class="comment">// Example of a "normal" setting for Kafka Streams</span></span><br><span class="line">streamsSettings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"kafka-broker-01:9092"</span>);</span><br><span class="line"><span class="comment">// Customize the Kafka consumer settings of your Streams application</span></span><br><span class="line">streamsSettings.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, <span class="number">60000</span>);</span><br><span class="line">StreamsConfig config = <span class="keyword">new</span> StreamsConfig(streamsSettings);</span><br></pre></td></tr></table></figure>
<h2 id="编写一个流处理应用程序">编写一个流处理应用程序</h2><p>Any Java application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).</p>
<p>Currently Kafka Streams provides two sets of APIs to define the processor topology:</p>
<ul>
<li>A low-level Processor API that lets you add and connect processors as well as interact directly with state stores.</li>
<li>A high-level Kafka Streams DSL that provides common data transformation operations in a functional programming style such as map and filter operations. The DSL is the recommended starting point for developers new to Kafka Streams, and should cover many use cases and stream processing needs.</li>
</ul>
<p>任何使用Kafka Streams客户端库的Java应用程序被认为是一个Kafka Streams应用程序。Kafka Streams应用程序的计算逻辑被定义为一个处理拓扑，它是流处理算子和流的一张图。目前支持两种API定义处理拓扑：  </p>
<ol>
<li>低级Processor API：允许你添加和连接Processor，以及和状态存储直接交互</li>
<li>高级DSL：以函数式变成的风格提供通用的数据转换算子</li>
</ol>
<p>首先需要在pom.xml中定义Kafka Streams的依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-streams<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.10.0.1<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.10.0.1<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Dependencies below are required/recommended only when using Apache Avro. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>io.confluent<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-avro-serializer<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>3.0.1<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.avro<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>avro<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.avro<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>avro-maven-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>你可以在应用程序代码的任何地方调用Kafka Streams库，通常是在main方法中。首先你需要创建一个KafkaStreams实例。KafkaStream构造器的第一个参数接收一个用来定义拓扑的builder（Kafka STreams DSL使用KStreamBuilder，Processor API使用TopologyBuilder）；第二个参数是StreamsConfig实例，定义了这个指定拓扑的配置。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.KafkaStreams;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KStreamBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.TopologyBuilder;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the builders to define the actual processing topology, e.g. to specify</span></span><br><span class="line"><span class="comment">// from which input topics to read, which stream operations (filter, map, etc.)</span></span><br><span class="line"><span class="comment">// should be called, and so on.</span></span><br><span class="line"></span><br><span class="line">KStreamBuilder builder = ...;  <span class="comment">// when using the Kafka Streams DSL</span></span><br><span class="line"><span class="comment">// OR</span></span><br><span class="line">TopologyBuilder builder = ...; <span class="comment">// when using the Processor API</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the configuration to tell your application where the Kafka cluster is,</span></span><br><span class="line"><span class="comment">// which serializers/deserializers to use by default, to specify security settings,</span></span><br><span class="line"><span class="comment">// and so on.</span></span><br><span class="line">StreamsConfig config = ...;</span><br><span class="line"></span><br><span class="line">KafkaStreams streams = <span class="keyword">new</span> KafkaStreams(builder, config);</span><br></pre></td></tr></table></figure>
<p>现在内部结果已经初始化完毕，不过处理工作还没有开始。你需要手动调用start()方法显示地启动Kafka Streams的线程：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Start the Kafka Streams threads</span></span><br><span class="line">streams.start();</span><br></pre></td></tr></table></figure>
<p>如果这个流处理应用程序还有其他实例运行在其他地方，Kafka Streams针对用户会透明地将任务从已经存在的实例<strong>重新分配</strong>到你刚刚启动的新实例上。为了捕获一些非预期的异常，你需要在启动应用程序之前设置一个<code>UncaughtExceptionHandler</code>异常处理器，这个处理器会在无论什么时候流处理线程因为非预期的异常而终结的时候被调用。</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">streams<span class="built_in">.</span>setUncaughtExceptionHandler(<span class="literal">new</span> <span class="keyword">Thread</span><span class="built_in">.</span>UncaughtExceptionHandler() &#123;</span><br><span class="line">    <span class="keyword">public</span> uncaughtException(<span class="keyword">Thread</span> t, throwable e) &#123;</span><br><span class="line">        <span class="comment">// here you should examine the exception and perform an appropriate action!</span></span><br><span class="line">    &#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>停止应用程序实例的方式是调用KafkaStreams的close()方法</p>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Stop the Kafka Streams threads</span></span><br><span class="line">streams.close<span class="comment">()</span>;</span><br></pre></td></tr></table></figure>
<p>为了保证你的应用程序在响应SIGTERM时优雅地退出，推荐在关闭钩子中调用KafkaStreams.close()方法，Java8中的关闭钩子使用方式如下：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add shutdown hook to stop the Kafka Streams threads</span></span><br><span class="line">Runtime<span class="built_in">.</span>getRuntime()<span class="built_in">.</span>addShutdownHook(<span class="literal">new</span> <span class="keyword">Thread</span>(streams<span class="tag">::close</span>));</span><br></pre></td></tr></table></figure>
<p>Java7中的而关闭钩子使用方式如下：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add shutdown hook</span></span><br><span class="line">Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="comment">// Stop the Kafka Streams threads</span></span><br><span class="line">      streams.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;));</span><br></pre></td></tr></table></figure>
<p>当一个特定的应用程序实例停止时，Kafka Streams会将运行在这个实例上的任何任务迁移到其他运行的实例上（假设还存在这样的应用程序实例）。下面我们会详细描述两种API的使用方式。</p>
<h3 id="Processor_API">Processor API</h3><p>As mentioned in the Concepts section, a stream processor is a node in the processor topology that represents a single processing step. With the Processor API users can define arbitrary stream processors that processes one received record at a time, and connect these processors with their associated state stores to compose the processor topology.</p>
<p>流处理算子是处理拓扑的一个节点，代表了一个单独的处理步骤。使用Processor API，用户可以定义任意的流处理算子，一次处理一条接收到的记录，并且将这些算子和它们对应的状态存储连接在一起，组成算子拓扑。用户可以实现Processor接口来定义定制的流算子，Processor接口有两个主要的方法：</p>
<p><strong>DEFINING A STREAM PROCESSOR</strong>  </p>
<p>Users can define their customized stream processor by implementing the Processor interface, which provides two main API methods: process() and punctuate().</p>
<ul>
<li>process() is called on each of the received record.</li>
<li>punctuate() is called periodically based on advanced record timestamps. For example, if processing-time is used as the record timestamp, then punctuate() will be triggered every specified period of time.</li>
</ul>
<ol>
<li>process()会在每个接收到的记录上调用</li>
<li>punctuate()会基于记录的时间撮被定时调用</li>
</ol>
<p>The Processor interface also has an init() method, which is called by the Kafka Streams library during task construction phase. Processor instances should perform any required initialization in this method. The init() method passes in a ProcessorContext instance, which provides access to the metadata of the currently processed record, including its source Kafka topic and partition, its corresponding message offset, and further such information . This context instance can also be used to schedule the punctuation period (via ProcessorContext#schedule()) for punctuate(), to forward a new record as a key-value pair to the downstream processors (via ProcessorContext#forward()), and to commit the current processing progress (via ProcessorContext#commit()).</p>
<p>Processor接口也有一个init()方法，它会在Kafka Streams库初始化任务的阶段调用。Processor实例需要在该方法上执行一些必须的初始化工作。init()方法传递了一个ProcessorContext实例，为当前处理的记录提供元数据的访问接口，包括输入源的Kafka主题和分区，对应的消息偏移量，以及更多的一些信息。这个上下文对象也可以被用来调度punctuation()方法的间隔（通过schedule方法），将新的记录作为键值对转发到下游的处理算子（通过forward方法），或者提交当前的处理进度（通过commit方法）。</p>
<p>下面的Processor实现类定义了一个简单的WordCount算法：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountProcessor</span> <span class="keyword">implements</span> <span class="title">Processor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> ProcessorContext context;</span><br><span class="line">  <span class="keyword">private</span> KeyValueStore&lt;String, Long&gt; kvStore;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessorContext context)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// keep the processor context locally because we need it in punctuate() and commit()</span></span><br><span class="line">      <span class="keyword">this</span>.context = context;</span><br><span class="line">      <span class="comment">// call this processor's punctuate() method every 1000 time units.</span></span><br><span class="line">      <span class="keyword">this</span>.context.schedule(<span class="number">1000</span>);</span><br><span class="line">      <span class="comment">// retrieve the key-value store named "Counts"</span></span><br><span class="line">      kvStore = (KeyValueStore) context.getStateStore(<span class="string">"Counts"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String dummy, String line)</span> </span>&#123;</span><br><span class="line">      String[] words = line.toLowerCase().split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">          Long oldValue = kvStore.get(word);</span><br><span class="line">          <span class="keyword">if</span> (oldValue == <span class="keyword">null</span>) &#123;</span><br><span class="line">              kvStore.put(word, <span class="number">1L</span>);</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              kvStore.put(word, oldValue + <span class="number">1L</span>);</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">punctuate</span><span class="params">(<span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">      KeyValueIterator&lt;String, Long&gt; iter = <span class="keyword">this</span>.kvStore.all();</span><br><span class="line">      <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">          KeyValue&lt;String, Long&gt; entry = iter.next();</span><br><span class="line">          context.forward(entry.key, entry.value.toString());</span><br><span class="line">      &#125;</span><br><span class="line">      iter.close();</span><br><span class="line">      <span class="comment">// commit the current processing progress</span></span><br><span class="line">      context.commit();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="comment">// close the key-value store</span></span><br><span class="line">      kvStore.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的实现中，执行了下列操作：</p>
<ul>
<li>In the init() method, schedule the punctuation every 1000 time units (the time unit is normally milliseconds, which in this example would translate to punctuation every 1 second) and retrieve the local state store by its name “Counts”.</li>
<li>In the process() method, upon each received record, split the value string into words, and update their counts into the state store (we will talk about this later in this section).</li>
<li>In the punctuate() method, iterate the local state store and send the aggregated counts to the downstream processor (we will talk about downstream processors later in this section), and commit the current stream state.</li>
</ul>
<ol>
<li>init()方法，调度punctuation每隔1000m个时间单元（这个时间单元通常是ms，这里表示每隔一秒调度一次punctuation），并且通过“Counts”名称获取本地的状态存储</li>
<li>process()方法，当接收到每条记录时，将字符串分成多个单词，并且将它们的次数更新到状态存储中</li>
<li>punctuate()方法，迭代本地状态存储，发送聚合次数给下游的处理算子，最后提交当前的流状态</li>
</ol>
<p><strong>DEFINING A STATE STORE 定义一个状态存储</strong>  </p>
<p>Note that the WordCountProcessor defined above can not only access the currently received record in the process() method, but also maintain processing states to keep recently arrived records for stateful processing needs such as aggregations and joins. To take advantage of these states, users can define a state store by implementing the StateStore interface (the Kafka Streams library also has a few extended interfaces such as KeyValueStore); in practice, though, users usually do not need to customize such a state store from scratch but can simply use the Stores factory to define a state store by specifying whether it should be persistent, log-backed, etc.</p>
<p>上面定义的WordCountProcessor不仅可以在process()方法中访问当前接收到的记录，也会维护处理状态，并保持最近到达的记录，可以被用于有状态的处理比如聚合和联合操作。为了利用这些状态的优势，用户可以实现自定义的StateStore接口。不过实际中用户通常不需要从头开始实现一个这样的状态存储，而只需要使用Stores的工厂类来定义一个状态存储。下面的示例中，定义了一个持久化的键值存储，名字叫做“Counts”，并且Key的类型是String，Value的类型是Long。</p>
<figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StateStoreSupplier</span> countStore = <span class="type">Stores</span>.create(<span class="string">"Counts"</span>)</span><br><span class="line">              .withKeys(<span class="type">Serdes</span>.<span class="type">String</span><span class="literal">()</span>)</span><br><span class="line">              .withValues(<span class="type">Serdes</span>.<span class="type">Long</span><span class="literal">()</span>)</span><br><span class="line">              .persistent<span class="literal">()</span></span><br><span class="line">              .build<span class="literal">()</span>;</span><br></pre></td></tr></table></figure>
<p><strong>CONNECTING PROCESSORS AND STORES 连接处理算子和状态</strong>  </p>
<p>Now that we have defined the processor and the state stores, we can now construct the processor topology by connecting these processors and state stores together by using the TopologyBuilder instance. In addition, users can add source processors with the specified Kafka topics to generate input data streams into the topology, and sink processors with the specified Kafka topics to generate output data streams out of the topology.</p>
<p>现在我们已经定义了处理算子和状态存储，我们可以开始构造处理拓扑：通过使用TopologyBuilder的实例来连接这些处理算子以及状态存储。另外，用户可以添加输入源处理算子（source processors），并且指定特定的Kafka主题，这样就可以（读取Kafka主题）生成输入数据流进入到拓扑中；也可以指定目标处理算子（sink processors），也会指定特定的Kafka主题，这样就可以（写入Kafka主题）生成输出数据流到拓扑之外。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line"></span><br><span class="line"><span class="comment">// add the source processor node that takes Kafka topic "source-topic" as input</span></span><br><span class="line">builder.addSource(<span class="string">"Source"</span>, <span class="string">"source-topic"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add the WordCountProcessor node which takes the source processor as its upstream processor</span></span><br><span class="line">    .addProcessor(<span class="string">"Process"</span>, () -&gt; <span class="keyword">new</span> WordCountProcessor(), <span class="string">"Source"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create the countStore associated with the WordCountProcessor processor</span></span><br><span class="line">    .addStateStore(countStore, <span class="string">"Process"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add the sink processor node that takes Kafka topic "sink-topic" as output</span></span><br><span class="line">    <span class="comment">// and the WordCountProcessor node as its upstream processor</span></span><br><span class="line">    .addSink(<span class="string">"Sink"</span>, <span class="string">"sink-topic"</span>, <span class="string">"Process"</span>);</span><br></pre></td></tr></table></figure>
<p>There are several steps in the above implementation to build the topology, and here is a quick walk through:<br>上面的实现中构造一个拓扑有几个步骤：</p>
<ul>
<li>A source processor node named “Source” is added to the topology using the addSource method, with one Kafka topic “source-topic” fed to it.</li>
<li>A processor node named “Process” with the pre-defined WordCountProcessor logic is then added as the downstream processor of the “Source” node using the addProcessor method.</li>
<li>A predefined persistent key-value state store countStore is created and associated to the “Process” node.</li>
<li>A sink processor node is then added to complete the topology using the addSink method, taking the “Process” node as its upstream processor and writing to a separate “sink-topic” Kafka topic.</li>
</ul>
<ol>
<li>一个叫做“Source”的源算子被添加到拓扑中，使用了addSource方法，并且有一个Kafka的主题“source-topic”会（将数据）喂到这个源算子里</li>
<li>一个叫做“Process”的处理节点，使用了预定义的WordCountProcessor逻辑，然后通过addProcessor方法将其添加到（作为）“Source”节点的下游处理算子</li>
<li>创建了一个预定义的持久化键值存储，保存了countStore，并且关联了“Process”节点</li>
<li>使用addSink方法添加了一个目标处理节点，构成了一个完整的拓扑。将“Process”节点作为它的上游处理算子，并且写到一个Kafka的输出主题</li>
</ol>
<p>In this defined topology, the “Process” stream processor node is considered a downstream processor of the “Source” node, and an upstream processor of the “Sink” node. As a result, whenever the “Source” node forward a newly fetched record from Kafka to its downstream “Process” node, WordCountProcessor#process() method is triggered to process the record and update the associated state store; and whenever context#forward() is called in the WordCountProcessor#punctuate() method, the aggregate key-value pair will be sent via the “Sink” processor node to the Kafka topic “sink-topic”. Note that in the WordCountProcessor implementation, users need to refer with the same store name “Counts” when accessing the key-value store; otherwise an exception will be thrown at runtime indicating that the state store cannot be found; also if the state store itself is not associated with the processor in the TopologyBuilder code, accessing it in the processor’s init() method will also throw an exception at runtime indicating the state store is not accessible from this processor.</p>
<p>在上面定义的拓扑中，“Process”流处理节点被认为是“Source”节点的下游处理算子，也是“Sink”节点的上游处理算子。结果就是：无论什么时候，当”Source“节点从Kafka拉取一条新的记录，并转发给下游的”Process“节点，WordCountProcessor#process()方法就会被触发，并且会处理这条记录，以及更新相应的状态存储；并且无论什么时候当在WordCountProcessor#punctuate()方法中调用context#forward()时，聚合的键值对会通过”Sink“处理节点发送到Kafka的输出主题“sink-topic”中。注意在WordCountProcessor的实现中，用户在获取键值存储时需要参考这里（构造拓扑）相同的状态存储名称“Counts”（即拓扑定义的Counts键值存储名称，在WordCountProcessor中为了获取这个键值存储，两者的名称必须是一致的），否则在运行时会抛出一个异常说状态存储未找到。如果状态存储本身没有和TopologyBuilder代码中的处理节点（Processor节点，只有WordCountProcessor这一个节点）相关联的话，在Processor的init方法中访问状态存储也会抛出运行时的异常（状态存储不能在当前Processor节点访问）。</p>
<p>With the defined processor topology, users can now start running a Kafka Streams application instance. Please read how to run a Kafka Streams application for details.</p>
<p>有了定义好的处理拓扑，用户现在就可以启动一个Kafka Streams应用程序实例了。</p>
<h3 id="Kafka_Streams_DSL">Kafka Streams DSL</h3><p>As mentioned in the Concepts section, a stream is an unbounded, continuously updating data set. With the Kafka Streams DSL users can define the processor topology by concatenating multiple transformation operations where each operation transforming one stream into other stream(s); the resulted topology then takes the input streams from source Kafka topics and generates the final output streams throughout its concatenated transformations. However, different streams may have different semantics in their data records:</p>
<p>一个流是一个无界的，持续更新的数据集。使用Kakfa Streams的DSL，用户可以通过连接多个转换操作的方式来定义处理拓扑，其中每个操作会从一个流转换到新的其他流。最终拓扑会将从Kafka源读取的主题作为输入流，并且在贯穿连接的转换操作中，生成最终的输出流。不过不同的流在它们的数据记录中，可能有不同的语义：</p>
<ul>
<li>In some streams, each record represents a new immutable datum in their unbounded data set; we call these record streams.</li>
<li>In other streams, each record represents a revision (or update) of their unbounded data set in chronological(按时间顺序) order; we call these changelog streams.</li>
</ul>
<ol>
<li>在一些流中，每条记录代表了无界数据集中新的不可变数据，我们叫做记录流（record streams）</li>
<li>在其他流中，每条记录代表了无界数据集中按照时间顺序排序的更新，我们叫做变更日志流（changelog streams）</li>
</ol>
<p>这两种类型的流都可以存储成Kafka的主题。不过，它们的计算语义则截然不同。举例有一个聚合操作，为指定的key计算记录的次数。对于记录流（record streams）而言，每条记录都是来自于Kafka主题中带有key的消息（比如一个页面浏览的数据流会以用户的编号作为key）：</p>
<p>Both of these two types of streams can be stored as Kafka topics. However, their computational semantics can be quite different. Take the example of an aggregation operation that counts the number of records for the given key. For record streams, each record is a keyed message from a Kafka topic (e.g., a page view stream keyed by user ids):</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># Example: a record stream for page view events</span></span><br><span class="line"><span class="preprocessor"># Notation is &lt;record key&gt; =&gt; &lt;record value&gt;</span></span><br><span class="line"><span class="number">1</span> =&gt; &#123;<span class="string">"time"</span>:<span class="number">1440557383335</span>, <span class="string">"user_id"</span>:<span class="number">1</span>, <span class="string">"url"</span>:<span class="string">"/home?user=1"</span>&#125;</span><br><span class="line"><span class="number">5</span> =&gt; &#123;<span class="string">"time"</span>:<span class="number">1440557383345</span>, <span class="string">"user_id"</span>:<span class="number">5</span>, <span class="string">"url"</span>:<span class="string">"/home?user=5"</span>&#125;</span><br><span class="line"><span class="number">2</span> =&gt; &#123;<span class="string">"time"</span>:<span class="number">1440557383456</span>, <span class="string">"user_id"</span>:<span class="number">2</span>, <span class="string">"url"</span>:<span class="string">"/profile?user=2"</span>&#125;</span><br><span class="line"><span class="number">1</span> =&gt; &#123;<span class="string">"time"</span>:<span class="number">1440557385365</span>, <span class="string">"user_id"</span>:<span class="number">1</span>, <span class="string">"url"</span>:<span class="string">"/profile?user=1"</span>&#125;</span><br></pre></td></tr></table></figure>
<p>The counting operation for record streams is trivial to implement: you can maintain a local state store that tracks the latest count for each key, and, upon receiving a new record, update the corresponding key by incrementing its count by one.</p>
<p>对记录流进行计数操作非常容易实现：你可以维护一个本地的状态存储，用来跟踪每个key的最近的次数。并且，当接收到一条新的记录时，更新对应key的值，把它的次数加1。</p>
<p>For changelog streams, on the other hand, each record is an update of the unbounded data set (e.g., a changelog stream for a user profile table, where the user id serves as both the primary key for the table and as the record key for the stream; here, a new record represents a changed row of the table). In practice you would usually store such streams in Kafka topics where log compaction is enabled.</p>
<p>对于变更日志流而言，每条记录都是对于无界数据集的一次更新（比如一个变更日志流是针对用户的个人信息表，用户的编号既作为表的主键，也会作为日志流记录的key，这里一条新记录表示的是表的一行更新）。实际应用中在Kafka的主题存储这样的路，应该开启日志压缩。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># Example: a changelog stream for a user profile table</span></span><br><span class="line"><span class="number">1</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557383335</span>, <span class="string">"user_id"</span>:<span class="number">1</span>, <span class="string">"email"</span>:<span class="string">"user1@aol.com"</span>&#125;</span><br><span class="line"><span class="number">5</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557383345</span>, <span class="string">"user_id"</span>:<span class="number">5</span>, <span class="string">"email"</span>:<span class="string">"user5@gmail.com"</span>&#125;</span><br><span class="line"><span class="number">2</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557383456</span>, <span class="string">"user_id"</span>:<span class="number">2</span>, <span class="string">"email"</span>:<span class="string">"user2@yahoo.com"</span>&#125;</span><br><span class="line"><span class="number">1</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557385365</span>, <span class="string">"user_id"</span>:<span class="number">1</span>, <span class="string">"email"</span>:<span class="string">"user1-new-email-addr@comcast.com"</span>&#125;</span><br><span class="line"><span class="number">2</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557385395</span>, <span class="string">"user_id"</span>:<span class="number">2</span>, <span class="string">"email"</span>:null&#125;  &lt;-- user has been deleted</span><br></pre></td></tr></table></figure>
<p>As a result the counting operation for changelog streams is no longer monotonically incrementing: you need to also decrement the counts when a delete update record is received on some given key as well. In addition, even for counting aggregations on an record stream, the resulting aggregate is no longer an record stream but a relation / table, which can then be represented as a changelog stream of updates on the table.</p>
<p>对变更日志流的计数操作不再是单调递增的了：当接收到指定key的一个删除更新记录时，你需要减少计数。另外，即使是在记录流上做聚合计数，聚合的结果也不是一个记录流，而是一张表，它代表的是在表上的变更日志流的更新。</p>
<p>The counting operation of a user profile changelog stream is peculiar(罕见的) because it will generate, for a given key, a count of either 0 (meaning the key does not exist or does not exist anymore) or 1 (the key exists) only. Multiple records for the same key are considered duplicate, old information of the most recent record, and thus will not contribute to the count.</p>
<p>For changelog streams developers usually prefer counting a non-primary-key field. We use the example above just for the sake of illustration.</p>
<p>对用户的个人信息变更日志流做计数操作是比较少见的，因为它为指定的key生成的计数要么是0（表示key不存在），要么是1（key存在）。相同key的多个记录被认为是重复的、相对当前最近记录而言是旧的信息，因此不会对计数结果产生影响。对于变更日志流，开发者通常会在非主键字段上计数，上面的示例仅仅是为了演示（有主键）。</p>
<p>One of the key design principles of the Kafka Streams DSL is to understand and distinguish between record streams and changelog streams and to provide operators with the correct semantics for these two different types of streams. More concretely, in the Kafka Streams DSL we use the KStream interface to represent record streams, and we use a separate KTable interface to represent changelog streams. The Kafka Streams DSL is therefore the recommended API to implement a Kafka Streams application. Compared to the lower-level Processor API, its benefits are:</p>
<p>Kafka Streams DSL的一个主要设计原则是理解和区分记录流合变更日志流，并且为这两种类型流的操作提供正确的语义。更具体地来说，在Kafka Streams DSL中，我们使用KStream接口来表示记录流，使用KTable接口代表变更日志流。所以Kafka Streams DSL是用来在实现Kafka Streams应用程序时推荐的API，和低级Processor API比较，它有几个优点：</p>
<ul>
<li>More concise and expressive code, particularly when using Java 8+ with lambda expressions.</li>
<li>Easier to implement stateful transformations such as joins and aggregations.</li>
<li>Understands the semantic differences of record streams and changelog streams, so that transformations such as aggregations work as expected depending on which type of stream they operate against.</li>
</ul>
<ol>
<li>更简洁、更具有表达力的代码，尤其是使用Java8的lambda表达式时</li>
<li>可以很容易地实现一个有状态的操作，比如联合和聚合操作</li>
<li>理解记录流和变更日志流的语义区别，这样转换操作（比如聚合）根据流的类型按照预期的方式工作</li>
</ol>
<h4 id="CREATING_SOURCE_STREAMS_FROM_KAFKA">CREATING SOURCE STREAMS FROM KAFKA</h4><p>Both KStream or KTable objects can be created as a source stream from one or more Kafka topics via KStreamBuilder, an extended class of TopologyBuilder used in the lower-level Processor API (for KTable you can only create the source stream from a single topic).</p>
<p>KStream和KTable对象都可以通过KStreamBuilder创建，作为从一个或多个Kafka主题的输入源流。KStreamBuilder是TopologyBuilder（低级的Processor API）的继承类。对于KTable，你只能从一个Kafka主题中创建一个输入源流。</p>
<table>
<thead>
<tr>
<th>Interface</th>
<th>How to instantiate</th>
</tr>
</thead>
<tbody>
<tr>
<td>KStream<k, v=""></k,></td>
<td>KStreamBuilder#stream(…)</td>
</tr>
<tr>
<td>KTable<k, v=""></k,></td>
<td>KStreamBuilder#table(…)</td>
</tr>
</tbody>
</table>
<p>When creating an instance, you may override the default serdes for record keys (K) and record values (V) used for reading the data from Kafka topics (see Data types and serdes for more details); otherwise the default serdes specified through StreamsConfig will be used.</p>
<p>当创建一个（KStream或KTable）实例时，需要指定输入源的Kafka主题，你可能需要指定读取记录的序列化器，如果不指定时，会使用StreamsConfig中指定的序列化器。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KStreamBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KTable;</span><br><span class="line"></span><br><span class="line">KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line"></span><br><span class="line"><span class="comment">// In this example we assume that the default serdes for keys and values are</span></span><br><span class="line"><span class="comment">// the String serde and the generic Avro serde, respectively.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a stream of page view events from the PageViews topic, where the key of</span></span><br><span class="line"><span class="comment">// a record is assumed to be the user id (String) and the value an Avro GenericRecord</span></span><br><span class="line"><span class="comment">// that represents the full details of the page view event.</span></span><br><span class="line">KStream&lt;String, GenericRecord&gt; pageViews = builder.stream(<span class="string">"PageViews"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a changelog stream for user profiles from the UserProfiles topic,</span></span><br><span class="line"><span class="comment">// where the key of a record is assumed to be the user id (String) and its value</span></span><br><span class="line"><span class="comment">// an Avro GenericRecord.</span></span><br><span class="line">KTable&lt;String, GenericRecord&gt; userProfiles = builder.table(<span class="string">"UserProfiles"</span>);</span><br></pre></td></tr></table></figure>
<h4 id="TRANSFORM_A_STREAM">TRANSFORM A STREAM</h4><p>KStream and KTable support a variety of transformation operations. Each of these operations can be translated into one or more connected processors into the underlying processor topology. Since KStream and KTable are strongly typed, all these transformation operations are defined as generics functions where users could specify the input and output data types.</p>
<p>KStream和KTable支持很多类型的转换操作。每种操作都可以翻译成底层处理拓扑的一个或多个相连起来的处理算子。由于KStream和KTable是强类型的，所有这些转换操作被定义为通用的函数，这样用户可以指定输入和输出的数据类型。</p>
<p>Some KStream transformations may generate one or more KStream objects (e.g., filter and map on KStream generate another KStream, while branch on KStream can generate multiple KStream) while some others may generate a KTable object (e.g., aggregation) interpreted(理解) as the changelog stream to the resulted relation. This allows Kafka Streams to continuously update the computed value upon arrivals of late records after it has already been produced to the downstream transformation operators. As for KTable, all its transformation operations can only generate another KTable (though the Kafka Streams DSL does provide a special function to convert a KTable representation into a KStream, which we will describe later). Nevertheless, all these transformation methods can be chained together to compose a complex processor topology.</p>
<p>有些KStream转换可能产生不止一个KStream对象（在KStream上进行过滤和映射会生成新的KStream，而在KStream上进行分支则会产生多个KStream），而有些可能产生一个KTable对象（比如聚合转换）。这就允许Kafka Streams即使在记录已经发送给下游的转换算子的情况下，在迟到的记录到来时，也可以持续地更新已经计算过的值。对于KTable而言，它的所有转换操作只会生成新的KTable（尽管Kafka Streams DSL提供额外的函数可以将一个KTable转换成KStream）。不仅如此，所有这些转换操作都可以被链接在一起，从而构造出一个复杂的处理拓扑。</p>
<p>We describe these transformation operations in the following subsections, categorizing them as two categories: stateless and stateful transformations.</p>
<p>下面我们会将这些转换操作分成两类：无状态的和有状态的转换。</p>
<h4 id="STATELESS_TRANSFORMATIONS">STATELESS TRANSFORMATIONS</h4><p>Stateless transformations include filter, filterNot, foreach, map, mapValues, selectKey, flatMap, flatMapValues, branch. Most of them can be applied to both KStream and KTable, where users usually pass a customized function to these functions as a parameter; e.g. a Predicate for filter, a KeyValueMapper for map, and so on. Stateless transformations, by definition, do not depend on any state for processing, and hence implementation-wise they do not require a state store associated with the stream processor.</p>
<p>无状态的转换包括：<code>filter, filterNot, foreach, map, mapValues, selectKey, flatMap, flatMapValues, branch</code>。它们中的大部分都可以被同时用在KStream和KTable上，用户通常需要为这些函数传递一个自定义的函数作为参数。比如对于filter算子，需要传递一个Predicate，对于map算子需要传递一个KeyValueMapper等等。无状态的操作，从定义上来说，不依赖于处理的任何状态，因此它们并不需要和流处理算子相关联的状态存储。</p>
<p>下面是mapValues分别使用Java8的lambda，以及Java7的示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Java8</span></span><br><span class="line">KStream&lt;Long, String&gt; uppercased =</span><br><span class="line">    nicknameByUserId.mapValues(nickname -&gt; nickname.toUpperCase());</span><br><span class="line"></span><br><span class="line"><span class="comment">//Java7</span></span><br><span class="line">KStream&lt;Long, String&gt; uppercased =</span><br><span class="line">    nicknameByUserId.mapValues(</span><br><span class="line">        <span class="keyword">new</span> ValueMapper&lt;String&gt;() &#123;</span><br><span class="line">            <span class="annotation">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">apply</span><span class="params">(String nickname)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> nickname.toUpperCase();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    );</span><br></pre></td></tr></table></figure>
<p>The function is applied to each record, and its result will trigger the creation a new record.  </p>
<p>这个函数会作用于每条记录，它的结果会创建一个新的记录。</p>
<h4 id="STATEFUL_TRANSFORMATIONS">STATEFUL TRANSFORMATIONS</h4><p>有状态的操作包括如下几个： </p>
<ul>
<li>joins (KStream/KTable): join, leftJoin, outerJoin</li>
<li>aggregations (KStream): countByKey, reduceByKey, aggregateByKey</li>
<li>aggregations (KTable): groupBy plus count, reduce, aggregate (via KGroupedTable)</li>
<li>general transformations (KStream): process, transform, transformValues</li>
</ul>
<p>Stateful transformations are transformations where the processing logic requires accessing an associated state for processing and producing outputs. For example, in join and aggregation operations, a windowing state is usually used to store all the records received so far within the defined window boundary. The operators can then access accumulated records in the store and compute based on them (see Windowing a Stream for details).</p>
<p>有状态的操作指的是转换的处理逻辑需要访问相关的状态，来做处理操作和产生结果。举例联合和聚合操作，会使用一个窗口状态，存储定义的窗口边界内接收到的所有记录。操作算子就可以从存储中访问收集到的记录，基于这些记录做计算。使用Java8的WordCoun示例：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// We assume message values represent lines of text.  For the sake of this example, we ignore</span></span><br><span class="line"><span class="comment">// whatever may be stored in the message keys.</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; textLines = <span class="attribute">...</span>;</span><br><span class="line"></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, Long&gt; wordCounts = textLines</span><br><span class="line">    <span class="comment">// Split each text line, by whitespace, into words.  The text lines are the message</span></span><br><span class="line">    <span class="comment">// values, i.e. we can ignore whatever data is in the message keys and thus invoke</span></span><br><span class="line">    <span class="comment">// `flatMapValues` instead of the more generic `flatMap`.</span></span><br><span class="line">    <span class="built_in">.</span>flatMapValues(value <span class="subst">-&gt; </span>Arrays<span class="built_in">.</span>asList(value<span class="built_in">.</span>toLowerCase()<span class="built_in">.</span>split(<span class="string">"\\W+"</span>)))</span><br><span class="line">    <span class="comment">// We will subsequently invoke `countByKey` to count the occurrences of words, so we use</span></span><br><span class="line">    <span class="comment">// `map` to ensure the key of each record contains the respective word.</span></span><br><span class="line">    <span class="built_in">.</span><span class="built_in">map</span>((key, word) <span class="subst">-&gt; </span><span class="literal">new</span> KeyValue&lt;&gt;(word, word))</span><br><span class="line">    <span class="comment">// Count the occurrences of each word (record key).</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// This will change the stream type from `KStream&lt;String, String&gt;` to</span></span><br><span class="line">    <span class="comment">// `KTable&lt;String, Long&gt;` (word -&gt; count).  We must provide a name for</span></span><br><span class="line">    <span class="comment">// the resulting KTable, which will be used to name e.g. its associated</span></span><br><span class="line">    <span class="comment">// state store and changelog topic.</span></span><br><span class="line">    <span class="built_in">.</span>countByKey(<span class="string">"Counts"</span>)</span><br><span class="line">    <span class="comment">// Convert the `KTable&lt;String, Long&gt;` into a `KStream&lt;String, Long&gt;`.</span></span><br><span class="line">    <span class="built_in">.</span>toStream();</span><br></pre></td></tr></table></figure>
<p>WordCount使用Java 7:</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Code below is equivalent to the previous Java 8+ example above.</span></span><br><span class="line">KStream&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; textLines = ...;</span><br><span class="line"></span><br><span class="line">KStream&lt;<span class="keyword">String</span>, Long&gt; wordCounts = textLines</span><br><span class="line">    .flatMapValues(<span class="keyword">new</span> ValueMapper&lt;<span class="keyword">String</span>, Iterable&lt;<span class="keyword">String</span>&gt;&gt;() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        <span class="keyword">public</span> Iterable&lt;<span class="keyword">String</span>&gt; apply(<span class="keyword">String</span> value) &#123;</span><br><span class="line">            <span class="keyword">return</span> Arrays.asList(value.toLowerCase().<span class="built_in">split</span>(<span class="string">"\\W+"</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .<span class="built_in">map</span>(<span class="keyword">new</span> KeyValueMapper&lt;<span class="keyword">String</span>, <span class="keyword">String</span>, KeyValue&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;&gt;() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        <span class="keyword">public</span> KeyValue&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; apply(<span class="keyword">String</span> <span class="variable">key</span>, <span class="keyword">String</span> word) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> KeyValue&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;(word, word);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .countByKey(<span class="string">"Counts"</span>)</span><br><span class="line">    .toStream();</span><br></pre></td></tr></table></figure>
<h4 id="WINDOWING_A_STREAM">WINDOWING A STREAM</h4><p>Windowing is a common prerequisite for stateful transformations which group records in a stream, for example, by their timestamps. A local state store is usually needed for a windowing operation to store recently received records based on the window interval, while old records in the store are purged after the specified window retention period. The retention time can be set via Windows#until().</p>
<p>窗口是有状态转换操作的基本条件，它会在流中对记录进行分组，比如根据时间撮的方式进行分组。一个窗口相关的操作通常需要一个本地状态存储，来保存最近接收到的记录，这些记录是基于窗口间隔，状态存储中旧的记录会在指定的窗口保留时间过去后会被删除。保留时间是通过Windows#until()设置的。Kafka Streams目前支持以下类型的窗口：</p>
<table>
<thead>
<tr>
<th>Window name</th>
<th>Behavior</th>
<th>Short description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tumbling time window（翻转窗口）</td>
<td>Time-based</td>
<td>Fixed-size, non-overlapping, gap-less windows</td>
</tr>
<tr>
<td>Hopping time window（跳跃时间窗口）</td>
<td>Time-based</td>
<td>Fixed-size, overlapping windows</td>
</tr>
<tr>
<td>Sliding time window（滑动时间窗口）</td>
<td>Time-based</td>
<td>Fixed-size, overlapping windows that work on differences between record timestamps</td>
</tr>
</tbody>
</table>
<p><strong>Tumbling time windows</strong> are a special case of hopping time windows and, like the latter, are windows based on time intervals. They model fixed-size, non-overlapping, gap-less windows. A tumbling window is defined by a single property: the window’s size. A tumbling window is a hopping window whose window size is equal to its advance interval. Since tumbling windows never overlap, a data record will belong to one and only one window.</p>
<p>翻转窗口是跳跃窗口的一个特例，和后者一样，所有的窗口都是基于时间间隔。窗口的大小是固定的，窗口之间不会重复，也没有间隙。一个翻转窗口之定义了窗口大小这个简单的属性，默认它的窗口大小和前进间隔相等。由于翻转窗口不会有重叠，所以一条记录指挥属于一个窗口。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A tumbling time window with a size 60 seconds (and, by definition, an implicit</span></span><br><span class="line"><span class="comment">// advance interval of 60 seconds).</span></span><br><span class="line"><span class="comment">// The window's name -- the string parameter -- is used to e.g. name the backing state store.</span></span><br><span class="line"><span class="keyword">long</span> windowSizeMs = <span class="number">60</span> * <span class="number">1000L</span>;</span><br><span class="line">TimeWindows.of(<span class="string">"tumbling-window-example"</span>, windowSizeMs);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The above is equivalent to the following code:</span></span><br><span class="line">TimeWindows.of(<span class="string">"tumbling-window-example"</span>, windowSizeMs).advanceBy(windowSizeMs);</span><br></pre></td></tr></table></figure>
<p><img src="https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/02/52/ASAQL-1.png" alt=""></p>
<p><strong>Hopping time windows</strong> are windows based on time intervals. They model fixed-sized, (possibly) overlapping windows. A hopping window is defined by two properties: the window’s size and its advance interval (aka “hop”). The advance interval specifies by how much a window moves forward relative to the previous one. For example, you can configure a hopping window with a size 5 minutes and an advance interval of 1 minute. Since hopping windows can overlap – and in general they do – a data record may belong to more than one such windows.</p>
<p>跳跃时间窗口是基于时间间隔的窗口，固定大小，但是窗口之间可能有重叠。跳跃窗口定义了两个属性：窗口大小和跳跃间隔（hop的中文意思是跳跃）。跳跃间隔指定了一个窗口相对于前一个窗口的移动大小。比如你可以配置一个跳跃窗口的大小为5分钟，跳跃间隔为1分钟。由于跳跃窗口允许重叠，所以一条记录可能属于不止一个窗口。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A hopping time window with a size of 5 minutes and an advance interval of 1 minute.</span></span><br><span class="line"><span class="comment">// The window's name -- the string parameter -- is used to e.g. name the backing state store.</span></span><br><span class="line"><span class="keyword">long</span> windowSizeMs = <span class="number">5</span> * <span class="number">60</span> * <span class="number">1000L</span>;</span><br><span class="line"><span class="keyword">long</span> advanceMs =    <span class="number">1</span> * <span class="number">60</span> * <span class="number">1000L</span>;</span><br><span class="line">TimeWindows.of(<span class="string">"hopping-window-example"</span>, windowSizeMs).advanceBy(advanceMs);</span><br></pre></td></tr></table></figure>
<p><img src="https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/02/52/ASAQL-2.png" alt=""></p>
<p>Pay attention, that tumbling and hopping time windows are aligned to the epoch and that the lower window time interval bound is inclusive, while the upper bound is exclusive.</p>
<p>Aligned to the epoch means, that the first window starts at timestamp zero. For example, hopping windows with size of 5000ms and advance of 3000ms, have window boundaries [0;5000),[3000;8000),…— and not [1000;6000),[4000;9000),… or even something “random” like [1452;6452),[4452;9452),…, which might be the case if windows get initialized depending on system/application start-up time, introducing non-determinism.</p>
<p>注意，翻转窗口和跳跃窗口是和时间点对齐的，所以窗口时间的下界（lower bound）被包含，而下界（upper bound）不被包含。和时间点对齐表示，第一个窗口从时间点为0开始。比如跳跃窗口的大小=5000ms，跳跃间隔=3000ms，对应的窗口边界分别是<code>[0;5000),[3000;8000),...</code>，而不是<code>[1000;6000),[4000;9000),...</code>，也不是<code>[1452;6452),[4452;9452),...</code>。窗口计数的示例如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">KStream&lt;String, GenericRecord&gt; viewsByUser = ...;</span><br><span class="line"></span><br><span class="line">KTable&lt;Windowed&lt;String&gt;, Long&gt; userCounts =</span><br><span class="line">    <span class="comment">// count users, using hopping windows of size 5 minutes that advance every 1 minute</span></span><br><span class="line">    viewsByUser.countByKey(TimeWindows.of(<span class="string">"GeoPageViewsWindow"</span>, <span class="number">5</span> * <span class="number">60</span> * <span class="number">1000L</span>).advanceBy(<span class="number">60</span> * <span class="number">1000L</span>));</span><br></pre></td></tr></table></figure>
<p>Unlike non-windowed aggregates that we have seen previously, windowed aggregates return a windowed KTable whose key type is Windowed<k>. This is to differentiate aggregate values with the same key from different windows. The corresponding window instance and the embedded key can be retrieved as Windowed#window() and Windowed#key(), respectively.</k></p>
<p>和前面看到的没有窗口的聚合不同的是，窗口聚合操作返回一个带有窗口的KTable，它的key是Windowed<k>，目的是区分不同窗口中存在相同的key（如果没有带窗口，那么相同的key在不同的窗口中就无法区分）。对应的窗口实例以及内置的key分别可以通过<code>Windowed#window()</code>和<code>Windowed#key()</code>获取到。</k></p>
<p><strong>Sliding windows</strong> are actually quite different from hopping and tumbling windows. A sliding window models a fixed-size window that slides continuously over the time axis; here, two data records are said to be included in the same window if the difference of their timestamps is within the window size. Thus, sliding windows are not aligned to the epoch, but on the data record timestamps. Pay attention, that in contrast to hopping and tumbling windows, lower and upper window time interval bounds are both inclusive. In Kafka Streams, sliding windows are used only for join operations, and can be specified through the JoinWindows class.</p>
<p>滑动窗口和跳跃窗口、翻转窗口都不同，它也有固定的窗口大小，不过是在时间轴上持续地滑动。比如有两条记录的时间撮差别是在窗口大小内的，这两条记录就会被包含在同一个窗口中。所以滑动窗口并不是和时间点对齐，而是和记录的时间撮对齐。注意和跳跃窗口、翻转窗口相反的是，滑动窗口会同时包含窗口边界的上界和下界。在Kafka Streams中，滑动窗口仅用于join操作。</p>
<p><img src="https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/02/52/ASAQL-3.png" alt=""></p>
<h4 id="JOINING_STREAMS">JOINING STREAMS</h4><p>Many stream processing applications can be coded as stream join operations. For example, applications backing an online shop might need to access multiple, updating database tables (e.g. sales prices, inventory, customer information) when processing a new record. These applications can be implemented such that they work on the tables’ changelog streams directly, i.e. without requiring to make a database query over the network for each record. In this example, the KTable concept in Kafka Streams would enable you to track the latest state (think: snapshot) of each table in a local key-value store, thus greatly reducing the processing latency as well as reducing the load of the upstream databases.</p>
<p>许多流处理应用程序都需要流式的join操作。比如在线商店应用程序在处理一条新记录时可能需要访问或更新多张数据库表（比如销售价格表，库存表，用户信息表）。这些应用程序可以在表的变更日志流（KTable是变更日志流，KStream是记录流）上直接实现，比如对每条记录不需要跨网络的数据库查询。在这个示例中，Kafka Streams的KTable概念使你可以在一个本地键值存储中跟踪每张表的最新状态（快照），因此可以很明显地减少处理延迟，以及减少上游数据库的压力。在Kafka Streams中，有下面两种的join操作：</p>
<ul>
<li>Join a KStream with another KStream or KTable.</li>
<li>Join a KTable with another KTable only.</li>
</ul>
<p>有三种join组合方式：</p>
<ul>
<li>KStream-to-KStream Joins are always windowed joins, since otherwise the join result size might grow infinitely in size. Here, a newly received record from one of the streams is joined with the other stream’s records within the specified window interval to produce one result for each matching pair based on user-provided ValueJoiner. A new KStream instance representing the result stream of the join is returned from this operator.</li>
<li>KTable-to-KTable Joins are join operations designed to be consistent(一致) with the ones in relational databases. Here, both changelog streams are materialized into local state stores to represent the latest snapshot of the their data table duals(双重). When a new record is received from one of the streams, it is joined with the other stream’s materialized state stores to produce one result for each matching pair based on user-provided ValueJoiner. A new KTable instance representing the result stream of the join, which is also a changelog stream of the represented table, is returned from this operator.</li>
<li>KStream-to-KTable Joins allow you to perform table lookups against a changelog stream (KTable) upon receiving a new record from another record stream (KStream). An example use case would be to enrich(使丰富) a stream of user activities (KStream) with the latest user profile information (KTable). Only records received from the record stream will trigger the join and produce results via ValueJoiner, not vice versa (i.e., records received from the changelog stream will be used only to update the materialized state store). A new KStream instance representing the result stream of the join is returned from this operator.</li>
</ul>
<ol>
<li><strong>KStream-to-KStream Join</strong>操作总是针对窗口的Join，否则Join结果的大小会无限膨胀。从其中一个流接收到的新记录会和另外一个流的记录进行Join，后者的流会指定窗口间隔，最后会基于用户提供的<code>ValueJoiner</code>为每个匹配对产生一个结果。这里返回的结果是一个新的KStream实例，代表了Join操作的结果流。</li>
<li><strong>KTable-to-KTable Join</strong>操作被设计为和关系型数据库类似的操作。两个变更日志流（KTable）都会被物化成本地状态存储，表示数据表的最近快照。当从其中的一个流接收到一条新记录，它会和另外一个流的物化状态存储进行join，并根据用户提供的ValueJoiner产生一个匹配的结果。返回的结果是新的KTable实例，代表Join操作的结果流，它也是一个变更日志流。</li>
<li><strong>KStream-to-KTable Join</strong>操作允许你在记录流（KStream）上接收到一条新记录时，从一个变更日志流（KTable）上执行表（级别的记录）查询。比如对一个用户活动流（KStream）使用最近的用户个人信息（KTable）进行信息增强。只有从记录流接收的记录才会触发join操作，并通过ValueJoiner产生结果，反过来则不行（比如从变更日志流中接收的新记录只会被用来更新物化的状态存储，而不会和KStream记录流进行join）。返回的结果是一个新的KStream，代表了Join操作的结果流。</li>
</ol>
<p>根据操作对象，不同操作类型支持不同的join语义：</p>
<table>
<thead>
<tr>
<th>Join operands</th>
<th>(INNER) JOIN</th>
<th>OUTER JOIN</th>
<th>LEFT JOIN</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>KStream-to-KStream</td>
<td>Supported</td>
<td>Supported</td>
<td>Supported</td>
<td>KStream</td>
</tr>
<tr>
<td>KTable-to-KTable</td>
<td>Supported</td>
<td>Supported</td>
<td>Supported</td>
<td>KTable</td>
</tr>
<tr>
<td>KStream-to-KTable</td>
<td>N/A</td>
<td>N/A</td>
<td>Supported</td>
<td>KStream</td>
</tr>
</tbody>
</table>
<p>Kafka Streams的Join语义类似于关系型数据库的操作符：</p>
<ul>
<li>Inner join produces new joined records when the join operator finds some records with the same key in the other stream / materialized store.</li>
<li>Outer join works like inner join if some records are found in the windowed stream / materialized store. The difference is that outer join still produces a record even when no records are found. It uses null as the value of missing record.</li>
<li>Left join is like outer join except that for KStream-KStream join it is always driven by record arriving from the primary stream; while for KTable-KTable join it is driven by both streams to make the result consistent with the left join of databases while only permits missing records in the secondary stream. In a KStream-KTable left join, a KStream record will only join a KTable record if the KTable record arrived before the KStream record (and is in the KTable). Otherwise, the join result will be null</li>
</ul>
<ol>
<li><strong>Inner join</strong>：当join操作符在两个流或物化存储中都找到相同key的记录，产生新的join记录。</li>
<li><strong>Outer join</strong>：如果在窗口流或物化视图中找到记录，则和inner join类似。不同的是，outer join即使在没有找到记录也会输出一条记录，对于缺失的记录使用null作为value。</li>
<li><strong>Left join</strong>：和outer join类似，不过对于KStream-KSteram的join（KStream left join KStream），它总是在主要流（A left join B，则A是主要的流）的记录到达时驱动的；对于KTable-KTable的join（KTable left join KTable），它是由两个流一起驱动，并且结果和left join左边的流是一致的，只允许右边流的记录缺失；对于KStream-KTable的left join（KStream left join KTable），一条KStream的记录只会和一条KTable的记录join，并且这条KTable的记录必须要在KStream的记录之前到达（当然必须在KTable中），否则join结果为null。</li>
</ol>
<p>Since stream joins are performed over the keys of records, it is required that joining streams are co-partitioned by key, i.e., their corresponding Kafka topics must have the same number of partitions and partitioned on the same key so that records with the same keys are delivered to the same processing thread. This is validated by Kafka Streams library at runtime (we talked about the threading model and data parallelism with more details in the Architecture section).</p>
<p>由于流的join是在记录的key上执行的，这就要求参与join的流能够按照key协同分区。比如它们（流）对应的Kafka主题必须有相等数量的分区，而且在相同的key上进行分区。这样相同key的记录会被发送到相同的处理线程。上面的限制会在Kafka Streams库中在运行时进行验证。</p>
<p>Join示例，使用Java8：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Key is user, value is number of clicks by that user</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, Long&gt; userClicksStream =  <span class="attribute">...</span>;</span><br><span class="line"><span class="comment">// Key is user, value is the geo-region of that user</span></span><br><span class="line">KTable&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; userRegionsTable = <span class="attribute">...</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// KStream-KTable join</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, RegionWithClicks&gt; userClicksWithRegion = userClicksStream</span><br><span class="line">    <span class="comment">// Null values possible: In general, null values are possible for region (i.e. the value of</span></span><br><span class="line">    <span class="comment">// the KTable we are joining against) so we must guard against that (here: by setting the</span></span><br><span class="line">    <span class="comment">// fallback region "UNKNOWN").</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Also, we need to return a tuple of (region, clicks) for each user.  But because Java does</span></span><br><span class="line">    <span class="comment">// not support tuples out-of-the-box, we must use a custom class `RegionWithClicks` to</span></span><br><span class="line">    <span class="comment">// achieve the same effect.  This class two fields -- the region (String) and the number of</span></span><br><span class="line">    <span class="comment">// clicks (Long) for that region -- as well as a matching constructor, which we use here.</span></span><br><span class="line">    <span class="built_in">.</span>leftJoin(userRegionsTable,</span><br><span class="line">      (clicks, region) <span class="subst">-&gt; </span><span class="literal">new</span> RegionWithClicks(region == <span class="built_in">null</span> ? <span class="string">"UNKNOWN"</span> : region, clicks));</span><br></pre></td></tr></table></figure>
<p>Join示例，使用Java7：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Key is user, value is number of clicks by that user</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, Long&gt; userClicksStream =  <span class="attribute">...</span>;</span><br><span class="line"><span class="comment">// Key is user, value is the geo-region of that user</span></span><br><span class="line">KTable&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; userRegionsTable = <span class="attribute">...</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// KStream-KTable join</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, RegionWithClicks&gt; userClicksWithRegion = userClicksStream</span><br><span class="line">    <span class="comment">// Null values possible: In general, null values are possible for region (i.e. the value of</span></span><br><span class="line">    <span class="comment">// the KTable we are joining against) so we must guard against that (here: by setting the</span></span><br><span class="line">    <span class="comment">// fallback region "UNKNOWN").</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Also, we need to return a tuple of (region, clicks) for each user.  But because Java does</span></span><br><span class="line">    <span class="comment">// not support tuples out-of-the-box, we must use a custom class `RegionWithClicks` to</span></span><br><span class="line">    <span class="comment">// achieve the same effect.  This class two fields -- the region (String) and the number of</span></span><br><span class="line">    <span class="comment">// clicks (Long) for that region -- as well as a matching constructor, which we use here.</span></span><br><span class="line">    <span class="built_in">.</span>leftJoin(userRegionsTable, <span class="literal">new</span> ValueJoiner&lt;Long, <span class="built_in">String</span>, RegionWithClicks&gt;() &#123;</span><br><span class="line">      @Override</span><br><span class="line">      <span class="keyword">public</span> RegionWithClicks apply(Long clicks, <span class="built_in">String</span> region) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">new</span> RegionWithClicks(region == <span class="built_in">null</span> ? <span class="string">"UNKNOWN"</span> : region, clicks);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>
<h4 id="APPLYING_A_CUSTOM_PROCESSOR">APPLYING A CUSTOM PROCESSOR</h4><p>Beyond the provided transformation operators, users can also specify any customized processing logic on their stream data via the KStream#process() method, which takes an implementation of the ProcessorSupplier interface as its parameter. This is essentially equivalent to the addProcessor() method in the Processor API.</p>
<p>The following example shows how to leverage, via the process() method, a custom processor that sends an email notification whenever a page view count reaches a predefined threshold.</p>
<p>除了Kafka Streams提供的转换操作（DSL），用户可以在流数据中通过KStream#process()方法指定定制的处理逻辑：将ProcessorSupplier接口的实现作为参数，这和Processor API的addProcessor()方法是等价的。下面的示例展示了通过process()方法如何使用自定义处理器，这个处理器会在当页面浏览计数到达指定的阈值时发送一个邮件通知。</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Send an email notification when the view count of a page reaches one thousand. </span></span><br><span class="line"><span class="comment">// JAVA8</span></span><br><span class="line">pageViews.countByKey(<span class="string">"PageViewCounts"</span>)</span><br><span class="line">         .filter((PageId pageId, <span class="keyword">Long</span> viewCount) -&gt; viewCount == <span class="number">1000</span>)</span><br><span class="line">         <span class="comment">// PopularPageEmailAlert is your custom processor that implements the</span></span><br><span class="line">         <span class="comment">// `Processor` interface, see further down below.</span></span><br><span class="line">         .process(() -&gt; <span class="keyword">new</span> PopularPageEmailAlert(<span class="string">"alerts@yourcompany.com"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// JAVA7</span></span><br><span class="line">pageViews.countByKey(<span class="string">"PageViewCounts"</span>)</span><br><span class="line">         .filter(</span><br><span class="line">            <span class="keyword">new</span> Predicate&lt;PageId, <span class="keyword">Long</span>&gt;() &#123;</span><br><span class="line">              <span class="keyword">public</span> <span class="keyword">boolean</span> test(PageId pageId, <span class="keyword">Long</span> viewCount) &#123;</span><br><span class="line">                <span class="keyword">return</span> viewCount == <span class="number">1000</span>;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">         .process(</span><br><span class="line">           <span class="keyword">new</span> ProcessorSupplier&lt;PageId, <span class="keyword">Long</span>&gt;() &#123;</span><br><span class="line">             <span class="keyword">public</span> Processor&lt;PageId, <span class="keyword">Long</span>&gt; get() &#123;</span><br><span class="line">               <span class="comment">// PopularPageEmailAlert is your custom processor that implements</span></span><br><span class="line">               <span class="comment">// the `Processor` interface, see further down below.</span></span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">new</span> PopularPageEmailAlert(<span class="string">"alerts@yourcompany.com"</span>);</span><br><span class="line">             &#125;</span><br><span class="line">           &#125;);</span><br></pre></td></tr></table></figure>
<p>上面的示例中PopularPageEmailAlert是一个自定义实现了Processor接口的流处理算子：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A processor that sends an alert message about a popular page to a configurable email address</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PopularPageEmailAlert</span> <span class="keyword">implements</span> <span class="title">Processor</span>&lt;<span class="title">PageId</span>, <span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> String emailAddress;</span><br><span class="line">  <span class="keyword">private</span> ProcessorContext;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">PopularPageEmailAlert</span><span class="params">(String emailAddress)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.emailAddress = emailAddress;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessorContext context)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.context = context;</span><br><span class="line">    <span class="comment">// Here you would perform any additional initializations</span></span><br><span class="line">    <span class="comment">// such as setting up an email client.</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">(PageId pageId, Long count)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Here would format and send the alert email.</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// In this specific example, you would be able to include information</span></span><br><span class="line">    <span class="comment">// about the page's ID and its view count (because the class implements</span></span><br><span class="line">    <span class="comment">// `Processor&lt;PageId, Long&gt;`).</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">punctuate</span><span class="params">(<span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Stays empty.  In this use case there would be no need for a periodical</span></span><br><span class="line">    <span class="comment">// action of this processor.</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Any code for clean up would go here.</span></span><br><span class="line">    <span class="comment">// This processor instance will not be used again after this call.</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>就像前面提到的，一个流处理算子可以通过调用ProcessorContext#getStateStore()方法访问任何可用的状态存储。可用指的是：这些状态存储的名称在调用KStream#process()方法时指定（注意和Processor#process()方法不同，KStream#process()是在构造拓扑时定义）。</p>
<h4 id="WRITING_STREAMS_BACK_TO_KAFKA">WRITING STREAMS BACK TO KAFKA</h4><p>Any streams may be (continuously) written back to a Kafka topic via KStream#to() and KTable#to().</p>
<p>任何的流都可能会通过KStream#to()和KTable#to()方法持续地（将记录）写到Kafka主题中。</p>
<figure class="highlight fsharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Write the stream userCountByRegion to the output topic 'RegionCountsTopic'</span></span><br><span class="line">userCountByRegion.<span class="keyword">to</span>(<span class="string">"RegionCountsTopic"</span>);</span><br></pre></td></tr></table></figure>
<p>Best practice: It is strongly recommended to manually create output topics ahead of time rather than relying on auto-creation of topics. First, auto-creation of topics may be disabled in your Kafka cluster. Second, auto-creation will always apply the default topic settings such as the replicaton factor, and these default settings might not be what you want for certain output topics (cf. auto.create.topics.enable=true in the Kafka broker configuration).</p>
<blockquote>
<p>最佳实践：推荐手动创建输出主题而不是依赖于自动创建。首先自动创建主题可能会被你的Kafka集群禁用掉；其二，自动创建会运用一些默认的设置，比如副本因子，而这些默认的设置可能在某些输出主题上不是你想要的。</p>
</blockquote>
<p>If your application needs to continue reading and processing the records after they have been written to a topic via to() above, one option is to construct a new stream that reads from the output topic:</p>
<p>如果你的应用程序需要持续读取并处理的记录是通过to()方法写到的输出主题，一种方式是从输出主题中构造新的流：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Write to a Kafka topic.</span></span><br><span class="line">userCountByRegion<span class="built_in">.</span><span class="keyword">to</span>(<span class="string">"RegionCountsTopic"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read from the same Kafka topic by constructing a new stream from the</span></span><br><span class="line"><span class="comment">// topic RegionCountsTopic, and then begin processing it (here: via `map`)</span></span><br><span class="line">builder<span class="built_in">.</span>stream(<span class="string">"RegionCountsTopic"</span>)<span class="built_in">.</span><span class="built_in">map</span>(<span class="attribute">...</span>)<span class="attribute">...</span>;</span><br></pre></td></tr></table></figure>
<p>Kafka Streams provides a convenience method called through() that is equivalent to the code above:</p>
<p>Kafka Streams还提供了一种方便的方式：调用through()方法和上面的代码是类似的：</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// <span class="string">`through`</span> combines <span class="keyword">write</span>-to-Kafka-topic <span class="keyword">and</span> <span class="keyword">read</span>-from-same-Kafka-topic operations</span><br><span class="line">userCountByRegion.through(<span class="string">"RegionCountsTopic"</span>).<span class="keyword">map</span>(...)...;</span><br></pre></td></tr></table></figure>
<p>Whenever data is read from or written to a Kafka topic, Streams must know the serdes to be used for the respective data records. By default the to() and through() methods use the default serdes defined in the Streams configuration. You can override these default serdes by passing explicit serdes to the to() and through() methods.</p>
<p>当数据从一个Kafka主题读取或者写入时，流必须知道记录使用的序列化方式，默认to()和through()方法使用了Streams配置中默认的序列化方式。你可以通过传递明确的序列化器给to()和through()方法来覆写这些默认的配置。</p>
<p>Besides writing the data back to Kafka, users can also apply a custom processor as mentioned above to write to any other external stores, for example, to materialize a data store, as stream sinks at the end of the processing.</p>
<p>除了将数据写回到Kafka，用户也可以像上面提到的那样运用一个自定义的处理器，并写到其他的外部存储介质中，比如物化数据存储，作为流处理的目标。</p>
<h2 id="运行流处理程序">运行流处理程序</h2><p>A Java application that uses the Kafka Streams library can be run just like any other Java application – there is no special magic or requirement on the side of Kafka Streams.</p>
<p>一个使用Kafka Streams客户端库的Java应用程序，它的运行方式和其他普通的Java应用程序一样，没有特殊的魔法，也没有额外的限制。比如你可以将你的Java应用程序打成一个fat jar包，然后启动：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># <span class="operator"><span class="keyword">Start</span> the application <span class="keyword">in</span> <span class="keyword">class</span> <span class="string">`com.example.MyStreamsApp`</span></span><br><span class="line"># <span class="keyword">from</span> the fat jar named <span class="string">`path-to-app-fatjar.jar`</span>.</span><br><span class="line">$ <span class="keyword">java</span> -cp <span class="keyword">path</span>-<span class="keyword">to</span>-app-fatjar.jar com.example.MyStreamsApp</span></span><br></pre></td></tr></table></figure>
<p>It is important to understand that, when starting your application as described above, you are actually launching what Kafka Streams considers to be one instance of your application. More than one instance of your application may be running at a time, and in fact the common scenario is that there are indeed multiple instances of your application running in parallel. See Parallelism Model for further information.</p>
<p>当启动应用程序时，在Kafka Streams看来，是启动了应用程序的一个实例。可能在同一个时间点，你的应用程序会同时运行多个实例，而实际上通常的场景的确是你的多个应用程序实例是并行执行的。</p>
<h3 id="水平扩展你的应用程序">水平扩展你的应用程序</h3><p>Kafka Streams makes your stream processing applications elastic and scalable: you can add and remove processing capacity dynamically during the runtime of your application, and you can do so without any downtime or data loss. This means that, unlike other stream processing technologies, with Kafka Streams you do not have to completely stop your application, recompile/reconfigure, and then restart. This is great not just for intentionally(故意地) adding or removing processing capacity, but also for being resilient(有弹性的) in the face of failures (e.g. machine crashes, network outages) and for allowing maintenance work (e.g. rolling upgrades).</p>
<p>Kafka Streams会让你的流处理应用程序可伸缩和可扩展。你可以在应用程序的运行时动态地添加或删除流处理能力，并且不需要停机维护，也不会丢失数据。这意味着，和其他流处理技术不同，使用Kafka Streams你不需要完全地停止你的应用程序，重新编译，重新配置，然后重启。对于故意地添加或删除处理能力，或者失败时的弹性机制，以及维护工作都是有好处的。</p>
<p>If you are wondering how this elasticity is actually achieved behind the scenes, you may want to read the Architecture chapter, notably the Parallelism Model section. In a nutshell(概括), Kafka Streams leverages existing functionality in Kafka, notably its group management functionality. This group management, which is built right into the Kafka wire protocol, is the foundation that enables the elasticity of Kafka Streams applications: members of a group will coordinate and collaborate(合作) jointly(共同地) on the consumption and processing of data in Kafka. On top of this foundation Kafka Streams provides some additional functionality, e.g. to enable stateful processing and to allow for fault-tolerante state in environment where application instances may come and go at any time.</p>
<p>如果你对如何实现扩展能力的背后机制感兴趣，可以阅读架构章节，尤其是并行模型那一部分。简单来说，Kafka Streams利用了Kafka已有的特性，尤其是组管理协议的功能。组管理协议构建在Kafka的协议之上，是Kafka Streams应用程序具有可伸缩性的基础：组的成员会协调合作，共同消费和处理Kafka中的数据。基于这些基础，Kafka Streams还提供了额外的功能，比如有状态的处理，以及在应用程序实例随时添加和删除的环境中，允许故障容错的状态存储。</p>
<h3 id="增加处理能力（Expand）">增加处理能力（Expand）</h3><p>If you need more processing capacity for your stream processing application, you can simply start another instance of your stream processing application, e.g. on another machine, in order to scale out. The instances of your application will become aware of each other and automatically begin to share the processing work. More specifically, what will be handed over from the existing instances to the new instances is (some of) the stream tasks that have been run by the existing instances. Moving stream tasks from one instance to another results in moving the processing work plus any internal state of these stream tasks (the state of a stream task will be re-created in the target instance by restoring the state from its corresponding changelog topic).</p>
<p>如果你需要为流处理应用程序添加更多的处理能力，你只需要在其他机器上简单地启动新的流处理应用程序实例，来达到扩展的目的。应用程序的所有示例都会彼此感知，并且自动地开始共享处理工作。更具体地说，从已有的实例移交给新实例的工作是在已有实例上运行的流任务。将流任务从一个实例移动到另一个实例的结果是，将这些流任务的处理工作以及内部状态都一起移动（一个流任务的状态会在目标实例中重新创建，从对应的变更日志主题中恢复数据来构造状态）。</p>
<p>The various instances of your application each run in their own JVM process, which means that each instance can leverage all the processing capacity that is available to their respective JVM process (minus the capacity that any non-Kafka-Streams part of your application may be using). This explains why running additional instances will grant your application additional processing capacity. The exact capacity you will be adding by running a new instance depends of course on the environment in which the new instance runs: available CPU cores, available main memory and Java heap space, local storage, network bandwidth, and so on. Similarly, if you stop any of the running instances of your application, then you are removing and freeing up the respective processing capacity.</p>
<p>应用程序的不同实例运行在它们各自的JVM进程中，这意味着每个实例可以利用它们对应的JVM进程的所有处理能力/资源（减去应用程序中不是Kafka Streams部分使用的资源）。这就解释了为什么运行额外的实例可以提升应用程序的处理能力。不过运行一个新实例期望新增加的处理能力当然会和新实例所在的环境有关：比如可用的CPU核，可用的主内存和Java堆空间大小，本地存储，网络带宽等等。同样，如果你停止了运行中的任意一个实例，你就删除并且释放掉相应的处理能力。</p>
<p><img src="http://img.blog.csdn.net/20161105125241586" alt="kstream expand"></p>
<p>Before adding capacity: only a single instance of your Kafka Streams application is running. At this point the corresponding Kafka consumer group of your application contains only a single member (this instance). All data is being read and processed by this single instance.</p>
<p>After adding capacity: now two additional instances of your Kafka Streams application are running, and they have automatically joined the application’s Kafka consumer group for a total of three current members. These three instances are automatically splitting the processing work between each other. The splitting is based on the Kafka topic partitions from which data is being read.</p>
<p>上图左边是添加处理能力之前，只有一个Kafka Streams应用程序实例在运行，这时你的应用程序对应的Kafka消费组只有一个成员（就是这个实例），所有的数据都通过这个唯一的实例读取并处理。右图是添加处理能力之后，现在增加了两个额外的应用程序运行实例，而且它们自动加入到应用程序对应的Kafka消费组中，这个消费组目前总共有3个成员。这三个实例两两之间都会自动地均摊处理工作。分摊是基于Kafka的主题分区，即（每个实例）从不同分区读取不同的数据。</p>
<h3 id="减少处理能力（Shrink）">减少处理能力（Shrink）</h3><p>If you need less processing capacity for your stream processing application, you can simply stop one or more running instances of your stream processing application, e.g. shut down 2 of 4 running instances. The remaining instances of your application will become aware that other instances were stopped and automatically take over the processing work of the stopped instances. More specifically, what will be handed over from the stopped instances to the remaining instances is the stream tasks that were run by the stopped instances. Moving stream tasks from one instance to another results in moving the processing work plus any internal state of these stream tasks (the state of a stream task will be re-created in the target instance by restoring the state from its corresponding changelog topic).</p>
<p>如果你的流处理应用程序需要较少的处理能力，你只需要停止一个或多个运行的流处理应用程序即可，比如将4个运行的实例关闭掉2个。剩余的应用程序实例会感知到其他实例已经被停止了，并且会自动接管这些停掉示例的处理工作。更具体的来说，从停止实例移交给剩余实例的工作是在停止实例上运行的流任务。将流任务从一个实例移动到另一个实例的结果是，将这些流任务的处理工作以及内部状态都一起移动。</p>
<p><img src="http://img.blog.csdn.net/20161105125255977" alt="kstream shrink"></p>
<p>If one of the application instances is stopped (e.g. intentional reduction of capacity, maintenance, machine failure), it will automatically leave the application’s consumer group, which causes the remaining instances to automatically take over the stopped instance’s processing work.</p>
<p>图中如果停止（故意减少容量，维护，或者机器故障都可能停止）了一个应用程序实例，它就会自动离开应用程序的消费组，并导致剩余的示例自动接管这个停止实例的处理工作。</p>
<h3 id="运行多少个应用程序实例">运行多少个应用程序实例</h3><p>How many instances can or should you run for your application? Is there an upper limit for the number of instances and, similarly, for the parallelism of your application? In a nutshell, the parallelism of a Kafka Streams application – similar to the parallelism of Kafka – is primarily determined by the number of partitions of the input topic(s) from which your application is reading. For example, if your application reads from a single topic that has 10 partitions, then you can run up to 10 instances of your applications (note that you can run further instances but these will be idle).</p>
<p>The number of topic partitions is the upper limit for the parallelism of your Kafka Streams application and thus for the number of running instances of your application.</p>
<p>那么到底可以或者应该运行多少个应用程序实例？是否有一个数量的上限，来并行化你的应用程序？简单来说，一个Kafka Streams应用程序的并行度，类似于Kafka的并行度，主要的决定因素是：应用程序所读取的输入主题的分区数量。比如你的应用程序读取的一个主题有10个分区，那么你就可以运行最多10个应用程序实例（虽然你可以运行更多的实例，但是会有一些实例是空闲的）。所以，主题分区的数量是你的流处理应用程序并行度的上限，因此也是你的应用程序运行实例的上限。</p>
<p>How to achieve(获得) a balanced processing workload across application instances to prevent processing hotpots: The balance of the processing work between application instances depends on factors such as how well data messages are balanced between partitions (think: if you have 2 topic partitions, having 1 million messages in each partition is better than having 2 million messages in the first partition and no messages in the second) and how much processing capacity is required to process the messages (think: if the time to process messages varies heavily, then it is better to spread the processing-intensive messages across partitions rather than storing these messages within the same partition).</p>
<p>那么怎么在应用程序实例之前来保证处理的负载是平衡的，防止发生处理热点。工作负载是否平衡的决定因素是分区的数据有多平衡（比如你有两个分区，每个分区有一百万条消息要比一个分区有两百万条消息，而另外一个分区没有一条消息要好的多），以及消息的处理能力（比如处理消息的时间变化很大，那么将处理比较耗时的消息分散在多个分区，要比这些消息都存储在一个分区也要好得多）。</p>
<p>If your data happens to be heavily skewed(倾斜) in the way described above, some application instances may become processing hotspots (say, when most messages would end up being stored in only 1 of 10 partitions, then the application instance that is processing this one partition would be performing most of the work while other instances might be idle). You can minimize the likelihood of such hotspots by ensuring better data balancing across partitions (i.e. minimizing data skew at the point in time when data is being written to the input topics in Kafka) and by over-partitioning the input topics (think: use 50 or 100 partitions instead of just 10), which lowers the probability that a small subset of partitions will end up storing most of the topic’s data.</p>
<p>如果你的数据恰巧倾斜的很严重，有一些应用程序实例就会变成处理热点。你可以通过确保数据在分区之间有更好的平衡来最小化出现这种热点的可能性（比如当大部分的消息都只存储在10个分区中的一个时，那么处理这个分区的应用程序实例就会处理大部分的工作，而其他实例则可能很空闲），或者对输入主题采用更多的分区数（比如在数据写入到Kafka的输入主题是就尽量最小化数据的倾斜）来减少一个很小的分区字节存储了大部分主题数据的这种可能性。</p>
<h2 id="数据类型和序列化">数据类型和序列化</h2><h2 id="应用重置工具">应用重置工具</h2><p>The Application Reset Tool allows you to quickly reset an application in order to reprocess its data from scratch – think: an application “reset” button. Scenarios when would you like to reset an application include:</p>
<p>应用程序重置工具允许你快速地重置一个应用程序，然后重新处理数据，可以认为是应用程序的一个“重置”按钮。需要重置应用程序的场景包括：</p>
<ul>
<li>Development and testing 开发和测试时</li>
<li>Addressing bugs in production 在生产环境定位问题时</li>
<li>Demos 演示</li>
</ul>
<p>However, resetting an application manually is a bit involved. Kafka Streams is designed to hide many details about operator state, fault-tolerance, and internal topic management from the user (especially when using Kafka Streams DSL). In general this hiding of details is very desirable(值得要的，令人满意的) but it also makes manually resetting an application more difficult. The Application Reset Tool fills this gap and allows you to quickly reset an application.</p>
<p>不过，手动方式重置一个应用程序需要做很多工作。Kafka Streams提供了一个重置工具，帮你隐藏了很多细节，比如操作状态，故障容错，内部的主题管理。</p>
<p><strong>用户主题和内部主题</strong>  </p>
<p>在Kafka Streams中，我们会区分用户主题和内部主题，这两种都是普通的Kafka主题，不过对于内部主题，有一些特定的命名约定。</p>
<p>用户主题包括输入主题、输出主题、临时主题。这些主题是用户创建或管理的，包括应用程序的输入和输出主题，以及通过through()方法指定的临时主题，临时主题实际上同时既是输出也是输入主题。</p>
<p>内部主题是由Kafka Streams底层自动创建的。比如针对状态存储的变更日志主题就是一个内部主题。内部主题的命名约定是：<application.id>-<operatorname>-<suffix>。</suffix></operatorname></application.id></p>
<p>应用程序重置工具做的工作有：  </p>
<ol>
<li>对输入主题：重置应用程序所有分区的消费者提交偏移量到0</li>
<li>对临时主题：跳到主题的最后，比如设置应用程序的消费者提交偏移量到每个分区的logSize（实际上就是分区的最后位置）</li>
<li>对内部主题：除了重置偏移量到0，还要删除内部主题</li>
</ol>
<p>应用程序重置工具不做的：</p>
<ol>
<li>不会重置应用程序的输出主题。如果任何的输出主题或者临时主题被下游的应用程序消费，那么调整这些下游应用程序是你自己的责任</li>
<li>不会重置应用程序实例的本地环境。同样删除应用程序实例运行所在机器的本地状态，也是你自己的责任。</li>
</ol>
<p><strong>步骤1：运行重置工具</strong>  </p>
<p>执行<code>bin/kafka-streams-application-reset</code>命令，需要指定如下的参数：</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Option (* = required)         Description</span><br><span class="line">-<span class="ruby">--------------------         -----------</span><br><span class="line"></span>* --application-id &lt;id&gt;       The Kafka Streams application ID (application.id)</span><br><span class="line">-<span class="ruby">-bootstrap-servers &lt;urls&gt;    <span class="constant">Comma</span>-separated list of broker urls with <span class="symbol">format:</span> <span class="constant">HOST1</span><span class="symbol">:PORT1</span>,<span class="constant">HOST2</span><span class="symbol">:PORT2</span></span><br><span class="line"></span>                                (default: localhost:9092)</span><br><span class="line">-<span class="ruby">-intermediate-topics &lt;list&gt;  <span class="constant">Comma</span>-separated list of intermediate user topics</span><br><span class="line"></span>-<span class="ruby">-input-topics &lt;list&gt;         <span class="constant">Comma</span>-separated list of user input topics</span><br><span class="line"></span>-<span class="ruby">-zookeeper &lt;url&gt;             <span class="constant">Format</span><span class="symbol">:</span> <span class="constant">HOST</span><span class="symbol">:POST</span></span><br><span class="line"></span>                                (default: localhost:2181)</span><br></pre></td></tr></table></figure>
<p>You can combine the parameters of the script as needed. For example, if an application should only restart from an empty internal state but not reprocess previous data, simply omit the parameters –input-topics and –intermediate-topics.</p>
<p>你可以任意组合上面的参数，比如如果应用程序只会从空的内部状态重启，不会重新处理已有的数据，可以忽略输入主题和临时主题这两个参数。</p>
<p>On intermediate topics: In general, we recommend to manually delete and re-create any intermediate topics before running the application reset tool. This allows to free disk space in Kafka brokers early on. It is important to first delete and re-create intermediate topics before running the application reset tool.</p>
<p>Not deleting intermediate topics and only using the application reset tool is preferable:</p>
<ul>
<li>when there are external downstream consumers for the application’s intermediate topics</li>
<li>during development, where manually deleting and re-creating intermediate topics might be cumbersome and often unnecessary</li>
</ul>
<p>关于临时主题：通常我们推荐在运行重置工具之前，手动删除然后重建临时主题。这样可以尽早释放Kafka的集群磁盘空间。什么时候不需要删除临时主题：</p>
<ol>
<li>存在外部的下游消费者订阅了应用程序的临时主题</li>
<li>在开发环境，手动删除并重建主题可能很繁琐，而且通常没有必要这么做</li>
</ol>
<p><strong>步骤2：重置本地环境</strong>  </p>
<p>Running the application reset tool (step 1) ensures that your application’s state – as tracked globally in the application’s configured Kafka cluster – is reset. However, by design the reset tool does not modify or reset the local environment of your application instances, which includes the application’s local state directory.</p>
<p>For a complete application reset you must also delete the application’s local state directory on any machines on which an application instance was run prior to restarting an application instance on the same machines. You can either use the API method KafkaStreams#cleanUp() in your application code or manually delete the corresponding local state directory (default location: /var/lib/kafka-streams/<application.id>, cf. state.dir configuration parameter).</application.id></p>
<p>运行步骤1的应用程序重置工具确保你的应用程序状态被重置（应用程序的状态实际上是在应用程序配置的Kafka集群被全局地跟踪）。不过，重置工具被设计的时候，并不会修改或重置应用程序实例的本地环境，包括应用程序的本地状态目录。</p>
<p>如果要彻底重置应用程序，你必须删除应用程序的本地状态目录，而且任何之前运行过的应用程序实例所在的机器都需要在重启应用程序实例之前删除干净。你可以在应用程序中调用KafkaStreams#cleanUp()方法，或者手动删除对应的本地状态目录（默认的路径是： /var/lib/kafka-streams/<application.id>，即state.dir的配置参数）来清理。</application.id></p>
<p><strong>示例</strong>  </p>
<p>Let’s imagine you are developing and testing an application locally and want to iteratively improve your application via run-reset-modify cycles. You might have code such as the following:</p>
<p>假设你在本地开发和测试一个应用程序，并且通过运行-重置-修改的循环开发模式来不断迭代提升你的应用程序，你可能需要这样的代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> ResetDemo &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> throws Exception </span>&#123;</span><br><span class="line">    <span class="comment">// Kafka Streams configuration</span></span><br><span class="line">    Properties streamsConfiguration = <span class="keyword">new</span> Properties();</span><br><span class="line">    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"my-streams-app"</span>);</span><br><span class="line">    <span class="comment">// ...and so on...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Define the processing topology</span></span><br><span class="line">    KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line">    builder.stream(<span class="string">"my-input-topic"</span>)</span><br><span class="line">        .selectKey(...)</span><br><span class="line">        .through(<span class="string">"rekeyed-topic"</span>)</span><br><span class="line">        .countByKey(<span class="string">"global-count"</span>)</span><br><span class="line">        .to(<span class="string">"my-output-topic"</span>);</span><br><span class="line"></span><br><span class="line">    KafkaStreams app = <span class="keyword">new</span> KafkaStreams(builder, streamsConfiguration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Delete the application's local state.</span></span><br><span class="line">    <span class="comment">// <span class="doctag">Note:</span> In real application you'd call `cleanUp()` only under</span></span><br><span class="line">    <span class="comment">// certain conditions.  See tip on `cleanUp()` below.</span></span><br><span class="line">    app.cleanUp();</span><br><span class="line"></span><br><span class="line">    app.start();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// <span class="doctag">Note:</span> In real applications you would register a shutdown hook</span></span><br><span class="line">    <span class="comment">// that would trigger the call to `app.close()` rather than</span></span><br><span class="line">    <span class="comment">// using the sleep-then-close example we show here.</span></span><br><span class="line">    Thread.sleep(<span class="number">30</span> * <span class="number">1000L</span>);</span><br><span class="line">    app.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Calling cleanUp() is safe but do so judiciously(明智的): It is always safe to call KafkaStreams#cleanUp() because the local state of an application instance can be recovered from the underlying internal changelog topic(s). However, to avoid the corresponding recovery overhead it is recommended to not call cleanUp() unconditionally and every time an application instance is restarted/resumed. A production application would therefore use e.g. command line arguments to enable/disable the cleanUp() call as needed.</p>
<p>调用cleanUp()方法是安全的，不过要谨慎调用：调用KafkaStreams#cleanUp()总是安全的，因为应用程序实例的本队状态可以从底层的内部变更日志主题恢复过来。不过，为了防止由此产生的恢复开销，推荐不要盲目调用cleanUp()，或者说在每次应用程序重启/恢复的时候就调用cleanUp()。生产环境下的应用程序因此会使用命令行的参数来允许或禁止调用cleanUp()。</p>
</blockquote>
<p>然后你就可以执行如下的“run-reset-modify”循环：</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run your application</span></span><br><span class="line">$ bin/kafka-<span class="command">run</span>-<span class="type">class</span> io.confluent.examples.streams.ResetDemo</span><br><span class="line"></span><br><span class="line"><span class="comment"># After stopping all application instances, reset the application</span></span><br><span class="line">$ bin/kafka-streams-<span class="type">application</span>-reset <span class="comment">--application-id my-streams-app \</span></span><br><span class="line">                                      <span class="comment">--input-topics my-input-topic \</span></span><br><span class="line">                                      <span class="comment">--intermediate-topics rekeyed-topic</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Now you can modify/recompile as needed and then re-run the application again.</span></span><br><span class="line"><span class="comment"># You can also experiment, for example, with different input data without</span></span><br><span class="line"><span class="comment"># modifying the application.</span></span><br></pre></td></tr></table></figure>
<p>EOF. 翻译完毕 @2016.11.5</p>
<h2 id="重置流处理应用程序">重置流处理应用程序</h2><p>Confluent关于重置的实现翻译：<a href="https://www.confluent.io/blog/data-reprocessing-with-kafka-streams-resetting-a-streams-application/" target="_blank" rel="external">https://www.confluent.io/blog/data-reprocessing-with-kafka-streams-resetting-a-streams-application/</a>  </p>
<h3 id="示例">示例</h3><p>As a running example we assume you have the following Kafka Streams application, and we subsequently demonstrate (1) how to make this application “reset ready” and (2) how to perform an actual application reset.</p>
<p>下面的Kafka Streams应用程序，我们会展示两个步骤：1）怎么让应用程序开始准备重置，2）怎么执行真正的应用程序重置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ResetDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// Kafka Streams configuration</span></span><br><span class="line">        <span class="keyword">final</span> Properties streamsConfiguration = <span class="keyword">new</span> Properties();</span><br><span class="line">        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"my-streams-app"</span>);</span><br><span class="line">        <span class="comment">// make sure to consume the complete topic via "auto.offset.reset = earliest"</span></span><br><span class="line">        streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line">        <span class="comment">// ...and so on...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// define the processing topology</span></span><br><span class="line">        <span class="keyword">final</span> KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line">        builder.stream(<span class="string">"my-input-topic"</span>)</span><br><span class="line">            .selectKey(...)</span><br><span class="line">            .through(<span class="string">"rekeyed-topic"</span>)</span><br><span class="line">            .countByKey(<span class="string">"global-count"</span>)</span><br><span class="line">            .to(<span class="string">"my-output-topic"</span>);</span><br><span class="line"></span><br><span class="line">         <span class="comment">// ...run application...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>This application reads data from the input topic “my-input-topic”, then selects a new record key, and then writes the result into the intermediate topic “rekeyed-topic” for the purpose of data re-partitioning. Subsequently, the re-partitioned data is aggregated by a count operator, and the final result is written to the output topic “my-output-topic”.  Note that in this blog post we don’t put the focus on what this topology is actually doing — the point is to have a running example of a typical topology that has input topics, intermediate topics, and output topics.</p>
<p>这个应用程序会从输入主题“my-input-topic”读取数据，然后选择新的记录key，将结果写到临时的主题“rekeyed-topic”，目的是将数据重新分区。随后，重新分区的数据会通过count算子被聚合，最终的结果写到输出主题“my-output-topic”。本篇博客我们不会将重点放到拓扑是怎么工作的，重点是典型的拓扑，会有输入主题，临时主题，输出主题。</p>
<h4 id="Step_1:_Prepare_your_application_for_resets">Step 1: Prepare your application for resets</h4><p>The first step is to make your application “reset ready”. For this, the only thing you need to do is to include a call to KafkaStreams#cleanUp() in your application code (for details about cleanUp() see Section ”Application Reset Tool Details”).</p>
<p>Calling cleanUp() is required because resetting a Streams applications consists of two parts: global reset and local reset. The global reset is covered by the new application reset tool (see “Step 2”), and the local reset is performed through the Kafka Streams API. Because it is a local reset, it must be performed locally for each instance of your application. Thus, embedding it in your application code is the most convenient way for a developer to perform a local reset of an application (instance).</p>
<p>步骤1是让你的应用程序准备好重置，你需要做的是在你的应用程序代码包含对KafkaStreams#cleanUp()方法的调用。调用cleanUp()是必要的，因为重置一个流应用程序包括两个部分：全局的重置和本地的重置。全局的重置会通过新的应用程序重置工具完成，本地的重置是通过Kafka Streams的API完成的。因为它（即调用Kafka Streams API）是一个本地的重置，你的应用程序的每个实例都应该本地地执行。因此将它嵌入到应用程序代码中，对于开发者而言这是本队重置一个应用程序实例的最方便做法。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ResetDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">         <span class="comment">// ...prepare your application configuration and processing topology...</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> KafkaStreams streams = <span class="keyword">new</span> KafkaStreams(builder, streamsConfiguration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Delete the application's local state.</span></span><br><span class="line">        <span class="comment">// <span class="doctag">Note:</span> In real application you'd call `cleanUp()` only under certain conditions.</span></span><br><span class="line">        <span class="comment">// See Confluent Docs for more details:</span></span><br><span class="line">        <span class="comment">// http://docs.confluent.io/3.0.1/streams/developer-guide.html#step-2-reset-the-local-environments-of-your-application-instances</span></span><br><span class="line">        streams.cleanUp();</span><br><span class="line"></span><br><span class="line">        streams.start();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Add shutdown hook to respond to SIGTERM and gracefully close Kafka Streams</span></span><br><span class="line">        Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="annotation">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                streams.close();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>At this point the application is ready for being reset (when needed), and you’d start one or multiple instances of your application as usual, possibly on different hosts.</p>
<p>现在应用程序已经准备好重置了，你启动一个或多个应用程序的方式跟平常一样，可能会在不同的节点启动多个实例。</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run your application</span></span><br><span class="line"><span class="comment"># For the sake of this short blog post we use the Kafka helper script</span></span><br><span class="line"><span class="comment"># `kafka-run-class` here, but your mileage may vary.</span></span><br><span class="line"><span class="comment"># You could also, for example, call `java …` directly. Remember that</span></span><br><span class="line"><span class="comment"># an application that uses the Kafka Streams library is a standard Java application.</span></span><br><span class="line"><span class="variable">$ </span>bin/kafka-run-<span class="class"><span class="keyword">class</span> <span class="title">io</span>.<span class="title">confluent</span>.<span class="title">examples</span>.<span class="title">streams</span>.<span class="title">ResetDemo</span></span></span><br></pre></td></tr></table></figure>
<h4 id="Step_2:_Reset_the_application">Step 2: Reset the application</h4><p>So what would we need to do to restart this application from scratch, i.e., not resume the processing from the point the application was stopped before, but rather to reprocess all its input data again?</p>
<p>First you must stop all running application instances and make sure the whole consumer group is not active anymore (you can use bin/kafka-consumer-groups to list active consumer groups). Typically, the consumer group should become inactive one minute after you stopped all the application instances. This is important because the reset behavior is undefined if you use the reset tool while some application instances are still running — the running instances might produce wrong results or even crash.</p>
<p>那么我们怎么从头开始重启这个应用程序呢，比如不是从应用程序上次停止的地方继续回复处理，而是重新处理所有的输入数据？</p>
<p>首先，你必须要停止所有正在运行的应用程序实例，确保整个消费组是不活动的（你可以使用kafka-consumer-groups来列出仍然存活的消费组）。通常，消费组会在你停止所有的应用程序实例之后的一分钟状态才为不活动。这非常重要，如果你使用了重置工具，但同时有一些应用程序仍然在运行，重置这个动作是不确定的，因为正在运行的示例可能会产生错误的结果，甚至挂掉（实际上就是说先停止所有的应用程序实例，然后检查消费组不活动，最后才可以使用重置工具）。</p>
<p>Once all application instances are stopped you can call the application reset tool as follows:</p>
<p>当所有的应用程序实例都停止后，你可以像下面那样使用应用程序重置工具：</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># After stopping all application instances, reset the application</span></span><br><span class="line">$ bin/kafka-streams-<span class="type">application</span>-reset <span class="comment">--application-id my-streams-app \</span></span><br><span class="line">                                      <span class="comment">--input-topics my-input-topic \</span></span><br><span class="line">                                      <span class="comment">--intermediate-topics rekeyed-topic \</span></span><br><span class="line">                                      <span class="comment">--bootstrap-servers brokerHost:9092 \</span></span><br><span class="line">                                      <span class="comment">--zookeeper zookeeperHost:2181</span></span><br></pre></td></tr></table></figure>
<p>As you can see, you only need to provide the application ID (“my-streams-app”) as specified in your application configuration, and the names of all input and intermediate topics. Furthermore, you might need to specify Kafka connection information (bootstrap servers) and Zookeeper connection information. Both parameters have default values localhost:9092 and localhost:2181, respectively. Those are convenient to use during development with a local single Zookeeper/single broker setup. For production/remote usage you need to provide appropriate host:port values.</p>
<p>可以看到，你需要提供一个应用程序编号（“my-streams-app”，这是在你的应用程序配置中指定的），以及所有的输入主题和临时主题的名称。此外，你可能还需要指定Kafka以及ZooKeeper的连接信息。</p>
<p>Once the application reset tool has completed its run (and its internal consumer is not active anymore), you can restart your application as usual, and it will now reprocess its input data from scratch again. We’re done!</p>
<p>一旦应用程序重置工具完成后，你可以像平常那样重启你的应用程序了，现在它就会从头开始重新处理输入数据了！</p>
<p>It’s important to highlight that, to prevent possible collateral(并行，附属) damage, the application reset tool does not reset the output topics of an application. If any output (or intermediate) topics are consumed by downstream applications, it is your responsibility to adjust those downstream applications as appropriate when you reset the upstream application.</p>
<p>很重要的一点是，为了防止可能的附属损坏，应用程序重置工具并不会重置输出主题。如果有任何的输出主题（或者临时主题）被下游的应用程序所消费，那么这就是你的责任来调整这些下游的应用程序了（当你重置上游的应用程序时）。</p>
<p>Use application reset tool with care and double-check its parameters: If you provide wrong parameter values (e.g. typos in application.id) or specify parameters inconsistently (e.g. specifying the wrong input topics for the application), this tool might invalidate the application’s state or even impact other applications, consumer groups, or Kafka topics of your Kafka cluster.</p>
<p>使用应用程序重置工具要很小心，并且要仔细检查它的参数：如果你提供了一个错误的参数值（比如应用程序编号写错了），或者指定了不一致的参数（比如指定了这个应用程序错误的输入主题），该工具可能会使得应用程序的状态失效，甚至影响其他的应用程序，消费组，或者你的Kafka集群的主题。</p>
<p>As we have shown above, using the new application reset tool you can easily reprocess data from scratch with Kafka Streams. Perhaps somewhat surprisingly, there’s actually a lot going on behind the scenes to make application resets work as easily.  The following sections are a deep dive on these internals for the curious reader.</p>
<p>使用新的应用程序重置工具，你可以使用Kafka Streams很容易地从头开始重新处理数据。你可能会觉得有点奇怪，不过实际上为了让应用程序重置工作的很简单，后台做了很多的工作。</p>
<h3 id="Behind_the_Scenes_of_Kafka_Streams">Behind the Scenes of Kafka Streams</h3><p>In this second part of the blog post we discuss those Kafka Streams internals that are required to understand the details of a proper application reset. Figure 1 shows a Kafka Streams application before its first run. The topology has as single input topic with two partitions. The current offset of each partition is zero (or there is no committed offsets and parameter auto.offset.reset = earliest is used). Also the topology writes into a single output topic with two partitions which are both empty. No offsets are shown as output topics are not consumed by the application itself. Furthermore, the topology contains a call to through(), thus, it writes/reads into/from an additional (intermediate) topic with two empty partitions and it contains a stateful operator.</p>
<p>下图展示了一个Kafka Streams应用程序第一次运行之前的状态。这个拓扑只有一个输入主题（两个分区）。当前每个分区的偏移量是0（或者说没有提交偏移量，而且使用了参数auto.offset.reset = earliest，表示没有偏移量或者偏移量超出时，会重置到分区的最开始位置，即偏移量=0的位置）。同时拓扑还会写到一个输出主题，同样也有两个分区，现在都还是空的。还没有偏移量，因为输出主题还没有从应用程序中消费数据。另外，拓扑包括了through()调用，因此它会写入/读取临时主题，这个临时主题也有两个空的分区，并且包括了有状态的操作算子。</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/application-reset-01_before-first-run.png" alt=""></p>
<p>After the Kafka Streams application was executed and stopped, the application state changed as shown in Figure 2. In the following we discuss the relevant parts of Figure 2 with regard to reprocessing.</p>
<p>Figure 2: The application after it was stopped. The small vertical arrows denote committed consumer offsets for the input and intermediate topics (colors denote the corresponding sub-topology). You can see, for example, that sub-topology A has so far written more data to the intermediate topic than sub-topology B has been able to consume (e.g. the last message written to partition 1 has offset 7, but B has only consumed messages up to offset 4). Also, sub-topology performs stateful operations and thus has created a local state store and an accompanying(伴随) internal changelog topic for this state stores.</p>
<p>当Kafka Streams应用程序执行（一段时间后），然后停止，应用程序的状态改变如下图。（分区上的）垂直箭头表示输入主题和临时主题的消费者提交偏移量。可以看到子拓扑A写入到临时主题的数据要比子拓扑B已经消费者的数据要多很多（A的临时主题分区1最近写入消息的偏移量是7，但是B只消费到了偏移量4的位置）。同时，子拓扑因为执行了有状态的操作，所以创建了一个本地状态存储，伴随了针对这个状态存储的内部变更日志主题。</p>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/application-reset-02_after-stopped.png" alt=""></p>
<h4 id="reprocessing_Input_Topics">reprocessing Input Topics</h4><p>Kafka Streams builds upon existing Kafka functionality to provide scalability and elasticity, security, fault-tolerance, and more. For reprocessing input topics from scratch, one important concern is Kafka Streams’ fault-tolerance mechanism. If an application is stopped and restarted, per default it does not reread previously processed data again, but it resumes processing where it left off when it was stopped (cf. committed input topic offsets in Figure 2).<br>Internally, Kafka Streams leverages Kafka’s consumer client to read input topics and to commit offsets of processed messages in regular intervals (see commit.interval.ms). Thus, on restart an application does not reprocess the data from its previous run. In case that a topic was already processed completely, the application will start up but then be idle and wait until new data is available for processing. So one of the steps we need to take care of when manually resetting an application is to ensure that input topics are reread from scratch. However, this step alone is not sufficient to get a correct reprocessing result.</p>
<p>Kafka Streams构建在Kafka已有的功能之上来提供扩展性、伸缩性、安全性、故障容错等等。对于从头开始重新处理输入主题，一个重要的关注点是Kafka Streams的故障容错机制。如果一个应用程序停止并重启，默认情况下它不会重新读取已经处理过的数据，而是从上一次停止时离开的地方恢复处理（图2中为输入主题提交偏移量）。内部实现中，Kafka Streams利用了Kafka的消费者客户端读取输入主题，然后定期地为处理完的消息提交偏移量。因此应用程序重启的时候，不会重新处理上一次运行过的数据。如果有一种场景是：一个主题已经都被处理完成了，应用程序启动之后会空转，直到有新数据可用时才（应用程序才）会处理。所以当手动重置一个应用程序时，要非常小心的一个步骤是：确保输入主题从头开始重新读取。不过，单独这个步骤并不足以得到一个正确的重新处理的结果。</p>
<h4 id="Sub-Topologies_and_Internal_State_in_Kafka_Streams">Sub-Topologies and Internal State in Kafka Streams</h4><p>First, an application can consist of multiple sub-topologies that are connected via intermediate or internal topics (cf. Figure 2 with two sub-topologies A and B). In Kafka Streams, intermediate topics are user-specified topics that are used as both an input and an output topic within a single application (e.g., a topic that is used in a call to through()). Internal topics are those topics that are created by Kafka Streams “under the hood” (e.g., internal repartitioning topics which are basically internal intermediate topics).</p>
<p>首先，一个应用程序可以包含多个子拓扑，它们会通过临时或中间主题互相连接（比如图2中两个子拓扑A和B是通过临时主题相连接在一起的）。在Kafka Streams中，临时的主题是用户指定的，在一个应用程序中会同时作为输入和输出主题（通过调用through()方法定义的就是临时主题）。内部的主题是由Kafka Streams底层创建的（内部重新分区的主题，基本上是内部的临时主题）。</p>
<p>If there are multiple sub-topologies, it might happen that an upstream sub-topology produces records faster into intermediate topics than a downstream sub-topology can consume (cf. committed intermediate topic offsets in Figure 2).  Such a consumption delta (cf. the notion of consumer lag in Kafka) within an application would cause problems in the context of an application reset because, after an application restart, the downstream sub-topology would resume reading from intermediate topics from the point where it stopped before the restart. While this behavior is very much desired during normal operations of your application, it would lead to data inconsistencies when resetting an application. For a proper application reset we must therefore tell the application to skip to the very end of any intermediate topics.</p>
<p>如果有多个子拓扑，有可能会发生上游的子拓扑生产记录到临时主题，要比下游子拓扑消费的速度快（比如图2中临时主题的提交偏移量）。应用程序的这种消费差距（即Kafka中消费者的落后进度，用lag表示）在一个应用程序的重置场景下可能会导致出现问题，因为当一个应用程序重启后，下游的子拓扑会在重启之前的上一次停止位置，从临时主题恢复读取数据。虽然这种行为在应用程序正常的操作时是你非常想要的结果，但是在重置一个应用程序时，则可能会导致数据的不一致性。对于正确的应用程序重置方式，我们必须告诉应用程序跳到任何一个临时主题的最后位置。</p>
<p>Second, for any stateful operation like aggregations or joins, the internal state of these operations is written to a local state store that is backed by an internal changelog topic (cf. sub-topology B in Figure 2). On application restart, Kafka Streams “detects” these changelog topics and any existing local state  data, and it ensures that the internal state is fully built up and ready before the actual processing starts. To reset an application we must therefore also reset the application’s internal state, which means we must delete all its local state stores and their corresponding internal changelog topics.</p>
<p>其次，对于任何有状态的操作比如聚合或联合，这些操作的内部状态会被写入到一个临时状态存储中，这个存储也依赖于一个内部的变更日志主题（比如图2的子拓扑B）。在应用程序重启是，Kafka Streams会检测到这些变更日志流，以及已经存在的本地状态数据，它会确保内部状态被完整地构建起来，并且会在实际的处理开始之前准备完毕。所以为了重置一个应用程序，我们也必须要重置应用程序的内部状态，这意味着我们必须要删除所有的本地状态存储，以及对应的内部变更日志主题。</p>
<p>If you are interested in more details than we could cover in this blog post, please take a look at Kafka Streams: Internal Data Management in the Apache Kafka wiki.</p>
<h4 id="Resetting_a_Kafka_Streams_Application_Manually">Resetting a Kafka Streams Application Manually</h4><p>In order to reprocess topics from scratch, it is required to reset the application state that consists of multiple parts as described above:<br>为了从头开始重新处理主题，重置应用程序的状态要重置以下数据：</p>
<ol>
<li>committed offsets of input topics 输入主题的提交偏移量</li>
<li>committed offsets of intermediate topics 临时主题的提交偏移量</li>
<li>content and committed offsets of internal topics 内容，以及内部主题的提交偏移量</li>
<li>local state store 本地状态存储</li>
</ol>
<p><img src="http://cdn2.hubspot.net/hubfs/540072/application-reset-03_after-reset_1.png" alt=""></p>
<p>Figure 3: The application after reset: (1) input topic offsets were reset to zero (2) intermediate topic offsets were advanced to end (3) internal topics were deleted (4) any local state stores were deleted (5) the output topic was not modified.</p>
<p>图3中应用程序在重置后：1）输入主题的偏移量被重置为0；2）临时主题的偏移量被跳跃到最后；3）内部主题被删除；4）任何的本地状态存储被删除；5）输出主题不会被修改</p>
<p><strong>Committed offsets of input topics</strong>: Internally, Kafka Streams leverages Kafka’s consumer client to read a topic and to commit offsets of processed messages in regular intervals (see commit.interval.ms). Thus, as a first step to reprocess data, the committed offsets need to be reset. This can be accomplished as follows: Write a special Kafka client application (e.g., leveraging Kafka’s Java consumer client or any other available language) that uses the application.id of your Kafka Streams application as its consumer group ID. The only thing this special client application does is to seek to offset zero for all partitions of all input topics and commit the offset (you should disable auto commit for this application). As this special application uses the same group ID as your Kafka Streams application (the application ID is used a consumer group ID internally), committing all offsets to zero allows your Streams application to consume its input topics from scratch when it is started again (cf. #1 in Figure 3).</p>
<p>输入主题的提交偏移量：在内部实现中，Kakfa Streams利用了Kafka消费者客户端来读取一个主题，并且定期地提交已经处理过的消息的偏移量。因此作为重新处理数据的第一个步骤，提交偏移量需要被重置。完成这一步可以这么做：编写一个特殊的Kafka消费者应用程序，使用流处理应用程序的<code>application.id</code>作为消费组名称。这个特殊的消费者唯一要做的事情是为所有输入主题的所有分区定位到偏移量=0，然后提交偏移量（这个客户端程序应该禁用自动提交偏移量）。由于这个特殊的消费者应用程序使用了和Kafka Streams应用程序相同的消费组编号（Kafka Streams在内部也会将application.id作为消费组编号），提交所有的偏移量到0，这样就允许你的流应用程序再次启动的时候可以从头开始消费输入主题（图3的步骤1）。</p>
<p><strong>Intermediate topics</strong>: For intermediate topics we must ensure to not consume any data from previous application runs. The simplest and recommended way is to delete and recreate those intermediate topics (recall that it is recommended to create user-specified topics manually before you run a Kafka Streams application). Additionally, it is required to reset the offsets to zero for the recreated topics (same as for input topics). Resetting the offsets is important because the application would otherwise pick up those invalid offsets on restart.</p>
<p>对于临时主题，我们要保证不能从上次应用程序运行的地方消费任何数据。最简单和推荐的方式是删除并重建这些临时主题（回顾下前面我们也说过推荐在运行Kafka Streams应用程序之前手动创建用户指定的主题）。另外，也需要将重新创建的主题的偏移量重置到0，这非常重要，否则应用程序在重启的时候可能会获得无效的偏移量。</p>
<p>As an alternative(供替代的选择), it is also possible to only modify the committed offsets for intermediate topics. You should consider this less invasive(侵入性) approach when there are other consumers for intermediate topics and thus deleting the topics is not possible. However, in contrast to modifying the offsets of input topics or deleted intermediate topic, the offsets for kept intermediate topics must be set to the largest value (i.e., to the current log-size) instead of zero, thus skipping over any not-yet consumed data. This ensure that, on restart, only data from the new run will be consumed by the application (cf. #2 in Figure 3). This alternative approach is used by the application reset tool.</p>
<p>除了删除和重建临时主题这个选择外，也可以值修改临时主题的提交偏移量。当临时主题有其他消费者时，可以采用这种方案较少侵入性的方案，因为（主题有消费者时）删除主题是不可能的。不过和修改输入主题、删除主题不同的是，临时主题的偏移量必须要设置到最大的值（当前的logSize），而不是0，这样就会跳过还没有被消费的任意数据。确保了在重启的时候，只有新的数据才会被应用程序消费（图3中的步骤2），应用程序重置工具使用的就是这种方法（修改临时主题的偏移量到最大）。总结下重置临时主题有两种方案：</p>
<ol>
<li>删除临时主题，重建临时主题，设置提交偏移量=0，提交偏移量</li>
<li>修改临时主题的提交偏移量到最大</li>
</ol>
<p><strong>Internal topics</strong>: Internal topics can simply be deleted (cf. #3 in Figure 3). As those are created automatically by Kafka Streams, the library can recreate them in the reprocessing case. Similar to deleting intermediate user topics, make sure that committed offsets are either deleted or set to zero.</p>
<p>内部主题可以简单地删除掉（图3的步骤3）。因为它们会被Kafka Streams自动创建，所以在重新处理的时候也会重新创建它们（内部主题）。和删除用户指定的临时主题一样，要确保提交偏移量要么删除，要么设置到0。</p>
<p>In order to delete those topics, you need to identify them. Kafka Streams creates two types of internal topics (repartitioning and state-backup) and uses the following naming convention (this naming convention could change in future releases however, which is one of the reasons we recommend the use of the application reset tool rather than manually resetting your applications):</p>
<p>为了删除内部主题，首先你要定位这些主题。Kafka Streams会创建两种类型的内部主题（重新分区和状态备份），并且会使用下面的命名约定（这个命名约定在未来的版本可能会变化，这也是推荐使用应用程序重置工具而不是手动重置应用程序的一个原因，因为应用程序重置工具可以帮我们隐藏这些内部细节，而手动重置则需要知道当前版本的命名约定）：</p>
<ul>
<li><applicationid>-<operatorname>-repartition 应用程序编号-操作符名称-repartition</operatorname></applicationid></li>
<li><applicationid>-<operatorname>-changelog 应用程序编号-操作符名称-changelog</operatorname></applicationid></li>
</ul>
<p>比如以我们前面一开始的示例为例，会创建内部主题：“my-streams-app-global-count-changelog”，因为countByKey()方法的操作符名称被指定为“global-count”。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ResetDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">// Kafka Streams configuration</span></span><br><span class="line">        <span class="keyword">final</span> Properties streamsConfiguration = <span class="keyword">new</span> Properties();</span><br><span class="line">        streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"my-streams-app"</span>);</span><br><span class="line">        <span class="comment">// make sure to consume the complete topic via "auto.offset.reset = earliest"</span></span><br><span class="line">        streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line">        <span class="comment">// ...and so on...</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// define the processing topology</span></span><br><span class="line">        <span class="keyword">final</span> KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line">        builder.stream(<span class="string">"my-input-topic"</span>)</span><br><span class="line">            .selectKey(...)</span><br><span class="line">            .through(<span class="string">"rekeyed-topic"</span>)</span><br><span class="line">            .countByKey(<span class="string">"global-count"</span>)</span><br><span class="line">            .to(<span class="string">"my-output-topic"</span>);</span><br><span class="line"></span><br><span class="line">         <span class="comment">// ...run application...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Local State Stores</strong>: Similar to internal topics, local state stores can just be deleted (cf. #4 in Figure 3). They will be recreated automatically by Kafka Streams. All state of an application (instance) is stored in the application’s state directory (cf. parameter state.dir, with default value /var/lib/kafka-streams in Confluent Platform releases and /tmp/kafka-streams for Apache Kafka releases). Within the state store directory, each application has its own directory tree within a sub-folder that is named after the application ID. The simplest way to delete the state store for an application is therefore rm -rf <state.dir>/<application.id> (e.g., rm -rf /var/lib/kafka-streams/my-streams-app).</application.id></state.dir></p>
<p>本地状态存储：类似于内部主题，本地状态存储也可以删除（图3的步骤4），它们也会被Kafka Streams自动重新创建。一个应用程序实例的所有状态都存储在应用程序的状态目录下（参数state.dir，默认是/tmp/kafka-streams）。在状态存储目录中，每个应用程序都有自己的目录树，子目录的名称是应用程序的编号。删除一个应用程序的状态存储最简单的方式是直接执行命令：<code>rm -rf &lt;state.dir&gt;/&lt;application.id&gt;</code>（比如<code>rm -rf /tmp/kafka-streams/my-streams-app</code>）。</p>
<p>After this discussion, we see that manually resetting a Stream application is cumbersome(累赘) and error-prone. Thus, we developed the new “Application Reset Tool” to simplify this process.</p>
<p>可以看到，手动重置一个流应用程序非常繁琐并且容易出错。因此我们开发了应用程序重置工具来简化这个流程。</p>
<h3 id="Application_Reset_Tool_Details">Application Reset Tool Details</h3><p>In more detail, the application reset tool (bin/kafka-streams-application-reset) performs the following actions (cf. Figure 3):</p>
<p>具体来说，应用程序重置工具（bin/kafka-streams-application-reset）执行了一下的操作（图3）：</p>
<ol>
<li>for any specified input topic, it resets all offsets to zero 对任何指定的输入主题，重置所有的偏移量到0</li>
<li>for any specified intermediate topic, seeks to the end for all partitions 对任何指定的临时主题，跳到所有分区的末尾</li>
<li>for all internal topic 对所有的内部主题<br>3.1 resets all offsets to zero 重置所有的偏移量到0<br>3.2 deletes the topic 删除（内部）主题</li>
</ol>
<p>To use the script, as a minimum you need to specify the application ID. For this case, only internal topics will be deleted. Additionally, you can specify input topics and/or intermediate topics. More details about resetting a Streams application, can be found in the Confluent documentation.</p>
<p>使用这个脚本，你最少必须要指定应用程序编号，这样只有内部主题会被删除。当然你也可以指定输入主题和临时主题（这样就会重置相应主题的偏移量）。</p>
<p>Pay attention, that the application reset tool only covers the “global reset” part. Additionally, to the global reset, for each application instance a local reset of the application state directory is required, too. (cf. #4 in Figure 3) This can be done directly within your application using the method KafkaStreams#cleanUp(). Calling cleanUp() is only valid as long as the application instance is not running (i.e., before start() or after close()).</p>
<p>注意：应用程序重置工具只覆盖“全局重置”的部分，除了全局重置，每个应用程序实例也需要本地重置应用程序的状态目录（图3的步骤4）。这可以通过在应用程序中调用KafkaStreams#cleanUp()来完成。调用cleanUp()方法只有在应用程序实例还没有运行的时候才是有效的（在start()之前，或者在close()之后）。</p>
<p>Because resetting the local state store is embedded in your code, there is no additional work to do for local reset — local reset is included in restarting an application instance. For global reset, a single run of the application reset tool is sufficient.</p>
<p>因为重置本地状态存储是内嵌在你的代码中的，所以本地重置没有额外的工作，即本地重置包含在重启应用程序实例的过程中。对于全局重置，只需要运行一次应用程序重置工具即可。</p>
<h4 id="重置时指定了新的应用程序编号会发生什么？">重置时指定了新的应用程序编号会发生什么？</h4><p>Before we close we want to discuss what happens when you configure your Kafka Streams application to use a new application ID. Until now this has been a common workaround(变通方法) for resetting an application manually.</p>
<p>在结束本篇博文之前，我们想要讨论下当你配置Kafka Streams应用程序时使用了一个新的应用程序编号会发生什么事。目前为止，这种方式实际上也是手动重置应用程序的一种变通方法。</p>
<p>On the positive side, renaming the application ID does cause your application to reprocess its input data from scratch. Why? When a new application ID is used, the application does not have any committed offsets, internal topics, or local state associated with itself, because all of those use the application ID in some way to get linked to a Kafka Streams application. Of course, you also need to delete and recreate all intermediate user topics.</p>
<p>实际上，重新命名应用程序编号确实会让你的应用程序会从头开始重新处理输入数据。当使用了新的应用程序编号，新的应用没有任何的提交偏移量、内部主题、或相关联的本地状态，因为所有这些（数据）都使用应用程序编号的某种方式来和一个Kafka Streams应用程序相关联。当然，你还是需要删除并重建用户指定的所有内部主题。</p>
<p>So why all the fuss(小题大作) about resetting a Kafka Streams application if we could use this workaround?</p>
<p>那么如果我们可以使用这种变通的方法（重命名应用程序编号）来重置Kafka Streams应用程序，为什么还要小题大做（开发一个应用程序重置工具呢）？</p>
<p>First, resetting an application is more than just enabling it to reprocess its input data. An important part of the reset is to also clean-up all internal data that is created by a running application in the background. For example, all the internal topics (that are no longer used) consume storage space in your Kafka cluster if nobody deletes them. Second, the same clean-up must be performed for data written to the local state directories. If not explicitly deleted, disk storage is wasted on those machines that hosted an application instance. Last but not least there is all kind of metadata like topic names, consumer groups, committed offsets that are not used any more and linger around(游荡，苟延残喘). For those reasons, using a new application ID is considered nothing more than a crude workaround to reset a Kafka Streams application, and we wouldn’t recommend its use for production scenarios.</p>
<p>首先，重置一个应用程序并不仅仅是为了让它能重新处理输入数据。重置的一个重要部分是清理后台运行的应用程序创建的所有内部数据。比如所有不会再被使用的内部主题，如果没有人去删除它们，就会消耗Kafka集群的存储空间。其次，同样的清理工作也必须针对写入到本地状态存储目录的数据。如果没有显示删除，就会浪费运行应用程序实例的所在机器的磁盘。最后，有很多的元数据比如主题名称，消费组，提交偏移量，这些都不会再被使用了。基于这些理由使用新的应用程序编号被认为是重置应用程序的一个粗糙方案，因此我们不推荐在生产环境中使用这个种方式。</p>
<p>One more hint at the end: if you do not use a Kafka Streams application anymore (e.g., it gets replaced by a new version of the application or is just not longer needed), we recommend to run the reset tool once to clean-up all the left-overs(剩余) of the retired application.</p>
<p>最后再提示一点：如果你不再使用一个Kafka Streams应用程序了（比如应用程序有新的版本替换了，或者就只是不用了），推荐运行一次重置工具来清理所有剩余的退役应用。</p>
<h1 id="KIP-28:Add_a_processor_client">KIP-28:Add a processor client</h1><p>翻译：<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-28+-+Add+a+processor+client" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/KAFKA/KIP-28+-+Add+a+processor+client</a>  </p>
<h2 id="动机">动机</h2><p>A common use case for Kafka is real-time processes that transform data from input topics to output topics. Today there are a couple of options available for users to process such data:</p>
<p>使用Kafka的一个典型用例是实时处理，从输入主题中转换数据到输出主题。现在用户有两种方式来处理这样的数据：</p>
<p>1.使用Kafka的生产者和消费者API，自己定义处理逻辑，比如</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create a producer and a consumer</span></span><br><span class="line">KafkaProducer producer = <span class="keyword">new</span> KafkaProducer(configs);</span><br><span class="line">KafkaConsumer consumer = <span class="keyword">new</span> KafkaConsumer(configs);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// start a thread with a producer and consumer client</span></span><br><span class="line"><span class="comment">// for data IO and execute processing logic</span></span><br><span class="line"><span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable &#123;</span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning) &#123;</span><br><span class="line">      <span class="comment">// read some data from up-stream Kafka</span></span><br><span class="line">      List&lt;Message&gt; inputMessages = consumer.poll();</span><br><span class="line"> </span><br><span class="line">      <span class="comment">// do some processing..</span></span><br><span class="line"> </span><br><span class="line">      <span class="comment">// send the output to the down-stream Kafka</span></span><br><span class="line">      producer.send(outputMessages);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;).start()</span><br></pre></td></tr></table></figure>
<p>2.使用成熟的流处理系统比如Storm、Samza、Spark Streaming、或者Flink，将Kafka作为它们的源/目标的流数据存储。</p>
<p>Both of those approaches have some downsides. Downsides of using the first option are that the producer and consumer APIs used for writing transformations are somewhat low level; simple examples are relatively simple, but any kind of more complex transformation tends to be a bit complicated. The opportunities(因素，机会) for a richer client to add value beyond what the producer and consumer do would be:</p>
<p>这两种方式都有优缺点，第一种方案的缺点是：用生产者和消费者API来写转换操作有些低级，简单的例子可能还好，稍微复杂的转换则不可取。提供一个富客户端相对原始的生产者和消费者有以下好处：</p>
<ol>
<li>Manage multi-threading and parallelism within a process. 在一个进程中管理多线程和并行</li>
<li>Manage partitioning assignment to processes / threads. 管理分区如何分配给进程或现场</li>
<li>Manage journaled local state storage. 管理本地状态存储（文件存储系统）</li>
<li>Manage offset commits and “exactly-once” guarantees as appropriate features are added in Kafka to support this. 管理偏移量的提交和正好一次的保证</li>
</ol>
<p>The second option, i.e. using a full stream processing framework can be a good solution but a couple of things tend to(趋向，易于) make it a bit heavy-weight (a brief and still-going survey can be found here):</p>
<p>第二种选项，使用流处理框架可能会是一个好的方案，不过下面这些附带产物很容易让它们变得非常重量级。</p>
<ol>
<li>These frameworks are poorly integrated with Kafka (different concepts, configuration, monitoring, terminology). For example, these frameworks only use Kafka as its stream data source / sink of the whole processing topology, while using their own in-memory format for storing intermediate data (RDD, Bolt memory map, etc). If users want to persist these intermediate results to Kafka as well, they need to break their processing into multiple topologies that need to be deployed separately, increasing operation and management costs.</li>
<li>These frameworks either duplicate or force the adoption(采用) of a packaging, deployment, and clustering solution. For example, in Storm you need to run a Storm cluster which is a separate thing that has to be monitored and operated. In an elastic environment like AWS, Mesos, YARN, etc this is sort of silly since then you have a Storm cluster inside the YARN cluster vs just directly running the jobs in Mesos; similarly Samza is tied up with YARN. </li>
<li><p>These frameworks can’t be integrated with existing services or applications. For example, you can’t just embed a light transformation library inside an existing app, but rather the entire framework that runs as a service.</p>
</li>
<li><p>这些框架与Kafka的集成很贫乏（不同的概念，配置，监控，术语）。比如这些框架都只会使用Kafka作为它们的整个处理拓扑中的流数据源或目标，但同时也会使用它们自己的内存格式来存储内部数据（RDD，Bolt内存字典）。如果用户想要持久化这些临时结果到Kafka中，他们需要将流处理分成多个部署独立的拓扑，而这显然增加了操作和维护的成本。</p>
</li>
<li>这些框架针对打包、部署、集群的方案会有重复或强制采用。比如在Storm中你需要运行一个独立的Storm集群，并需要监控和管理这个集群。</li>
<li>这些框架不能和已有的服务或应用程序继承。比如你不能简单地将一个轻量级的转换客户端库嵌入到已有的程序中，而是让整个框架作为一个服务运行。</li>
</ol>
<h2 id="Processor客户端提议">Processor客户端提议</h2><p>We want to propose another standalone “processor” client besides the existing producer and consumer clients for processing data consumed from Kafka and storing results back to Kafka. </p>
<p>除了已经存在的生产者和消费者客户端，我们想要提供一个新的标准“Processor”客户端，它会处理从Kafka消费的数据，然后将结果存储回Kafka。</p>
<p><strong>Data Processing 数据处理</strong>  </p>
<p>A processor computes on a stream of messages, with each message composed as a key-value pair.<br>Processor receives one message at a time and does not have access to the whole data set at once.</p>
<ol>
<li>Per-message processing: this is the basic function that can be triggered once a new message has arrived from the stream.</li>
<li>Time-triggered processing: this function can be triggered whenever a specified time period has elapsed. It can be used for windowing computation, for example.</li>
</ol>
<p>一个Processor会在消息流上做计算，每个消息由键值对组成。Processor一次只接收一条消息，并不需要一次访问所有的数据。</p>
<ol>
<li>每条消息处理：这是一个最基本的功能，当一条心的消息从流中到达时，应该触发一次处理逻辑</li>
<li>时间触发处理：当一个指定的时间间隔过去后，这个函数应该被触发。比如，它可以用在窗口计算</li>
</ol>
<p><strong>Compossible Processing 共存处理</strong>  </p>
<p>Multiple processors should be able to chained up to form a DAG (i.e. the processor topology) for complex processing logic.<br>Users can define such processor topology in a exploring REPL manner: make an initial topology, deploy and run, check the results and intermediate values, and pause the job and edit the topology on-the-fly.</p>
<p>针对复杂的处理逻辑，多个Processor应该被链接在一起，形成一个DAG（Processor处理拓扑）。用户可以定义采用REPL的方式定义这样的处理拓扑：创建和初始化一个拓扑，部署和运行，检查结果和中间数据，暂停作业，编辑拓扑。</p>
<p><strong>Local State Storage 本地状态存储</strong>  </p>
<p>Users can create state storage inside a processor that can be accessed locally.<br>For example, a processor may retain a (usually most recent) subset of data for a join, aggregation / non-monolithic operations.</p>
<p>用户可以在一个Processor中创建状态存储，并且只能在本地访问（所以叫做本地状态存储）。比如一个Processor可能会保存数据的子集（通常是最近的数据）用在join操作，聚合操作。</p>
<h3 id="Processor接口">Processor接口</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ProcessorContext</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">send</span><span class="params">(String topic, Object key, Object value)</span></span>;  <span class="comment">// send the key value-pair to a Kafka topic</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">schedule</span><span class="params">(<span class="keyword">long</span> timestamp)</span></span>;                      <span class="comment">// repeatedly schedule the punctuation function for the period</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">commit</span><span class="params">()</span></span>;                                      <span class="comment">// commit the current state, along with the upstream offset and the downstream sent data</span></span><br><span class="line">    <span class="function">String <span class="title">topic</span><span class="params">()</span></span>;                                     <span class="comment">// return the Kafka record's topic of the current processing key-value pair</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">()</span></span>;                                    <span class="comment">// return the Kafka record's partition id of the current processing key-value pair</span></span><br><span class="line">    <span class="function"><span class="keyword">long</span> <span class="title">offset</span><span class="params">()</span></span>;                                      <span class="comment">// return the Kafka record's offset of the current processing key-value pair</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Processor</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt;  </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessorContext context)</span></span>;           <span class="comment">// initialize the processor</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">(K1 key, V1 value)</span></span>;                <span class="comment">// process a key-value pair</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">punctuate</span><span class="params">()</span></span>;                              <span class="comment">// process when the the scheduled time has reached</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;                                  <span class="comment">// close the processor</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ProcessorDef</span> </span>&#123;</span><br><span class="line">    <span class="function">Processor <span class="title">instance</span><span class="params">()</span></span>;                          <span class="comment">// create a new instance of the processor from its definition</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TopologyBuilder</span> </span>&#123;</span><br><span class="line">    <span class="comment">// add a source node to the topology which generates incoming traffic with the specified Kafka topics</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> TopologyBuilder <span class="title">addSource</span><span class="params">(String name, String... topics)</span> </span>&#123; ... &#125;  </span><br><span class="line">    <span class="comment">// add a sink node to the topology with the specified parent nodes that sends out-going traffic to the specified Kafka topics</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> TopologyBuilder <span class="title">addSink</span><span class="params">(String name, String topic, String... parentNames)</span> </span>&#123; ... &#125; </span><br><span class="line">    <span class="comment">// add a processor node to the topology with the specified parent nodes</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> TopologyBuilder <span class="title">addProcessor</span><span class="params">(String name, ProcessorDef definition, String... parentNames)</span> </span>&#123; ... &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>用户可以使用创建的Processor拓扑创建处理作业：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProcessorJob</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">MyProcessorDef</span> <span class="keyword">implements</span> <span class="title">ProcessorDef</span> </span>&#123;</span><br><span class="line">        <span class="annotation">@Override</span></span><br><span class="line">        <span class="keyword">public</span> Processor&lt;String, Integer&gt; instance() &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> Processor&lt;String, Integer&gt;() &#123;</span><br><span class="line">                <span class="keyword">private</span> ProcessorContext context;</span><br><span class="line">                <span class="keyword">private</span> KeyValueStore&lt;String, Integer&gt; kvStore;</span><br><span class="line"> </span><br><span class="line">                <span class="annotation">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessorContext context)</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">this</span>.context = context;</span><br><span class="line">                    <span class="keyword">this</span>.context.schedule(<span class="keyword">this</span>, <span class="number">1000</span>);</span><br><span class="line">                    <span class="keyword">this</span>.kvStore = <span class="keyword">new</span> InMemoryKeyValueStore&lt;&gt;(<span class="string">"local-state"</span>, context);</span><br><span class="line">                &#125;</span><br><span class="line"> </span><br><span class="line">                <span class="annotation">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key, Integer value)</span> </span>&#123;</span><br><span class="line">                    Integer oldValue = <span class="keyword">this</span>.kvStore.get(key);</span><br><span class="line">                    <span class="keyword">if</span> (oldValue == <span class="keyword">null</span>) &#123;</span><br><span class="line">                        <span class="keyword">this</span>.kvStore.put(key, value);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="keyword">int</span> newValue = oldValue + value;</span><br><span class="line">                        <span class="keyword">this</span>.kvStore.put(key, newValue);</span><br><span class="line">                    &#125;</span><br><span class="line">                    context.commit();</span><br><span class="line">                &#125;</span><br><span class="line"> </span><br><span class="line">                <span class="annotation">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">punctuate</span><span class="params">(<span class="keyword">long</span> streamTime)</span> </span>&#123;</span><br><span class="line">                    KeyValueIterator&lt;String, Integer&gt; iter = <span class="keyword">this</span>.kvStore.all();</span><br><span class="line">                    <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">                        Entry&lt;String, Integer&gt; entry = iter.next();</span><br><span class="line">                        System.out.println(<span class="string">"["</span> + entry.key() + <span class="string">", "</span> + entry.value() + <span class="string">"]"</span>);</span><br><span class="line">                        context.forward(entry.key(), entry.value());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"> </span><br><span class="line">                <span class="annotation">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                    <span class="keyword">this</span>.kvStore.close();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamingConfig config = <span class="keyword">new</span> StreamingConfig(<span class="keyword">new</span> Properties());</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// build topology</span></span><br><span class="line">        TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line">        builder.addSource(<span class="string">"SOURCE"</span>, <span class="string">"topic-source"</span>);</span><br><span class="line">               .addProcessor(<span class="string">"PROCESS"</span>, <span class="keyword">new</span> MyProcessorDef(), <span class="string">"SOURCE"</span>);</span><br><span class="line">               .addSink(<span class="string">"SINK"</span>, <span class="string">"topic-sink"</span>, <span class="string">"PROCESS"</span>);</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// start process</span></span><br><span class="line">        KafkaStreaming streaming = <span class="keyword">new</span> KafkaStreaming(builder, config);</span><br><span class="line">        streaming.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的API示例了低级消费者/生产者接口的抽象，比如consumer.poll() / commit(), producer.send(callback), producer.flush()。</p>
<h3 id="High-level_Stream_DSL">High-level Stream DSL</h3><p>除了Processor API，我们也会引入高级Stream DSL，覆盖了常见的处理实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">KStream</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">//Creates a new stream consists of all elements of this stream which satisfy a predicate</span></span><br><span class="line">    KStream&lt;K, V&gt; filter(Predicate&lt;K, V&gt; predicate);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Creates a new stream by transforming key-value pairs by a mapper to all elements of this stream</span></span><br><span class="line">    &lt;K1, V1&gt; KStream&lt;K1, V1&gt; map(KeyValueMapper&lt;K, V, K1, V1&gt; mapper);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Creates a new stream by transforming valuesa by a mapper to all values of this stream</span></span><br><span class="line">    &lt;V1&gt; KStream&lt;K, V1&gt; mapValues(ValueMapper&lt;V, V1&gt; mapper);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Creates a new stream by applying a flat-mapper to all elements of this stream</span></span><br><span class="line">    &lt;K1, V1&gt; KStream&lt;K1, V1&gt; flatMap(KeyValueMapper&lt;K, V, K1, ? extends Iterable&lt;V1&gt;&gt; mapper);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Creates a new stream by applying a flat-mapper to all values of this stream</span></span><br><span class="line">    &lt;V1&gt; KStream&lt;K, V1&gt; flatMapValues(ValueMapper&lt;V, ? extends Iterable&lt;V1&gt;&gt; processor);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Creates a new windowed stream using a specified window instance.</span></span><br><span class="line">    KStreamWindowed&lt;K, V&gt; with(Window&lt;K, V&gt; window);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Creates an array of streams from this stream. Each stream in the array corresponds to a predicate in supplied predicates in the same order.</span></span><br><span class="line">    KStream&lt;K, V&gt;[] branch(Predicate&lt;K, V&gt;... predicates);</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">sendTo</span><span class="params">(String topic)</span></span>;   <span class="comment">//Sends key-value to a topic.</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Sends key-value to a topic, also creates a new stream from the topic.</span></span><br><span class="line">    <span class="comment">//This is mostly used for repartitioning and is equivalent to calling sendTo(topic) and from(topic).</span></span><br><span class="line">    KStream&lt;K, V&gt; through(String topic);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//Processes all elements in this stream by applying a processor.</span></span><br><span class="line">    &lt;K1, V1&gt; KStream&lt;K1, V1&gt; process(KafkaProcessor&lt;K, V, K1, V1&gt; processor);</span><br><span class="line">    <span class="comment">// .. more operators</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">KStreamWindowed</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">KStream</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/**</span><br><span class="line">     * Creates a new stream by joining this windowed stream with the other windowed stream.</span><br><span class="line">     * Each element arrived from either of the streams is joined with elements with the same key in another stream.</span><br><span class="line">     * The resulting values are computed by applying a joiner.</span><br><span class="line">     */</span></span><br><span class="line">    &lt;V1, V2&gt; KStream&lt;K, V2&gt; join(KStreamWindowed&lt;K, V1&gt; other, ValueJoiner&lt;V, V1, V2&gt; joiner);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">/**</span><br><span class="line">     * Creates a new stream by joining this windowed stream with the other windowed stream.</span><br><span class="line">     * Each element arrived from either of the streams is joined with elements with the same key in another stream</span><br><span class="line">     * if the element from the other stream has an older timestamp.</span><br><span class="line">     * The resulting values are computed by applying a joiner.</span><br><span class="line">     */</span></span><br><span class="line">    &lt;V1, V2&gt; KStream&lt;K, V2&gt; joinPrior(KStreamWindowed&lt;K, V1&gt; other, ValueJoiner&lt;V, V1, V2&gt; joiner);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用高级接口，用户的程序可以很简单：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KStreamJob</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        StreamingConfig config = <span class="keyword">new</span> StreamingConfig(props);</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// build the topology</span></span><br><span class="line">        KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line"> </span><br><span class="line">        KStream&lt;String, String&gt; stream1 = builder.from(<span class="string">"topic1"</span>);</span><br><span class="line"> </span><br><span class="line">        KStream&lt;String, Integer&gt; stream2 =</span><br><span class="line">            stream1.map((key, value) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(key, <span class="keyword">new</span> Integer(value)))</span><br><span class="line">                   .filter(((key, value) -&gt; <span class="keyword">true</span>));</span><br><span class="line"> </span><br><span class="line">        KStream&lt;String, Integer&gt;[] streams = stream2</span><br><span class="line">            .branch((key, value) -&gt; value &gt; <span class="number">10</span>,</span><br><span class="line">                    (key, value) -&gt; value &lt;= <span class="number">10</span>);</span><br><span class="line">  </span><br><span class="line">        streams[<span class="number">0</span>].sendTo(<span class="string">"topic2"</span>);</span><br><span class="line">        streams[<span class="number">1</span>].sendTo(<span class="string">"topic3"</span>);</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// start the process</span></span><br><span class="line">        KafkaStreaming kstream = <span class="keyword">new</span> KafkaStreaming(builder, config);</span><br><span class="line">        kstream.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="架构设计">架构设计</h2><p>下面我们会总结一些重要的架构设计要点：</p>
<p><img src="http://img.blog.csdn.net/20161106115808194" alt="kstream arch"></p>
<h3 id="分区分布">分区分布</h3><p>As shown in the digram above, each KStream process could have multiple threads (#.threads configurable in the properties), with each thread having a separate consumer and producer. So the first question is how can we distribute the partitions of the subscribed topics in the source processor among all the processes / threads. There are a couple of common cases for partition management in KStream:</p>
<p>如上图所示，每个KStream进程允许有多个线程（配置文件指定的线程数量），每个线程都有一个独立的消费者和生产者。所以第一个问题是：对于源处理算子订阅主题的分区，我们怎么分布这些分区。在KStream中有一些通用的分区管理场景：</p>
<ol>
<li>Co-partitioning: for windowed-joins.</li>
<li>Sticky partitioning: for stateful processing, users may want to have a static mapping from stream partitions to process threads.</li>
<li><p>N-way partitioning: when we have stand-by processor instances, users may want to assign a single stream partition to multiple process threads.</p>
</li>
<li><p>协调分区，针对窗口的join</p>
</li>
<li>粘性分区：对于有状态的操作，从流分区到处理线程，用户可能会用静态的映射方式</li>
<li>多路分区：当我们有备用的Processor实例，用户可能想要将一个流应用程序分配到多个处理线程上</li>
</ol>
<p>These use cases would require more flexible assignments than today’s server-side strategies, so we need to extend the consumer coordinator protocol in the way that:</p>
<ol>
<li>Consumers send JoinGroup with their subscribed topics, and receive the JoinGroup responses with the list of members in the group and the list of topic-partitions.</li>
<li>All consumers will get the same lists, and they can execute the same deterministic partition assignment algorithm to get their assigned topic-partitions.</li>
</ol>
<p>这些用例都需要比现有的服务端策略有更加灵活的分配方式，所以我们需要扩展消费者的协调协议：</p>
<ol>
<li>消费者发送带有订阅主题的JoinGroup，接收到带有消费组成员的JoinGroup响应，以及主题分区列表</li>
<li>所有消费者接收到相同的列表，执行相同的分区分配算法，来得到属于它们自己的主题分区</li>
</ol>
<p>With this new assignment protocol (details of this change can be found here), we distribute the partitions among worker thread as the following:</p>
<p>使用新的分配协议，我们会用下面的方式将分区在所有工作线程上进行分布：</p>
<p>0.Upon starting the KStream process, user-specified number of KStream threads will be created. There is no shared variables between threads and no synchronization barriers as well hence these threads will execute completely asynchronously. Hence we will describe the behavior of a single thread in all the following steps.  </p>
<p>在启动KStream进程时，指定数量的KStream线程会被创建。线程之间没有共享的变量，也没有同步的屏障，所以这些线程会完全异步地执行。所以在下面的步骤中我们只会描述一个线程的行为，其他线程都是类似的。</p>
<p>1.Thread constructs the user-specified processor topology without initializing it just in order to extract the list of subscribed topics from the topology.   </p>
<p>线程会构造用户指定的处理拓扑，但是不会初始化它，仅仅只是为了从拓扑中抽取中订阅的主题列表</p>
<p>2.Thread uses its consumer’s partitionsFor() to fetch the metadata for each of the subscribed topics to get a information of topic -&gt; #.partitions.   </p>
<p>线程使用消费者的partitionsFor()方法获取订阅的每个主题的元数据，得到topic和分区数量的信息</p>
<p>3.Thread now triggers consumer’s subscribe() with the subscribed topics, which will then applies the new rebalance protocol. The join-group request will be instantiated as follows (for example):  </p>
<p>现在线程会调用消费者的subscribe()方法，传递订阅的主题，然后会运用新的平衡协议。JoinGroup请求实例化对象如下：</p>
<figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">JoinGroupRequest =&gt;</span><br><span class="line">  GroupId                 =&gt; <span class="string">"KStream-[JobName]"</span></span><br><span class="line">  GroupType               =&gt; <span class="string">"KStream"</span></span><br></pre></td></tr></table></figure>
<p>And the assignor interface is implemented as follows: 分配分区的接口如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">List&lt;TopicPartition&gt; <span class="title">assign</span><span class="params">(String consumerId, //消费者编号</span><br><span class="line">                            Map&lt;String, Integer&gt; partitionsPerTopic,  //每个主题有多少个分区</span><br><span class="line">                            List&lt;ConsumerMetadata&lt;T&gt;&gt; consumers)</span> </span>&#123; <span class="comment">//所有的消费者元数据</span></span><br><span class="line"> </span><br><span class="line">   <span class="comment">// 1. trigger user-customizable grouping function to group the partitions into groups. 将分区进行分组</span></span><br><span class="line">   <span class="comment">// 2. assign partitions to consumers at the granularity of partition-groups. 以分区分组的粒度将分区分配给消费者</span></span><br><span class="line">   <span class="comment">// 3*. persist the assignment result using commit-offset to Kafka. 持久化分配信息</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The interface of the grouping function is the following, it is very similar to the assign() interface above, with the only difference that it does not have the consumer-lists. </p>
<p>分组函数的接口如下，它和上面的assign()接口方法很类似，唯一的区别是没有消费者列表</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">PartitionGrouper</span> </span>&#123;</span><br><span class="line">  <span class="comment">//Group partitions into partition groups</span></span><br><span class="line">  List&lt;Set&lt;TopicPartition&gt;&gt; group(Map&lt;String, Integer&gt; partitionsPerTopic);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>So after the rebalance completes, each partition-group will be assigned as a whole to the consumers, i.e. no partitions belonging to the same group will be assigned to different consumers. The default grouping function maps partitions with the same id across topics to into a group (i.e. co-partitioning). </p>
<p>在平衡完成后，每个分区分组都会作为一个整体分配给消费者，也就是说不会有一个分区，在同一个组中，但被分配给不同的消费者（同一组中的分区总是分配给同一个消费者）。默认的分组方法会在多个主题之间，将相同的分区编号映射到同一个组中（比如协调分区）。</p>
<p>4.Upon getting the partition-groups, thread creates one task for each partition-group. And for each task, constructs the processor topology AND initializes the topology with the task context.</p>
<p>在得到分区分组后，线程会为每个分区分组创建一个任务。对每个任务，都会构造处理拓扑，然后使用任务的上下文信息初始化拓扑</p>
<pre><code>a. Initialization <span class="keyword">process</span> will trigger Processor.init() <span class="keyword">on</span> each processor <span class="keyword">in</span> the topology following topology<span class="attribute">'s</span> DAG order.
b. <span class="keyword">All</span> user-specified local states will also be created during the initialization <span class="keyword">process</span> (we will talk about this later <span class="keyword">in</span> the later sections).
c. Creates the <span class="keyword">record</span> queue <span class="keyword">for</span> each one <span class="keyword">of</span> the task<span class="attribute">'s</span> associated partition-<span class="keyword">group</span><span class="attribute">'s</span> partitions, so that <span class="keyword">when</span> consumers fetches <span class="keyword">new</span> messages, it will put them into the corresponding queue.

a.初始化过程会在拓扑的每个Processor上调用rocessor.init()方法，每个Processor按照拓扑的DAG顺序依次初始化  
b.所有用户指定的本地状态也会在初始化过程中被创建  
c.为每个任务关联的分区分组的每个分区创建一个记录队列（每个分区都有一个对了），这样当消费者拉取新消息时，可以将它们放到对应的队列中
</code></pre><p>Hence all tasks’ topologies have the same “skeleton” but different processor / state instantiated objects; in addition, partitions are also synchronized at the tasks basis (we will talk about partition-group synchronization in the next section).</p>
<p>所以所有任务的拓扑都有相同的“骨架”，但是有不同的处理/状态实例化对象；另外，分区也会在任务的基础上进行同步。</p>
<p>5.When rebalance is triggered, the consumers will read its last persisted partition assignment from Kafka and checks if the following are true when comparing with the new assignment result:</p>
<p>当平衡触发时，消费者会从Kafka中读取最近持久化的分区分配，并检查下面的条件，和新的分配结果比较是否成立</p>
<pre><code><span class="operator">a</span>. Existing partitions are still assigned <span class="built_in">to</span> <span class="operator">the</span> same partition-groups.
b. New partitions are assigned <span class="built_in">to</span> <span class="operator">the</span> existing partition-groups instead <span class="operator">of</span> creating <span class="built_in">new</span> groups.
c. Partition groups are assigned <span class="built_in">to</span> <span class="operator">the</span> specific consumers instead <span class="operator">of</span> randomly / <span class="built_in">round</span>-robin.

<span class="operator">a</span>.已经存在的分区仍然分配给相同的分区组  
b.新的分区分配给已经存在的分区组，而不是创建新的组  
c.分区组分配给指定的消费者，而不是采用随机或者轮询方式
</code></pre><p>For a), since the partition-group’s associated task-id is used as the corresponding change-log partition id, if a partition gets migrated from one group to another during the rebalance, its state will be no longer valid; for b) since we cannot (yet) dynamically change the #.partitions from the consumer APIs, dynamically adding more partition-groups (hence tasks) will cause change log partitions possibly not exist yet. for c) there are some more advanced partitioning setting such as sticky-partitioning / consistent hashing that we want to support in the future, which may then require additionally.</p>
<p>对于a)，由于分区组关联的任务编号作为对应的变更日志主题的分区编号，如果在平衡式，一个分区从一组迁移到另一个组，它的状态可能不再有效；对于b），由于我们不能在消费者API中动态地修改分区数量，如果动态地添加更多的分区组（以及任务），会导致变更日志主题的分区可能还不存在；对于c），未来我们想要支持一些更加高级的分区方式比如粘性分区/一致性哈希，这可能就需要我们将分区组分配给指定的消费者。</p>
<h3 id="流时间和同步">流时间和同步</h3><p>Time in the stream processing is very important. Windowing operations (join and aggregation) are defined by time. Since Kafka can replay stream, wall-clock based time (system time) may not make sense due to delayed messages / out-of-order messages. Hence we need to define a “time” for each stream according to its progress. We call it stream time.</p>
<p>流处理中的时间非常重要。窗口函数操作（比如聚合和联合）都是通过时间定义的。由于Kafka可以重放流，基于系统时间的时钟对于延迟的、无需的消息可能没有多大意义。所以我们需要为每个流根据它的处理进度定义一个时间，这个时间叫做流的时间。</p>
<p><strong>Stream Time</strong>  </p>
<p>A stream is defined to abstract all the partitions of the same topic within a task, and its name is the same as the topic name. For example if a task’s assigned partitions are {Topic1-P1, Topic1-P2, Topic1-P3, Topic2-P1, Topic2-P2}, then we treat this task as having two streams: “Topic1” and “Topic2” where “Topic1” represents three partitions P1 P2 and P3 of Topic1, and stream “Topic2” represents two partitions P1 and P2 of Topic2.</p>
<p>一个流的定义是在一个任务中，对相同主题的所有分区的抽象，它的名称和主题的名称相同。比如一个任务分配的分区有：{Topic1-P1, Topic1-P2, Topic1-P3, Topic2-P1, Topic2-P2}，那么我们就会认为这个任务有两个流：“Topic1”和“Topic2”，其中流“Topic1”代表了Topic1主题的三个分区P1，P2和P3，而流“Topic2”代表了Topic2主题的两个分区P1和P2。</p>
<p>Each message in a stream has to have a timestamp to perform window based operations and punctuations. Since Kafka message does not have timestamp in the message header, users can define a timestamp extractor based on message content that is used in the source processor when deserializing the messages. This extractor can be as simple as always returning the current system time (or wall-clock time), or it can be an Avro decoder that gets the timestamp field specified in the record schema. </p>
<p>流中的每条消息都必须有一个时间撮，才可以执行基于窗口的操作。由于Kafka消息头中没有时间撮（在新版本中其实已经有时间撮了），用户在序列化消息时，源处理算子会基于消息内容定义一个时间撮解析器。这个解析器可以简单地返回当前的系统时间（即时钟时间），或者是一个能够从记录的Schema获取时间撮字段的Avro解码器。</p>
<p>In addition, since Kafka supports multiple producers sending message to the same topic, brokers may receive messages in order that is not strictly following their timestamps (i.e. out-of-order messages). Therefore, we cannot simply define the “stream time” as the timestamp of the currently processed message in the stream hence that time can move back and forth.</p>
<p>此外，由于Kafka支持多个生产者发送消息到同一个主题，Broker接收到消息的顺序可能并不是严格按照它们的时间撮（即乱序的消息）。因此我们不能简单地将流中当前处理过的消息的时间撮作为“流时间”，因为那个时间可能会来回地移动。</p>
<p>We can define the “stream time” as a monotonically increasing value as the following:</p>
<ol>
<li>For each assigned partition, the thread maintains a record queue for buffering the fetched records from the consumer.</li>
<li>Each message has an associated timestamp that is extracted from the timestamp extractor in the message content.</li>
<li>The partition time is defined as the lowest message timestamp value in its buffer.<br> a. When the lowest timestamp corresponding record gets processed by the thread, the partition time possibly gets advanced.<br> b. The partition time will NOT gets reset to a lower value even if a later message was put in a buffer with a even lower timestamp.</li>
<li>The stream time is defined as the lowest partition timestamp value across all its partitions in the task:<br> a. Since partition times are monotonically increasing, stream times are also monotonically increasing.</li>
<li>Any newly created streams through the upstream processors inherits the stream time of the parents; for joins, the bigger parent’s stream time is taken.</li>
</ol>
<p>我们可以定义“流时间”是一个单调递增的值：</p>
<ol>
<li>对每个分配的分区，线程维护了一个记录队列，用来缓冲从消费者拉取到的记录</li>
<li>每条消息都有一个关联的时间撮，它是从消息内容中用时间撮解析器抽取出来的</li>
<li>分区的时间会被定义为缓冲区中最低的消息时间撮<br>3.1 当最低时间撮对应的记录被线程处理后，分区的时间可能会增长<br>3.2 分区时间不会被重置为一个更低的值，即使一条迟到的消息放到缓冲区，而它的时间撮比分区时间还要低  </li>
<li>流时间被定义为任务中所有分区的最低的分区时间<br>4.1 由于分区时间是单调递增的，所以流时间也是单调递增的  </li>
<li>任何通过上游处理节点新创建的流都继承了所有父节点的流时间。对于join操作而言，会选择所有父节点中最大的流时间作为它的流时间</li>
</ol>
<p><strong>Stream Synchronization</strong></p>
<p>When joining two streams, their progress need to be synchronized. If they are out of sync, a time window based join becomes faulty. Say a delay of one stream is negligible(微不足道的) and a delay of the other stream is one day, doing join over 10 minutes window does not make sense. To handle this case, we need to make sure that the consumption rates of all partitions within each task’s assigned partition-group are “synchronized”. Note that each thread may have one or more tasks, but it does not need to synchronize the partitions across tasks’ partition-groups.</p>
<p>当联合两个流时，它们的进度需要被同步。如果它们状态不同步，基于时间窗口的join就会出错。比如一个流的延迟很小，但是另一个流的延迟有一天，在做10分钟的窗口join时就没有意义了。为了处理这种场景，我们要确保每个任务分配的分区组中所有分区的消费速率是同步的。注意：由于每个线程可能有多个任务，但是并不需要在任务的分区组之间同步。</p>
<p>Work thread synchronizes the consumption within each one of such groups through consumer’s pause / resume APIs as following:</p>
<ol>
<li>When one un-paused partition is a head of time (partition time defined as above) beyond some defined threshold with other partitions, notify the corresponding consumer to pause.</li>
<li>When one paused partition is a head of time below some defined with other partitions, notify the corresponding consumer to un-pause.</li>
</ol>
<p>工作线程同步分区组中每个分区的消费进度，是通过消费者的pause/resume API完成的：</p>
<ol>
<li>当一个还没暂停的分区比其他分区的时间（这个时间指的是分区的时间）<strong>超前</strong>定义的阈值，通知对应的消费者暂停</li>
<li>当一个暂停的分区比其他分区的时间<strong>落后</strong>定义的阈值，通知对应的消费者不要暂停（即恢复）</li>
</ol>
<p>Two streams that are joined together have to be in the same task, and their represented partition lists have to match each other. That is, for example, a stream representing P1, P2 and P3 can be joined with another stream also representing P1, P2 and P3.</p>
<p>两个join的流必须在同一个任务中，它们对应的分区列表必须互相匹配。举例一个流有P1,P2,P3三个分区，可以和另外一个也有三个分区的流进行join（如果另外一个流的分区不是3个，就无法join）。</p>
<h3 id="本地状态管理">本地状态管理</h3><p>Users can create one or more state stores during their processing logic, and each task will have a state manager that keeps an instance of each specified store inside the task. Since a single store instance will not be shared across multiple partition groups, and each partition group will only be processed by a single thread, this guarantees any store will not be accessed concurrently by multiple thread at any given time.</p>
<p>用户可以在流处理逻辑中创建一个或多个状态存储，每个任务在其任务内部都有一个状态管理器，保存了每个指定存储的实例。由于一个单一的存储不会在多个分区组中共享，而且每个分区组都只会被一个线程处理，这就保证了在任何时间，都不会有多个线程并发地访问任何的存储（所以访问存储是线程安全的）。</p>
<p><strong>Log-backed State Storage</strong>  </p>
<p>Each state store will be backed up by a different Kafka change log topic, and each instance of the store correlates to one partition of the topic, such that:</p>
<p>每个状态存储后台都是一个不同的Kafka变更日志主题，每个存储实例都会存储主题的一个分区</p>
<figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#. tasks == #. partition groups == #. store instances <span class="keyword">for</span> <span class="keyword">each</span> state store == #.partitions <span class="keyword">of</span> the change log <span class="keyword">for</span> <span class="keyword">each</span> state store</span><br></pre></td></tr></table></figure>
<p>For example, if a processor instance consumes from upstream Kafka topic “topic-A” with 4 partitions, and creates two stores, namely store1 and store2, and user groups the 4 partitions into {topic-A-p1, topic-A-p2} and {topic-A-p3, topic-A-p4}; then two change log topics, for example namely “topic-store1-changelog” and “topic-store2-changelog”, need to be created beforehand, each with two partitions.</p>
<p>比如有一个Processor实例从上游有4个分区的Kafka主题“topic-A”消费数据，并创建了两个存储即store1和store2，用户将这4个分区分成{topic-A-p1, topic-A-p2}和{topic-A-p3, topic-A-p4}，那么就需要事先创建两个主题：”topic-store1-changelog”和”topic-store2-changelog”，每个主题有两个分区。</p>
<p>After processor writes to a store instance, it first sends the change message to its corresponding changelog topic partition. When user calls commit() in his processor, KStream needs to flush both the store instance as well as the producer sending to the changelog, as well as committing the offset in the upstream Kafka. If these three operations cannot be done atomically, then if there is a crash in between this operations duplicates could be generated since the upstream Kafka committing offset is executed in the last step; if there three operations can be done atomically, then we can guarantee “exactly-once” semantics.</p>
<p>当Processor写入存储实例，它首先会将变更消息发送到对应的变更日志主题分区中。当用户在Processor中调用commit()方法时，KStream需要同时刷新存储实例、生产者发送到变更日志的消息（之前的发送不一定真正写入，只有flush时才会确保消息真正写入）、提交上游Kafka的偏移量。如果这三个操作不能以原子操作完成，那么如果在这些步骤中发生崩溃，就会生成重复的数据，因为上游的Kafka提交偏移量是在最后一步执行的（和消费者的处理类似，最后提交偏移量，只能保证至少一次，但数据可能会重复）；如果这三个步骤能原子地完成，我们就可以保证“正好一次”的语义了。</p>
<p><strong>Persisting and Restoring State</strong>  </p>
<p>When we close a KStream instance, the following steps are executed:</p>
<ol>
<li>Flush all store’s state as mentioned above.</li>
<li>Write the change log offsets for all stores into a local offset checkpoint file. The existence of the offset checkpoint file indicates if the instance was cleanly shutdown.</li>
</ol>
<p>当我们关闭一个KStream示例时，会执行下面的步骤：</p>
<ol>
<li>刷新上面提到的所有状态存储</li>
<li>将所有存储的变更日志偏移量写到本地的偏移量检查点文件中。是否存在偏移量检查点文件，可以用力爱判断实例是否关闭的很干净（如果没有，说明KStream没有被彻底关闭）。</li>
</ol>
<p>Upon (re-)starting the KStream instance:</p>
<ol>
<li>Try to read the local offset checkpoint file into memory, and delete the file afterwards.</li>
<li>Check the offset of the corresponding change log partition read from the checkpoint file.<br> a. If the offset is read successfully, load the previously flushed state and replay the change log from the read offset up to the log-end-offset.<br> b. Otherwise, do not load the previously flushed state and replay the change log from the beginning up to the log-end-offset.</li>
</ol>
<p>重启KStream实例时：</p>
<ol>
<li>尝试读取本地的偏移量检查点文件到内存中，然后删除删除这个文件</li>
<li>读取检查点文件，检查对应的变更日志分区的偏移量<br>2.1 如果成功读取了偏移量，加载之前（关闭时）刷新的状态，从读取的偏移量到日志文件的最后，重放变更日志<br>2.2 否则，不要加载之前刷新的状态，也不需要重放变更日志</li>
</ol>
<h3 id="工作流程总结">工作流程总结</h3><p>下面总结Kafka Streams处理的步骤：</p>
<p><strong>启动</strong>  </p>
<p>Upon user calling KafkaStreaming.start(), the process instance creates the worker threads given user specified #.threads. In each worker thread:</p>
<ol>
<li>Construct the producer and consumer client, extract the subscription topic names from the topology.</li>
<li>Let the consumer to subscribe to the topics and gets the assigned partitions.</li>
<li>Trigger the grouping function with the assigned partitions get the returned list of partition-groups (hence tasks) with associated ids.</li>
<li>Initialize each task by:<br> a. Creates a record queue for buffering the fetched records for each partition.<br> b. Initialize a topology instance for the task from the builder with a newly created processor context.<br> c. Initialize the state manager of the task and constructs / resumes user defined local states.  </li>
<li>Runs the loop at its own pace until notified to be shutdown: there is no synchronization between these threads. In each iteration of the loop:<br> a. Thread checks if the record queues are empty / low, and if yes calls consumer.poll(timeout) / consumer.poll(0) to re-fill the buffer.<br> b. Choose one record from the queues and process it through the processor topology.<br> c. Check if some of the processors’ punctuate functions need to be triggered, and if yes, execute the function.<br> d. Check if user calls commit() during the processing of this records; if yes commit the offset / flush the local state / flush the producer.</li>
</ol>
<p>在调用KafkaStreaming.start()时，Processor实例会创建指定数量的工作线程，每个线程中：  </p>
<ol>
<li>构造生产者和消费者客户端，从拓扑中解析订阅的主题名称</li>
<li>消费者订阅主题，并得到分配的分区</li>
<li>使用分配的分区触发分组方法，返回分区分组列表以及对应的编号（任务）</li>
<li>初始化每个任务<br>4.1 为每个分区创建一个记录队列，用来缓冲每个分区的拉取记录<br>4.2 从Builder中为任务初始化拓扑实例<br>4.3 初始化任务的状态管理器，构造或恢复用户定义的本地状态  </li>
<li>以自己的步伐运行循环，直到收到关闭的通知，在这些线程中不需要同步，在循环的每次迭代中：<br>5.1 线程检查记录队列空了或者记录很少，则调用onsumer.poll(timeout) / consumer.poll(0)重新填充缓冲区<br>5.2 从队列中选择一条记录，并将其放入处理拓扑中处理<br>5.3 检查一些Processor的punctuate方法是否需要触发，如果需要则执行函数<br>5.4 在处理记录时，检查是否可以调用commit()，如果是，则提交偏移量、刷新本地状态、刷新生产者</li>
</ol>
<blockquote>
<p>这里的Processor实例指的是拓扑中的处理节点，而不是拓扑本身，也不是指应用程序实例</p>
</blockquote>
<p><strong>关闭</strong>  </p>
<p>Upon user calling KafkaStreaming.shutdown(), the following steps are executed:</p>
<ol>
<li>Commit / flush each partition-group’s current processing state as described in the local state management section.</li>
<li>Close the embedded producer and consumer clients.</li>
</ol>
<p>当用户调用KafkaStreaming.shutdown()，会执行以下步骤：</p>
<ol>
<li>提交或刷新每个分区组的当前处理状态</li>
<li>关闭内置的生产者和消费者客户端</li>
</ol>
<p>一些重要的类：</p>
<ol>
<li>PartitionGroup: a set of partitions along with their queuing buffers and timestamp extraction logic. 分区的集合，联通它们的队列缓冲区，以及时间撮解析逻辑</li>
<li>ProcessorStateManager: the manager of the local states within a task. 任务的本地状态管理器</li>
<li>ProcessorTopology: the instance of the topology generated by the TopologyBuilder. 通过TopologyBuilder生成的拓扑实例</li>
<li>StreamTask: the task of the processing tasks unit, which include a ProcessorTopology, a ProcessorStateManager and a PartitionGroup. 处理任务的单元，包括前面三个概念</li>
<li>StreamThread: contains multiple StreamTasks, a Consumer and a Producer client. 包括多个流任务，一个生产者、消费者客户端</li>
<li>KStreamFilter/Map/Branch/…: implementations of high-level KStream topology builder operators. 实现高级KStream拓扑构造器的算子</li>
</ol>
<h1 id="KIP-67:Queryable_state_for_Kafka_Streams">KIP-67:Queryable state for Kafka Streams</h1><p>翻译：<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-67%3A+Queryable+state+for+Kafka+Streams" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/KAFKA/KIP-67%3A+Queryable+state+for+Kafka+Streams</a>  </p>
<p>Today a Kafka Streams application will implicitly create state. This state is used for storing intermediate data such as aggregation results. The state is also used to store KTable’s data when they are materialized. The problem this document addresses is that this state is hidden from application developers and they cannot access it directly. The DSL allows users to make a copy of the data (using the through operator) but this leads to a doubling in the amount of state that is kept. In addition, this leads to extra IOs to external databases/key value stores that could potentially slow down the entire pipeline. Here is a simple example that illustrates the problem:</p>
<p>一个Kafka Streams应用程序通常都会在后台隐式地创建状态。这个状态用来存储临时数据，比如聚合的结果。状态也会被用在当物化KTable时存储KTable数据。这篇文档要解决的问题是状态对于应用开发者是隐藏的，他们不能直接访问状态。DSL操作允许用户使用through操作符复制数据，但是导致了需要保存的状态数量翻倍。另外，也导致了和外部数据库/键值存储产生额外的IO开销，这可能会降低整个数据管道的响应。下面模拟了这个问题：</p>
<figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> KTable&lt;String, Long&gt; wordCounts = textLine</span><br><span class="line"><span class="number">2</span>    .flatMapValues(value<span class="function"> -&gt;</span>Arrays.asList(value.toLowerCase().split(<span class="string">"\\W+"</span>)))</span><br><span class="line"><span class="number">3</span>    .map<span class="function"><span class="params">((key, word) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(word, word)</span><br><span class="line"><span class="number">4</span>    .countByKey(<span class="string">"StoreName"</span>)</span><br><span class="line"><span class="number">5</span> wordCounts.<span class="keyword">to</span>(Serdes.String(), Serdes.Long(), <span class="string">"streams-wordcount-output"</span>);</span></span></span><br></pre></td></tr></table></figure>
<p>In line 4, the aggregation already maintains state in a store called StoreName, however that store cannot be directly queried by the developer. Instead, the developer makes a copy of the data in that store into a topic called streams-wordcount-output. Subsequently, the developer might instantiate its own database after reading the data from that topic (this step is not shown above). This is shown in illustration (a):</p>
<p>在第四行中，聚合操作维护了一个状态存储叫做“StoreName”，不过这个存储不能直接被开发者用来查询。相反，开发者会在这个存储中复制数据到一个叫做“streams-wordcount-output”的主题。然后，开发者可能会在从这个主题读到数据后，实例化自己的数据库（这里没有展示出用法）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Confluent Kafka Streams Documentation 中文翻译：  &lt;a href=&quot;http://docs.confluent.io/3.0.1/streams/introduction.html&quot;&gt;http://docs.confluent.io/3.0.1/streams/introduction.html&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
</feed>
