<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Apache Spark入门 | zqhxuyuan</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Apache Spark小白入门教程">
<meta property="og:type" content="article">
<meta property="og:title" content="Apache Spark入门">
<meta property="og:url" content="http://github.com/zqhxuyuan/2015/06/20/2015-06-20-Apache-Spark/index.html">
<meta property="og:site_name" content="zqhxuyuan">
<meta property="og:description" content="Apache Spark小白入门教程">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark1.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark2.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark3.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark4.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark5.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark7.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark6.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark8.png">
<meta property="og:updated_time" content="2015-12-19T13:24:14.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Apache Spark入门">
<meta name="twitter:description" content="Apache Spark小白入门教程">
  
    <link rel="alternative" href="/atom.xml" title="zqhxuyuan" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://avatars1.githubusercontent.com/u/1088525?v=3&amp;s=180" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">任何忧伤,都抵不过世界的美丽</a></h1>
		</hgroup>

		
				


		
			<div id="switch-btn" class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
						<li>关于我</li>
						
					</ul>
				</div>
			</div>
		

		<div id="switch-area" class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives/">归档</a></li>
				        
							<li><a href="/tags/">标签</a></li>
				        
							<li><a href="/about/">关于</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<ul class="social">
							
								<li id="新浪微博"><a class="新浪微博" target="_blank" href="http://weibo.com/xuyuantree" title="新浪微博"></a></li>
					        
								<li id="GitHub"><a class="GitHub" target="_blank" href="http://github.com/zqhxuyuan" title="GitHub"></a></li>
					        
						</ul>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/apex/" style="font-size: 10px;">apex</a> <a href="/tags/cassandra/" style="font-size: 18.57px;">cassandra</a> <a href="/tags/clojure/" style="font-size: 10px;">clojure</a> <a href="/tags/drill/" style="font-size: 17.14px;">drill</a> <a href="/tags/druid/" style="font-size: 14.29px;">druid</a> <a href="/tags/elasticsearch/" style="font-size: 10px;">elasticsearch</a> <a href="/tags/geode/" style="font-size: 10px;">geode</a> <a href="/tags/graph/" style="font-size: 14.29px;">graph</a> <a href="/tags/hbase/" style="font-size: 15.71px;">hbase</a> <a href="/tags/ignite/" style="font-size: 10px;">ignite</a> <a href="/tags/kafka/" style="font-size: 20px;">kafka</a> <a href="/tags/ops/" style="font-size: 12.86px;">ops</a> <a href="/tags/redis/" style="font-size: 11.43px;">redis</a> <a href="/tags/scala/" style="font-size: 12.86px;">scala</a> <a href="/tags/spark/" style="font-size: 14.29px;">spark</a> <a href="/tags/storm/" style="font-size: 15.71px;">storm</a> <a href="/tags/timeseries/" style="font-size: 12.86px;">timeseries</a> <a href="/tags/work/" style="font-size: 12.86px;">work</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">BIG(DATA)</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">任何忧伤,都抵不过世界的美丽</a></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<a href="/" class="profilepic">
				<img lazy-src="https://avatars1.githubusercontent.com/u/1088525?v=3&amp;s=180" class="js-avatar">
			</a>
			<hgroup>
			  <h1 class="header-author"><a href="/" title="回到主页">任何忧伤,都抵不过世界的美丽</a></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives/">归档</a></li>
		        
					<li><a href="/tags/">标签</a></li>
		        
					<li><a href="/about/">关于</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
						<ul class="social">
							
								<li id="新浪微博"><a class="新浪微博" target="_blank" href="http://weibo.com/xuyuantree" title="新浪微博"></a></li>
					        
								<li id="GitHub"><a class="GitHub" target="_blank" href="http://github.com/zqhxuyuan" title="GitHub"></a></li>
					        
						</ul>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-2015-06-20-Apache-Spark" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/06/20/2015-06-20-Apache-Spark/" class="article-date">
  	<time datetime="2015-06-19T16:00:00.000Z" itemprop="datePublished">2015-06-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Apache Spark入门
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>
	</div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <p>Apache Spark小白入门教程</p>
<a id="more"></a>
<h2 id="Spark_Shell">Spark Shell</h2><p>➜  spark-1.4.0-bin-hadoop2.6  <code>bin/spark-shell</code></p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  '_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   <span class="keyword">version</span> 1.4.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using <span class="keyword">Scala</span> <span class="keyword">version</span> 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_25)</span><br><span class="line">15/06/28 10:36:07 INFO ui.SparkUI: Started SparkUI at http:<span class="comment">//127.0.0.1:4040</span></span><br><span class="line">15/06/28 10:36:07 INFO repl.SparkILoop: Created spark context..</span><br><span class="line">Spark context available <span class="keyword">as</span> <span class="keyword">sc</span>.</span><br><span class="line">15/06/28 10:36:08 INFO hive.HiveContext: Initializing execution hive, <span class="keyword">version</span> 0.13.1</span><br><span class="line">15/06/28 10:36:23 INFO repl.SparkILoop: Created sql context (with Hive support)..</span><br><span class="line">SQL context available <span class="keyword">as</span> sqlContext.</span><br></pre></td></tr></table></figure>
<h2 id="Basic_RDD_Operation">Basic RDD Operation</h2><p>第一个例子: 统计一个文本文件的单词数量.<br>调用sc的textFile(fileName)会生成一个MapPartitionsRDD  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textFile = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"README.md"</span>)</span></span></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: <span class="function"><span class="title">ensureFreeSpace</span><span class="params">(<span class="number">63424</span>)</span></span> called with curMem=<span class="number">0</span>, maxMem=<span class="number">278019440</span></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: Block broadcast_0 stored as values <span class="keyword">in</span> memory (estimated size <span class="number">61.9</span> KB, free <span class="number">265.1</span> MB)</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: <span class="function"><span class="title">ensureFreeSpace</span><span class="params">(<span class="number">20061</span>)</span></span> called with curMem=<span class="number">63424</span>, maxMem=<span class="number">278019440</span></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: Block broadcast_0_piece0 stored as bytes <span class="keyword">in</span> memory (estimated size <span class="number">19.6</span> KB, free <span class="number">265.1</span> MB)</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.BlockManagerInfo</span>: Added broadcast_0_piece0 <span class="keyword">in</span> memory on localhost:<span class="number">58638</span> (size: <span class="number">19.6</span> KB, free: <span class="number">265.1</span> MB)</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO spark<span class="class">.SparkContext</span>: Created broadcast <span class="number">0</span> from textFile at &lt;console&gt;:<span class="number">21</span></span><br><span class="line">textFile: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br></pre></td></tr></table></figure>
<p>调用上面生成的textFile RDD的count()会触发一个Action.  </p>
<figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.count()</span><br><span class="line">java.net.ConnectException: Call <span class="keyword">From</span> hadoop/<span class="number">127.0</span>.<span class="number">0.1</span> <span class="keyword">to</span> localhost:<span class="number">9000</span> failed <span class="keyword">on</span> connection exception: java.net.ConnectException: 拒绝连接; <span class="keyword">For</span> more details see:  http:<span class="comment">//wiki.apache.org/hadoop/ConnectionRefused</span></span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native <span class="function"><span class="keyword">Method</span>)</span><br><span class="line">    ...</span><br><span class="line"><span class="title">Caused</span> <span class="title">by</span>:</span> java.net.ConnectException: 拒绝连接</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.checkConnect(Native <span class="function"><span class="keyword">Method</span>)</span><br><span class="line">	...</span></span><br></pre></td></tr></table></figure>
<p>由于本机已经安装了Hadoop,使用的是伪分布式模式,所以Spark会读取Hadoop的配置信息.<br>我们这里先不启动Hadoop,使用本地模式,要手动添加file:///并使用绝对路径读取文本文件  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile</span><br><span class="line">res1: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br></pre></td></tr></table></figure>
<p>重新构造读取本地文本文件的textFile RDD</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textFile = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"file:///home/hadoop/soft/spark-1.4.0-bin-hadoop2.6/README.md"</span>)</span></span></span><br><span class="line">textFile: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">3</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br></pre></td></tr></table></figure>
<p>触发RDD的Action: count  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.count()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">44</span>:<span class="number">07</span> INFO scheduler.DAGScheduler: Job <span class="number">0</span> finished: count at &lt;console&gt;:<span class="number">24</span>, took <span class="number">0.275609</span> s</span><br><span class="line">res2: Long = <span class="number">98</span></span><br></pre></td></tr></table></figure>
<p>又一个Action RDD : 输出文本文件的第一行  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.first()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">44</span>:<span class="number">27</span> INFO scheduler.DAGScheduler: Job <span class="number">1</span> finished: first at &lt;console&gt;:<span class="number">24</span>, took <span class="number">0.017917</span> s</span><br><span class="line">res3: String = <span class="preprocessor"># Apache Spark</span></span><br></pre></td></tr></table></figure>
<h2 id="More_RDD_Operations">More RDD Operations</h2><p>1.统计包含了Spark这个单词一共有几行</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val linesWithSpark = textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>)</span><br><span class="line">linesWithSpark: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">4</span>] at <span class="attribute">filter</span> at &lt;console&gt;:<span class="number">23</span></span><br><span class="line">scala&gt; textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>).<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<p>2.文本文件中长度最长的那一行,它一共有多少个单词</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.<span class="function"><span class="title">map</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>.size).<span class="function"><span class="title">reduce</span><span class="params">((a, b)</span></span> =&gt; <span class="keyword">if</span> (<span class="tag">a</span> &gt; b) <span class="tag">a</span> <span class="keyword">else</span> b)</span><br></pre></td></tr></table></figure>
<p>3.MapReduce WordCount</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val wordCounts = textFile.<span class="function"><span class="title">flatMap</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>).<span class="function"><span class="title">map</span><span class="params">(word =&gt; (word, <span class="number">1</span>)</span></span>).<span class="function"><span class="title">reduceByKey</span><span class="params">((a, b)</span></span> =&gt; <span class="tag">a</span> + b)</span><br><span class="line">wordCounts: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[(String, Int)] = ShuffledRDD[<span class="number">9</span>] at reduceByKey at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt;  wordCounts.<span class="function"><span class="title">collect</span><span class="params">()</span></span></span><br><span class="line">res6: Array[(String, Int)] = <span class="function"><span class="title">Array</span><span class="params">((package,<span class="number">1</span>)</span></span>, (this,<span class="number">1</span>), (Version<span class="string">"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (cluster.,1), (its,1), ([run,1), (general,2), (have,1), (pre-built,1), (locally.,1), (locally,2), (changed,1), (sc.parallelize(1,1), (only,1), (several,1), (This,2), (basic,1), (Configuration,1), (learning,,1), (documentation,3), (YARN,,1), (graph,1), (Hive,2), (first,1), (["</span>Specifying,<span class="number">1</span>), (<span class="string">"yarn-client"</span>,<span class="number">1</span>), (page](http:<span class="comment">//spark.apache.org/documentation.html),1), ([params]`.,1), (application,1), ([project,2), (prefer,1), (SparkPi,2), (&lt;http://spark.apache.org/&gt;,1), (engine,1), (version,1), (file,1), (documentation,,1), (MASTER,1), (example,3), (distribution.,1), (are,1), (params,1), (scala&gt;,1), (systems.,1...</span></span><br></pre></td></tr></table></figure>
<p>4.Cache</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  linesWithSpark.cache()</span><br><span class="line">res7: linesWithSpark.type = MapPartitionsRDD[<span class="number">4</span>] at filter at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">47</span>:<span class="number">11</span> INFO scheduler.DAGScheduler: Job <span class="number">5</span> finished: count at &lt;console&gt;:<span class="number">26</span>, took <span class="number">0.054036</span> s</span><br><span class="line">res8: Long = <span class="number">19</span></span><br><span class="line"></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">47</span>:<span class="number">14</span> INFO scheduler.DAGScheduler: Job <span class="number">6</span> finished: count at &lt;console&gt;:<span class="number">26</span>, took <span class="number">0.016638</span> s</span><br><span class="line">res9: Long = <span class="number">19</span></span><br></pre></td></tr></table></figure>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">val textFile = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"file:///home/hadoop/soft/spark-1.4.0-bin-hadoop2.6/README.md"</span>)</span></span></span><br><span class="line">〇 textFile.<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br><span class="line">① textFile.<span class="function"><span class="title">first</span><span class="params">()</span></span></span><br><span class="line">val linesWithSpark = textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>)</span><br><span class="line">② textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>).<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br><span class="line">textFile.<span class="function"><span class="title">map</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>.size).<span class="function"><span class="title">reduce</span><span class="params">((a, b)</span></span> =&gt; <span class="keyword">if</span> (<span class="tag">a</span> &gt; b) <span class="tag">a</span> <span class="keyword">else</span> b)</span><br><span class="line">③ val wordCounts = textFile.<span class="function"><span class="title">flatMap</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>).<span class="function"><span class="title">map</span><span class="params">(word =&gt; (word, <span class="number">1</span>)</span></span>).<span class="function"><span class="title">reduceByKey</span><span class="params">((a, b)</span></span> =&gt; <span class="tag">a</span> + b)</span><br><span class="line">④ wordCounts.<span class="function"><span class="title">collect</span><span class="params">()</span></span></span><br><span class="line">linesWithSpark.<span class="function"><span class="title">cache</span><span class="params">()</span></span></span><br><span class="line">⑤ linesWithSpark.<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br><span class="line">⑥ linesWithSpark.<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br><span class="line">⑦ linesWithSpark.<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<h2 id="Spark_Shell_UI">Spark Shell UI</h2><p><a href="http://127.0.0.1:4040" target="_blank" rel="external">http://127.0.0.1:4040</a></p>
<h3 id="Jobs,_Stages,_Storage">Jobs, Stages, Storage</h3><p>Jobs: 上面每个Action RDD编号对应了下图中的Job Id.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark1.png" alt=""></p>
<p>Stages: 上面有8个Job, 但是Stages多了一个. 其实是④的<code>collect</code>有两个stage  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark2.png" alt=""></p>
<p>Storage: 在Cache的时候才有</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark3.png" alt=""></p>
<h3 id="查看Stage">查看Stage</h3><p>在Jobs中点击Job Id=4的collect RDD(输出WordCount的结果). 在下方的列表中可以看到有2个Stages<br>仔细观察列表的最后面两列, 分别是Shuffle Read和Shuffle Write.<br>其中map会进行Shuffle Write, collect会进行Shuffle Read</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark4.png" alt=""></p>
<p>点击Stage Id=4的map. 它的DAG可视化图和上面的概览图的左侧是一样的</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark5.png" alt=""></p>
<p>Spark的WebUI还提供了一个EventTime,可以很清楚地看到每个阶段消耗的时间</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark7.png" alt=""></p>
<p>回退,点击Stage Id=5的collect</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark6.png" alt=""></p>
<hr>
<h2 id="Spark_Standalone_集群安装">Spark Standalone 集群安装</h2><p>准备工作:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>master无密码ssh到slaves(将master的pub追加到所有slaves的authorized_keys)</span><br><span class="line"><span class="number">2.</span>关闭所有节点的防火墙(chkconfig iptables off)</span><br><span class="line"><span class="number">3.</span>安装scala-<span class="number">2.10</span>,并设置~/.bashrc</span><br></pre></td></tr></table></figure>
<p>cd $SPARK_HOME<br>vi conf/spark-env.sh  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> JAVA_HOME=/usr/java/jdk1<span class="number">.7</span><span class="number">.0</span>_51</span><br><span class="line"><span class="keyword">export</span> SCALA_HOME=/usr/install/scala-<span class="number">2.10</span><span class="number">.5</span></span><br><span class="line"><span class="keyword">export</span> HADOOP_HOME=/usr/install/hadoop</span><br><span class="line"><span class="keyword">export</span> HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_IP=dp0652</span><br><span class="line"><span class="keyword">export</span> MASTER=spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="preprocessor">#export SPARK_LOCAL_IP=dp0652</span></span><br><span class="line"><span class="keyword">export</span> SPARK_LOCAL_DIRS=/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_WEBUI_PORT=<span class="number">8082</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_PORT=<span class="number">7077</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_CORES=<span class="number">1</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_INSTANCES=<span class="number">1</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_MEMORY=<span class="number">8</span>g</span><br></pre></td></tr></table></figure>
<p>vi conf/slaves</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">dp0652</span></span><br><span class="line">dp0653</span><br><span class="line">dp0655</span><br><span class="line">dp0656</span><br><span class="line">dp0657</span><br></pre></td></tr></table></figure>
<p>将spark目录分发到集群的其他节点</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0653:/usr/install</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0655:/usr/install</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0656:/usr/install</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0657:/usr/install</span><br></pre></td></tr></table></figure>
<p>由于集群中dp0652和dp0653的内存比较大, 我们修改了这两个节点的spark-env.sh  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> SPARK_WORKER_INSTANCES=<span class="number">2</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_MEMORY=<span class="number">20</span>g</span><br></pre></td></tr></table></figure>
<p>启动集群, 在master上启动即可.  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>]$ sbin/start-all<span class="class">.sh</span></span><br><span class="line">starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.master</span><span class="class">.Master</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.master</span><span class="class">.Master-1-dp0652</span><span class="class">.out</span></span><br><span class="line">dp0656: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0656</span><span class="class">.out</span></span><br><span class="line">dp0655: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0655</span><span class="class">.out</span></span><br><span class="line">dp0657: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0657</span><span class="class">.out</span></span><br><span class="line">dp0652: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0652</span><span class="class">.out</span></span><br><span class="line">dp0653: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0653</span><span class="class">.out</span></span><br><span class="line">dp0652: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-2-dp0652</span><span class="class">.out</span></span><br><span class="line">dp0653: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-2-dp0653</span><span class="class">.out</span></span><br></pre></td></tr></table></figure>
<p>在master和slaves上查看Spark进程</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ jps -lm</span><br><span class="line"><span class="number">40708</span> org.apache.spark.deploy.master.Master --ip dp0652 --port <span class="number">7077</span> --webui-port <span class="number">8082</span></span><br><span class="line"><span class="number">41095</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8082</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="number">40926</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ ssh dp0653</span><br><span class="line">Last login: Thu Jul  <span class="number">2</span> <span class="number">09</span>:<span class="number">07</span>:<span class="number">17</span> <span class="number">2015</span> from <span class="number">192.168</span><span class="number">.6</span><span class="number">.140</span></span><br><span class="line">[qihuang.zheng@dp0653 ~]$ jps -lm</span><br><span class="line"><span class="number">27153</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8082</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="number">27029</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line">[qihuang.zheng@dp0653 ~]$ <span class="built_in">exit</span></span><br><span class="line">logout</span><br><span class="line">Connection to dp0653 closed.</span><br><span class="line">[qihuang.zheng@dp0652 logs]$ ssh dp0655</span><br><span class="line">Last login: Thu Jul  <span class="number">2</span> <span class="number">08</span>:<span class="number">55</span>:<span class="number">05</span> <span class="number">2015</span> from <span class="number">192.168</span><span class="number">.6</span><span class="number">.140</span></span><br><span class="line">[qihuang.zheng@dp0655 ~]$ jps -lm</span><br><span class="line"><span class="number">8766</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line">[qihuang.zheng@dp0655 ~]$</span><br></pre></td></tr></table></figure>
<p>在master上查看web ui: <a href="http://dp0652:8082/" target="_blank" rel="external">http://dp0652:8082/</a></p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark8.png" alt=""></p>
<h3 id="遇到一些问题">遇到一些问题</h3><p><strong>1.如果配置了SPARK_LOCAL_IP, 但是并没有在slaves上修改为自己的IP,则会报错:</strong>  </p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">15/07/02 09:04:08 <span class="keyword">ERROR</span> netty.NettyTransport: failed to bind to /192.168.6.52:0, shutting down Netty transport</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.<span class="keyword">net</span>.BindException: Failed to bind to: /192.168.6.52:0: Service 'sparkWorker' failed after 16 retries!</span><br><span class="line">        at org.jboss.netty.<span class="keyword">bootstrap</span>.ServerBootstrap.bind(ServerBootstrap.java:272)</span><br><span class="line">        at akka.remote.transport.netty.NettyTransport$<span class="label">$anonfun</span><span class="label">$listen</span><span class="label">$1</span>.apply(NettyTransport.<span class="keyword">scala</span>:393)</span><br><span class="line">        at akka.remote.transport.netty.NettyTransport$<span class="label">$anonfun</span><span class="label">$listen</span><span class="label">$1</span>.apply(NettyTransport.<span class="keyword">scala</span>:389)</span><br><span class="line">        at <span class="keyword">scala</span>.util.Success$<span class="label">$anonfun</span><span class="label">$map</span><span class="label">$1</span>.apply(Try.<span class="keyword">scala</span>:206)</span><br><span class="line">        at <span class="keyword">scala</span>.util.Try$.apply(Try.<span class="keyword">scala</span>:161)</span><br><span class="line">        at <span class="keyword">scala</span>.util.Success.map(Try.<span class="keyword">scala</span>:206)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.Future$<span class="label">$anonfun</span><span class="label">$map</span><span class="label">$1</span>.apply(Future.<span class="keyword">scala</span>:235)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.Future$<span class="label">$anonfun</span><span class="label">$map</span><span class="label">$1</span>.apply(Future.<span class="keyword">scala</span>:235)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.impl.CallbackRunnable.<span class="keyword">run</span>(Promise.<span class="keyword">scala</span>:32)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.processBatch<span class="label">$1</span>(BatchingExecutor.<span class="keyword">scala</span>:67)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.apply<span class="label">$mcV</span><span class="label">$sp</span>(BatchingExecutor.<span class="keyword">scala</span>:82)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.apply(BatchingExecutor.<span class="keyword">scala</span>:59)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.apply(BatchingExecutor.<span class="keyword">scala</span>:59)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.BlockContext$.withBlockContext(BlockContext.<span class="keyword">scala</span>:72)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>.<span class="keyword">run</span>(BatchingExecutor.<span class="keyword">scala</span>:58)</span><br><span class="line">        at akka.dispatch.TaskInvocation.<span class="keyword">run</span>(AbstractDispatcher.<span class="keyword">scala</span>:41)</span><br><span class="line">        at akka.dispatch.ForkJoinExecutorConfigurator<span class="label">$AkkaForkJoinTask</span>.exec(AbstractDispatcher.<span class="keyword">scala</span>:393)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinPool<span class="label">$WorkQueue</span>.runTask(ForkJoinPool.java:1339)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinWorkerThread.<span class="keyword">run</span>(ForkJoinWorkerThread.java:107)</span><br><span class="line">15/07/02 09:04:09 INFO remote.RemoteActorRefProvider<span class="label">$RemotingTerminator</span>: Shutting down remote daemon.</span><br><span class="line">15/07/02 09:04:09 INFO remote.RemoteActorRefProvider<span class="label">$RemotingTerminator</span>: Remote daemon shut down; proceeding with flushing remote transports.</span><br><span class="line">15/07/02 09:04:09 INFO util.Utils: Shutdown hook called</span><br></pre></td></tr></table></figure>
<p>原因分析: SPARK_LOCAL_IP指的是本机IP地址,因此分发到集群的不同节点上,都要到各自的节点修改为自己的IP地址.<br>如果集群节点比较多,则比较麻烦, 可以用SPARK_LOCAL_DIRS代替.</p>
<p><strong>2.如果没有配置export MASTER, 在worker上会报错:</strong>  </p>
<figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">40</span>:<span class="number">51</span> INFO worker.Worker: Retrying connection <span class="keyword">to</span> master (attempt <span class="preprocessor"># 12)</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">40</span>:<span class="number">51</span> INFO worker.Worker: Connecting <span class="keyword">to</span> master akka.tcp://sparkMaster<span class="constant">@dp0652</span>:<span class="number">7077</span>/user/Master...</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">40</span>:<span class="number">51</span> WARN Remoting: Tried <span class="keyword">to</span> associate <span class="keyword">with</span> unreachable remote address [akka.tcp://sparkMaster<span class="constant">@dp0652</span>:<span class="number">7077</span>].</span><br><span class="line">Address is <span class="built_in">now</span> gated <span class="keyword">for</span> <span class="number">5000</span> ms, all messages <span class="keyword">to</span> this address will be delivered <span class="keyword">to</span> dead letters.</span><br><span class="line">Reason: 拒绝连接: dp0652/<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">7077</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">41</span>:<span class="number">23</span> ERROR worker.Worker: RECEIVED SIGNAL <span class="number">15</span>: SIGTERM</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">41</span>:<span class="number">23</span> INFO util.Utils: <span class="built_in">Shutdown</span> hook called</span><br></pre></td></tr></table></figure>
<p>导致的后果是虽然slaves上都启动了Worker进程(使用jps查看),但是在Master上并没有看到workers. 这时候应该查看Master上的日志.<br>master上启动成功显示的日志是spark@dp0652:7077. 而上面却显示的是sparkMaster@dp0652:7077. 所以应该手动export MASTER  </p>
<p><strong>3.最后成功启动集群, 在Master上的日志:</strong>  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Spark <span class="string">Command:</span> <span class="regexp">/usr/</span>java<span class="regexp">/jdk1.7.0_51/</span>bin<span class="regexp">/java -cp /</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/sbin/</span>..<span class="regexp">/conf/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/spark-assembly-1.4.0-hadoop2.6.0.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-rdbms-<span class="number">3.2</span><span class="number">.9</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/datanucleus-core-3.2.10.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-api-jdo-<span class="number">3.2</span><span class="number">.6</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span> -Xms512m -Xmx512m -<span class="string">XX:</span>MaxPermSize=<span class="number">128</span>m org.apache.spark.deploy.master.Master --ip dp0652 --port <span class="number">7077</span> --webui-port <span class="number">8082</span></span><br><span class="line">========================================</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">49</span> INFO master.<span class="string">Master:</span> Registered signal handlers <span class="keyword">for</span> [TERM, HUP, INT]</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> WARN util.<span class="string">NativeCodeLoader:</span> Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> INFO spark.<span class="string">SecurityManager:</span> Changing view acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> INFO spark.<span class="string">SecurityManager:</span> Changing modify acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> INFO spark.<span class="string">SecurityManager:</span> <span class="string">SecurityManager:</span> authentication disabled; ui acls disabled; users with view <span class="string">permissions:</span> Set(qihuang.zheng); users with modify <span class="string">permissions:</span> Set(qihuang.zheng)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO slf4j.<span class="string">Slf4jLogger:</span> Slf4jLogger started</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO <span class="string">Remoting:</span> Starting remoting</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO <span class="string">Remoting:</span> Remoting started; listening on <span class="string">addresses :</span>[akka.<span class="string">tcp:</span><span class="comment">//sparkMaster@dp0652:7077]</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'sparkMaster'</span> on port <span class="number">7077.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">Server:</span> jetty-<span class="number">8.</span>y.z-SNAPSHOT</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">AbstractConnector:</span> Started SelectChannelConnector<span class="annotation">@dp</span><span class="number">0652:</span><span class="number">6066</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO util.<span class="string">Utils:</span> Successfully started service on port <span class="number">6066.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO rest.<span class="string">StandaloneRestServer:</span> Started REST server <span class="keyword">for</span> submitting applications on port <span class="number">6066</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO master.<span class="string">Master:</span> Starting Spark master at <span class="string">spark:</span><span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO master.<span class="string">Master:</span> Running Spark version <span class="number">1.4</span><span class="number">.0</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">Server:</span> jetty-<span class="number">8.</span>y.z-SNAPSHOT</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">AbstractConnector:</span> Started SelectChannelConnector@<span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">8082</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'MasterUI'</span> on port <span class="number">8082.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO ui.<span class="string">MasterWebUI:</span> Started MasterWebUI at <span class="string">http:</span><span class="comment">//192.168.6.52:8082</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO master.<span class="string">Master:</span> I have been elected leader! New <span class="string">state:</span> ALIVE</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">35398</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">60106</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">50995</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">55994</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">34020</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">56</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">55912</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">56</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">35846</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br></pre></td></tr></table></figure>
<p><strong>在53的其中一个Worker上的日志:</strong>  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Spark <span class="string">Command:</span> <span class="regexp">/usr/</span>java<span class="regexp">/jdk1.7.0_51/</span>bin<span class="regexp">/java -cp /</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/sbin/</span>..<span class="regexp">/conf/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/spark-assembly-1.4.0-hadoop2.6.0.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-api-jdo-<span class="number">3.2</span><span class="number">.6</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/datanucleus-core-3.2.10.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-rdbms-<span class="number">3.2</span><span class="number">.9</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span> -Xms512m -Xmx512m -<span class="string">XX:</span>MaxPermSize=<span class="number">128</span>m org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> <span class="string">spark:</span><span class="comment">//dp0652:7077</span></span><br><span class="line">========================================</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO worker.<span class="string">Worker:</span> Registered signal handlers <span class="keyword">for</span> [TERM, HUP, INT]</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> WARN util.<span class="string">NativeCodeLoader:</span> Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO spark.<span class="string">SecurityManager:</span> Changing view acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO spark.<span class="string">SecurityManager:</span> Changing modify acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO spark.<span class="string">SecurityManager:</span> <span class="string">SecurityManager:</span> authentication disabled; ui acls disabled; users with view <span class="string">permissions:</span> Set(qihuang.zheng); users with modify <span class="string">permissions:</span> Set(qihuang.zheng)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">53</span> INFO slf4j.<span class="string">Slf4jLogger:</span> Slf4jLogger started</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">53</span> INFO <span class="string">Remoting:</span> Starting remoting</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO <span class="string">Remoting:</span> Remoting started; listening on <span class="string">addresses :</span>[akka.<span class="string">tcp:</span><span class="comment">//sparkWorker@192.168.6.53:55994]</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'sparkWorker'</span> on port <span class="number">55994.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Starting Spark worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">55994</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Running Spark version <span class="number">1.4</span><span class="number">.0</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Spark <span class="string">home:</span> <span class="regexp">/usr/</span>install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO server.<span class="string">Server:</span> jetty-<span class="number">8.</span>y.z-SNAPSHOT</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO server.<span class="string">AbstractConnector:</span> Started SelectChannelConnector@<span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">8081</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'WorkerUI'</span> on port <span class="number">8081.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO ui.<span class="string">WorkerWebUI:</span> Started WorkerWebUI at <span class="string">http:</span><span class="comment">//192.168.6.53:8081</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Connecting to master akka.<span class="string">tcp:</span><span class="comment">//sparkMaster@dp0652:7077/user/Master...</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Successfully registered with master <span class="string">spark:</span><span class="comment">//dp0652:7077</span></span><br></pre></td></tr></table></figure>
<h3 id="spark-shell_&amp;_spark-submit">spark-shell &amp; spark-submit</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --master spark:<span class="comment">//dp0652:7077 --executor-memory 4g</span></span><br><span class="line"></span><br><span class="line">bin/spark-submit --master spark:<span class="comment">//dp0652:7077 \</span></span><br><span class="line">  --<span class="keyword">class</span> org.apache.spark.examples.SparkPi \</span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">2</span> \</span><br><span class="line">  lib/spark-examples-<span class="number">1.4</span><span class="number">.0</span>-hadoop2<span class="number">.6</span><span class="number">.0</span>.jar <span class="number">1000</span></span><br></pre></td></tr></table></figure>

      
    </div>
    
  </div>
  
    
<div class="copyright">
  <p><span>本文标题:</span><a href="/2015/06/20/2015-06-20-Apache-Spark/">Apache Spark入门</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 任何忧伤,都抵不过世界的美丽 的个人博客">任何忧伤,都抵不过世界的美丽</a></p>
  <p><span>发布时间:</span>2015年06月20日 - 00时00分</p>
  <p><span>最后更新:</span>2015年12月19日 - 21时24分</p>
  <p>
    <span>原始链接:</span><a href="/2015/06/20/2015-06-20-Apache-Spark/" title="Apache Spark入门">http://github.com/zqhxuyuan/2015/06/20/2015-06-20-Apache-Spark/</a>
    <span class="btn" data-clipboard-text="原文: http://github.com/zqhxuyuan/2015/06/20/2015-06-20-Apache-Spark/　　作者: 任何忧伤,都抵不过世界的美丽" title="点击复制文章链接">
        <i class="fa fa-clipboard"></i>
    </span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" title="中国大陆 (CC BY-NC-SA 3.0 CN)">"署名-非商用-相同方式共享 3.0"</a> 转载请保留原文链接及作者。</p>
  <script src="/js/clipboard.min.js"></script>
  <script> var clipboard = new Clipboard('.btn'); </script>
</div>
<style type="text/css">
  .copyright p .btn {
    margin-left: 1em;
  }
  .copyright:hover p .btn::after {
    content: "复制"
  }
  .copyright p .btn:hover {
      color: gray;
      cursor: pointer;
    };
</style>



<nav id="article-nav">
  
    <div id="article-nav-newer" class="article-nav-title">
      <a href="/2015/06/25/2015-06-25-Spark-SQL/">
        Spark SQL入门
      </a>
    </div>
  
  
    <div id="article-nav-older" class="article-nav-title">
      <a href="/2015/05/01/hello-world/">
        Hello World
      </a>
    </div>
  
</nav>

  
</article>

<!-- 默认显示文章目录，在文章---前输入toc: false关闭目录 -->
<!-- Show TOC and tocButton in default, Hide TOC via putting "toc: false" before "---" at [post].md -->
<div id="toc" class="toc-article">
<strong class="toc-title">文章目录</strong>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark_Shell"><span class="toc-number">1.</span> <span class="toc-text">Spark Shell</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Basic_RDD_Operation"><span class="toc-number">2.</span> <span class="toc-text">Basic RDD Operation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#More_RDD_Operations"><span class="toc-number">3.</span> <span class="toc-text">More RDD Operations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark_Shell_UI"><span class="toc-number">4.</span> <span class="toc-text">Spark Shell UI</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Jobs,_Stages,_Storage"><span class="toc-number">4.1.</span> <span class="toc-text">Jobs, Stages, Storage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#查看Stage"><span class="toc-number">4.2.</span> <span class="toc-text">查看Stage</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark_Standalone_集群安装"><span class="toc-number">5.</span> <span class="toc-text">Spark Standalone 集群安装</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#遇到一些问题"><span class="toc-number">5.1.</span> <span class="toc-text">遇到一些问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-shell_&_spark-submit"><span class="toc-number">5.2.</span> <span class="toc-text">spark-shell & spark-submit</span></a></li></ol></li></ol>
</div>
<style type="text/css">
  .left-col .switch-btn {
    display: none;
  }
  .left-col .switch-area {
    display: none;
  }
</style>

<input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">
<script type="text/javascript">
  var toc_button= document.getElementById("tocButton");
  var toc_div= document.getElementById("toc");
  /* Show or hide toc when click on tocButton.
  通过点击设置的按钮显示或者隐藏文章目录.*/
  toc_button.onclick=function(){
  if(toc_div.style.display=="none"){
  toc_div.style.display="block";
  toc_button.value="隐藏目录";
  document.getElementById("switch-btn").style.display="none";
  document.getElementById("switch-area").style.display="none";
  }
  else{
  toc_div.style.display="none";
  toc_button.value="显示目录";
  document.getElementById("switch-btn").style.display="block";
  document.getElementById("switch-area").style.display="block";
  }
  }
    if ($(".toc").length < 1) {
        $("#toc").css("display","none");
        $("#tocButton").css("display","none");
        $(".switch-btn").css("display","block");
        $(".switch-area").css("display","block");
    }
</script>


    <style>
        .toc {
            white-space: nowrap;
            overflow-x: hidden;
        }
    </style>

    <script>
        $(document).ready(function() {
            $(".toc li a").mouseover(function() {
                var title = $(this).attr('href');
                $(this).attr("title", title);
            });
        })
    </script>




<div class="share">
	<div class="bdsharebuttonbox">
	<a href="#" class="bds_more" data-cmd="more"></a>
	<a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
	<a href="#" class="bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
	<a href="#" class="bds_copy" data-cmd="copy" title="复制网址"></a>
	<a href="#" class="bds_mail" data-cmd="mail" title="通过邮件分享"></a>
	<a href="#" class="bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
	</div>
	<script>
	window._bd_share_config={
		"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
	</script>
</div>







    <style type="text/css">
    #scroll {
      display: none;
    }
    </style>
    <div class="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
    </div>


  
  
    
    <div  class="post-nav-button">
    <a href="/2015/06/25/2015-06-25-Spark-SQL/" title="上一篇: Spark SQL入门">
    <i class="fa fa-angle-left"></i>
    </a>
    <a href="/2015/05/01/hello-world/" title="下一篇: Hello World">
    <i class="fa fa-angle-right"></i>
    </a>
    </div>
  



    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2016 任何忧伤,都抵不过世界的美丽
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的静态博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减双栏 Hexo 博客主题">Yelee</a> by MOxFIVE
        </div>
    </div>
    <div class="visit">
      <span id="busuanzi_container_site_pv" style='display:none'>
        <span id="site-visit" >本站到访数: 
        <span id="busuanzi_value_site_uv"></span>
        </span>
      </span>
      <span id="busuanzi_container_page_pv" style='display:none'>
        <span id="page-visit">, 本页阅读量: 
        <span id="busuanzi_value_page_pv"></span>
        </span>
      </span>
    </div>
  </div>
</footer>
    </div>
    

<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>

<script>
  var backgroundnum = 5;
  var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));

  $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
</script>




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
<a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
<a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>