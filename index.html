<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>任何忧伤,都抵不过世界的美丽</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="任何忧伤,都抵不过世界的美丽">
<meta property="og:url" content="http://github.com/zqhxuyuan/index.html">
<meta property="og:site_name" content="任何忧伤,都抵不过世界的美丽">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="任何忧伤,都抵不过世界的美丽">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="任何忧伤,都抵不过世界的美丽" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="/img/avatar.png" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">zqhxuyuan</a></h1>
		</hgroup>

		
				


		
			<div id="switch-btn" class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						<div class="icon-wrap icon-link hide" data-idx="2">
							<div class="loopback_l"></div>
							<div class="loopback_r"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						<li>友情链接</li>
						
						
						<li>关于我</li>
						
					</ul>
				</div>
			</div>
		

		<div id="switch-area" class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives/">归档</a></li>
				        
							<li><a href="/tags/">标签</a></li>
				        
							<li><a href="/about/">关于</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<ul class="social">
							
								<li id="新浪微博"><a class="新浪微博" target="_blank" href="http://weibo.com/xuyuantree" title="新浪微博"></a></li>
					        
								<li id="GitHub"><a class="GitHub" target="_blank" href="http://github.com/zqhxuyuan" title="GitHub"></a></li>
					        
						</ul>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/cassandra/" style="font-size: 13.33px;">cassandra</a> <a href="/tags/drill/" style="font-size: 20px;">drill</a> <a href="/tags/druid/" style="font-size: 10px;">druid</a> <a href="/tags/elasticsearch/" style="font-size: 13.33px;">elasticsearch</a> <a href="/tags/spark/" style="font-size: 16.67px;">spark</a> <a href="/tags/storm/" style="font-size: 16.67px;">storm</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
			        
			          <a target="_blank" class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
			        
			          <a target="_blank" class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
			        
			        </div>
				</section>
				

				
				
				<section class="switch-part switch-part4">
				
					<div id="js-aboutme">BIG</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">zqhxuyuan</a></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<a href="/" class="profilepic">
				<img lazy-src="/img/avatar.png" class="js-avatar">
			</a>
			<hgroup>
			  <h1 class="header-author"><a href="/" title="回到主页">zqhxuyuan</a></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives/">归档</a></li>
		        
					<li><a href="/tags/">标签</a></li>
		        
					<li><a href="/about/">关于</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
						<ul class="social">
							
								<li id="新浪微博"><a class="新浪微博" target="_blank" href="http://weibo.com/xuyuantree" title="新浪微博"></a></li>
					        
								<li id="GitHub"><a class="GitHub" target="_blank" href="http://github.com/zqhxuyuan" title="GitHub"></a></li>
					        
						</ul>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap">
  
    <article id="post-2015-12-02-Hello-Druid" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/12/02/2015-12-02-Hello-Druid/" class="article-date">
  	<time datetime="2015-12-01T16:00:00.000Z" itemprop="datePublished">2015-12-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/12/02/2015-12-02-Hello-Druid/">Druid入门</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <p><a href="http://druid.io/docs/latest/tutorials/tutorial-a-first-look-at-druid.html" target="_blank" rel="external">http://druid.io/docs/latest/tutorials/tutorial-a-first-look-at-druid.html</a>  </p>
<p>数据源是Wikipedia的编辑日志. 当这种事件发生时,会push到IRC通道. 我们的示例会实时读取IRC通道的数据写入到Druid的实时节点中.  </p>
<blockquote>
<p>Each event has a timestamp indicating the time of the edit (in UTC time),<br>a list of dimensions indicating various metadata about the event (such as information about the user editing the page and where the user is a bot),<br>and a list of metrics associated with the event (such as the number of characters added and deleted).</p>
</blockquote>
<p>事件的定义: 时间戳, 事件的元数据:<code>维度</code>, 事件关联的<code>指标</code>.<br>Dimensions (things to filter on)    参与事件过滤<br>Metrics (things to aggregate over)  要聚合的字段  </p>
<h2 id="Hello_Druid">Hello Druid</h2><p>1.启动ZooKeeper:  <code>zkServer.sh start</code>  </p>
<p>2.启动示例服务器: </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  druid-<span class="number">0.8</span>.<span class="number">2</span>  ./run_example_server<span class="class">.sh</span></span><br><span class="line">This will run <span class="tag">a</span> stand-alone version of Druid</span><br><span class="line">+ java -Xmx512m -Duser.timezone=UTC -Dfile.encoding=UTF-<span class="number">8</span> -Ddruid<span class="class">.realtime</span><span class="class">.specFile</span>=/Users/zhengqh/Soft/druid-<span class="number">0.8</span>.<span class="number">2</span>/examples/wikipedia/wikipedia_realtime<span class="class">.spec</span> </span><br><span class="line">-Ddruid<span class="class">.extensions</span><span class="class">.localRepository</span>=./extensions-repo <span class="string">'-Ddruid.extensions.remoteRepositories=[]'</span> -Ddruid<span class="class">.publish</span><span class="class">.type</span>=noop </span><br><span class="line">-classpath <span class="string">'/Users/zhengqh/Soft/druid-0.8.2/../config/realtime:/Users/zhengqh/Soft/druid-0.8.2/examples/wikipedia:/Users/zhengqh/Soft/druid-0.8.2/config/_common:/Users/zhengqh/Soft/druid-0.8.2/config/realtime:/Users/zhengqh/Soft/druid-0.8.2/lib/*'</span> </span><br><span class="line">io<span class="class">.druid</span><span class="class">.cli</span><span class="class">.Main</span> example realtime      ⬅️ 启动一个实时节点</span><br></pre></td></tr></table></figure>
<p>3.启动示例客户端:  </p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  druid-<span class="number">0.8</span><span class="number">.2</span>  ./run_example_client.sh</span><br><span class="line">This will <span class="command">run</span> a query <span class="keyword">against</span> a stand-alone <span class="property">version</span> <span class="keyword">of</span> Druid.</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">   <span class="string">"queryType"</span>:<span class="string">"timeBoundary"</span>,</span><br><span class="line">   <span class="string">"dataSource"</span>:<span class="string">"wikipedia"</span></span><br><span class="line">&#123;</span><br></pre></td></tr></table></figure>
<p>timeBoundary表示所有事件的时间边界, 下面是两次查询的结果:  </p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[ &#123;</span><br><span class="line">  "<span class="attribute">timestamp</span>" : <span class="value"><span class="string">"2015-12-02T01:40:47.555Z"</span></span>,</span><br><span class="line">  "<span class="attribute">result</span>" : <span class="value">&#123;</span><br><span class="line">    "<span class="attribute">minTime</span>" : <span class="value"><span class="string">"2015-12-02T01:40:47.555Z"</span></span>,</span><br><span class="line">    "<span class="attribute">maxTime</span>" : <span class="value"><span class="string">"2015-12-02T01:42:21.869Z"</span></span><br><span class="line">  </span>&#125;</span><br><span class="line"></span>&#125; ]</span><br><span class="line"></span><br><span class="line">[ &#123;</span><br><span class="line">  "<span class="attribute">timestamp</span>" : <span class="value"><span class="string">"2015-12-02T01:40:47.555Z"</span></span>,</span><br><span class="line">  "<span class="attribute">result</span>" : <span class="value">&#123;</span><br><span class="line">    "<span class="attribute">minTime</span>" : <span class="value"><span class="string">"2015-12-02T01:40:47.555Z"</span></span>,</span><br><span class="line">    "<span class="attribute">maxTime</span>" : <span class="value"><span class="string">"2015-12-02T01:42:52.597Z"</span></span><br><span class="line">  </span>&#125;</span><br><span class="line"></span>&#125; ]</span><br></pre></td></tr></table></figure>
<p>可以看到每次查询只会有一个timeBucket,因为默认是所有的记录都放在一个timeBucket里.   </p>
<p>4.queryType设置为timeseries, granularity时间粒度为minute,表示每分钟的数据为一个timeBucket:  </p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">➜  druid-<span class="number">0</span>.<span class="number">8.2</span>  cat timeseries.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"queryType"</span>: <span class="string">"timeseries"</span>,</span><br><span class="line">    <span class="string">"dataSource"</span>: <span class="string">"wikipedia"</span>,</span><br><span class="line">    <span class="string">"intervals"</span>: [ <span class="string">"2010-01-01/2020-01-01"</span> ],</span><br><span class="line">    <span class="string">"granularity"</span>: <span class="string">"minute"</span>,</span><br><span class="line">    <span class="string">"aggregations"</span>: [</span><br><span class="line">        &#123;<span class="string">"type"</span>: <span class="string">"longSum"</span>, <span class="string">"fieldName"</span>: <span class="string">"count"</span>, <span class="string">"name"</span>: <span class="string">"edit_count"</span>&#125;,</span><br><span class="line">        &#123;<span class="string">"type"</span>: <span class="string">"doubleSum"</span>, <span class="string">"fieldName"</span>: <span class="string">"added"</span>, <span class="string">"name"</span>: <span class="string">"chars_added"</span>&#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">➜  druid-<span class="number">0</span>.<span class="number">8.2</span>  curl -X POST <span class="string">'http://localhost:8084/druid/v2/?pretty'</span> -H <span class="string">'content-type: application/json'</span>  -d  @timeseries.json</span><br><span class="line">[ &#123;</span><br><span class="line">  <span class="string">"timestamp"</span> : <span class="string">"2015-12-02T01:40:00.000Z"</span>,</span><br><span class="line">  <span class="string">"result"</span> : &#123;</span><br><span class="line">    <span class="string">"chars_added"</span> : <span class="number">18400.0</span>,</span><br><span class="line">    <span class="string">"edit_count"</span> : <span class="number">43</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;, &#123;</span><br><span class="line">  <span class="string">"timestamp"</span> : <span class="string">"2015-12-02T01:41:00.000Z"</span>,</span><br><span class="line">  <span class="string">"result"</span> : &#123;</span><br><span class="line">    <span class="string">"chars_added"</span> : <span class="number">45920.0</span>,</span><br><span class="line">    <span class="string">"edit_count"</span> : <span class="number">153</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;, &#123;</span><br><span class="line">  <span class="string">"timestamp"</span> : <span class="string">"2015-12-02T01:42:00.000Z"</span>,</span><br><span class="line">  <span class="string">"result"</span> : &#123;</span><br><span class="line">    <span class="string">"chars_added"</span> : <span class="number">104842.0</span>,</span><br><span class="line">    <span class="string">"edit_count"</span> : <span class="number">150</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;, &#123;...</span><br></pre></td></tr></table></figure>
<p>5.topN查询: <code>select page,sum(count) as edit_count from x where country=&#39;US&#39; group by page order by edit_count desc</code> </p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">➜  druid-<span class="number">0</span>.<span class="number">8.2</span>  cat topn.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"queryType"</span>: <span class="string">"topN"</span>,</span><br><span class="line">  <span class="string">"dataSource"</span>: <span class="string">"wikipedia"</span>,</span><br><span class="line">  <span class="string">"granularity"</span>: <span class="string">"all"</span>,</span><br><span class="line">  <span class="string">"dimension"</span>: <span class="string">"page"</span>,</span><br><span class="line">  <span class="string">"metric"</span>: <span class="string">"edit_count"</span>,</span><br><span class="line">  <span class="string">"threshold"</span> : <span class="number">10</span>,</span><br><span class="line">  <span class="string">"aggregations"</span>: [</span><br><span class="line">    &#123;<span class="string">"type"</span>: <span class="string">"longSum"</span>, <span class="string">"fieldName"</span>: <span class="string">"count"</span>, <span class="string">"name"</span>: <span class="string">"edit_count"</span>&#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"filter"</span>: &#123; <span class="string">"type"</span>: <span class="string">"selector"</span>, <span class="string">"dimension"</span>: <span class="string">"country"</span>, <span class="string">"value"</span>: <span class="string">"United States"</span> &#125;,</span><br><span class="line">  <span class="string">"intervals"</span>: [<span class="string">"2012-10-01T00:00/2020-01-01T00"</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">➜  druid-<span class="number">0</span>.<span class="number">8.2</span>  curl -X POST <span class="string">'http://localhost:8084/druid/v2/?pretty'</span> -H <span class="string">'content-type: application/json'</span>  -d @topn.json</span><br><span class="line">[ &#123;</span><br><span class="line">  <span class="string">"timestamp"</span> : <span class="string">"2015-12-02T01:40:47.555Z"</span>,</span><br><span class="line">  <span class="string">"result"</span> : [ &#123;</span><br><span class="line">    <span class="string">"page"</span> : <span class="string">"Hillel_at_the_University_of_Illinois_at_Urbana-Champaign"</span>,</span><br><span class="line">    <span class="string">"edit_count"</span> : <span class="number">4</span></span><br><span class="line">  &#125;, &#123;</span><br><span class="line">    <span class="string">"page"</span> : <span class="string">"All_Together_Now"</span>,</span><br><span class="line">    <span class="string">"edit_count"</span> : <span class="number">2</span></span><br><span class="line">  &#125;, &#123;</span><br><span class="line">  .....]  //TOTAL <span class="number">10</span> RECORDS</span><br><span class="line">&#125; ]%</span><br></pre></td></tr></table></figure>
<p>5.RealTime实时节点的服务器日志:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">③ timeBoundary</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">42</span>:<span class="number">22</span>,<span class="number">513</span> INFO [timeBoundary_wikipedia_[<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z]] com.metamx.emitter.core.LoggingEmitter - Event [&#123;<span class="string">"feed"</span>:<span class="string">"metrics"</span>,<span class="string">"timestamp"</span>:<span class="string">"2015-12-02T01:42:22.512Z"</span>,<span class="string">"service"</span>:<span class="string">"realtime"</span>,<span class="string">"host"</span>:<span class="string">"localhost:8084"</span>,<span class="string">"metric"</span>:<span class="string">"query/partial/time"</span>,<span class="string">"value"</span>:<span class="number">63</span>,<span class="string">"dataSource"</span>:<span class="string">"wikipedia"</span>,<span class="string">"id"</span>:<span class="string">"b9474f46-fa0f-44a9-b4e2-5a1aba27c85c"</span>,<span class="string">"type"</span>:<span class="string">"timeBoundary"</span>&#125;]</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">42</span>:<span class="number">22</span>,<span class="number">514</span> INFO [timeBoundary_wikipedia_[<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z]] com.metamx.emitter.core.LoggingEmitter - Event [&#123;<span class="string">"feed"</span>:<span class="string">"metrics"</span>,<span class="string">"timestamp"</span>:<span class="string">"2015-12-02T01:42:22.513Z"</span>,<span class="string">"service"</span>:<span class="string">"realtime"</span>,<span class="string">"host"</span>:<span class="string">"localhost:8084"</span>,<span class="string">"metric"</span>:<span class="string">"query/wait/time"</span>,<span class="string">"value"</span>:<span class="number">18</span>,<span class="string">"dataSource"</span>:<span class="string">"wikipedia"</span>,<span class="string">"id"</span>:<span class="string">"b9474f46-fa0f-44a9-b4e2-5a1aba27c85c"</span>,<span class="string">"type"</span>:<span class="string">"timeBoundary"</span>&#125;]</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">42</span>:<span class="number">22</span>,<span class="number">622</span> INFO [qtp940621403-<span class="number">23</span>] com.metamx.emitter.core.LoggingEmitter - Event [&#123;<span class="string">"feed"</span>:<span class="string">"metrics"</span>,<span class="string">"timestamp"</span>:<span class="string">"2015-12-02T01:42:22.620Z"</span>,<span class="string">"service"</span>:<span class="string">"realtime"</span>,<span class="string">"host"</span>:<span class="string">"localhost:8084"</span>,<span class="string">"metric"</span>:<span class="string">"query/time"</span>,<span class="string">"value"</span>:<span class="number">352</span>,<span class="string">"context"</span>:<span class="string">"&#123;\"queryId\":\"b9474f46-fa0f-44a9-b4e2-5a1aba27c85c\",\"timeout\":300000&#125;"</span>,<span class="string">"dataSource"</span>:<span class="string">"wikipedia"</span>,<span class="string">"duration"</span>:<span class="string">"PT94670899200S"</span>,<span class="string">"hasFilters"</span>:<span class="string">"false"</span>,<span class="string">"id"</span>:<span class="string">"b9474f46-fa0f-44a9-b4e2-5a1aba27c85c"</span>,<span class="string">"interval"</span>:[<span class="string">"0000-01-01T00:00:00.000Z/3000-01-01T00:00:00.000Z"</span>],<span class="string">"remoteAddress"</span>:<span class="string">"127.0.0.1"</span>,<span class="string">"type"</span>:<span class="string">"timeBoundary"</span>&#125;]</span><br><span class="line"></span><br><span class="line">④ timeseries</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">45</span>:<span class="number">40</span>,<span class="number">470</span> INFO [timeseries_wikipedia_[<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z]] com.metamx.emitter.core.LoggingEmitter - Event [&#123;<span class="string">"feed"</span>:<span class="string">"metrics"</span>,<span class="string">"timestamp"</span>:<span class="string">"2015-12-02T01:45:40.470Z"</span>,<span class="string">"service"</span>:<span class="string">"realtime"</span>,<span class="string">"host"</span>:<span class="string">"localhost:8084"</span>,<span class="string">"metric"</span>:<span class="string">"query/partial/time"</span>,<span class="string">"value"</span>:<span class="number">51</span>,<span class="string">"dataSource"</span>:<span class="string">"wikipedia"</span>,<span class="string">"duration"</span>:<span class="string">"PT315532800S"</span>,<span class="string">"hasFilters"</span>:<span class="string">"false"</span>,<span class="string">"id"</span>:<span class="string">"ce112856-ce91-4ff4-ae47-06894843e2a6"</span>,<span class="string">"interval"</span>:[<span class="string">"2010-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z"</span>],<span class="string">"numComplexMetrics"</span>:<span class="string">"0"</span>,<span class="string">"numMetrics"</span>:<span class="string">"2"</span>,<span class="string">"type"</span>:<span class="string">"timeseries"</span>&#125;]</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">45</span>:<span class="number">40</span>,<span class="number">471</span> INFO [timeseries_wikipedia_[<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z]] com.metamx.emitter.core.LoggingEmitter - Event [&#123;<span class="string">"feed"</span>:<span class="string">"metrics"</span>,<span class="string">"timestamp"</span>:<span class="string">"2015-12-02T01:45:40.470Z"</span>,<span class="string">"service"</span>:<span class="string">"realtime"</span>,<span class="string">"host"</span>:<span class="string">"localhost:8084"</span>,<span class="string">"metric"</span>:<span class="string">"query/wait/time"</span>,<span class="string">"value"</span>:<span class="number">1</span>,<span class="string">"dataSource"</span>:<span class="string">"wikipedia"</span>,<span class="string">"duration"</span>:<span class="string">"PT315532800S"</span>,<span class="string">"hasFilters"</span>:<span class="string">"false"</span>,<span class="string">"id"</span>:<span class="string">"ce112856-ce91-4ff4-ae47-06894843e2a6"</span>,<span class="string">"interval"</span>:[<span class="string">"2010-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z"</span>],<span class="string">"numComplexMetrics"</span>:<span class="string">"0"</span>,<span class="string">"numMetrics"</span>:<span class="string">"2"</span>,<span class="string">"type"</span>:<span class="string">"timeseries"</span>&#125;]</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">45</span>:<span class="number">40</span>,<span class="number">476</span> INFO [qtp940621403-<span class="number">22</span>] com.metamx.emitter.core.LoggingEmitter - Event [&#123;<span class="string">"feed"</span>:<span class="string">"metrics"</span>,<span class="string">"timestamp"</span>:<span class="string">"2015-12-02T01:45:40.475Z"</span>,<span class="string">"service"</span>:<span class="string">"realtime"</span>,<span class="string">"host"</span>:<span class="string">"localhost:8084"</span>,<span class="string">"metric"</span>:<span class="string">"query/time"</span>,<span class="string">"value"</span>:<span class="number">151</span>,<span class="string">"context"</span>:<span class="string">"&#123;\"queryId\":\"ce112856-ce91-4ff4-ae47-06894843e2a6\",\"timeout\":300000&#125;"</span>,<span class="string">"dataSource"</span>:<span class="string">"wikipedia"</span>,<span class="string">"duration"</span>:<span class="string">"PT315532800S"</span>,<span class="string">"hasFilters"</span>:<span class="string">"false"</span>,<span class="string">"id"</span>:<span class="string">"ce112856-ce91-4ff4-ae47-06894843e2a6"</span>,<span class="string">"interval"</span>:[<span class="string">"2010-01-01T00:00:00.000Z/2020-01-01T00:00:00.000Z"</span>],<span class="string">"remoteAddress"</span>:<span class="string">"127.0.0.1"</span>,<span class="string">"type"</span>:<span class="string">"timeseries"</span>&#125;]</span><br><span class="line"></span><br><span class="line">内存数据持久化成Segment数据文件  </span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">52</span>,<span class="number">551</span> INFO [chief-wikipedia[<span class="number">0</span>]] io.druid.segment.realtime.plumber.RealtimePlumber - Submitting persist runnable <span class="keyword">for</span> dataSource[wikipedia]</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">52</span>,<span class="number">581</span> INFO [wikipedia-incremental-persist] io.druid.segment.realtime.plumber.RealtimePlumber - DataSource[wikipedia], Interval[<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z], Metadata [null] persisting Hydrant[FireHydrant&#123;index=io.druid.segment.incremental.OnheapIncrementalIndex@<span class="number">749297</span>a4, queryable=io.druid.segment.ReferenceCountingSegment@<span class="number">4421</span>cbc6, count=<span class="number">0</span>&#125;]</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">52</span>,<span class="number">621</span> INFO [wikipedia-incremental-persist] io.druid.guice.PropertiesModule - Loading properties from common.runtime.properties</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">52</span>,<span class="number">623</span> INFO [wikipedia-incremental-persist] io.druid.guice.PropertiesModule - Loading properties from runtime.properties</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">52</span>,<span class="number">666</span> INFO [wikipedia-incremental-persist] io.druid.segment.IndexMerger - Starting persist <span class="keyword">for</span> interval[<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">52.507</span>Z], rows[<span class="number">1</span>,<span class="number">374</span>]</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">52</span>,<span class="number">877</span> INFO [wikipedia-incremental-persist] io.druid.segment.IndexMerger - outDir[/tmp/realtime/basePersist/wikipedia/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z_2015-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">0</span>/v8-tmp] completed index.drd in <span class="number">21</span> millis.</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">53</span>,<span class="number">157</span> INFO [wikipedia-incremental-persist] io.druid.segment.IndexMerger - outDir[/tmp/realtime/basePersist/wikipedia/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z_2015-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">0</span>/v8-tmp] completed dim conversions in <span class="number">277</span> millis.</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">53</span>,<span class="number">833</span> INFO [wikipedia-incremental-persist] io.druid.segment.IndexMerger - outDir[/tmp/realtime/basePersist/wikipedia/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z_2015-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">0</span>/v8-tmp] completed walk through of <span class="number">1</span>,<span class="number">374</span> rows in <span class="number">675</span> millis.</span><br><span class="line">...</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">54</span>,<span class="number">099</span> INFO [wikipedia-incremental-persist] io.druid.segment.IndexMerger - Starting dimension[user] with cardinality[<span class="number">468</span>]</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">54</span>,<span class="number">146</span> INFO [wikipedia-incremental-persist] io.druid.segment.IndexMerger - Completed dimension[user] in <span class="number">47</span> millis.</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">54</span>,<span class="number">147</span> INFO [wikipedia-incremental-persist] io.druid.segment.IndexMerger - outDir[/tmp/realtime/basePersist/wikipedia/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z_2015-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">0</span>/v8-tmp] completed inverted.drd in <span class="number">313</span> millis.</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">49</span>:<span class="number">54</span>,<span class="number">188</span> INFO [wikipedia-incremental-persist] io.druid.segment.IndexIO$DefaultIndexIOHandler - Converting v8[/tmp/realtime/basePersist/wikipedia/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z_2015-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">0</span>/v8-tmp] to v9[/tmp/realtime/basePersist/wikipedia/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z_2015-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">⑤ topN</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">55</span>:<span class="number">04</span>,<span class="number">188</span> INFO [topN_wikipedia_[<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z]] io.druid.offheap.OffheapBufferPool - Allocating <span class="keyword">new</span> intermediate processing buffer[<span class="number">0</span>] of size[<span class="number">100</span>,<span class="number">000</span>,<span class="number">000</span>]</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">55</span>:<span class="number">04</span>,<span class="number">319</span> INFO [topN_wikipedia_[<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z]] com.metamx.emitter.core.LoggingEmitter - Event [&#123;<span class="string">"feed"</span>:<span class="string">"metrics"</span>,<span class="string">"timestamp"</span>:<span class="string">"2015-12-02T01:55:04.319Z"</span>,<span class="string">"service"</span>:<span class="string">"realtime"</span>,<span class="string">"host"</span>:<span class="string">"localhost:8084"</span>,<span class="string">"metric"</span>:<span class="string">"query/partial/time"</span>,<span class="string">"value"</span>:<span class="number">191</span>,<span class="string">"dataSource"</span>:<span class="string">"wikipedia"</span>,<span class="string">"dimension"</span>:<span class="string">"page"</span>,<span class="string">"duration"</span>:<span class="string">"PT228787200S"</span>,<span class="string">"hasFilters"</span>:<span class="string">"true"</span>,<span class="string">"id"</span>:<span class="string">"837752d5-7fa8-447f-bf21-b843827577db"</span>,<span class="string">"interval"</span>:[<span class="string">"2012-10-01T00:00:00.000Z/2020-01-01T00:00:00.000Z"</span>],<span class="string">"numComplexMetrics"</span>:<span class="string">"0"</span>,<span class="string">"numMetrics"</span>:<span class="string">"1"</span>,<span class="string">"threshold"</span>:<span class="string">"10"</span>,<span class="string">"type"</span>:<span class="string">"topN"</span>&#125;]</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">55</span>:<span class="number">04</span>,<span class="number">320</span> INFO [topN_wikipedia_[<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">00</span>:<span class="number">00.000</span>Z/<span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T02:<span class="number">00</span>:<span class="number">00.000</span>Z]] com.metamx.emitter.core.LoggingEmitter - Event [&#123;<span class="string">"feed"</span>:<span class="string">"metrics"</span>,<span class="string">"timestamp"</span>:<span class="string">"2015-12-02T01:55:04.319Z"</span>,<span class="string">"service"</span>:<span class="string">"realtime"</span>,<span class="string">"host"</span>:<span class="string">"localhost:8084"</span>,<span class="string">"metric"</span>:<span class="string">"query/wait/time"</span>,<span class="string">"value"</span>:<span class="number">1</span>,<span class="string">"dataSource"</span>:<span class="string">"wikipedia"</span>,<span class="string">"dimension"</span>:<span class="string">"page"</span>,<span class="string">"duration"</span>:<span class="string">"PT228787200S"</span>,<span class="string">"hasFilters"</span>:<span class="string">"true"</span>,<span class="string">"id"</span>:<span class="string">"837752d5-7fa8-447f-bf21-b843827577db"</span>,<span class="string">"interval"</span>:[<span class="string">"2012-10-01T00:00:00.000Z/2020-01-01T00:00:00.000Z"</span>],<span class="string">"numComplexMetrics"</span>:<span class="string">"0"</span>,<span class="string">"numMetrics"</span>:<span class="string">"1"</span>,<span class="string">"threshold"</span>:<span class="string">"10"</span>,<span class="string">"type"</span>:<span class="string">"topN"</span>&#125;]</span><br><span class="line"><span class="number">2015</span>-<span class="number">12</span>-<span class="number">02</span>T01:<span class="number">55</span>:<span class="number">04</span>,<span class="number">337</span> INFO [qtp940621403-<span class="number">26</span>] com.metamx.emitter.core.LoggingEmitter - Event [&#123;<span class="string">"feed"</span>:<span class="string">"metrics"</span>,<span class="string">"timestamp"</span>:<span class="string">"2015-12-02T01:55:04.336Z"</span>,<span class="string">"service"</span>:<span class="string">"realtime"</span>,<span class="string">"host"</span>:<span class="string">"localhost:8084"</span>,<span class="string">"metric"</span>:<span class="string">"query/time"</span>,<span class="string">"value"</span>:<span class="number">289</span>,<span class="string">"context"</span>:<span class="string">"&#123;\"queryId\":\"837752d5-7fa8-447f-bf21-b843827577db\",\"timeout\":300000&#125;"</span>,<span class="string">"dataSource"</span>:<span class="string">"wikipedia"</span>,<span class="string">"duration"</span>:<span class="string">"PT228787200S"</span>,<span class="string">"hasFilters"</span>:<span class="string">"true"</span>,<span class="string">"id"</span>:<span class="string">"837752d5-7fa8-447f-bf21-b843827577db"</span>,<span class="string">"interval"</span>:[<span class="string">"2012-10-01T00:00:00.000Z/2020-01-01T00:00:00.000Z"</span>],<span class="string">"remoteAddress"</span>:<span class="string">"127.0.0.1"</span>,<span class="string">"type"</span>:<span class="string">"topN"</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>6.总结:  </p>
<p>The most basic Druid setup: a single realtime node. We streamed in some data and queried it.<br>Realtime nodes collect very recent data and periodically hand that data off to the rest of the Druid cluster.<br>最基本的Druid设置, 单一的RealTime节点. 实时导入数据并查询这些数据. 实时节点会收集最近的数据,每隔一段时间将数据转换/保存到Druid集群中.  </p>
<p>Druid collects each individual event and packages them together in a container known as a segment.<br>Segments contain data over some span of time. 每条事件被收集并打包成Druid的segment容器,这个容器内的数据包含了一段时间的所有数据.  </p>
<hr>
<h2 id="Druid_Cluster(单机伪分布式)">Druid Cluster(单机伪分布式)</h2><p>Druid.io实时OLAP数据分析存储系统介绍: <a href="http://lxw1234.com/archives/2015/11/563.htm" target="_blank" rel="external">http://lxw1234.com/archives/2015/11/563.htm</a></p>
<p>0.依赖组件和配置文件  </p>
<p><code>config/_common/common.runtime.properties</code>文件定义了Druid外部依赖的三个组件:<br>ZooKeeper, MySQL, DeepStorage(HDFS). 在单机下, 可以设置为本地的ZK, derby, local本地文件系统.  </p>
<table>
<thead>
<tr>
<th>ConfigFile</th>
<th>Node</th>
<th>Port</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>config/coordinator/runtime.properties</code></td>
<td>协调节点</td>
<td>8081</td>
</tr>
<tr>
<td><code>config/historical/runtime.properties</code></td>
<td>历史节点</td>
<td>8083</td>
</tr>
<tr>
<td><code>config/broker/runtime.properties</code></td>
<td>Broker</td>
<td>8082</td>
</tr>
<tr>
<td><code>config/realtime/runtime.properties</code></td>
<td>实时节点</td>
<td>8084  </td>
</tr>
</tbody>
</table>
<p>历史节点和实时节点类似于Worker|DataNode,在集群环境可以有多个.<br>在集群环境下, 在每个节点都要修改配置文件的druid.host为本机的IP地址.  </p>
<p>1.启动协调节点    </p>
<p>Coordinator nodes are in charge of load assignment and distribution. 协调节点负责加载任何和分发任务.<br>Coordinator nodes monitor the status of the cluster and command historical nodes to assign and drop segments. </p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -<span class="constant">Xmx256m</span> -<span class="constant">Duser</span>.timezone=<span class="constant">UTC</span> -<span class="constant">Dfile</span>.encoding=<span class="constant">UTF</span>-<span class="number">8</span> -classpath config/<span class="symbol">_common:</span>config/<span class="symbol">coordinator:</span><span class="class"><span class="keyword">lib</span>/* <span class="title">io</span>.<span class="title">druid</span>.<span class="title">cli</span>.<span class="title">Main</span> <span class="title">server</span> <span class="title">coordinator</span></span></span><br></pre></td></tr></table></figure>
<p>2.启动历史节点  </p>
<p>Historical nodes are the workhorses of a cluster and are in charge of 历史节点会加载历史segments<br>loading historical segments and making them available for queries. 让这些segments能够被查询使用<br>Realtime nodes hand off segments to historical nodes 实时节点的数据会转换到历史节点上   </p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -<span class="constant">Xmx256m</span> -<span class="constant">Duser</span>.timezone=<span class="constant">UTC</span> -<span class="constant">Dfile</span>.encoding=<span class="constant">UTF</span>-<span class="number">8</span> -classpath config/<span class="symbol">_common:</span>config/<span class="symbol">historical:</span><span class="class"><span class="keyword">lib</span>/* <span class="title">io</span>.<span class="title">druid</span>.<span class="title">cli</span>.<span class="title">Main</span> <span class="title">server</span> <span class="title">historical</span></span></span><br></pre></td></tr></table></figure>
<p>3.启动Broker  </p>
<p>Broker nodes are responsible for figuring out which historical and/or realtime nodes correspond to which queries. 更像Cassandra中的Coordinator<br>They also merge partial results from these nodes in a scatter/gather fashion. 确定查询要分发到那个历史节点和实时节点,并聚合这些节点的数据返回给客户端.  </p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -<span class="constant">Xmx256m</span> -<span class="constant">Duser</span>.timezone=<span class="constant">UTC</span> -<span class="constant">Dfile</span>.encoding=<span class="constant">UTF</span>-<span class="number">8</span> -classpath config/<span class="symbol">_common:</span>config/<span class="symbol">broker:</span><span class="class"><span class="keyword">lib</span>/* <span class="title">io</span>.<span class="title">druid</span>.<span class="title">cli</span>.<span class="title">Main</span> <span class="title">server</span> <span class="title">broker</span></span></span><br></pre></td></tr></table></figure>
<p>4.启动实时节点  </p>
<p>因为是测试环境,将间隔时间设少点, 下面表示窗口的大小为5分钟, 每隔一分钟统计过去五分钟的数据. 每隔3分钟持久化中间数据.  </p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"segmentGranularity"</span>: <span class="string">"FIVE_MINUTE"</span>,</span><br><span class="line"><span class="string">"intermediatePersistPeriod"</span>: <span class="string">"PT3m"</span>,</span><br><span class="line"><span class="string">"windowPeriod"</span>: <span class="string">"PT1m"</span>,</span><br></pre></td></tr></table></figure>
<p>启动RealTime节点, 使用上面修改过的spec. 可以看到只有实时节点添加了specFile,因为实时节点负责读取数据源.    </p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -<span class="constant">Xmx512m</span> -<span class="constant">Duser</span>.timezone=<span class="constant">UTC</span> -<span class="constant">Dfile</span>.encoding=<span class="constant">UTF</span>-<span class="number">8</span> -<span class="constant">Ddruid</span>.realtime.specFile=examples/wikipedia/wikipedia_realtime.spec -classpath config/<span class="symbol">_common:</span>config/<span class="symbol">realtime:</span><span class="class"><span class="keyword">lib</span>/* <span class="title">io</span>.<span class="title">druid</span>.<span class="title">cli</span>.<span class="title">Main</span> <span class="title">server</span> <span class="title">realtime</span></span></span><br></pre></td></tr></table></figure>
<p>5.All Things Up</p>
<p>Once the real-time node starts up, it should begin ingesting data and handing that data off to the rest of the Druid cluster.<br>You can use a web UI located at coordinator_ip:port to view the status of data being loaded.<br>Once data is handed off from the real-time nodes to historical nodes, the historical nodes should begin serving segments.<br>一旦启动了实时节点,数据就会流入并转换到Druid的集群中. 当数据从实时节点转换到历史节点,历史节点就可以用其上的segments服务于查询了.<br>当然数据在实时节点的时候是存在于实时节点的内存上, 当然查询也是没有问题的. 数据转换到历史节点是因为这些数据不再是实时数据了而是历史数据.      </p>
<p>6.客户端查询</p>
<p>向Broker(8082)发起查询请求, Broker会同时向实时节点和历史节点发起请求.  </p>
<p>curl -X POST ‘<a href="http://localhost:8082/druid/v2/?pretty" target="_blank" rel="external">http://localhost:8082/druid/v2/?pretty</a>‘ -H ‘content-type: application/json’ -d@examples/wikipedia/query.body</p>
<p>如果一开始就向历史节点(8083)发起查询请求,当实时节点还没运行足够长时间,不会转换数据到历史节点. 所以下面的查询刚开始没有数据<br>因为窗口大小为5分钟,所以实时节点至少五分钟后才会转换数据到历史节点(虽然持久化间隔为3分钟,但第一次还是需要5分钟的窗口填满)  </p>
<p>curl -X POST ‘<a href="http://localhost:8083/druid/v2/?pretty" target="_blank" rel="external">http://localhost:8083/druid/v2/?pretty</a>‘ -H ‘content-type: application/json’ -d@examples/wikipedia/query.body</p>
<p>实时节点(8084)的内存中保存有最近(索引过后)的数据,并且这部分数据还没转存到历史节点. 当历史节点回馈说它已经完全加载了segment:这里面包括了两个步骤<br>A.实时节点数据转存到历史节点, B.历史节点加载Segment到内存中. 同时实时节点就会丢弃这个segment. 因为只需要有一份数据存在即可.  </p>
<p>curl -X POST ‘<a href="http://localhost:8084/druid/v2/?pretty" target="_blank" rel="external">http://localhost:8084/druid/v2/?pretty</a>‘ -H ‘content-type: application/json’ -d@examples/wikipedia/query.body</p>
<p>如果你仅仅是需要查询数据, 只需要向Broker发起请求即可, 因为Broker会向实时节点和历史节点发起请求并将它们的结果合并起来返回给客户端.  </p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/druid/">druid</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-11-29-StreamCQL-application" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/11/29/2015-11-29-StreamCQL-application/" class="article-date">
  	<time datetime="2015-11-28T16:00:00.000Z" itemprop="datePublished">2015-11-29</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/29/2015-11-29-StreamCQL-application/">StreamCQL源码阅读(4) 应用程序执行</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <a id="more"></a>
<h2 id="前戏:_CQL代码结构">前戏: CQL代码结构</h2><p>之前我们并没有梳理CQL部分的代码结构, 在分析了差不多的代码之后, 来看看每个部分都一一对应:  </p>
<p><img src="http://img.blog.csdn.net/20151130093405880" alt="cql"></p>
<p>还没有涉及的包括: PhysicalPlan,物理计划/逻辑计划优化器,executors执行器.    </p>
<h2 id="正文:_submitApplication">正文: submitApplication</h2><p>历经千辛万苦, 终于回到SubmitTask的submitApplication, 创建物理计划Executor,并执行Application.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">submitApplication</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">new</span> PhysicalPlanExecutor().execute(context.getApp());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="api-Application_-&gt;_application-Application">api.Application -&gt; application.Application</h3><p>api的Application是流处理执行计划应用程序, 封装的是CQL语句构建而成的应用程序:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;                      </span><br><span class="line">    <span class="keyword">private</span> String applicationId = <span class="keyword">null</span>;                    <span class="comment">//应用id</span></span><br><span class="line">    <span class="keyword">private</span> String applicationName = <span class="keyword">null</span>;                  <span class="comment">//应用名称</span></span><br><span class="line">    <span class="keyword">private</span> TreeMap&lt;String, String&gt; confs;                  <span class="comment">//整个应用程序中用到的配置属性,也包含用户自定义的配置属性</span></span><br><span class="line">    <span class="keyword">private</span> String[] userFiles;                             <span class="comment">//用户自定义添加的一些文件</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;UserFunction&gt; userFunctions;               <span class="comment">//用户自定义的函数,udf和udaf都在这个里面</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;Schema&gt; schemas = <span class="keyword">new</span> ArrayList&lt;Schema&gt;(); <span class="comment">//执行计划中的所有的schema</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;Operator&gt; operators = <span class="keyword">null</span>;                <span class="comment">//执行计划中所有的操作,包含输入、输出和计算操作      </span></span><br><span class="line">    <span class="keyword">private</span> List&lt;OperatorTransition&gt; opTransition = <span class="keyword">null</span>;   <span class="comment">//整个执行计划中所有的连接线，定义了operator之间的连接关系</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>还记得上一篇中在buildApplication时,由SplitContext拆分算子,组合算子会把operators和transitions都设置到Application里吗?  </p>
</blockquote>
<p>application.Application针对Schema和Operator采用Manager管理类(实际上底层的存储结构都是Map)来操作:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String appName;                 <span class="comment">//应用程序名称</span></span><br><span class="line">    <span class="keyword">private</span> EventTypeMng streamSchema;      <span class="comment">//所有Schema集合</span></span><br><span class="line">    <span class="keyword">private</span> OperatorMng operatorManager;    <span class="comment">//算子集合</span></span><br><span class="line">    <span class="keyword">private</span> StreamingConfig conf;           <span class="comment">//系统级别的配置属性</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Application在<code>API</code>和<code>物理计划</code>是不同的对象, 同样API阶段的<code>Operator</code>在物理计划中对应的是<code>IRichOperator</code>.  </p>
<h4 id="IRichOperator">IRichOperator</h4><p>OperatorMng管理的算子包括输入算子(addInputStream),输出算子(addOutputStream),功能算子(addFunctionStream).  </p>
<blockquote>
<p>虽然两个Operator处于不同的阶段, 但是总的来说都可以分为输入,输出和Function.  </p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">① application ⬇️                                            ② api ⬇️</span><br><span class="line">IRichOperator (com.huawei.streaming.operator)               Operator (com.huawei.streaming.api.opereators)</span><br><span class="line">    |-- AbsOperator                                             |-- FunctionStreamOperator</span><br><span class="line">        |-- FunctionOperator                                    |-- InnerOutputSourceOperator      </span><br><span class="line">            |-- JoinFunctionOp                                  |-- SplitterOperator</span><br><span class="line">                |-- DataSourceFunctionOp                        |-- OutputStreamOperator</span><br><span class="line">            |-- AggFunctionOp                                   |-- InputStreamOperator</span><br><span class="line">            |-- SplitOp                                         |-- InnerInputSourceOperator</span><br><span class="line">            |-- UnionFunctionOp                                 |-- InnerFunctionOperator</span><br><span class="line">            |-- SelfJoinFunctionOp                                      |-- FunctorOperator</span><br><span class="line">            |-- FunctorOp                                               |-- UnionOperator</span><br><span class="line">            |-- FilterFunctionOp                                        |-- FilterOperator</span><br><span class="line">        |-- OutputOperator                                              |-- BasicAggFunctionOperator</span><br><span class="line">        |-- FunctionStreamOperator                                              |-- JoinFunctionOperator</span><br><span class="line">        |-- InputOperator                                                       |-- AggregateOperator</span><br><span class="line">                                                                                |-- BaseDataSourceOperator</span><br></pre></td></tr></table></figure>
<p><code>IRichOperator流处理算子</code>基本接口: 所有的流处理相关的算子实现，都来源于这个算子, 所有的外部Storm实现均依赖于这个接口  </p>
<p>正常的CQL insert语句只有一个输出,所以在前面的SplitContext中会有一个outputStreamName和一系列的operators和transitions.<br>但是这里对于IRichOperator流算子, 它是构成Storm的Topology组件,就必须考虑算子之间数据的流动,一个Bolt可能有多个输入和输出.    </p>
<p><img src="http://img.blog.csdn.net/20151201165952194" alt="stream_inout"></p>
<blockquote>
<p>对于一个算子而言,输入数据可以有多个.但是输出是只有一个! 这就好比最终的select只会有一个输出schema: select输出的数据作为算子的输出.<br>所以IRichOperator的输入getInputStream和getInputSchema都是集合, 而输出的getOutputStream和getOutputSchema都是单一的.<br>Update: 输入输出这里其实看流，反正是一个流一个名称。都是允许多输入多输出的。一个算子就是一个流，一个算子的多个实例都属于一个流。  </p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">IRichOperator</span> <span class="keyword">extends</span> <span class="title">IOperator</span>, <span class="title">Configurable</span></span>&#123;     <span class="comment">//① 流处理算子基本接口</span></span><br><span class="line">    <span class="function">String <span class="title">getOperatorId</span><span class="params">()</span></span>;                     <span class="comment">//获取算子id</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getParallelNumber</span><span class="params">()</span></span>;                    <span class="comment">//获取算子并发度</span></span><br><span class="line">    <span class="function">List&lt;String&gt; <span class="title">getInputStream</span><span class="params">()</span></span>;              <span class="comment">//获取输入流名称, 多个输入流</span></span><br><span class="line">    <span class="function">String <span class="title">getOutputStream</span><span class="params">()</span></span>;                   <span class="comment">//获取输出流名称</span></span><br><span class="line">    Map&lt;String, IEventType&gt; getInputSchema();   <span class="comment">//获取输入schema  ⬅️ &lt;key是输入流名称,IEventType是输入流的Schema&gt;</span></span><br><span class="line">    <span class="function">IEventType <span class="title">getOutputSchema</span><span class="params">()</span></span>;               <span class="comment">//获取输出schema  ⬅️ </span></span><br><span class="line">    Map&lt;String, GroupInfo&gt; getGroupInfo();      <span class="comment">//获取分组信息</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Operator</span> </span>&#123;                         <span class="comment">// ② 每个计算单元成为一个Operator,定义了各类操作.分为Source和InnerFunction</span></span><br><span class="line">    <span class="keyword">private</span> String id;                          <span class="comment">//算子ID</span></span><br><span class="line">    <span class="keyword">private</span> String name;                        <span class="comment">//算子名称</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> parallelNumber;                 <span class="comment">//并行度</span></span><br><span class="line">    <span class="keyword">private</span> TreeMap&lt;String, String&gt; args;       <span class="comment">//每个operator的参数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OperatorTransition</span> </span>&#123;               <span class="comment">// ③ 各种Operator的连接关系,定义了从一个operator到另外一个operator的连接</span></span><br><span class="line">    <span class="keyword">private</span> String id;                          <span class="comment">//当前连接的id</span></span><br><span class="line">    <span class="keyword">private</span> String streamName;                  <span class="comment">//流名称</span></span><br><span class="line">    <span class="keyword">private</span> String fromOperatorId;              <span class="comment">//发起连接的Operator id</span></span><br><span class="line">    <span class="keyword">private</span> String toOperatorId;                <span class="comment">//接收连接的Operator id</span></span><br><span class="line">    <span class="keyword">private</span> DistributeType distributedType;     <span class="comment">//数据获取类型,仅仅在非sourceOperator中存在</span></span><br><span class="line">    <span class="keyword">private</span> String distributedFields;           <span class="comment">//数据分发字段, 仅在distributedType为field的时候生效</span></span><br><span class="line">    <span class="keyword">private</span> String schemaName;                  <span class="comment">//流上进行数据传输的时候的schema名称</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>还记得上一章在创建Transition时会指定分组字段和分组策略吗? 和这里的GroupBy应该是会有点血缘关系的. 当然算子还少不了输入输出Schema.   </p>
</blockquote>
<h4 id="PhysicalPlanExecutor_-&gt;_ExecutorPlanGenerator">PhysicalPlanExecutor -&gt; ExecutorPlanGenerator</h4><p>PhysicalPlanExecutor.execute传入的是API的Application, 而submit(app)的app是application.Application.<br>怎么转换: 通过ExecutorPlanGenerator物理计划生成器生成可执行的计划. 可执行指的是可以运行在Storm引擎.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Application apiApplication)</span> </span>&#123;</span><br><span class="line">    LOG.info(<span class="string">"start to execute application &#123;&#125;"</span>, apiApplication.getApplicationId());        </span><br><span class="line">    parseUserDefineds(apiApplication, isStartFromDriver);   <span class="comment">//① 准备工作</span></span><br><span class="line">    com.huawei.streaming.application.Application app = generatorPlan(apiApplication);   <span class="comment">// ⬅️ API.App-&gt;application.App</span></span><br><span class="line">    submit(app);                    <span class="comment">//④ 提交Application</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//由物理执行计划api.Application生成可执行计划application.Application(最终生成的，可以提交的应用程序)</span></span><br><span class="line"><span class="keyword">private</span> com.huawei.streaming.application.<span class="function">Application <span class="title">generatorPlan</span><span class="params">(Application apiApplication)</span> </span>&#123;</span><br><span class="line">    preExecute(apiApplication);     <span class="comment">//执行器执行之前的钩子</span></span><br><span class="line">    <span class="keyword">new</span> PhysicPlanChecker().check(apiApplication);</span><br><span class="line">    <span class="comment">//② 用户自定义的处理: 执行计划的组装, 构建application, 表达式的解析被延迟到这里来实现</span></span><br><span class="line">    com.huawei.streaming.application.Application app = generator.generate(apiApplication);  </span><br><span class="line">    preSubmit(app);                 <span class="comment">//提交执行计划之前的钩子</span></span><br><span class="line">    executorChecker.check(app);     <span class="comment">//③ 执行计划检查</span></span><br><span class="line">    <span class="keyword">return</span> app;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>①准备工作<code>parseUserDefineds</code>: 注册jar包,注册函数,打包等(类似storm中需要上传jar包).    </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">24</span> | INFO  | [main] | 👉 start to execute application example | com.huawei.streaming.cql.executor.PhysicalPlanExecutor (PhysicalPlanExecutor.java:<span class="number">127</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">25</span> | INFO  | [main] | start to unzip jar stream-storm-<span class="number">1.0</span>-jar-with-dependencies.jar | com.huawei.streaming.cql.executor.mergeuserdefinds.JarExpander (JarExpander.java:<span class="number">79</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">25</span> | INFO  | [main] | unzip jar /<span class="keyword">private</span>/var/folders/xc/x0b8crk9667ddh1zhfs29_zr0000gn/T/da6d53114b1f49458c0e6329553b1ff9/stream-storm-<span class="number">1.0</span>-jar-with-dependencies.jar to /<span class="keyword">private</span>/var/folders/xc/x0b8crk9667ddh1zhfs29_zr0000gn/T/da6d53114b1f49458c0e6329553b1ff9/jartmp | com.huawei.streaming.cql.executor.mergeuserdefinds.JarExpander (JarExpander.java:<span class="number">91</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">30</span> | INFO  | [main] | finished to unzip jar to dir | com.huawei.streaming.cql.executor.mergeuserdefinds.JarExpander (JarExpander.java:<span class="number">84</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">30</span> | INFO  | [main] | start to copy ch | com.huawei.streaming.cql.executor.mergeuserdefinds.JarFilesMerger (JarFilesMerger.java:<span class="number">82</span>)</span><br><span class="line">...</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">38</span> | INFO  | [main] | finished to package jar | com.huawei.streaming.cql.executor.mergeuserdefinds.JarPacker (JarPacker.java:<span class="number">68</span>)</span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | 👉 start to generator executor application <span class="keyword">for</span> app example | com.huawei.streaming.cql.executor.ExecutorPlanGenerator (ExecutorPlanGenerator.java:<span class="number">102</span>)</span><br></pre></td></tr></table></figure>
<p>生成的可执行计划会: ①设置系统配置参数, 解析Application中的②<code>Schema</code>和③<code>Operators</code>, 最终返回的是可执行的Application.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> com.huawei.streaming.application.<span class="function">Application <span class="title">generate</span><span class="params">(Application vap)</span> </span>&#123;</span><br><span class="line">    LOG.info(<span class="string">"start to generator executor application for app "</span> + vap.getApplicationId());</span><br><span class="line">    apiApplication = vap;           <span class="comment">//这个是API的Application</span></span><br><span class="line">    createEmptyApplication(vap.getApplicationId());</span><br><span class="line">    parseUserDefineds(vap);         <span class="comment">//① 用户自定义的处理(系统配置参数)</span></span><br><span class="line">    parseSchemas();                 <span class="comment">//② 解析所有的Schema，构建schema信息</span></span><br><span class="line">    parseOperators();               <span class="comment">//③ 解析所有的Operator,构建OperatorInfo. 整理Operator中的上下级关系</span></span><br><span class="line">    <span class="keyword">return</span> executorApp;             <span class="comment">//返回的是可执行的Application</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="parseSchemas:_Schema_-&gt;_TupleEventType">parseSchemas: Schema -&gt; TupleEventType</h3><p>在第一篇中Topology的CQL语句:  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">INPUT</span> STREAM s(<span class="keyword">id</span> <span class="built_in">INT</span>, <span class="keyword">name</span> <span class="keyword">STRING</span>, <span class="keyword">type</span> <span class="built_in">INT</span>) <span class="keyword">SOURCE</span> randomgen PROPERTIES ( timeUnit = <span class="string">"SECONDS"</span>, <span class="keyword">period</span> = <span class="string">"1"</span>, eventNumPerperiod = <span class="string">"1"</span>, isSchedule = <span class="string">"true"</span> );</span></span><br><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">OUTPUT</span> STREAM rs(<span class="keyword">type</span> <span class="built_in">INT</span>, cc <span class="built_in">INT</span>) SINK consoleOutput;</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM rs               #<span class="keyword">INSERT</span> <span class="keyword">STATEMENT</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">type</span>, <span class="keyword">COUNT</span>(<span class="keyword">id</span>) <span class="keyword">as</span> cc        #<span class="keyword">SELECT</span> <span class="keyword">STATEMENT</span>      </span><br><span class="line"><span class="keyword">FROM</span> s[<span class="keyword">RANGE</span> <span class="number">20</span> SECONDS BATCH]      #<span class="keyword">FROM</span> CLAUSE</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">id</span> &gt; <span class="number">5</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">type</span>;</span>         #WHERE CLAUSE/GROUPBY CLAUSE</span><br></pre></td></tr></table></figure>
<p>上面有两个流,分别是输入流s, 输出流rs. 因此会将这两个Schema添加到可执行Application中.       </p>
<blockquote>
<p>对于输入输出而言,最重要的就是Schema了, 在输入和输出这一对双胞胎眼中,数据进来和出去的格式非常重要.<br>因为外部数据进来,和暴露数据给外部,最重要的是格式. 你中间不管怎么处理,它们都不管. Only Schema! Give Me the Data!  </p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | AddEventType enter, the eventtypeName is:s. | com.huawei.streaming.event.EventTypeMng (EventTypeMng.java:<span class="number">73</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | AddEventType enter, the eventtypeName is:rs. | com.huawei.streaming.event.EventTypeMng (EventTypeMng.java:<span class="number">73</span>)</span><br></pre></td></tr></table></figure>
<p>parseSchemaToIEvent会将API的Application中所有的Schema都转换为IEvent事件:TupleEventType.<br>Schema中的Column对应TupleEventType的Attribute. Schema的streamName/id对应了下面的name/事件类型.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TupleEventType</span> <span class="keyword">implements</span> <span class="title">IEventType</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;                                <span class="comment">//schemaName,表名</span></span><br><span class="line">    <span class="keyword">private</span> Attribute[] schema;                         <span class="comment">//schemas, 所有列</span></span><br><span class="line">    <span class="keyword">private</span> String[] attNames;                          <span class="comment">//所有列的列名</span></span><br><span class="line">    <span class="keyword">private</span> Class&lt; ? &gt;[] attTypes;                      <span class="comment">//所有列的列类型</span></span><br><span class="line">    <span class="keyword">private</span> HashMap&lt;String, Integer&gt; attid;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Schema的管理类用Map结构保存schemaName/eventTypeName和对应的Schema/TupleEventType: 表名-&gt;表结构.   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EventTypeMng</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, IEventType&gt; schemas;            <span class="comment">//MAP: 数据类型名称 =&gt; 具体数据类型</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addEventType</span><span class="params">(IEventType schema)</span> </span>&#123;       <span class="comment">//⬅️ executorApp.addEventSchema(tupleEventType)</span></span><br><span class="line">        schemas.put(schema.getEventTypeName(), schema); <span class="comment">//数据类型|事件类型|表名schemaName|streamName流名称</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="parseOperators:_算子解析">parseOperators: 算子解析</h3><blockquote>
<p>前面第一篇的时候submit之前每条CQL语句都有start to parse cql过一次了,这里为什么还会再次parse?<br>答: 前面只是LazyTask懒解析,其实还是没有开始的. 那为什么要在这里才开始? 因为解析完schema后, 就该轮到operator的解析了.<br>不过有一点不同的是, 最开始的parse是对整个CQL语句, 这里只解析了部分, 比如表达式,groupby,聚合.  <code>WHYYY?</code></p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">解析算子: <span class="number">1</span>)解析二元表达式(属性值表达式) id &gt; <span class="number">5</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | start to parse cql : (s.id &gt; <span class="number">5</span>) | com.huawei.streaming.cql.semanticanalyzer.parser.ApplicationParser (ApplicationParser.java:<span class="number">44</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | Parse Completed | com.huawei.streaming.cql.semanticanalyzer.parser.ApplicationParser (ApplicationParser.java:<span class="number">69</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | start to create binary Expressions. | com.huawei.streaming.cql.executor.expressioncreater.PropertyValueExpressionCreator (BinaryExpressionCreator.java:<span class="number">54</span>)  ⬅️</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | Parse Completed, cql : s.type,  count( s.id )  | com.huawei.streaming.cql.semanticanalyzer.parser.SelectClauseParser (SelectClauseParser.java:<span class="number">68</span>)</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>)解析 group by</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | start to parse cql : s.type | com.huawei.streaming.cql.semanticanalyzer.parser.GroupbyClauseParser (GroupbyClauseParser.java:<span class="number">45</span>)   ⬅️</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | Parse Completed | com.huawei.streaming.cql.semanticanalyzer.parser.GroupbyClauseParser (GroupbyClauseParser.java:<span class="number">68</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | start to parse cql : s.type | com.huawei.streaming.cql.semanticanalyzer.parser.GroupbyClauseParser (GroupbyClauseParser.java:<span class="number">45</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | Parse Completed | com.huawei.streaming.cql.semanticanalyzer.parser.GroupbyClauseParser (GroupbyClauseParser.java:<span class="number">68</span>)</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>)聚合算子 count(id)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | start to create aggregate service | com.huawei.streaming.cql.executor.operatorviewscreater.AggregateServiceViewCreator (AggregateServiceViewCreator.java:<span class="number">89</span>)  ⬅️</span><br></pre></td></tr></table></figure>
<p>不仅仅是Driver.run调用了ApplicationParser.parse. 从调用树还能看到有XXXInfoCreator,ViewCreator.(<code>还记得聚合也是一种查询吗</code>)</p>
<p><img src="http://img.blog.csdn.net/20151130172800418" alt="stream-opinfo"></p>
<p>算子解析: 这里的解析是为了使得输入和输出算子统一，避免用户自定义和系统内置的算子对外表现不一致处理起来的麻烦<br>由于输入和输出算子中存在特例，即针对文件，tcp，kafka等编写了特例, 所以需要①首先将他们抽象化，之后再来处理  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">parseOperators</span><span class="params">()</span></span>&#123;</span><br><span class="line">    Map&lt;String, Operator&gt; opts = formatOperators();                     <span class="comment">//① 输入输出算子抽象化</span></span><br><span class="line">    Map&lt;String, AbsOperator&gt; opMappings = createOperatorInfos(opts);    <span class="comment">//② 算子解析 ⬅️  Operator -&gt; AbsOperator</span></span><br><span class="line">    combineOperators(opMappings);                                       <span class="comment">//③ 整理算子顺序</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (Entry&lt;String, AbsOperator&gt; et : opMappings.entrySet())&#123;        <span class="comment">//④ 添加算子到Application</span></span><br><span class="line">        IRichOperator operator = et.getValue();</span><br><span class="line">        <span class="comment">//如果没有输入，也算是input</span></span><br><span class="line">        <span class="keyword">if</span> (operator <span class="keyword">instanceof</span> InputOperator)&#123;</span><br><span class="line">            executorApp.addInputStream(operator);</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//如果没有输出，也算是output</span></span><br><span class="line">        <span class="keyword">if</span> (operator <span class="keyword">instanceof</span> OutputOperator)&#123;</span><br><span class="line">            executorApp.addOutputStream(operator);</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//不是输入输出,就是功能算子</span></span><br><span class="line">        executorApp.addFunctionStream(operator);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>② Operator是算子, AbsOperator则是流处理算子(继承IRichOperator).它们的转换由createOperatorInfos-&gt;OperatorInfoCreatorFactory完成.<br>④ 和Schema的管理一样,算子的管理类OperatorMng用三个Map分别管理输入,输出,功能算子. Map的key是operatorId,value是Operator算子本身.   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OperatorMng</span></span>&#123;    </span><br><span class="line">    <span class="keyword">private</span> List&lt;IRichOperator&gt; sortedFunctions;        <span class="comment">//DFG排序后的功能算子列表，作为创建Storm拓扑顺序的基础(输出和功能算子组成--&gt;Bolt)</span></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, IRichOperator&gt; inputs;          <span class="comment">//输入算子 --&gt; Spout</span></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, IRichOperator&gt; functions;       <span class="comment">//功能算子 --&gt; Bolt</span></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, IRichOperator&gt; outputs;         <span class="comment">//输出算子 --&gt; Bolt</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>上面parseOperators的两个Map:opts和opMapping以及OperatorMng的三个Map的Key,Value都是operatorID-&gt;AbsOperator实例.  </p>
</blockquote>
<h4 id="createOperatorInfos">createOperatorInfos</h4><p>operators是API中的算子, 要转换为可执行计划对应的算子, 通过OperatorInfoCreatorFactory工厂类根据operator上的注解<br>先取得OperatorInfoCreator的具体实现类,再调用createInstance, 在具体的OperatorInfoCreator实现类中才完成转换.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Map&lt;String, AbsOperator&gt; createOperatorInfos(Map&lt;String, Operator&gt; operators) &#123;</span><br><span class="line">    Map&lt;String, AbsOperator&gt; opMappings = Maps.newHashMap();</span><br><span class="line">    <span class="keyword">for</span> (Operator op : operators.values()) &#123;</span><br><span class="line">        AbsOperator opinfo = createOperatorInfo(op);</span><br><span class="line">        opMappings.put(opinfo.getOperatorId(), opinfo);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> opMappings;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> AbsOperator <span class="title">createOperatorInfo</span><span class="params">(Operator operator)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> OperatorInfoCreatorFactory.createOperatorInfo(apiApplication, operator, executorApp.getStreamSchema(), <span class="keyword">this</span>.systemConfig);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>假设Operator是AggregateOperator,它的注解是AggregaterInfoCreator.所以最终调用的是AggregaterInfoCreator.createInstance创建AggFunctionOp.  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="annotation">@OperatorInfoCreatorAnnotation</span>(<span class="type">AggregaterInfoCreator</span>.<span class="keyword">class</span>)</span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">AggregateOperator</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">BasicAggFunctionOperator</span></span></span><br></pre></td></tr></table></figure>
<p>接口OperatorInfoCreator创建每个算子的实例, createInstance参数分别是: 执行计划信息,xml执行计划中的算子信息(哪里的xml?),schema信息,系统配置信息.<br>下图是OperatorInfoCreator的实现类, 创建的算子实例除了FunctionOperator,还有InputOperator,OutputOperator,FunctionStreamOperator.  </p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="constant">OperatorInfoCreator</span> (c.h.s.c.e.operatorinfocreater)     创建的算子实例               父类</span><br><span class="line">    |-- <span class="constant">AggregaterInfoCreator</span>                           <span class="constant">AggFunctionOp</span>           <span class="prompt">&gt;&gt; </span><span class="constant">FunctionOperator</span></span><br><span class="line">    |-- <span class="constant">DataSourceInfoOperatorCreator</span>                   <span class="constant">FunctionOperator</span>        -- <span class="constant">FunctionOperator</span></span><br><span class="line">    |-- <span class="constant">FunctorInfoCreator</span>                              <span class="constant">FunctorOp</span>               <span class="prompt">&gt;&gt; </span><span class="constant">FunctionOperator</span></span><br><span class="line">    |-- <span class="constant">FilterInfoCreator</span>                               <span class="constant">FilterFunctionOp</span>        <span class="prompt">&gt;&gt; </span><span class="constant">FunctionOperator</span>     </span><br><span class="line">    |-- <span class="constant">InputInfoCreator</span>                                                           <span class="constant">InputOperator</span></span><br><span class="line">    |-- <span class="constant">UnionInfoCreator</span>                                <span class="constant">UnionFunctionOp</span>         <span class="prompt">&gt;&gt; </span><span class="constant">FunctionOperator</span></span><br><span class="line">    |-- <span class="constant">OutputInfoCreator</span>                                                          <span class="constant">OutputOperator</span></span><br><span class="line">    |-- <span class="constant">SplitterInfoCreator</span>                             <span class="constant">SplitOp</span>                 <span class="prompt">&gt;&gt; </span><span class="constant">FunctionOperator</span></span><br><span class="line">    |-- <span class="constant">JoinInfoOperatorCreator</span>                         <span class="constant">JoinFunctionOp</span>          <span class="prompt">&gt;&gt; </span><span class="constant">FunctionOperator</span></span><br><span class="line">    |-- <span class="constant">FunctionStreamInfoCreator</span>                                                  <span class="constant">FunctionStreamOperator</span></span><br></pre></td></tr></table></figure>
<p>以AggregaterInfoCreator将<code>AggregateOperator</code>转换为<code>AggFunctionOp</code>为例: 由于AggregateOperator本身包含了Window对象<br>以及filterBeforeAggregate,filterAfterAggregate等字符串, 所以根据这些数据构造相应的对象,并最终构造出AggFunctionOp.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> AbsOperator <span class="title">createInstance</span><span class="params">(Application vapp, Operator operator, EventTypeMng streamschema, Map&lt;String, String&gt; systemConfig)</span> </span>&#123;</span><br><span class="line">    LOG.debug(<span class="string">"start to create aggregate operator"</span>);        <span class="comment">//LOG</span></span><br><span class="line">    prepare(vapp, operator, systemConfig);</span><br><span class="line">    <span class="comment">//窗口,过滤,表达式     </span></span><br><span class="line">    WindowViewCreator creater = <span class="keyword">new</span> WindowViewCreator();</span><br><span class="line">    IWindow window = creater.create(inputSchemas, aggOperator.getWindow(), <span class="keyword">this</span>.applicationConfig);</span><br><span class="line">    FilterView filterView = createFilterView();</span><br><span class="line">    IExpression bexpr = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (filterView != <span class="keyword">null</span>) bexpr = filterView.getBoolexpr();</span><br><span class="line">    <span class="comment">//聚合结果合并     </span></span><br><span class="line">    AggResultSetParameters pars = createResultSetMergeParmeters(streamschema, window, bexpr);</span><br><span class="line">    IAggResultSetMerge resultSetMerge = <span class="keyword">new</span> AggResultSetMergeViewCreator(pars).create();        <span class="comment">// ⬅️ </span></span><br><span class="line">    <span class="comment">//聚合算子</span></span><br><span class="line">    AggFunctionOp aggFunctionOp = <span class="keyword">new</span> AggFunctionOp(window, filterView, resultSetMerge, OutputTypeAnalyzer.createOutputType(aggOperator.getWindow()));</span><br><span class="line">    <span class="comment">//系统参数</span></span><br><span class="line">    StreamingConfig config = <span class="keyword">new</span> StreamingConfig();</span><br><span class="line">    <span class="keyword">if</span> (operator.getArgs() != <span class="keyword">null</span>) config.putAll(operator.getArgs());</span><br><span class="line">    config.putAll(<span class="keyword">this</span>.applicationConfig);</span><br><span class="line">    aggFunctionOp.setConfig(config);</span><br><span class="line">    <span class="comment">//设置并行度和ID,完成流算子的构建</span></span><br><span class="line">    <span class="keyword">return</span> OperatorInfoCreatorFactory.buildStreamOperator(operator, aggFunctionOp);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面生成AggFunctionOp动用了WindowViewCreator创建Window,FilterView,AggResultSetMergeViewCreator创建IAggResultSetMerge. 最后组装成AggFunctionOp.   </p>
<blockquote>
<p>上一章AggregateSplitter拆分算子的时候解析From子句时就创建了FilterBeforeWindow的FilterOperator和AggregateOperator,<br>这里也有Window和Filter,不过是FilterBeforeAggregate. 这两个对象分别对应了AggregateOperator的Window对象和filterBeforeAggregate字符串.<br>这是因为拆分算子的时候创建的FilterOperator是FilterBeforeWindow, 创建的AggregateOperator本身包含了FilterBeforeAggregate和Window.   </p>
<p>IAggResultSetMerge表示聚合结果合并. 将IAggResultSetMerge设置给AggFunctionOp, 后面初始化Op的时候会用到这个对象来创建处理视图.<br>A.结合前面的代码阅读体验, 比如第一章创建完Task和SemanticAnalyzer都会初始化.这里创建完Op也会初始化流计算算子.<br>B.实际上对象创建完不就是被使用嘛! 如果有依赖的对象,可以设置到构造函数中,在实际使用对象的时候获取依赖对象进行必要计算,充分体现JAVA的面向对象思想.<br>C.数据的传递:从语法解析结果到语义解析结果到拆分的算子再到这里的流计算算子. 数据都是层层传递并不断更新或添加新的数据结构满足不同阶段的对象构建.     </p>
</blockquote>
<h4 id="combineOperators">combineOperators</h4><blockquote>
<p>上一篇重点介绍了拆分算子SplitContext(Operator)和组合算子OperatorCombiner(连线). 这里也不例外,在创建完具体的AbsOperator算子后,也需要组合.   </p>
</blockquote>
<p>combineOperators会将算子用OperatorTransition进行连接: 梳理operatorInfo之间的上下级关系  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">combineOperators</span><span class="params">(Map&lt;String, AbsOperator&gt; operatorInfos)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//算子之间的连接, 获取算子通过apiApplication.getOperators(), 获取连线也是通过apiApplication</span></span><br><span class="line">    <span class="keyword">for</span> (OperatorTransition ot : apiApplication.getOpTransition()) &#123;</span><br><span class="line">        <span class="comment">//获取连接的入口和出口算子</span></span><br><span class="line">        String fromOpId = ot.getFromOperatorId();</span><br><span class="line">        String toOpId = ot.getToOperatorId();</span><br><span class="line">        String streamName = ot.getStreamName();</span><br><span class="line">        DistributeType distributedType = ot.getDistributedType();</span><br><span class="line">        String distributedFields = ot.getDistributedFields();</span><br><span class="line">        <span class="comment">//连接的Schema, 对于From和To都是使用相同的Schema</span></span><br><span class="line">        String outputSchemaName = ot.getSchemaName();</span><br><span class="line">        distributedFields = ExecutorUtils.removeStreamName(distributedFields);            </span><br><span class="line">        TupleEventType outputSchema = (TupleEventType)(executorApp.getEventType(outputSchemaName));</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//operatorInfos是所有的算子集合, 根据传入的fromOpId或者toOpId,从集合中找出对应的算子</span></span><br><span class="line">        combineFromTransition(operatorInfos, fromOpId, streamName, outputSchema);</span><br><span class="line">        combineToTransition(operatorInfos, toOpId, streamName, distributedType, distributedFields, outputSchema);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>FromTransition: 连线的from算子的输出是outputSchema  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">combineFromTransition</span><span class="params">(Map&lt;String, AbsOperator&gt; operatorInfos, String fromOpId, String streamName, TupleEventType outputSchema)</span></span>&#123;</span><br><span class="line">    sConfig.put(StreamingConfig.STREAMING_INNER_OUTPUT_SCHEMA, outputSchema);</span><br><span class="line">    sConfig.put(StreamingConfig.STREAMING_INNER_OUTPUT_STREAM_NAME, streamName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ToTransition: 连线的to算子的输入是outputSchema. <code>这里outputSchema命名为schema似乎更好</code>  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">combineToTransition</span><span class="params">(Map&lt;String, AbsOperator&gt; operatorInfos, String toOpId, String streamName,</span><br><span class="line">    DistributeType distributedType, String distributedFields, TupleEventType outputSchema)</span> </span>&#123;</span><br><span class="line">    sConfig.put(StreamingConfig.STREAMING_INNER_INPUT_STREAM_NAME, streamName);</span><br><span class="line">    sConfig.put(StreamingConfig.STREAMING_INNER_INPUT_SCHEMA, outputSchema);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>类似于Graph中的顶点A -&gt; 边 -&gt; 顶点B. Transition就类似于边, 连接着左右两边的算子, 分别是From算子和To算子.  </p>
<h2 id="StormApplication">StormApplication</h2><p>SubmitTask.submitApplication -&gt; PhysicalPlanExecutor.execute -&gt; PhysicalPlanExecutor.submit(application.Application) -&gt;<br>StormApplication.launch -&gt; createTopology 创建拓扑, 对于Storm的程序而言, 构成拓扑的组件包括Spouts和Bolts.<br>这些数据都来自于Application的输入,输出和功能算子. 由于Storm只有两种组件Spout和Bolt, 所以输入算子归于Spout,输出和功能算子都属于Bolt.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">createSpouts</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    List&lt; ? extends IRichOperator&gt; sources = getInputStreams(); <span class="comment">//获得所有源算子信息: OperatorMng.inputs ⬅️</span></span><br><span class="line">    checkInputStreams(sources);</span><br><span class="line">    <span class="keyword">for</span> (IRichOperator input : sources) &#123;</span><br><span class="line">        StormSpout spout = <span class="keyword">new</span> StormSpout();                    <span class="comment">//将算子设置到为StormSpout中</span></span><br><span class="line">        spout.setOperator(input);                               <span class="comment">//Spout接收数据时,将使用设置的算子开始处理</span></span><br><span class="line">        builder.setSpout(input.getOperatorId(), spout, input.getParallelNumber());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">createBolts</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    List&lt;IRichOperator&gt; orderedFunOp = genFunctionOpsOrder();   <span class="comment">//获取已经排好序的功能算子,包含output算子  ⬅️</span></span><br><span class="line">    <span class="keyword">for</span> (IRichOperator operator : orderedFunOp) &#123;</span><br><span class="line">        setOperatorGrouping(operator);                          <span class="comment">//设置Bolt的分组策略  ⬅️</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>哪些Operator算子会作为Bolt, OutpoutBolt, Spout都是由OperatorMng管理的比如getInputStreams,genFunctionOpsOrder<br>这样创建的Bolt会直接依赖于对应的Operator, 在处理Bolt时,就不需要再判断是哪一种类型的Operator了.<br>所以正是由于对算子的种类进行了分离(输入,输出,功能)才使得处理Storm的component时变得容易.<br>一旦根据算子创建并组织完构成Topology的Spouts和Bolts, 就可以提交拓扑给Storm集群执行了. DONE😄  </p>
</blockquote>
<h3 id="Bolt_Grouping">Bolt Grouping</h3><p>在开发Storm应用程序时, 一般是在Storm的Topology代码中创建Bolt并直接设置Bolt的分组策略.<br>假设有这样的Topology, Bolt1输出到Bolt3和Bolt4, Bolt2输出到Bolt3(一个Bolt可以有多个输出,也可以由多个输入).    </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">             |------ |Bolt4|</span><br><span class="line">|Bolt1| -----|</span><br><span class="line">             |------ |Bolt3|</span><br><span class="line">|Bolt2| -----|</span><br><span class="line"></span><br><span class="line"><span class="comment">//Bolt1有两个输出流, 输出字段都是一样的, 两个输出流的名称stream-id不一样</span></span><br><span class="line">builder.setBolt(<span class="string">"bolt1"</span>, <span class="keyword">new</span> Bolt1(), <span class="number">2</span>)        </span><br><span class="line">            declarer.declareStream(<span class="string">"streamA"</span>, <span class="keyword">new</span> Fields(<span class="string">"f1"</span>,<span class="string">"f2"</span>))</span><br><span class="line">            declarer.declareStream(<span class="string">"streamB"</span>, <span class="keyword">new</span> Fields(<span class="string">"f1"</span>,<span class="string">"f2"</span>))</span><br><span class="line"><span class="comment">//Bolt2只有一个输出流</span></span><br><span class="line">builder.setBolt(<span class="string">"bolt2"</span>, <span class="keyword">new</span> Bolt2(), <span class="number">2</span>)</span><br><span class="line">            declarer.declareStream(<span class="string">"streamC"</span>, <span class="keyword">new</span> Fields(<span class="string">"f1"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">//Bolt3接收Bolt1的streamA流使用字段分组, 接收Bolt2的streamC流使用shuffle分组</span></span><br><span class="line">builder.setBolt(<span class="string">"bolt3"</span>, <span class="keyword">new</span> Bolt3(), <span class="number">4</span>)</span><br><span class="line">        .fieldsGrouping(<span class="string">"bolt1"</span>, <span class="string">"streamA"</span>, <span class="keyword">new</span> Field(<span class="string">"f1"</span>))</span><br><span class="line">        .shuffleGrouping(<span class="string">"bolt2"</span>, <span class="string">"streamC"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//Bolt4接收Bolt1的streamB流使用字段分组, 分组字段是Bolt1产生的f2字段.  </span></span><br><span class="line">builder.setBolt(<span class="string">"bolt4"</span>, <span class="keyword">new</span> Bolt4(), <span class="number">3</span>)</span><br><span class="line">        .fieldsGrouping(<span class="string">"bolt1"</span>, <span class="string">"streamB"</span>, , <span class="keyword">new</span> Field(<span class="string">"f2"</span>))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>setBolt时设置的componentId/operatorId都是自己Bolt, 分组时的componentId则是输入的componentId/operatorId.  </p>
</blockquote>
<p>通过解析CQL的分组以及算子/组件之间的连接, 现在就不需要在Topology写死了. 因此需要框架能够动态地构建Topology.  </p>
<blockquote>
<p>为什么IRichOperator的getInputStream()和getOutputStream()表示的是输入流和输出流的名称, 而不是输入流对象和输出流对象(比如算子本身).<br>这是因为Operator算子会用于Topology的Spout/Bolt, 创建完Spout/Bolt之后, 用于构建Topology其他必要的信息除了分组外,<br>还有Storm的component-id对应算子的id 和 Storm的stream-id对应算子的输入/输出流名称  </p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setOperatorGrouping</span><span class="params">(IRichOperator operator)</span> </span>&#123;</span><br><span class="line">    BoltDeclarer bolt = createBoltDeclarer(operator);</span><br><span class="line">    <span class="comment">//一个Bolt可能有多个输入即多个InputStream, 同时输出也可能有多个: 设置不同的Grouping策略</span></span><br><span class="line">    <span class="comment">//注意: Bolt设置分组时的componentId是其输入源的ComponentId,而不是自己的componentId, 自己是在builder.setBolt时设置的</span></span><br><span class="line">    <span class="keyword">for</span> (String strname : operator.getInputStream()) &#123;              <span class="comment">//strname是当前算子的输入流名称</span></span><br><span class="line">        GroupInfo groupInfo = operator.getGroupInfo().get(strname); <span class="comment">//算子的分组信息</span></span><br><span class="line">        setBoltGrouping(bolt, strname, groupInfo);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setBoltGrouping</span><span class="params">(BoltDeclarer bolt, String strname, GroupInfo groupInfo)</span> </span>&#123;        </span><br><span class="line">    DistributeType distribute = groupInfo.getDitributeType();</span><br><span class="line">    <span class="keyword">switch</span> (distribute) &#123;</span><br><span class="line">        <span class="keyword">case</span> FIELDS:</span><br><span class="line">            Fields fields = <span class="keyword">new</span> Fields(groupInfo.getFields());</span><br><span class="line">            <span class="comment">//根据输入流的名称, 获取这个输入流是个什么算子, 为的是获得这个输入算子的operatorId,作为分组策略的第一个参数</span></span><br><span class="line">            IRichOperator operator = getOperatorByOutputStreamName(strname);</span><br><span class="line">            <span class="comment">//字段分组三个参数分别表示: componentId, streamId, fields. 这里的componentId表示从哪个数据源接入数据,而不是当前算子的operatorId</span></span><br><span class="line">            bolt.fieldsGrouping(operator.getOperatorId(), strname, fields);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="comment">//... 其他分组类型 ...   </span></span><br><span class="line">        <span class="keyword">default</span>:               </span><br><span class="line">            setDefaultBoltGrouping(bolt, strname);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setDefaultBoltGrouping</span><span class="params">(BoltDeclarer bolt, String strname)</span> </span>&#123;</span><br><span class="line">    IRichOperator operator = getOperatorByOutputStreamName(strname);</span><br><span class="line">    <span class="comment">//shuffle分组两个参数分别表示: 输入流的operatorId/componentId, streamId</span></span><br><span class="line">    bolt.shuffleGrouping(operator.getOperatorId(), strname);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Operator算子的id会作为Storm中Spout/Bolt的component-id, 而Operator的输入流/输出流名称是作为Spout/Bolt的stream-id.<br>component-id只是用于区别不同的组件,或者用于从哪个输入组件获取数据. 而stream-id则可以作为分流/多流/合并流等.  </p>
</blockquote>
<h3 id="Bolt_Creation">Bolt Creation</h3><p>createBolts设置Operator的分组策略, 首先创建IRichBolt,并返回Bolt的声明BoltDeclarer,以便后续操作可以在BoltDeclarer继续进行(比如上面的分组策略).  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> BoltDeclarer <span class="title">createBoltDeclarer</span><span class="params">(IRichOperator operator)</span></span>&#123;</span><br><span class="line">    IRichBolt bolt;</span><br><span class="line">    <span class="keyword">if</span> ((operator <span class="keyword">instanceof</span> FunctionOperator) || (operator <span class="keyword">instanceof</span> FunctionStreamOperator)) &#123;</span><br><span class="line">        bolt = createStormBolt(operator);</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        bolt = createOutputStormBolt(operator);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> builder.setBolt(operator.getOperatorId(), bolt, operator.getParallelNumber());</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> IRichBolt <span class="title">createOutputStormBolt</span><span class="params">(IRichOperator f)</span></span>&#123;</span><br><span class="line">    StormOutputBolt outputbolt = <span class="keyword">new</span> StormOutputBolt();     </span><br><span class="line">    outputbolt.setOperator(f);          <span class="comment">//类似于StormSpout将算子赋值,真正执行时会使用算子进行操作</span></span><br><span class="line">    <span class="keyword">return</span> outputbolt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StormSpout,StormBolt,StormOutputBolt都是对Storm的组件的封装. 除了继承各自的IRichSpout和IRichBolt外,还要实现StreamAdapter接口的setOperator方法.<br>流处理算子适配接口: 依靠这个接口，将流处理的算子注入到具体的Storm的Spout/Bolt中. <code>创建Bolt为啥不用构造函数一句话的事儿: new StormBolt(operator)</code>  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StormSpout</span> <span class="keyword">implements</span> <span class="title">IRichSpout</span>, <span class="title">StreamAdapter</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> IRichOperator input;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setOperator</span><span class="params">(IRichOperator operator)</span></span>&#123;</span><br><span class="line">        input = operator;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StormOutputBolt</span> <span class="keyword">implements</span> <span class="title">IRichBolt</span>, <span class="title">StreamAdapter</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> OutputCollector outputCollector;</span><br><span class="line">    <span class="keyword">private</span> OutputOperator output;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setOperator</span><span class="params">(IRichOperator operator)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.output = (OutputOperator)operator;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StormBolt的execute方法  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple input)</span> </span>&#123;</span><br><span class="line">    String sourceStreamName = input.getSourceStreamId();        <span class="comment">//获取Tuple的输入流stream-id</span></span><br><span class="line">    List&lt;String&gt; inStreams = functionStream.getInputStream();   <span class="comment">//输入流名称列表,因为一个Bolt可以有多个输入流</span></span><br><span class="line">    <span class="keyword">for</span> (String streamName : inStreams) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!sourceStreamName.equals(streamName)) <span class="keyword">continue</span>;     <span class="comment">//只有Tuple的输入流stream-id和IRichOperator的输入流名称相同时,才处理这个Tuple</span></span><br><span class="line">        TupleEvent event = TupleTransform.tupeToEvent(input, functionStream.getInputSchema().get(streamName));</span><br><span class="line">        functionStream.execute(streamName, event);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | start to submit application example | com.huawei.streaming.cql.executor.PhysicalPlanExecutor (PhysicalPlanExecutor.java:<span class="number">201</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | reset submit jar to /<span class="keyword">private</span>/var/folders/xc/x0b8crk9667ddh1zhfs29_zr0000gn/T/example<span class="number">.5f</span>8ed0baaeb243a49308fd75144cf715.jar | com.huawei.streaming.storm.StormApplication (StormApplication.java:<span class="number">314</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | Using defaults.yaml from resources | backtype.storm.utils.Utils (Utils.java:<span class="number">253</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | The baseSleepTimeMs [<span class="number">2000</span>] the maxSleepTimeMs [<span class="number">60000</span>] the maxRetries [<span class="number">5</span>] | backtype.storm.utils.StormBoundedExponentialBackoffRetry (StormBoundedExponentialBackoffRetry.java:<span class="number">47</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | Using defaults.yaml from resources | backtype.storm.utils.Utils (Utils.java:<span class="number">253</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | GenFunctionOpsOrder enter. | com.huawei.streaming.application.OperatorMng (OperatorMng.java:<span class="number">205</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | Using defaults.yaml from resources | backtype.storm.utils.Utils (Utils.java:<span class="number">253</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | Generated ZooKeeper secret payload <span class="keyword">for</span> MD5-digest: -<span class="number">5668598407594625313</span>:-<span class="number">5703359794945963404</span> | backtype.storm.StormSubmitter (StormSubmitter.java:<span class="number">82</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">39</span> | INFO  | [main] | Uploading topology jar /<span class="keyword">private</span>/var/folders/xc/x0b8crk9667ddh1zhfs29_zr0000gn/T/example<span class="number">.5f</span>8ed0baaeb243a49308fd75144cf715.jar to assigned location: storm-local/nimbus/inbox/stormjar-a5b95134-e3f2-<span class="number">431</span>d-b675-<span class="number">924</span>d8c468cf3.jar | backtype.storm.StormSubmitter (StormSubmitter.java:<span class="number">371</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">40</span> | INFO  | [main] | Successfully uploaded topology jar to assigned location: storm-local/nimbus/inbox/stormjar-a5b95134-e3f2-<span class="number">431</span>d-b675-<span class="number">924</span>d8c468cf3.jar | backtype.storm.StormSubmitter (StormSubmitter.java:<span class="number">396</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">40</span> | INFO  | [main] | Finished submitting topology: example | backtype.storm.StormSubmitter (StormSubmitter.java:<span class="number">248</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">40</span> | INFO  | [main] | <span class="keyword">delete</span> user packed jar after submit | com.huawei.streaming.cql.executor.PhysicalPlanExecutor (PhysicalPlanExecutor.java:<span class="number">156</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">40</span> | INFO  | [main] | unRegister jars from <span class="keyword">class</span> loader. | com.huawei.streaming.cql.DriverContext (DriverContext.java:<span class="number">427</span>)</span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Source/">Source</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/storm/">storm</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-11-27-StreamCQL-operator" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/11/27/2015-11-27-StreamCQL-operator/" class="article-date">
  	<time datetime="2015-11-26T16:00:00.000Z" itemprop="datePublished">2015-11-27</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/27/2015-11-27-StreamCQL-operator/">StreamCQL源码阅读(3) 拆分组合算子</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="前戏:_buildApplication">前戏: buildApplication</h2><p>上篇在解析Schema的时候分析了CQL中一些常用的Statement syntax和对应的语法/语义解析器结果,<br>现在继续ApplicationBuilder.buildApplication中parseSchemas的下一步splitOperators.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">buildApplication</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    app = <span class="keyword">new</span> Application(applicationName);</span><br><span class="line">    parseSchemas();</span><br><span class="line">    List&lt;SplitContext&gt; splitContexts = splitOperators();            <span class="comment">//拆分算子  ⬅️</span></span><br><span class="line">    SplitContext splitContext = combineOperators(splitContexts);    <span class="comment">//组合算子, 将拆分算子列表转换为只有一个SplitContext  ⬅️</span></span><br><span class="line">    changeUnionOperators(splitContext);</span><br><span class="line">    changeSchemaAfterAggregate(splitContext);</span><br><span class="line">    app.setOperators(splitContext.getOperators());                  <span class="comment">//拆分结果包含了operatots和transitions</span></span><br><span class="line">    app.setOpTransition(splitContext.getTransitions());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>combineOperators会创建OperatorCombiner, 并调用combine方法将splitContexts合并起来,终于打印了日志中看到的:<code>combine all split contexts</code>(解析submit之后).<br>构建Application的主要工作就是Split和Combine,最后将SplitContext的operators和transitions设置到Application对象中,完成应用程序的构建,在这基础上再进行物理优化.    </p>
<p>为什么要先拆分, 后面又要再组合?😖 以下面的CQL为例:  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">insert</span> <span class="keyword">into</span> stream s1           <span class="keyword">INSERT</span>  </span><br><span class="line"><span class="keyword">select</span> <span class="keyword">type</span>,<span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">count</span>     <span class="keyword">AGGREGATE</span>  </span><br><span class="line"><span class="keyword">from</span> s0 (<span class="keyword">id</span>&gt;<span class="number">10</span>)[<span class="keyword">ROWS</span> <span class="number">10</span>]        <span class="keyword">FROM</span>: FilterBeforeWindow  </span><br><span class="line"><span class="keyword">having</span> <span class="keyword">count</span>(<span class="keyword">id</span>)&gt;<span class="number">10</span>             <span class="keyword">HAVING</span>: Expression  </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">type</span>                   GROUPBY</span></span><br></pre></td></tr></table></figure>
<p>首先需要通过之前分析的AnalyzeContext解析出系统中所有的算子.<br>① INSERT INTO SELECT, 即INSERT语句中包含了SELECT子句. 对于INSERT而言只要确定输出流名称<br>② SELECT语句则包含比较多的算子, FilterBeforeWindow,Having,GroupBy,COUNT聚合  </p>
<blockquote>
<p>只要将CQL中的算子都拆分出来, 才能进一步进行整理. 所以拆分算子的工作类似于为每个关键词进行归类.<br>试想一下: 要整理很多杂乱的东西, 首先对每件物品进行归类, 最后再进行总的汇总😊.  </p>
</blockquote>
<h2 id="正文:_Split_and_Combine_Operators">正文: Split and Combine Operators</h2><p>第一步SplitOperators拆分算子: 创建对应的Splitter,调用其split方法.   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> List&lt;SplitContext&gt; <span class="title">splitOperators</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    List&lt;SplitContext&gt; splitContexts = Lists.newArrayList();</span><br><span class="line">    <span class="keyword">for</span> (AnalyzeContext pContext : parseContexts) &#123;         <span class="comment">//parseContexts是语义解析器结果列表</span></span><br><span class="line">        parseAutoCreatePipeStream(splitContexts, pContext); <span class="comment">//由于schema推断的存在,中间的流schema没有通过create input语句定义,所以显示的创建一个create input的解析内容</span></span><br><span class="line">        parseSubQueryOperators(splitContexts, pContext);    <span class="comment">//解析子查询</span></span><br><span class="line">        SplitContext context = OperatorSplitter.split(buildUtils, pContext);    <span class="comment">//算子拆分 ⬅️ </span></span><br><span class="line">        splitContexts.add(context);                         <span class="comment">//每个AnalyzeContext拆分后都对应一个SplitContext</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> splitContexts;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>AnalyzeContext是CQL语句的语义解析结果, CQL的上下文信息都封装在语义解析结果里面.  由于语义解析是个比较大的切面,<br>需要把语义解析结果分成更细粒度, 即算子. 可以认为AnalyzeContext -&gt; SplitContext的转换是将任务更加具体化.  </p>
</blockquote>
<p>OperatorSplitter的splitters采用static块提前添加了系统中也有的算子拆分类. 结合上面的splitOperators就是一个双层循环了:<br>针对Application的每一个AnalyzeContext, 判断哪个Splitter可以解析这个AnalyzeContext(每个AnalyzeContext只会对应一个Splitter).   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SplitContext <span class="title">split</span><span class="params">(BuilderUtils buildUtils, AnalyzeContext parseContext)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (Splitter splitter : splitters) &#123;</span><br><span class="line">        <span class="keyword">if</span> (splitter.validate(parseContext)) &#123;</span><br><span class="line">            <span class="keyword">return</span> createSplitter(splitter.getClass(), buildUtils).split(parseContext);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个具体的Splitter都实现了validate方法根据传入的AnalyzeContext实现类(pContext)用来验证能否进行解析  </p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Splitter                                AnalyzeContext</span><br><span class="line">    |<span class="string">-- SelectSplitter                      </span>|<span class="string">-- SelectAnalyzeContext     </span><br><span class="line">            </span>|<span class="string">-- DataSourceSplitter          </span>|</span><br><span class="line">            |<span class="string">-- AggregateSplitter           </span>|</span><br><span class="line">            |<span class="string">-- JoinSplitter                </span>|</span><br><span class="line">    |<span class="string">-- InsertSplitter                      </span>|<span class="string">-- InsertAnalyzeContext &gt;&gt; InsertOnlyAnalyzeContext</span><br><span class="line">    </span>|<span class="string">-- SourceOperatorSplitter              </span>|<span class="string">-- CreateStreamAnalyzeContext</span><br><span class="line">    </span>|<span class="string">-- MultiInsertSplitter                 </span>|<span class="string">-- MultiInsertStatementAnalyzeContext</span><br><span class="line">    </span>|<span class="string">-- UserOperatorSplitter                </span>|<span class="string">-- InsertUserOperatorStatementAnalyzeContext</span></span><br></pre></td></tr></table></figure>
<p>比如AggregateSplitter的validate方法会验证是不是SelectAnalyzeContext.     </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">validate</span><span class="params">(AnalyzeContext parseContext)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!(parseContext <span class="keyword">instanceof</span> SelectAnalyzeContext))    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    SelectAnalyzeContext selectAnalyzeContext = (SelectAnalyzeContext)parseContext;</span><br><span class="line">    FromClauseAnalyzeContext clauseContext = selectAnalyzeContext.getFromClauseContext();</span><br><span class="line">    <span class="keyword">if</span> (clauseContext.getJoinexpression() != <span class="keyword">null</span>)          <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">if</span> (clauseContext.getCombineConditions().size() != <span class="number">0</span>)   <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;  <span class="comment">//属于select,然后既不是combine，又不是join，那么就是aggregate</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>普通的select可以看做是aggregate,比如<code>select count(id) from a where</code>就是一种聚合. 因为select子句是count(id)<br>但是如果是<code>select id from a join b on a.id=b.id</code>因为有join操作就不是aggregate了.<br>所以可以看到SelectSplitter针对这两种语句分成了AggregateSplitter和JoinSplitter.  </p>
</blockquote>
<p>只有验证成功,才可以在此Splitter上调用split: 根据AnalyzeContext创建Operator算子, 即根据语义分析结果拆分内容</p>
<h3 id="SourceOperatorSplitter_-&gt;_Input/Output_Operator">SourceOperatorSplitter -&gt; Input/Output Operator</h3><p>SourceOperatorSplitter的split会创建Input和Output算子和临时的pipe算子.   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SourceOperatorSplitter</span> <span class="keyword">implements</span> <span class="title">Splitter</span> </span>&#123;   <span class="comment">//源算子拆分,包括输入算子和输出算子</span></span><br><span class="line">    <span class="keyword">private</span> SplitContext result = <span class="keyword">new</span> SplitContext();</span><br><span class="line">    <span class="keyword">private</span> CreateStreamAnalyzeContext context;             <span class="comment">//由validate保证,所以下面的split方法可以强转</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SplitContext <span class="title">split</span><span class="params">(AnalyzeContext parseContext)</span> </span>&#123;</span><br><span class="line">        context = (CreateStreamAnalyzeContext)parseContext;  <span class="comment">//语义解析结果</span></span><br><span class="line">        setParallelNumber();</span><br><span class="line">        addToInput();                       <span class="comment">//创建输入算子并加入到result中: InputStreamOperator</span></span><br><span class="line">        addToOutput();                      <span class="comment">//创建输出算子并加入到result中: OutputStreamOperator</span></span><br><span class="line">        addToPipe();                        <span class="comment">//连接算子: FilterOperator</span></span><br><span class="line">        result.setParseContext(context);    <span class="comment">//最后都要将AnalyzeContext设置到SplitContext中,说不定后面还是需要它的父亲(context)站出来撑腰呢.</span></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上篇语法结构的<code>C</code>部分,AnalyzeContext.analyze会将<code>语法解析的上下文结果</code>设置到<code>语义解析结果</code>中. 这里创建的<code>算子</code>则进一步依赖于语义解析结果.<br>所谓任何事物都是可以追朔到源头的: <strong><code>StatementContext-&gt;AnalyzeContext-&gt;Operator-&gt;SplitContext-&gt;Application</code></strong>, Operator并不是一步登天,与生俱来的.<br>可以看到context中的RecordReaderClass,DeserializerClass,ReadWriterProperties,SerDeProperties依次<code>登场</code>并进入到Operator的<code>戏局</code>里.<br>所以Operator沿袭了<code>祖先</code>的上下文数据, 现在新的世界格局将是以Operator为<code>主角</code>的了, 祖先们就可以<code>隐退江湖</code>了.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">addToInput</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (context.getDeserializerClassName() != <span class="keyword">null</span> &amp;&amp; context.getSerializerClassName() == <span class="keyword">null</span>) &#123;</span><br><span class="line">        Operator inop = createInputSourceOperator();    <span class="comment">//创建算子</span></span><br><span class="line">        <span class="keyword">if</span> (inputConverter.validate(inop)) &#123;</span><br><span class="line">            inop = inputConverter.convert(inop);</span><br><span class="line">        &#125;</span><br><span class="line">        result.addOperators(inop);      <span class="comment">//这个很重要,将新创建的算子添加到SplitContext中, 后面才会在设置到Application中 ⬅️ </span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> InputStreamOperator <span class="title">createInputSourceOperator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    String operatorName = getOperatorName(context.getRecordReaderClassName(),<span class="string">"Input"</span>);</span><br><span class="line">    <span class="comment">//创建输入流算子</span></span><br><span class="line">    InputStreamOperator op = <span class="keyword">new</span> InputStreamOperator(buildUtils.getNextOperatorName(operatorName), parallelNumber);</span><br><span class="line">    <span class="comment">//设置输入算子的属性: 反序列化类, 读取记录类</span></span><br><span class="line">    op.setName(context.getStreamAlias());</span><br><span class="line">    op.setDeserializerClassName(context.getDeserializerClassName());</span><br><span class="line">    op.setRecordReaderClassName(context.getRecordReaderClassName());</span><br><span class="line">    <span class="comment">//反序列化类的属性和读取记录的属性, 对应CQL最原始的properties. </span></span><br><span class="line">    op.setArgs(<span class="keyword">new</span> TreeMap&lt;String, String&gt;());</span><br><span class="line">    op.getArgs().putAll(context.getReadWriterProperties());</span><br><span class="line">    op.getArgs().putAll(context.getSerDeProperties());</span><br><span class="line">    <span class="keyword">return</span> op;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>pipe stream算子属于中间算子，本来是不会对应任何算子，只要解析出schema即可的. 但是为了和CQL的整体规则一致，便于后面创建算子之间的连线，<br>所以这里创建一个空的filter算子，不带任何过滤。 这样，就可以在优化器阶段将这个filter算子优化掉  </p>
<p>每个operator的参数用一个Map args来保存, 比如上面输入算子的参数包括了ReadWriterProperties和SerDeProperties.<br>因为执行器不知道每个operator中到底需要哪些参数，所以只能都放在map中, 由上层客户端进行填充，并在底层运行时检测</p>
</blockquote>
<h3 id="InsertSplitter_-&gt;_Insert_Operator">InsertSplitter -&gt; Insert Operator</h3><p>insert into语句的AnalyzeContext包含了outputStreamName和select子句, 输出流可以直接设置到SplitContext结果中. 而select子句需要<br>再次调用查询相关的Splitter(返回值也是SplitContext), 并将其结果产生的operators和transitions添加到insert的SplitContext中.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InsertSplitter</span> <span class="keyword">implements</span> <span class="title">Splitter</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> InsertAnalyzeContext context;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SplitContext <span class="title">split</span><span class="params">(AnalyzeContext parseContext)</span> </span>&#123;</span><br><span class="line">        context = (InsertAnalyzeContext)parseContext;</span><br><span class="line">        result.setOutputStreamName(context.getOutputStreamName());</span><br><span class="line">        <span class="comment">//insert中包含了select, 所以要先创建select算子, 调用SelectSplitter.split</span></span><br><span class="line">        SplitContext selectResult = OperatorSplitter.split(buildUtils, context.getSelectContext());</span><br><span class="line">        <span class="comment">//将select的结果算子和连接都加入到insert算子中</span></span><br><span class="line">        result.getOperators().addAll(selectResult.getOperators());</span><br><span class="line">        result.getTransitions().addAll(selectResult.getTransitions());</span><br><span class="line">        result.setParseContext(context);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>SourceOperatorSplitter和InsertSplitter设置到SplitContext result中的内容并不是一样的. 这是由于不同的AnalyzeContext所存储的<br>数据也是不同的, 数据源只需要序列化类,读取类等. 而插入语句则有输出流和输入源(select). 所以你<code>吃的是什么草, 挤出来的奶也是不一样的</code>.  </p>
</blockquote>
<h3 id="SelectSplitter">SelectSplitter</h3><p>SelectSplitter包含了<code>DataSource,①Aggregate,②JoinSplitter</code>.针对select语句的拆分以及Schema分为:  </p>
<p>1、最一般的select子句。<br>只有一个schema, 输入和输出都是(同)一个schema, 不论有没有窗口，都必须放在聚合算子(AggregateSplitter)中。<br>只要有where，就都放在functor算子(表达式)中，在优化器中，再进行调整，可以改为filter或者继承再聚合算子中。  </p>
<blockquote>
<p>单单一个select为什么要添加聚合算子?<br>答: 聚合不一定就是group by, 可能是filter过滤,limit限制条数等.<br>而select后面是可以跟上filter或者limit等. 聚合还可以是count,sum等.  </p>
</blockquote>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">                     |<span class="operator"><span class="keyword">SELECT</span>子句</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> stream s0 <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> s0 <span class="keyword">where</span> <span class="keyword">id</span>&gt;<span class="number">10</span></span><br><span class="line">                      <span class="comment">---------------        ----------- </span></span><br><span class="line">                      <span class="keyword">Aggregate</span>              Functor(expression)</span></span><br></pre></td></tr></table></figure>
<p>2、Join: 多个Join的schema，一个outputschema. 先查询出多个表所有的列，再在functor算子中进行列过滤。  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> s0.<span class="keyword">id</span>,s1.<span class="keyword">name</span> <span class="keyword">from</span> s0 <span class="keyword">join</span> s1 <span class="keyword">on</span> s0.<span class="keyword">id</span> = s1.<span class="keyword">id</span></span><br><span class="line">                     <span class="comment">------- -------</span></span><br><span class="line">                     s0元数据 s1元数据</span><br><span class="line">                            ⬇️</span><br><span class="line">       <span class="comment">-----------------------------</span></span><br><span class="line">       Functor列过滤</span></span><br></pre></td></tr></table></figure>
<p>3、Groupby: 聚合算子<br>4、orderby: 同一般select子句<br>5、Join语句中不支持聚合和groupby，至少目前不支持<br>6、三种过滤<br>A.窗口之后的过滤：where，放在聚合算子中<br>B.窗口之前的过滤：filter，前面加一个filter算子, 但是这样就牵扯到schema的变化，这个就麻烦一些了。先解析出所有的列，再进行过滤。<br>C.聚合之后的过滤：having，放在聚合算子中  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> <span class="keyword">type</span>,<span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">FROM</span> s0 <span class="keyword">where</span> code=<span class="string">'test'</span> (<span class="keyword">id</span>&gt;<span class="number">10</span>)[<span class="keyword">ROWS</span> <span class="number">10</span>] <span class="keyword">HAVING</span> <span class="keyword">count</span>(<span class="keyword">id</span>)&gt;<span class="number">100</span> </span><br><span class="line">                              <span class="comment">----------------  -------          --------------------</span></span><br><span class="line">                              A⬇                B⬇️              <span class="keyword">C</span>⬇️</span><br><span class="line">                              <span class="keyword">Aggregate</span>         Filter+Agg       <span class="keyword">Aggregate</span></span></span><br></pre></td></tr></table></figure>
<p>总结下：<br>1、<code>聚合算子</code>是必须有的。<br>2、Orderby必须放在独立sort算子中<br>3、limit放在output中作为限制，但是目前还不支持<br>4、<code>一个select语句，无论如何拆分，都只有一个输出schema</code>. (至少目前是这样,后面在优化器中会进行调整,将where中的一些列加入到select中,进行一些列变换)<br>5、Join时候，先查询该流<code>所有列</code>的Join结果，之后再进行<code>列过滤</code><br>6、Sort、Join、Aggregate算子都是按照<code>字段</code>进行分发，其他都是<code>随机</code>分发  </p>
<p>抽象类SelectSplitter的splitFromClause交给子类(Join,DataSource,AggregatePlitter)自己去实现:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">SelectSplitter</span> <span class="keyword">implements</span> <span class="title">Splitter</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> SplitContext result = <span class="keyword">new</span> SplitContext();       <span class="comment">//算子的拆分结果, 数据主要由AnalyzeContext而来</span></span><br><span class="line">    <span class="keyword">private</span> SelectAnalyzeContext selectAnalyzeContext;      <span class="comment">//AnalyzeContext语义解析结果    </span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SplitContext <span class="title">split</span><span class="params">(AnalyzeContext parseContext)</span> </span>&#123;</span><br><span class="line">        initParameters(parseContext);   <span class="comment">//初始化, 由于Select语句包含了很多子句,因此还要在这里定义其他子句的AnalyzeContext.</span></span><br><span class="line">        setParallelNumber();</span><br><span class="line">        splitFromClause();              <span class="comment">//抽象方法</span></span><br><span class="line">        result.setParseContext(selectAnalyzeContext);       <span class="comment">//设置到SplitContext结果中</span></span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> SelectClauseAnalyzeContext selectClauseContext; <span class="comment">//既然要解析Slect语句,就要解析它包含的所有子句! 正如前面的insert也要先获得select!</span></span><br><span class="line">    <span class="keyword">private</span> FromClauseAnalyzeContext fromClauseContext;</span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initParameters</span><span class="params">(AnalyzeContext parseContext)</span> </span>&#123;</span><br><span class="line">        selectAnalyzeContext = (SelectAnalyzeContext)parseContext;</span><br><span class="line">        selectClauseContext = selectAnalyzeContext.getSelectClauseContext();</span><br><span class="line">        fromClauseContext = selectAnalyzeContext.getFromClauseContext();</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于层级嵌套: 比如insert中嵌套了select. 所以解析insert时要先解析select,再把select设置到insert中.<br>同样select语句包含了更多的子句,比如select子句,from子句, 所以也要把旗下包含的所有子句都解析完了,自己才是完整的可用的.  </p>
</blockquote>
<h3 id="AggregateSplitter_-&gt;_FilterOp_+_AggregateOp_+_Transition">AggregateSplitter -&gt; FilterOp + AggregateOp + Transition</h3><p>AggregateSplitter的父类是SelectSplitter, 而Select包含From子句. From中可以有①FilterOperator: <code>filter before window</code><br>流前的过滤: <code>FROM transform (evnetid&gt;10)[range UNBOUNDED]</code>. 其中[]表示window, 而[]前面的()则是filter过滤.<br>②拆分AggregateOperator, 因为聚合算子可能包括多种聚合操作, 如果存在则都设置到AggregateOperator对应的字段中.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">splitFromClause</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">//获取Select中From子句的语义解析结果</span></span><br><span class="line">    FromClauseAnalyzeContext clauseContext = getFromClauseContext();    <span class="comment">//定义在父类SelectSplitter中,初始化时由Select获取</span></span><br><span class="line">    String streamName = clauseContext.getInputStreams().get(<span class="number">0</span>);</span><br><span class="line">    <span class="comment">//① 在Window操作前可以有Filter操作, 拆分出Filter算子, 在父类SelectSplitter中实现</span></span><br><span class="line">    FilterOperator fop = splitFiterBeforeWindow(streamName);</span><br><span class="line">    <span class="comment">//② 聚合算子</span></span><br><span class="line">    AggregateOperator aggregateOperator = splitAggregateOperator(clauseContext, streamName);</span><br><span class="line">    <span class="comment">//③ 创建算子之间的连接</span></span><br><span class="line">    OperatorTransition transition = createTransition(fop, aggregateOperator, streamName);</span><br><span class="line">    </span><br><span class="line">    getResult().addOperators(fop);                  <span class="comment">//过滤算子</span></span><br><span class="line">    getResult().addOperators(aggregateOperator);    <span class="comment">//聚合算子</span></span><br><span class="line">    getResult().addTransitions(transition);         <span class="comment">//从过滤算子到聚合算子的连线</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">protected</span> FilterOperator <span class="title">splitFiterBeforeWindow</span><span class="params">(String streamName)</span> </span>&#123;</span><br><span class="line">    FromClauseAnalyzeContext clauseContext = getFromClauseContext();</span><br><span class="line">    <span class="comment">//新创建一个Filter过滤算子, from stream(id&gt;1)[RANGE 10s] 可以在流之后,窗口之前的中间存在Filter过滤: 在进入窗口前过滤</span></span><br><span class="line">    FilterOperator fop = <span class="keyword">new</span> FilterOperator(buildUtils.getNextOperatorName(<span class="string">"Filter"</span>), parallelNumber);</span><br><span class="line">    <span class="comment">//从From语义解析结果中获取filterBeforeWindow对应当前stream的filter表达式,比如上面的id&gt;1</span></span><br><span class="line">    ExpressionDescribe expression = clauseContext.getFilterBeForeWindow().get(streamName);</span><br><span class="line">    fop.setFilterExpression(expression.toString());                     <span class="comment">//过滤条件表达式,比如id&gt;1</span></span><br><span class="line">    fop.setOutputExpression(createFilterOutputExpression(streamName));  <span class="comment">//输出列</span></span><br><span class="line">    <span class="keyword">return</span> fop;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> AggregateOperator <span class="title">splitAggregateOperator</span><span class="params">(FromClauseAnalyzeContext clauseContext, String streamName)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//创建新的聚合算子</span></span><br><span class="line">    AggregateOperator aggop = <span class="keyword">new</span> AggregateOperator(getBuildUtils().getNextOperatorName(<span class="string">"Aggregator"</span>), getParallelNumber());</span><br><span class="line">    parseWindow(clauseContext, streamName, aggop);  <span class="comment">//解析窗口. Window其实也是From的一部分,所以需要从From子句中获取Windows设置到aggop里</span></span><br><span class="line">    parseWhere(aggop);                              <span class="comment">//解析过滤: setFilterBeforeAggregate,在聚合之前的过滤.</span></span><br><span class="line">    aggop.setFilterAfterAggregate(parseHaving());   <span class="comment">//解析having</span></span><br><span class="line">    aggop.setGroupbyExpression(parseGroupby());     <span class="comment">//解析分组</span></span><br><span class="line">    aggop.setOrderBy(parseOrderBy());               <span class="comment">//解析排序</span></span><br><span class="line">    aggop.setLimit(parseLimit());                   <span class="comment">//解析限制</span></span><br><span class="line">    aggop.setOutputExpression(getSelectClauseContext().toString());     <span class="comment">//输出表达式: select关键字后面的都是输出</span></span><br><span class="line">    <span class="keyword">return</span> aggop;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>AggregateOperator对Select子句的解析最后都是字符串.  </p>
</blockquote>
<table>
<thead>
<tr>
<th>aggregation</th>
<th>setXXX</th>
<th>AnalyzerContext</th>
<th>parseXXX</th>
</tr>
</thead>
<tbody>
<tr>
<td>window</td>
<td>setWindow</td>
<td>FromClauseAnalyzeContext.windows.get(streamName)</td>
<td>parseWindow</td>
</tr>
<tr>
<td>where</td>
<td>setFilterBeforeAggregate</td>
<td>FilterClauseAnalzyeContext whereClauseContext</td>
<td>parseWhere</td>
</tr>
<tr>
<td>having</td>
<td>setFilterAfterAggregate</td>
<td>FilterClauseAnalzyeContext havingClauseContext</td>
<td>parseHaving</td>
</tr>
<tr>
<td>group by</td>
<td>setGroupbyExpression</td>
<td>SelectClauseAnalyzeContext groupbyClauseContext</td>
<td>parseGroupby</td>
</tr>
<tr>
<td>order by</td>
<td>setOrderBy</td>
<td>OrderByClauseAnalyzeContext</td>
<td>parseOrderBy</td>
</tr>
<tr>
<td>limit</td>
<td>setLimit</td>
<td>LimitClauseAnalzyeContext</td>
<td>parseLimit</td>
</tr>
<tr>
<td>output exp</td>
<td>setOutputExpression</td>
<td>SelectClauseAnalyzeContext selectClauseContext</td>
<td>getSelectClauseContext</td>
</tr>
</tbody>
</table>
<p>通过上面的AggregateSplitter.split方法,我们知道了聚合算子由FilterBeforeWindow,Window,FilterBeforeAggregate,Having,GroupBy,OrderBy,Limit组成.  </p>
<blockquote>
<p>语法结构中并没有Aggregate这种类型, 但是我们发现Aggregate用到的这些和Select语句中包含的子句都差不多. 其中两个Window可以认为是From子句.<br>FilterBeforeAggregate是Where子句, 这样Select语句的所有部分就和Aggregate都吻合了! 所以说Select也是一种Aggregate! (^_^这不是巧合吧) </p>
</blockquote>
<h4 id="AggregateOperator">AggregateOperator</h4><p><img src="http://img.blog.csdn.net/20151126173430754" alt="stream-operators"></p>
<p>AggregateOperator聚合算子:包含了window,以及window前后的filter操作. 当然还少不了count,sum之类的UDAF函数计算和UDF函数计算(BasicAggFunctionOperator)<br>AggregateOperator &gt;&gt; BasicAggFunctionOperator &gt;&gt; InnerFunctionOperator 这些类的字段正好对应了聚合算子的所有组成部分.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AggregateOperator</span> <span class="keyword">extends</span> <span class="title">BasicAggFunctionOperator</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> Window window;                  <span class="comment">//窗口 ==&gt; From子句</span></span><br><span class="line">    <span class="keyword">private</span> String filterBeforeAggregate;   <span class="comment">//filter的过滤条件 ==&gt; Where子句</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BasicAggFunctionOperator</span> <span class="keyword">extends</span> <span class="title">InnerFunctionOperator</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String filterAfterAggregate;    <span class="comment">//聚合类的过滤条件, 这里都是udaf函数, 过滤一定发生在数据聚合之后, 这里的表达式一定使用的是outputSchema中的列名称</span></span><br><span class="line">    <span class="keyword">private</span> String groupbyExpression;       <span class="comment">//分组的表达式</span></span><br><span class="line">    <span class="keyword">private</span> String orderBy;                 <span class="comment">//排序: 允许有多个字段，之间按照逗号分割, 允许出现udf和udaf函数</span></span><br><span class="line">    <span class="keyword">private</span> Integer limit;                  <span class="comment">//窗口的输出限制</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InnerFunctionOperator</span> <span class="keyword">extends</span> <span class="title">Operator</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String outputExpression;        <span class="comment">//输出的列定义,不光有单纯的列，还有udf以及udaf函数</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>过滤条件是一个字符串形式的逻辑表达式, 允许有and,or以及大括号和udf函数, 但是绝对不允许出现udaf函数，因为这没有聚合操作<br><strong>过滤发生在数据进入窗口之后，聚合之前</strong>. 比如 (a&gt;1 and a &lt;100) or (b is not null) 就是where的过滤， </p>
<p>InnerFunctionOperator功能性算子，主要为系统提供window，join，order by，group by等聚合操作。<br>这里的名称和operator包中的不一样. 这里定义的这些operator，主要是进行执行计划的序列化和反序列化的。<br>所有的数据类型全部是<code>字符串类型</code>，之后还要经过语法的解析，物理执行计划的优化之后，才会在application中提交。 </p>
<h4 id="Transition">Transition</h4><p>别忘了,还有createTransition创建连线哦: 在AggregateSplitter.splitFromClause中fromOp是FilterBeforeWindow FilterOperator, toOp是AggregateOperator.  </p>
<p>算子之间进行连接, 涉及到分组策略, Storm中Bolt可以指定怎么根据输入源进行分组, 即输入源将数据怎么分流到当前Bolt. 分组是有一定依据的,不能胡乱连接.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> OperatorTransition <span class="title">createTransition</span><span class="params">(Operator fromOp, Operator toOp, String streamName)</span> </span>&#123;</span><br><span class="line">    FromClauseAnalyzeContext clauseContext = getFromClauseContext();</span><br><span class="line">    DistributeType distype = DistributeType.SHUFFLE;</span><br><span class="line">    String disFields = <span class="keyword">null</span>;</span><br><span class="line">    Schema schema = clauseContext.getInputSchemas().get(<span class="number">0</span>);</span><br><span class="line">    <span class="comment">//如果有GroupBy,分组策略就是字段分组(distype=FIELDS), 比如group by type, 则按照type字段分组(disFields=type).  </span></span><br><span class="line">    <span class="keyword">if</span> (getGroupbyClauseContext() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        disFields = removeDataSourceColumnsFromGroupbyExpression(schema, getGroupbyClauseContext().toString());</span><br><span class="line">        distype = DistributeType.FIELDS;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//现在是在SelectSpiltter中, 所以可能是Aggregate,Join,DataSource中任意一种.  </span></span><br><span class="line">    <span class="keyword">if</span> (toOp <span class="keyword">instanceof</span> JoinFunctionOperator) &#123;</span><br><span class="line">        List&lt;Schema&gt; inputSchemas = getFromClauseContext().getInputSchemas();</span><br><span class="line">        schema = BaseAnalyzer.getSchemaByName(streamName, inputSchemas);</span><br><span class="line">        <span class="keyword">if</span> (((JoinFunctionOperator)toOp).getJoinType() != JoinType.CROSS_JOIN) &#123;</span><br><span class="line">            disFields = getJoinExpression(schema);</span><br><span class="line">            distype = DistributeType.FIELDS;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//创建起始算子到结束算子之间的连线, 并确定分组策略,分组字段, 发起连接的算子的schema信息</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> OperatorTransition(buildUtils.getNextStreamName(), fromOp, toOp, distype, disFields, schema);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>现在似乎要逐步用Storm的例子来理解了,否则算子之间为什么要进行连接? 连接之后就确定了上游算子怎么发送数据到下游算子. 假设Storm有两个Bolt: Bolt1和Bolt2.<br>Bolt1过滤数据,输出(word,count)两个字段. 为了能使得对Bolt1过滤后的数据进行分流,Bolt2使用Bolt1的word字段进行FieldGrouping. 这样Bolt1相同的word字段<br>只会到相同的Bolt任务中, 不同的word字段会分发到不同的Bolt2任务. 所以可以把Bolt1看做过滤算子, Bolt2看做是聚合算子, 中间存在分组连接对Bolt1的数据分流.  </p>
<p>那么具体下游算子要怎么接收上游算子的数据呢? 不用担心! 这里先只是创建连接,只要有连接,就都好办了,找对关系找对门路是最重要的!</p>
</blockquote>
<h4 id="SplitContext">SplitContext</h4><p>现在不难看出SplitContext的作用是各类语义分析结果拆分内容. 和前面几个XXXContext一样都只是保存数据的介质(还记得StatementContext,AnalyzeContext吗)<br>常见的CQL语句格式<code>insert into stream s0 select</code>中insert语句确定了<code>outputStreamName</code>, select语句确定算子拆分的结果:<code>operators和transitions</code>.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SplitContext</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> List&lt;OperatorTransition&gt; transitions = <span class="keyword">new</span> ArrayList&lt;OperatorTransition&gt;();</span><br><span class="line">    <span class="keyword">private</span> List&lt;Operator&gt; operators = <span class="keyword">new</span> ArrayList&lt;Operator&gt;();</span><br><span class="line">    <span class="keyword">private</span> String outputStreamName;        <span class="comment">//输出的流名称: 指的是在CQL中显示指定输出流名称的。例如insert into 之类的语句</span></span><br><span class="line">    <span class="keyword">private</span> AnalyzeContext parseContext;    <span class="comment">//CQL解析结果: 通过这个结果，可以进行多个CQL之间的连接</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>每一条CQL语句都对应一个AnalyzeContext, 每个AnalyzeContext都有一个Splitter用来创建算子. 即使上面我们创建了Transition连接, 但也是在同一个CQL语句内的!<br>而一个完整的Topology是要求能把多个CQL语句的上下文都串联起来组成一个DAG图的. 所以这就是在SplitContext中保留AnalyzeContext的含义: It’ time to Combine!  </p>
</blockquote>
<h3 id="OperatorCombiner">OperatorCombiner</h3><p>现在我们知道为什么要进行合并了,因为如果仅仅是拆分每一条CQL语句, 这样最后都是一段一段的,我们需要把这些一段一段拼接成完整的图.<br>目前只有两种CQL语句: 一种是流定义create stream语句, 一种是insert into语句. 对于流定义没有算子之间的连接,但是schema还是需要的.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> SplitContext <span class="title">combine</span><span class="params">(List&lt;SplitContext&gt; splitContexts)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; splitContexts.size(); i++) &#123;</span><br><span class="line">        combineSplitContext(splitContexts.get(i));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">combineSplitContext</span><span class="params">(SplitContext context)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//原始每一条CQL的算子和连线不能被破坏的! 要重新加入最后的返回结果中. </span></span><br><span class="line">    result.getOperators().addAll(context.getOperators());</span><br><span class="line">    result.getTransitions().addAll(context.getTransitions());</span><br><span class="line">    <span class="comment">//如果是SourceOperatorSplitter对应的CreateStreamAnalyzeContext, 是没有transition的.  </span></span><br><span class="line">    <span class="keyword">if</span> (context.getParseContext() <span class="keyword">instanceof</span> CreateStreamAnalyzeContext)&#123;</span><br><span class="line">        addSchemasFromCreateStream(context);    <span class="comment">//添加到inputStreams,outputStreams,pipeStreams</span></span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//合并时新添加的只是CQL与CQL之间的连线! 对于算子已经都是完整的了,不会缺胳膊断腿的,不需要再添加. ⬅️</span></span><br><span class="line">    createTransition(context);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">createTransition</span><span class="params">(SplitContext context)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//MultiInsertStatementAnalyzeContext</span></span><br><span class="line">    <span class="comment">//InsertUserOperatorStatementAnalyzeContext</span></span><br><span class="line">    InsertAnalyzeContext insertContext = (InsertAnalyzeContext)context.getParseContext();</span><br><span class="line">    createFromTransition(context, insertContext);</span><br><span class="line">    createToTransition(context, insertContext);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Splitter中SourceOperatorSplitter是没有operators和transitions, InsertSplitter的算子和连线则依赖于SelectSpitter.<br>所以如果是CreateStreamAnalyzeContext则不会调用createTransition, 只有三种insert才会调用:Insert,MultiInsert,UDFInsert    </p>
</blockquote>
<p>将多个算子组合起来, 组建算子之间的上下级关系. 算子之间的连线，有两种来源：<br>1、算子是由<code>一条CQL语句</code>拆分出多个算子组成，这样，连线就可以在拆分的时候确定。<br>2、算子是由<code>多条CQL语句</code>组合而来，通过使用<code>insert into select from</code>这样的语句，就可以实现多个算子之间的级联。<br>甚至可以改变算子之间的连接关系。比如在aggregate算子之前加入union算子, 在aggregate算子之后加入split算子。  </p>
<p>为每个<code>insert into select</code>语句解析出来的结果加入上下文连线。<br>CQL语句之间的连线，必然从inputStream或者PipeStream发起，连接到outputStream或者PipeStream.  </p>
<p><img src="http://img.blog.csdn.net/20151129160212682" alt="input-pipe-output"></p>
<h4 id="PipeStream">PipeStream</h4><p>splitOperators在拆分之前, 会首先解析是否需要创建之前没有声明过的流. 如果不存在,则创建PipeStream.   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">parseAutoCreatePipeStream</span><span class="params">(List&lt;SplitContext&gt; splitContexts, AnalyzeContext pcontext)</span> </span>&#123;</span><br><span class="line">    parseAutoCreatePipeStreamForInsert(splitContexts, pcontext);</span><br><span class="line">    parseAutoCreatePipeStreamForMultiInsert(splitContexts, pcontext);</span><br><span class="line">    parseAutoCreatePipeStreamForUserOperator(splitContexts, pcontext);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">parseAutoCreatePipeStreamForInsert</span><span class="params">(List&lt;SplitContext&gt; splitContexts, AnalyzeContext pcontext)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (pcontext <span class="keyword">instanceof</span> InsertAnalyzeContext) &#123;</span><br><span class="line">        InsertAnalyzeContext ipc = (InsertAnalyzeContext)pcontext;</span><br><span class="line">        <span class="keyword">if</span> (!ipc.isPipeStreamNotCreated()) <span class="keyword">return</span>;      <span class="comment">//允许类型不是输入/输出流的流不存在   </span></span><br><span class="line">        SplitContext sc = createPipeStreamSplitContext(ipc.getOutputSchema());</span><br><span class="line">        splitContexts.add(sc);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> SplitContext <span class="title">createPipeStreamSplitContext</span><span class="params">(Schema schema)</span> </span>&#123;</span><br><span class="line">    LOG.info(<span class="string">"create pipe Stream while stream is not created!"</span>);</span><br><span class="line">    CreateStreamAnalyzeContext pipe = <span class="keyword">new</span> CreateStreamAnalyzeContext();</span><br><span class="line">    pipe.setSchema(schema);</span><br><span class="line">    pipe.setStreamName(schema.getId());</span><br><span class="line">    <span class="keyword">return</span> OperatorSplitter.split(buildUtils, pipe);    <span class="comment">//输入输出流使用SourceOperatorSplitter拆分算子</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>比如下面的<code>multi insert</code>语句, 其中teststream是事先创建好的, 而s1,s2,s3都没有创建过, 在解析的时候判断这些insert对应的stream没有创建过,<br>就会创建这些pipe-stream. 因为insert的schema取决于select, 所以这些pipe-stream的schema等于InsertAnalyzeContext的outputSchema.   </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM teststream</span><br><span class="line">  <span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s1 <span class="keyword">SELECT</span> *</span><br><span class="line">  <span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s2 <span class="keyword">SELECT</span> a</span><br><span class="line">  <span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s3 <span class="keyword">SELECT</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">WHERE</span> <span class="keyword">id</span> &gt; <span class="number">10</span></span><br><span class="line">PRARLLEL <span class="number">4</span>;</span></span><br></pre></td></tr></table></figure>
<p>判断PipeStream是否创建过在InsertStatementAnalyzer.analyze中. 只要是insert语句, 如果在系统已有的schemas中不存在, 就设置setPipeStreamNotCreated=true<br>这样parseAutoCreatePipeStreamForInsert在判断到isPipeStreamNotCreated才会创建createPipeStreamSplitContext. 否则schema已经存在就不再需要创建了.  </p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (checkSchemaExists(streamName)) &#123;</span><br><span class="line">    context.setOutputSchema(getSchemaByName(streamName));</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    context.setPipeStreamNotCreated(true);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="From_&amp;_To_Transition">From &amp; To Transition</h4><p>首先找到insert into语句中计算出来的连线的起点。找到对应的算子. 然后根据起点的schema名称，找到对应的流名称，创建连线  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">createFromTransition</span><span class="params">(SplitContext context, InsertAnalyzeContext insertContext)</span> </span>&#123;</span><br><span class="line">    List&lt;OperatorTransition&gt; startTransitions = context.getFirstTransitons();</span><br><span class="line">    <span class="keyword">for</span> (OperatorTransition transition : startTransitions) &#123;</span><br><span class="line">        Operator op = context.getOperatorById(transition.getFromOperatorId());</span><br><span class="line">        String startStreamName = transition.getSchemaName();</span><br><span class="line">        SplitContext fromContext = getFromSplitContext(startStreamName);</span><br><span class="line">        Schema schema = getInputSchema(startStreamName, insertContext);</span><br><span class="line">        String nextStreamName = buildUtils.getNextStreamName();</span><br><span class="line">        </span><br><span class="line">        Operator fromOp = fromContext.getLastOperator();</span><br><span class="line">        OperatorTransition fromtransition = <span class="keyword">new</span> OperatorTransition(nextStreamName, fromOp, op, DistributeType.SHUFFLE, <span class="keyword">null</span>, schema);</span><br><span class="line">        result.addTransitions(fromtransition);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">createToTransition</span><span class="params">(SplitContext context, InsertAnalyzeContext insertContext)</span> </span>&#123;</span><br><span class="line">    Set&lt;Operator&gt; ops = getLastOperator(context);</span><br><span class="line">    <span class="keyword">for</span> (Operator op : ops) &#123;</span><br><span class="line">        String startStreamName = insertContext.getOutputStreamName();</span><br><span class="line">        SplitContext toContext = getToSplitContext(startStreamName);</span><br><span class="line">        Schema schema = insertContext.getOutputSchema();</span><br><span class="line">        String nextStreamName = buildUtils.getNextStreamName();</span><br><span class="line">        </span><br><span class="line">        Operator toOp = toContext.getFirstOperator();</span><br><span class="line">        OperatorTransition totransition = <span class="keyword">new</span> OperatorTransition(nextStreamName, op, toOp, DistributeType.SHUFFLE, <span class="keyword">null</span>, schema);</span><br><span class="line">        result.addTransitions(totransition);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="物理优化">物理优化</h3><p>拆分并组合算子后,因为每个算子之间都通过transition来连接(没有通过连线连接的算子是一个孤岛,是不会起作用的),所以可以进一步优化.<br>changeUnionOperators:union算子替换和changeSchemaAfterAggregate: 替换所有的having和orderby这些在聚合之后的表达式.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhysicOptimizer</span> <span class="keyword">implements</span> <span class="title">Optimizer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Application <span class="title">optimize</span><span class="params">(Application app)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">new</span> AggregateConverter().optimize(app);</span><br><span class="line">        <span class="keyword">new</span> FilterPruner().optimize(app);</span><br><span class="line">        <span class="keyword">new</span> SameStreamCombiner().optimize(app);</span><br><span class="line">        <span class="keyword">new</span> SameTransitionPruner().optimize(app);</span><br><span class="line">        <span class="keyword">return</span> app;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Source/">Source</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/storm/">storm</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-11-26-StreamCQL-schema" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/11/26/2015-11-26-StreamCQL-schema/" class="article-date">
  	<time datetime="2015-11-25T16:00:00.000Z" itemprop="datePublished">2015-11-26</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/26/2015-11-26-StreamCQL-schema/">StreamCQL源码阅读(2) 语法和语义解析</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="前戏:_SemanticAnalyzer">前戏: SemanticAnalyzer</h2><p>客户端提交的CQL语句经过Application.parse返回StatementContext(语法解析结果),再进入语义解析.<br>如果是SubmitTask会触发之前的LazyTask链条依次进行语义解析. 语义解析结果AnalyzeContext最后用于构建Application.<br>语义解析跟CQL语法相关, 不过首先我们来看下构建Application的第一步是解析schemas.    </p>
<h2 id="正文一:_Schema">正文一: Schema</h2><p>parseSchemas解析Schema: 循环每个AnalyzeContext的getCreatedSchemas,最后设置为Application的schemas.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">parseSchemas</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    List&lt;Schema&gt; schemas = Lists.newArrayList();</span><br><span class="line">    <span class="keyword">for</span> (AnalyzeContext context : parseContexts) &#123;</span><br><span class="line">        schemas.addAll(context.getCreatedSchemas());</span><br><span class="line">    &#125;</span><br><span class="line">    app.setSchemas(schemas);    <span class="comment">//设置到Application里 ⬅️</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="create_stream_schema">create stream schema</h3><p>那么每个AnalyzeContext的CreatedSchemas是在什么时候设置进来的? 以AnalyzeContext的一个实现类CreateStreamAnalyzeContext为例:  </p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CreateStreamAnalyzeContext.setSchema(Schema)  (com.huawei.streaming.cql.semanticanalyzer.analyzecontext)</span><br><span class="line">    <span class="string">|--CreatePipeStreamAnalyzer.analyze()  </span></span><br><span class="line">    <span class="string">|--ApplicationBuilder.createPipeStreamSplitContext(Schema)  (com.huawei.streaming.cql.builder)</span></span><br><span class="line">    <span class="string">|--CreateOutputStreamAnalyzer.analyze()  </span></span><br><span class="line">    <span class="string">|--FromClauseAnalyzer.createNewStreamContext(String, Schema)  </span></span><br><span class="line">    <span class="string">|--CreateInputStreamAnalyzer.analyze()       ⬅️</span></span><br></pre></td></tr></table></figure>
<p>AnalyzeContext的setSchema方法会被对应的SemanticAnalyzer实现类的analyze调用,比如CreateInputStreamAnalyzer:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> AnalyzeContext <span class="title">analyze</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">//createInputStreamParseContext这个是ParseContext语法解析上下文对应的是CreateInputStatementContext</span></span><br><span class="line">    String streamName = createInputStreamParseContext.getStreamName();</span><br><span class="line">    ColumnNameTypeListContext columns = createInputStreamParseContext.getColumns();  <span class="comment">//列的信息在语法内容中</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//下面的AnalyzeContext就是CreateStreamAnalyzeContext</span></span><br><span class="line">    getAnalyzeContext().setStreamName(streamName);</span><br><span class="line">    getAnalyzeContext().setSchema(createSchema(streamName, columns));  <span class="comment">//根据streamName和columns创建Schema</span></span><br><span class="line">    </span><br><span class="line">    setSerDeDefine();               <span class="comment">//序列化反序列化定义, 包括设置类setSerDeClass和属性setSerDeProperties</span></span><br><span class="line">    setSourceDefine();              <span class="comment">//数据源定义</span></span><br><span class="line">    setParallelNumber();            <span class="comment">//并行度</span></span><br><span class="line">    <span class="keyword">return</span> getAnalyzeContext();     <span class="comment">//返回语义分析的结果,所以上面几个set动作其实都是往AnalyzeContext设置内容的.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从RDBMS的角度来理解Schema: 如果把streamName看做table, schema就表示表元数据: 表是由列组成的.<br>streamName和columns信息最初都是存放在语法内容的上下文中ParseContext, 而这里的语义分析上下文AnalyzeContext需要根据语法创建对应的表结构.<br>创建表元数据createSchema很简单了,就是new一个Schema,并将columns的每一列创建一个Column对象,加入到Schema中.<br>现在我们知道了Schema的来龙去脉了. 其实语义分析的基础是语法内容, 根据语法产生的数据,构建语义需要的数据结构.  </p>
<p>以设置序列化反序列化方式为例:setSerDeDefine()-&gt;setSerDeClass+setSerDeProperties, 只不过把语法解析的内容取出来设置到语义解析结果中.   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setSerDeClass</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">//1.首先从语法解析结果中获取出反序列化类</span></span><br><span class="line">    ClassNameContext deserClassName = createInputStreamParseContext.getDeserClassName();</span><br><span class="line">    setSerDeByCQL(deserClassName);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setSerDeByCQL</span><span class="params">(ClassNameContext deserClassName)</span> </span>&#123;</span><br><span class="line">    String newDeserClassName = deserClassName.getClassName();</span><br><span class="line">    <span class="comment">//2.然后设置到语义解析结果中</span></span><br><span class="line">    getAnalyzeContext().setDeserializerClassName(newDeserClassName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>能不能把语法和语义的这两个上下文结果数据合并在一起? 作者在注释中也提到了这个问题. 如果修改了语法,对应的语义也要一起修改(SemanticAnalyzerFactory).   </p>
</blockquote>
<h3 id="insert_&amp;_select_schema">insert &amp; select schema</h3><p>对于insert的Schemas, 来自于select, 而select又会再次调用from的getCreatedSchemas,下面是三个AnalyzeContext获取创建的Schemas:      </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Schema&gt; <span class="title">getCreatedSchemas</span><span class="params">()</span> </span>&#123;                   <span class="comment">// InsertAnalyzeContext</span></span><br><span class="line">    List&lt;Schema&gt; schemas = Lists.newArrayList();</span><br><span class="line">    schemas.addAll(selectContext.getCreatedSchemas());      <span class="comment">// 1.Insert使用了Select ⬅️</span></span><br><span class="line">    schemas.addAll(<span class="keyword">super</span>.getCreatedSchemas());</span><br><span class="line">    <span class="keyword">return</span> schemas;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Schema&gt; <span class="title">getCreatedSchemas</span><span class="params">()</span> </span>&#123;                   <span class="comment">// SelectAnalyzeContext</span></span><br><span class="line">    List&lt;Schema&gt; schemas = Lists.newArrayList();</span><br><span class="line">    schemas.addAll(fromClauseContext.getCreatedSchemas());  <span class="comment">// 2.Select使用了From的 ⬅️</span></span><br><span class="line">    schemas.addAll(<span class="keyword">super</span>.getCreatedSchemas());</span><br><span class="line">    <span class="keyword">return</span> schemas;</span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Schema&gt; <span class="title">getCreatedSchemas</span><span class="params">()</span> </span>&#123;                   <span class="comment">// FromClauseAnalyzeContext</span></span><br><span class="line">    List&lt;Schema&gt; schemas = <span class="keyword">new</span> ArrayList&lt;Schema&gt;();</span><br><span class="line">    <span class="keyword">for</span> (Entry&lt;String, InsertAnalyzeContext&gt; et : subQueryForStream.entrySet()) &#123;</span><br><span class="line">        String streamName = et.getKey();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; inputSchemas.size(); i++) &#123;     <span class="comment">// 4.最终都是来源于这里的</span></span><br><span class="line">            <span class="keyword">if</span> (streamName.equals(inputSchemas.get(i).getStreamName())) &#123;</span><br><span class="line">                schemas.add(inputSchemas.get(i));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        InsertAnalyzeContext sp = et.getValue();</span><br><span class="line">        schemas.addAll(sp.getCreatedSchemas());             <span class="comment">// 3.我勒个去,又回到了Insert ⬅️</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> schemas;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>对于其他的AnalyzeContext, getCreatedSchemas一般是没有的. 表结构实际上只有输入输出流和insert/select语句才有.  </p>
</blockquote>
<h2 id="正文二:_CQL_Statement_syntax">正文二: CQL Statement syntax</h2><p>下面结合官方的CQL语法文档配合上对应的StatementContext,AnalyzeContext一起分析常用的几种语法结构:  </p>
<h3 id="Create_Input_Stream_Statement_syntax">Create Input Stream Statement syntax</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">INPUT</span> STREAM streamName </span><br><span class="line">columnList [streamComment]      ①数据列</span><br><span class="line">[serdeClause]                   ②反序列化</span><br><span class="line">sourceClause                    ③数据读取/数据源</span><br><span class="line">[parallelClause];</span>               ④可选的算子并行度</span><br></pre></td></tr></table></figure>
<p>Create Input Stream 语句定义了输入流的<code>①数据列名称,③数据读取方式和②数据反序列化方式</code>。<br>SERDE 定义了数据的反序列化方式,即如何将从inputStream中读取的数据解析成对应的完整的流的Schema。<br>系统配置了默认的序列化和反序列化类,当使用系统默认反序列化类时,SERDE子句可以省略,同时,SERDE 的相关配置属性也可以省略。<br>SOURCE 定义了数据从什么地方读取,比如从MQ消息队列中读取或者文件等其他方式读取。SOURCE 语句一定不能省略。<br>ParallelClause 语句指定了该输入算子的并发数量。</p>
<p>使用系统内置反序列化类读取算子示例  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">INPUT</span> STREAM example</span><br><span class="line">(eventId <span class="built_in">INT</span>, eventDesc <span class="keyword">STRING</span>)</span><br><span class="line">SERDE simpleSerDe PROPERTIES (separator = <span class="string">"|"</span>)</span><br><span class="line"><span class="keyword">SOURCE</span> TCPClientInput PROPERTIES (<span class="keyword">server</span> = <span class="string">"127.0.0.1"</span>, port = <span class="string">"9999"</span>)</span><br><span class="line"><span class="keyword">Parallel</span> <span class="number">2</span>;</span></span><br></pre></td></tr></table></figure>
<p>输入流语句的Context即输入流的语法解析内容对应了语法结构的组成部分.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CreateInputStatementContext</span> <span class="keyword">extends</span> <span class="title">CreateStreamStatementContext</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> ClassNameContext deserClassName;                <span class="comment">//② 反序列化类: SERDE  </span></span><br><span class="line">    <span class="keyword">private</span> StreamPropertiesContext deserProperties;        <span class="comment">//   反序列化属性</span></span><br><span class="line">    <span class="keyword">private</span> ClassNameContext sourceClassName;               <span class="comment">//③ 数据源类: SOURCE  </span></span><br><span class="line">    <span class="keyword">private</span> StreamPropertiesContext sourceProperties;       <span class="comment">//   数据源属性</span></span><br><span class="line">    <span class="keyword">private</span> ParallelClauseContext parallelNumber;           <span class="comment">//④ 并行度: PARALLEL  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>streamName和columns是在父类CreateStreamStatementContext中定义的.<br>CreateInputStreamAnalyzer语义分析的结果已经在上面分析过了. 接下来我们看看insert和select语法,<br>分别按照A.语法结构定义(statement syntax), B.语法解析结果(statement context), C.语义解析(analyze) D.语义解析结果(analyze context).<br>最终结果都是为了得到语义解析结果. 一般都是获取语法解析的结果(StatementContext)经过计算最后设置到语义解析结果中(AnalyzeContext).    </p>
</blockquote>
<h3 id="Insert_Statement_syntax">Insert Statement syntax</h3><p><code>A</code>.Insert包含三种<strong>语法结构</strong>:  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--1.Single Insert</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM transformTemp <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> transform;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--2.不包含子查询的 MultiInsert</span></span><br><span class="line">FROM teststream</span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s1 <span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s2 <span class="keyword">SELECT</span> a</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s3 <span class="keyword">SELECT</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">WHERE</span> <span class="keyword">id</span> &gt; <span class="number">10</span> </span><br><span class="line"><span class="keyword">Parallel</span> <span class="number">4</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--3.包含子查询的 MultiInsert</span></span><br><span class="line">FROM</span><br><span class="line">(</span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> <span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">as</span> <span class="keyword">id</span>, <span class="string">'sss'</span> <span class="keyword">as</span> <span class="keyword">name</span></span><br><span class="line"><span class="keyword">FROM</span> testStream(<span class="keyword">id</span> &gt;<span class="number">5</span> )[<span class="keyword">RANGE</span> <span class="number">1</span> SECONDS SLIDE] <span class="keyword">GROUP</span> <span class="keyword">BY</span> ss</span><br><span class="line">)</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s1 <span class="keyword">SELECT</span> *</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s2 <span class="keyword">SELECT</span> a</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s3 <span class="keyword">SELECT</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">WHERE</span> <span class="keyword">id</span> &gt; <span class="number">10</span>;</span></span><br></pre></td></tr></table></figure>
<p>可以将数据导入一个未定义的流中,但是如果有要将多个流的数据导入一个新流,这么几个导入语句生成的schema列名称和列类型必须相同。<br>允许将select结果导入到一个不存在的流中,只要这个流不是输入和输出流,系统就会自动创建该流。  </p>
<p>MultiInsert 语句一般用来进行单流数据的分割,它可以将一个流的数据,按照不同的处理规则,在处理完毕之后,发送至指定流。从而达到单流变多流的目的。<br>MultiInsert 语句只有一个From子句,该子句中,只允许进行简单流的定义,不允许出现窗口等复杂语法。</p>
<p><code>B</code>.InsertStatementContext对应第一种的<strong>语法解析结果</strong>(没有并行度), MultiInsertStatementContext对应2.3两种的多级插入语法解析结果(有并行度).  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InsertStatementContext</span> <span class="keyword">extends</span> <span class="title">ParseContext</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String streamName;                  <span class="comment">//insert into stream s1的streamName是s1</span></span><br><span class="line">    <span class="keyword">private</span> SelectStatementContext select;      <span class="comment">//select * from ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MultiInsertStatementContext</span> <span class="keyword">extends</span> <span class="title">ParseContext</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> FromClauseContext from;             <span class="comment">//FROM ...</span></span><br><span class="line">    <span class="keyword">private</span> List&lt;MultiInsertContext&gt; inserts;   <span class="comment">//insert into ... insert into ...</span></span><br><span class="line">    <span class="keyword">private</span> ParallelClauseContext parallel;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>C</code>.insert语句的<strong>语义解析</strong>(multi insert的类似:解析from以及多条insert语句):  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">InsertStatementAnalyzer</span> <span class="keyword">extends</span> <span class="title">BaseAnalyzer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> InsertAnalyzeContext context = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> InsertStatementContext insertContext;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> AnalyzeContext <span class="title">analyze</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String streamName = insertContext.getStreamName();  <span class="comment">//insertContext: InsertStatementContext</span></span><br><span class="line">        context.setOutputStreamName(streamName);</span><br><span class="line">        <span class="keyword">if</span> (checkSchemaExists(streamName)) &#123;</span><br><span class="line">            context.setOutputSchema(getSchemaByName(streamName));</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            context.setPipeStreamNotCreated(<span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        analyzeSelectStatement();   <span class="comment">//分析Select语句</span></span><br><span class="line">        createOutputStream();       <span class="comment">//创建输出流</span></span><br><span class="line">        <span class="keyword">return</span> context;             <span class="comment">//返回的是语义分析的结果AnalyzeContext: InsertAnalyzeContext</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>D</code>.语义解析结果AnalyzeContext: 对于第一种的insert语句(单条insert),包含了select子句.<br>InsertStatementContext 语法解析结果中含有 SelectStatementContext select.<br>InsertAnalyzeContext 语义解析结果中也含有 SelectAnalyzeContext selectContext.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">analyzeSelectStatement</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">//insertContext.getSelect()返回的是InsertStatementContext中的SelectStatementContext, 对应分析器就是SelectStatementAnalyzer</span></span><br><span class="line">    SemanticAnalyzer selectAnalzyer = SemanticAnalyzerFactory.createAnalyzer(insertContext.getSelect(), getAllSchemas());</span><br><span class="line">    <span class="keyword">if</span> (selectAnalzyer <span class="keyword">instanceof</span> SelectStatementAnalyzer) &#123;</span><br><span class="line">        <span class="comment">//创建查询语句的语义分析器, 设置查询的输出流名称为当前insert语句的输出流名称. </span></span><br><span class="line">        <span class="comment">//比如insert into stream s1 select ... 表示查询语句的输出是s1.  </span></span><br><span class="line">        ((SelectStatementAnalyzer)selectAnalzyer).setOutputStreamName(context.getOutputStreamName());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//InsertAnalyzeContext context是insert语句的语义解析结果. </span></span><br><span class="line">    <span class="comment">//还是要对查询分析器SelectStatementAnalyzer经过分析analyze得到查询语义解析结果SelectAnalyzeContext</span></span><br><span class="line">    context.setSelectContext((SelectAnalyzeContext)selectAnalzyer.analyze());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>insert中包含了select语句, 所以在语义解析insert语句时, 要首先对其包含的select进行语义解析.  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">    SelectStatementContext <span class="operator"><span class="keyword">select</span> <span class="comment">-----------------------------</span></span><br><span class="line">          |                                                   ⬇️</span><br><span class="line">    InsertStatementContext insertContext                      ⬇️  insertContext.getSelect() = SelectStatementContext</span><br><span class="line">          |                                                   ⬇️  根据ParaseContext创建对应的SemanticAnalyzer  </span><br><span class="line">InsertStatementAnalyzer.<span class="keyword">analyze</span>()                             ⬇️  再调用语义解析器的<span class="keyword">analyze</span>方法进行语义解析,返回值为语义解析结果AnalyzeContext</span><br><span class="line">                       |<span class="comment">--analyzeSelectStatement() -- SelectStatementAnalyzer.analyze()</span></span><br><span class="line">                       |<span class="comment">--createOutputStream()                                |</span></span><br><span class="line">                       |<span class="comment">--InsertAnalyzeContext  &lt;-----------------------------|--SelectAnalyzeContext</span></span><br><span class="line">                               <span class="keyword">context</span>              <span class="keyword">context</span>.setSelectContext</span></span><br></pre></td></tr></table></figure>
<p>创建输出流, select的schema也就是insert的schema.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">createOutputStream</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">//select的Schema中应该已经包含了完整的元数据,比如columns, 这里创建的输出流只是更改了输出流的名称.  </span></span><br><span class="line">    Schema outputSchema = context.getSelectContext().getSelectClauseContext().getOutputSchema();</span><br><span class="line">    outputSchema.setId(context.getOutputStreamName());          </span><br><span class="line">    outputSchema.setName(context.getOutputStreamName());        </span><br><span class="line">    outputSchema.setStreamName(context.getOutputStreamName());</span><br><span class="line">    context.setOutputSchema(outputSchema);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Select_Statement_syntax">Select Statement syntax</h3><p><code>A</code>.查询语句的语法结构:  </p>
<figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SelectClause </span><br><span class="line">FromClause </span><br><span class="line"><span class="string">[WhereClause]</span> </span><br><span class="line"><span class="string">[GroupByClause]</span> </span><br><span class="line"><span class="string">[HavingClause]</span> </span><br><span class="line"><span class="string">[OrderbyClause]</span> </span><br><span class="line"><span class="string">[LimitClause]</span> </span><br><span class="line"><span class="string">[ParallelClause]</span>;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Select语句(Statement)中的每个子句(Clause)都有自己的语法结构,因此都有自己的语法解析和语义解析.  </p>
</blockquote>
<p><code>B</code>.Select语句包含了多个子句, 比如select <em> from table. 则`select </em><code>对应select子句,</code>from table`对应from子句.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SelectStatementContext</span> <span class="keyword">extends</span> <span class="title">ParseContext</span> </span>&#123;  <span class="comment">//②</span></span><br><span class="line">    <span class="keyword">private</span> SelectClauseContext select;</span><br><span class="line">    <span class="keyword">private</span> FromClauseContext from;                         <span class="comment">//③</span></span><br><span class="line">    <span class="keyword">private</span> WhereClauseContext where;</span><br><span class="line">    <span class="keyword">private</span> GroupbyClauseContext groupby;</span><br><span class="line">    <span class="keyword">private</span> HavingClauseContext having;</span><br><span class="line">    <span class="keyword">private</span> OrderbyClauseContext orderby;</span><br><span class="line">    <span class="keyword">private</span> LimitClauseContext limit;</span><br><span class="line">    <span class="keyword">private</span> ParallelClauseContext parallel;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>C</code>.查询语句的语义分析(from子句的解析要优先于select子句).  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SelectStatementAnalyzer</span> <span class="keyword">extends</span> <span class="title">BaseAnalyzer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> SelectAnalyzeContext context;                   <span class="comment">//① StatementAnalyzer的analyze结果为AnalyzerContext</span></span><br><span class="line">    <span class="keyword">private</span> SelectStatementContext selectContext;           <span class="comment">//② 语义解析StatementAnalyzer依赖于语法解析StatementContext</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> SelectAnalyzeContext <span class="title">analyze</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        fromAnalyzer();                                     <span class="comment">//③</span></span><br><span class="line">        selectAnalyzer();</span><br><span class="line">        resetOutputColumnTypes();</span><br><span class="line">        whereAnalyzer();</span><br><span class="line">        groupbyAnalyzer();</span><br><span class="line">        havingAnalyzer();</span><br><span class="line">        orderByAnalyzer();</span><br><span class="line">        limitAnalyzer();</span><br><span class="line">        parallelAnalyzer();</span><br><span class="line">        dataSourceQueryArgumentsAnalyzer();</span><br><span class="line">        <span class="keyword">return</span> context;                                     <span class="comment">//①</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>D</code>.查询的语义解析结果SelectAnalyzeContext(和SelectWithOutFromAnalyzeContext)    </p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">一次查询: 带有from的比如select <span class="keyword">*</span> from table</span><br><span class="line">SelectAnalyzeContext ---------------|<span class="string"></span><br><span class="line">      </span>|<span class="string">                             </span>|<span class="string">--FromClauseAnalyzeContext fromClauseContext</span><br><span class="line">      </span>|<span class="string">                             </span>|<span class="string">--ParallelClauseAnalyzeContext parallelClauseContext</span><br><span class="line">      </span>|<span class="string">                             </span>|<span class="string">++SelectStatementContext context   //②</span><br><span class="line">      </span>|</span><br><span class="line">多次查询: 不带from的,比如前面multi insert,对同一个流查询多次</span><br><span class="line">SelectWithOutFromAnalyzeContext ----|<span class="string"></span><br><span class="line">                                    </span>|<span class="string">--SelectClauseAnalyzeContext selectClauseContext</span><br><span class="line">                                    </span>|<span class="string">--FilterClauseAnalzyeContext whereClauseContext</span><br><span class="line">                                    </span>|<span class="string">--SelectClauseAnalyzeContext groupbyClauseContext</span><br><span class="line">                                    </span>|<span class="string">--OrderByClauseAnalyzeContext orderbyClauseContext</span><br><span class="line">                                    </span>|<span class="string">--FilterClauseAnalzyeContext havingClauseContext</span><br><span class="line">                                    </span>|<span class="string">--LimitClauseAnalzyeContext limitClauseContext</span><br><span class="line">                                    </span>|<span class="string">++MultiSelectContext context</span></span><br></pre></td></tr></table></figure>
<p>下面的context是SelectAnalyzeContext, 不同解析器解析的结果也是AnalyzeContext,分别设置到SelectAnalyzeContext对应的字段中.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">fromAnalyzer</span><span class="params">()</span> </span>&#123;                                         <span class="comment">//    ②         ③</span></span><br><span class="line">    SemanticAnalyzer analyzer = SemanticAnalyzerFactory.createAnalyzer(selectContext.getFrom(), getAllSchemas());</span><br><span class="line">    context.setFromClauseContext((FromClauseAnalyzeContext)analyzer.analyze());</span><br><span class="line">&#125;   <span class="comment">//①            ③</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">groupbyAnalyzer</span><span class="params">()</span> </span>&#123;   </span><br><span class="line">    SemanticAnalyzer analyzer = SemanticAnalyzerFactory.createAnalyzer(selectContext.getGroupby(), getInputSchemas());</span><br><span class="line">    context.setGroupbyClauseContext((SelectClauseAnalyzeContext)analyzer.analyze());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>语法解析Parser</code> - <code>语法解析结果ParseContext</code> – <code>语义解析SemanticAnalyzer</code> – <code>语义解析结果AnalyzeContext</code></p>
<p><img src="http://img.blog.csdn.net/20151127193205608" alt="stream-select"></p>
<h3 id="DataSource_Statement_syntax">DataSource Statement syntax</h3><p>数据源和 From 子句一起结合使用,dataSourceBody 语法在 from 字句中也进行了定义  </p>
<p>数据源的查询参数分为查询Schema定义(SCHEMA)和数据源查询(QUERY)两个部分。<br>Schema 的定义同 Create Input Stream 中的 schema 定义,主要用来指定数据源查询结果 的列数量、名称、类型,便于进行下一步处理。</p>
<p>RDB 数据读取,支持多行数据读取,同时支持 CQL UDF 以及窗口和聚合运算<br>QUERY 内部的参数顺序固定,不同的数据源,有不同的参数。<br>RDB 的 SQL 中,如果不包含 Where,就会一次查询出多行记录。和原始流做了 Join 之后,最终输出多条结果。  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--数据源定义</span></span><br><span class="line"><span class="operator"><span class="keyword">CREATE</span> DATASOURCE rdbdatasource         #dataSourceName</span><br><span class="line"><span class="keyword">SOURCE</span> RDBDataSource                    #className</span><br><span class="line">PROPERTIES (                            #datasourceProperties     </span><br><span class="line"><span class="keyword">url</span> = <span class="string">"jdbc:postgresql://127.0.0.1:1521/streaming"</span>, </span><br><span class="line">username = <span class="string">"55B5B07CF57318642D38F0CEE0666D26"</span>, </span><br><span class="line"><span class="keyword">password</span> = <span class="string">"55B5B07CF57318642D38F0CEE0666D26"</span> );</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--数据源查询</span></span><br><span class="line"><span class="operator"><span class="keyword">insert</span> <span class="keyword">into</span> rs <span class="keyword">select</span> rdb.<span class="keyword">id</span>,s.<span class="keyword">id</span>,<span class="keyword">count</span>(rdb.<span class="keyword">id</span>),<span class="keyword">sum</span>(s.<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">from</span> S[<span class="keyword">rows</span> <span class="number">10</span> slide],                  #普通的流可以和DataSource进行<span class="keyword">join</span> </span><br><span class="line">DATASOURCE rdbdatasource                #dataSourceName, 前面定义好的数据源</span><br><span class="line">[                                       #dataSourceBody</span><br><span class="line"><span class="keyword">SCHEMA</span> (<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">String</span>,<span class="keyword">type</span> <span class="built_in">int</span>),                                       #dataSourceSchema</span><br><span class="line"><span class="keyword">QUERY</span> (<span class="string">"select rid as id,rname,rtype from rdbtable where id = ? "</span>, s.<span class="keyword">id</span>)    #dataSourceQuery</span><br><span class="line">] rdb                                   #sourceAlias</span><br><span class="line"><span class="keyword">where</span> rdb.<span class="keyword">name</span> <span class="keyword">like</span> <span class="string">'%hdd%'</span>             #dataSourceArguments</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> rdb.<span class="keyword">id</span>,s.<span class="keyword">id</span>;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>原始流s的id会作为查询条件,传入数据源的SQL查询语句中, 同时数据源本身也有自己的查询条件.  </p>
</blockquote>
<p>CreateDataSource相关的语法解析,语义解析和CreateStreamStatement类似,就不分析了. </p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Source/">Source</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/storm/">storm</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-11-24-StreamCQL-task" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/11/24/2015-11-24-StreamCQL-task/" class="article-date">
  	<time datetime="2015-11-23T16:00:00.000Z" itemprop="datePublished">2015-11-24</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/24/2015-11-24-StreamCQL-task/">StreamCQL源码阅读(1) 提交任务</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <p>大数据上的流式SQL引擎—StreamCQL: <a href="http://www.csdn.net/article/2015-11-13/2826204" target="_blank" rel="external">http://www.csdn.net/article/2015-11-13/2826204</a></p>
<h1 id="Introduce">Introduce</h1><p>CQL（Continuous Query Language），持续查询语言，用于数据流上的查询语言。相对于传统的SQL，CQL加入了窗口的概念，使得数据可以一直保存在内存中，由此可以快速进行大量内存计算，CQL的输出结果为数据流在某一时刻的计算结果。  </p>
<p>CQL是建立在Storm基础上的类SQL查询语言，它解决了Storm原生API使用复杂，上手难度高，很多基本功能缺失的问题，提升了流处理产品的易用性。  </p>
<p>在CQL设计语法之初，通过参考市面上现有的CEP产品的语法，发现这些产品都不算是全部的SQL语句，即仅仅使用SQL语句还不能运行，还必须依靠一些客户端的代码。 这样就给使用带来了一些不便， 用户必须学习客户端API，比较繁琐，上手难度也比较大。  </p>
<p>所以，CQL设计目标就是，用纯粹的SQL语句再加上一些命令，就可以完成所有的任务发布以及执行，这样，就可以通过SQL接口，直接进行任务的下发，统一了客户端接口。对于有一定SQL基础的用户，只需要掌握一些CQL比较特殊的语法，比如窗口或者流定义的语法，就可以写出可运行的CQL程序，大大降低了上手难度。  </p>
<p><strong>关键概念</strong></p>
<p><strong>Stream(流)</strong>：流是一组（无穷）元素的集合，流上的每个元素都属于同一个schema；每个元素都和逻辑时间有关；即流包含了元组和时间的双重属性。留上的任何一个元素，都可以用Element<tuple,time>的方式来表示，tuple是元组，包含了数据结构和数据内容，Time就是该数据的逻辑时间。</tuple,time></p>
<p><strong>Window(窗口)</strong>：窗口是流处理中解决事件的无边界（unbounded）及流动性的一种重要手段，把事件流在某一时刻变成静态的视图，以便进行类似数据库表的各种查询操作。在stream上可以定义window，窗口有两种类型，时间窗口（time-based）和记录窗口（row-based）。两种窗口都支持两种模式，滑动（slide）和跳动（tumble）。</p>
<p><strong>Expression(表达式)</strong>：符号和运算符的一种组合，CQL解析引擎处理该组合以获取单个值。简单表达式可以是常量、变量或者函数，可以用运算符将两个或者多个简单表达式联合起来构成更复杂的表达式。</p>
<h1 id="QuickStart">QuickStart</h1><p>1.startup zk and storm local  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh <span class="operator"><span class="keyword">start</span></span><br><span class="line"></span><br><span class="line">nohup <span class="keyword">bin</span>/storm nimbus &amp;</span><br><span class="line">nohup <span class="keyword">bin</span>/storm ui &amp;</span><br><span class="line">nohup <span class="keyword">bin</span>/storm supervisor &amp;</span></span><br></pre></td></tr></table></figure>
<p>2.build and run cql client</p>
<figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="label">cd</span> <span class="keyword">StreamCQL</span><br><span class="line"></span><span class="keyword">mvn </span>clean install</span><br><span class="line"><span class="label">cd</span> cql-<span class="keyword">binary/target</span><br><span class="line"></span><span class="label">tar</span> xvf <span class="keyword">stream-cql-bianry-1.0.tar.gz</span><br><span class="line"></span><span class="label">cd</span> <span class="keyword">stream-cql-bianry-1.0</span><br><span class="line"></span><span class="keyword">bin/cql</span></span><br></pre></td></tr></table></figure>
<p>3.create first topology:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">INPUT</span> STREAM s(<span class="keyword">id</span> <span class="built_in">INT</span>, <span class="keyword">name</span> <span class="keyword">STRING</span>, <span class="keyword">type</span> <span class="built_in">INT</span>)</span><br><span class="line"><span class="keyword">SOURCE</span> randomgen PROPERTIES ( timeUnit = <span class="string">"SECONDS"</span>, <span class="keyword">period</span> = <span class="string">"1"</span>, eventNumPerperiod = <span class="string">"1"</span>, isSchedule = <span class="string">"true"</span> );</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">OUTPUT</span> STREAM rs(<span class="keyword">type</span> <span class="built_in">INT</span>, cc <span class="built_in">INT</span>)</span><br><span class="line">SINK consoleOutput;</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM rs <span class="keyword">SELECT</span> <span class="keyword">type</span>, <span class="keyword">COUNT</span>(<span class="keyword">id</span>) <span class="keyword">as</span> cc</span><br><span class="line"><span class="keyword">FROM</span> s[<span class="keyword">RANGE</span> <span class="number">20</span> SECONDS BATCH]</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">id</span> &gt; <span class="number">5</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">type</span>;</span></span><br><span class="line"></span><br><span class="line">SUBMIT APPLICATION example;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>输入流: 随机数,每秒生成一个事件<br>输出流: 控制台, 每隔20秒输出一次, 只统计id&gt;5,根据type分组,求和<br>提交应用程序, 相当于创建了一个Storm的Topology.  </p>
</blockquote>
<p>4.A complicate topology:  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">INPUT</span> STREAM s1(<span class="keyword">name</span> <span class="keyword">STRING</span>)</span><br><span class="line"><span class="keyword">SOURCE</span> RANDOMGEN PROPERTIES ( timeUnit = <span class="string">"SECONDS"</span>, <span class="keyword">period</span> = <span class="string">"1"</span>, eventNumPerperiod = <span class="string">"1"</span>, isSchedule = <span class="string">"true"</span> );</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">OUTPUT</span> STREAM s2(c1 <span class="keyword">STRING</span>)</span><br><span class="line">SINK kafakOutput PROPERTIES ( topic = <span class="string">"cqlOut"</span>, zookeepers = <span class="string">"127.0.0.1:2181"</span>, brokers = <span class="string">"127.0.0.1:9092"</span> );</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">INPUT</span> STREAM s3( c1 <span class="keyword">STRING</span>)</span><br><span class="line"><span class="keyword">SOURCE</span> KafkaInput PROPERTIES (groupid = <span class="string">"cqlClient"</span>, topic = <span class="string">"cqlInput"</span>, zookeepers = <span class="string">"127.0.0.1:2181"</span>, brokers = <span class="string">"127.0.0.1:9092"</span> )</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">OUTPUT</span> STREAM s4(c1 <span class="keyword">STRING</span>)</span><br><span class="line">SINK consoleOutput;</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s2 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> s1;</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s4 <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> s3;</span></span><br><span class="line"></span><br><span class="line">SUBMIT APPLICATION cql_kafka_example;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>输入流s1 发射数据 到kafka输出流s2<br>kafka输入流 发射数据 到控制台输出流s4</p>
</blockquote>
<p>5.查看拓扑: <a href="http://localhost:8080" target="_blank" rel="external">http://localhost:8080</a> </p>
<h1 id="Architecture">Architecture</h1><p>StreamCQL的代码由三部分组成: cql,streaming,adapter分别对应下面的三个组件.  </p>
<p><img src="http://img.blog.csdn.net/20151125162710289" alt="stream-arch"></p>
<blockquote>
<p>客户端提交的<code>CQL语句</code>会由执行计划生成器<code>ExecutorPlanGenerator</code>生成<code>可运行的任务</code>,最终由<code>Storm适配器</code>组装Topology提交执行.</p>
</blockquote>
<p>StreamCQL对应的Storm拓扑:  </p>
<p><img src="http://img.blog.csdn.net/20151125162722310" alt="stream-storm"></p>
<blockquote>
<p>至少有一个输入和输出. Component之间可以组合比如Select,Join等.  </p>
</blockquote>
<p>Window example:  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--按照type对窗口内数据进行分组,每组容量为10</span></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> transformEvent[<span class="keyword">ROWS</span> <span class="number">10</span> SLIDE <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">TYPE</span>];</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">--时间排序窗,一般用来解决数据乱序问题</span></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> transformEvent[<span class="keyword">RANGE</span> <span class="number">1000</span> MILLISECONDS <span class="keyword">SORT</span> <span class="keyword">BY</span> dte];</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--事件驱动时间滑动窗</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM rs <span class="keyword">sum</span>(OrderPrice),<span class="keyword">avg</span>(OrderPrice),<span class="keyword">count</span>(OrderPrice)</span><br><span class="line"><span class="keyword">FROM</span> transformEvent[<span class="keyword">RANGE</span> <span class="number">10</span> SECONDS SLIDE <span class="keyword">TRIGGER</span> <span class="keyword">by</span> TS <span class="keyword">EXCLUDE</span> <span class="keyword">now</span>];</span></span><br><span class="line"></span><br><span class="line"><span class="comment">--保存周期为一个自然天的分组窗</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM rs <span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span>,<span class="keyword">count</span>(<span class="keyword">id</span>)</span><br><span class="line"><span class="keyword">FROM</span> transformEvent[<span class="keyword">RANGE</span> TODAY ts <span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">TYPE</span>] </span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">id</span> &gt; <span class="number">5</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">TYPE</span> <span class="keyword">HAVING</span> <span class="keyword">id</span> &gt; <span class="number">10</span>;</span></span><br></pre></td></tr></table></figure>
<p>Split example:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM teststream</span><br><span class="line">  <span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s1 <span class="keyword">SELECT</span> *</span><br><span class="line">  <span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s2 <span class="keyword">SELECT</span> a</span><br><span class="line">  <span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM s3 <span class="keyword">SELECT</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">WHERE</span> <span class="keyword">id</span> &gt; <span class="number">10</span></span><br><span class="line">PRARLLEL <span class="number">4</span>;</span></span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20151126091936179" alt="stream-split"></p>
<blockquote>
<p>原始的spout输入流会分成三个输出流. 所以中间用一个SplitBolt来作为中间介质.  </p>
</blockquote>
<h1 id="From_log_to_see_StreamCQL">From log to see StreamCQL</h1><blockquote>
<p>熟悉一个开源框架的流程, 可以先跑一个测试例子, 查看打印的日志信息, 通过日志的顺序, 可以大致熟悉整体的流程.<br>当然要求框架本身的日志信息足够明了, StreamCQL做的不错. 这种方式的优点是不至于不知道要从哪里看起来. </p>
</blockquote>
<h2 id="CQLClient_to_Driver">CQLClient to Driver</h2><p><code>bin/cql</code>会开启一个CQLClient客户端, 当输入<code>;</code>表示一个语句的终结时,就会触发一次CQL语句的编译执行等.</p>
<p>Driver.run是CQL的运行起点</p>
<ul>
<li>1、编译</li>
<li>2、语义分析</li>
<li>3、命令执行</li>
<li>4、返回结果</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">(String cql)</span> </span>&#123;</span><br><span class="line">    ParseContext parseContext = parser.parse(cql);  <span class="comment">//CQL解析 ⬅️</span></span><br><span class="line">    saveAllChangeableCQLs(cql, parseContext);</span><br><span class="line">    preDriverRun(context, parseContext);</span><br><span class="line">    executeTask(parseContext);                      <span class="comment">//执行任务</span></span><br><span class="line">    postDriverRun(context, parseContext);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">executeTask</span><span class="params">(ParseContext parseContext)</span> </span>&#123;</span><br><span class="line">    mergeConfs();</span><br><span class="line">    Task task = TaskFactory.createTask(context, parseContext, config, analyzeHooks);</span><br><span class="line">    task.execute(parseContext);                     <span class="comment">//执行任务 ⬅️</span></span><br><span class="line">    context.setQueryResult(task.getResult());       <span class="comment">//返回结果(查询性的命令比如show,get会有结果,其他没有结果)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>初始的语法解析类是ApplicationParser. parse方法采用visitor访问者模式遍历CQL语句. 感兴趣的可以进入CQLParser查看具体的解析过程.     </p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">IParser (com.huawei.streaming.cql.semanticanalyzer.parser)</span><br><span class="line">    <span class="string">|-- OrderbyClauseParser </span></span><br><span class="line">    <span class="string">|-- GroupbyClauseParser </span></span><br><span class="line">    <span class="string">|-- ApplicationParser                   ⬅️</span></span><br><span class="line">    <span class="string">|-- SelectClauseParser </span></span><br><span class="line">    <span class="string">|-- DataSourceArgumentsParser</span></span><br></pre></td></tr></table></figure>
<p>ParseContext的实现类很多,基本上CQL语法的每一部分都会对应一个语法解析器.    </p>
<figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ParseContext (com.huawei.streaming.cql.semanticanalyzer.parser.<span class="keyword">context</span>)</span><br><span class="line">    |<span class="comment">-- CreateStreamStatementContext        语句</span></span><br><span class="line">            |<span class="comment">-- CreatePipeStatementContext </span></span><br><span class="line">            |<span class="comment">-- CreateInputStatementContext </span></span><br><span class="line">            |<span class="comment">-- CreateOutputStatementContext </span></span><br><span class="line">    |<span class="comment">-- FromClauseContext                   子句</span></span><br><span class="line">    |<span class="comment">-- RangeWindowContext                  窗口</span></span><br><span class="line">    |<span class="comment">-- ...</span></span><br></pre></td></tr></table></figure>
<h2 id="Input,Output,Insert">Input,Output,Insert</h2><p>ApplicationParser.parse返回的ParseContext具体针对特定的CQL语句返回的是什么类型? 这个类型对于创建什么类型的任务非常重要.<br>因为这个是创建一个新的Stream, 所以ParseContext是<code>CreateInputStatementContext</code>.  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2015-11-25 02:32:19 | INFO  | [main] | <span class="operator"><span class="keyword">start</span> <span class="keyword">to</span> <span class="keyword">parse</span> cql : <span class="keyword">CREATE</span> <span class="keyword">INPUT</span> STREAM s</span><br><span class="line">    (<span class="keyword">id</span> <span class="built_in">INT</span>, <span class="keyword">name</span> <span class="keyword">STRING</span>, <span class="keyword">type</span> <span class="built_in">INT</span>)</span><br><span class="line"><span class="keyword">SOURCE</span> randomgen</span><br><span class="line">    PROPERTIES ( timeUnit = <span class="string">"SECONDS"</span>, <span class="keyword">period</span> = <span class="string">"1"</span>,</span><br><span class="line">        eventNumPerperiod = <span class="string">"1"</span>, isSchedule = <span class="string">"true"</span> ) | com.huawei.streaming.cql.semanticanalyzer.parser.ApplicationParser (ApplicationParser.<span class="keyword">java</span>:<span class="number">44</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">19</span> | INFO  | [<span class="keyword">main</span>] | <span class="keyword">Parse</span> Completed | com.huawei.streaming.cql.semanticanalyzer.parser.ApplicationParser (ApplicationParser.<span class="keyword">java</span>:<span class="number">69</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">19</span> | INFO  | [<span class="keyword">main</span>] | <span class="keyword">start</span> <span class="keyword">to</span> <span class="keyword">execute</span> <span class="keyword">CREATE</span> <span class="keyword">INPUT</span> STREAM s (<span class="keyword">id</span> <span class="built_in">INT</span>, <span class="keyword">name</span> <span class="keyword">STRING</span>, <span class="keyword">type</span> <span class="built_in">INT</span>) <span class="keyword">SOURCE</span> randomgen PROPERTIES ( <span class="string">'timeUnit'</span> = <span class="string">'SECONDS'</span>, <span class="string">'period'</span> = <span class="string">'1'</span>, <span class="string">'eventNumPerperiod'</span> = <span class="string">'1'</span>, <span class="string">'isSchedule'</span> = <span class="string">'true'</span> ) | com.huawei.streaming.cql.tasks.LazyTask (LazyTask.<span class="keyword">java</span>:<span class="number">62</span>)</span></span><br></pre></td></tr></table></figure>
<p>CreateStreamStatementContext.createTask创建的是LazyTask. 它的execute方法只是把当前ParseContext加入到DriverContext的parseContexts中.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(ParseContext parseContext)</span> </span>&#123;</span><br><span class="line">    context.addParseContext(parseContext);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同样输出流经过ApplicationParser.parse返回的是<code>CreateOutputStatementContext</code>,它也继承了CreateStreamStatementContext.  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2015-11-25 02:32:20 | INFO  | [main] | <span class="operator"><span class="keyword">start</span> <span class="keyword">to</span> <span class="keyword">parse</span> cql : <span class="keyword">CREATE</span> <span class="keyword">OUTPUT</span> STREAM rs</span><br><span class="line">    (<span class="keyword">type</span> <span class="built_in">INT</span>, cc <span class="built_in">INT</span>)</span><br><span class="line">SINK consoleOutput | com.huawei.streaming.cql.semanticanalyzer.parser.ApplicationParser (ApplicationParser.<span class="keyword">java</span>:<span class="number">44</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">20</span> | INFO  | [<span class="keyword">main</span>] | <span class="keyword">Parse</span> Completed | com.huawei.streaming.cql.semanticanalyzer.parser.ApplicationParser (ApplicationParser.<span class="keyword">java</span>:<span class="number">69</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">20</span> | INFO  | [<span class="keyword">main</span>] | <span class="keyword">start</span> <span class="keyword">to</span> <span class="keyword">execute</span> <span class="keyword">CREATE</span> <span class="keyword">OUTPUT</span> STREAM rs (<span class="keyword">type</span> <span class="built_in">INT</span>, cc <span class="built_in">INT</span>) SINK consoleOutput | com.huawei.streaming.cql.tasks.LazyTask (LazyTask.<span class="keyword">java</span>:<span class="number">62</span>)</span></span><br></pre></td></tr></table></figure>
<p>insert语句是<code>InsertStatementContext</code>, 输入输出插入这些都是延迟执行的任务,并不需要立即执行,因为需要根据上下文构造一个完整的DAG拓扑图.   </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2015-11-25 02:32:20 | INFO  | [main] | <span class="operator"><span class="keyword">start</span> <span class="keyword">to</span> <span class="keyword">parse</span> cql : <span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM rs <span class="keyword">SELECT</span> <span class="keyword">type</span>, <span class="keyword">COUNT</span>(<span class="keyword">id</span>) <span class="keyword">as</span> cc</span><br><span class="line">    <span class="keyword">FROM</span> s[<span class="keyword">RANGE</span> <span class="number">20</span> SECONDS BATCH]</span><br><span class="line">    <span class="keyword">WHERE</span> <span class="keyword">id</span> &gt; <span class="number">5</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">type</span> | com.huawei.streaming.cql.semanticanalyzer.parser.ApplicationParser (ApplicationParser.<span class="keyword">java</span>:<span class="number">44</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">20</span> | INFO  | [<span class="keyword">main</span>] | <span class="keyword">Parse</span> Completed | com.huawei.streaming.cql.semanticanalyzer.parser.ApplicationParser (ApplicationParser.<span class="keyword">java</span>:<span class="number">69</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">20</span> | INFO  | [<span class="keyword">main</span>] | <span class="keyword">start</span> <span class="keyword">to</span> <span class="keyword">execute</span> <span class="keyword">INSERT</span> <span class="keyword">INTO</span> STREAM rs <span class="keyword">SELECT</span> <span class="keyword">type</span>, <span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">AS</span> cc <span class="keyword">FROM</span> s[<span class="keyword">RANGE</span> <span class="number">20</span> SECONDS BATCH] <span class="keyword">WHERE</span> <span class="keyword">id</span> &gt; <span class="number">5</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">type</span> | com.huawei.streaming.cql.tasks.LazyTask (LazyTask.<span class="keyword">java</span>:<span class="number">62</span>)</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Submit">Submit</h2><p>提交应用程序,经过parse返回的是<code>SubmitApplicationContext</code>,创建的Task是SubmitTask.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">23</span> | INFO  | [main] | start to parse cql : SUBMIT APPLICATION example | com.huawei.streaming.cql.semanticanalyzer.parser.ApplicationParser (ApplicationParser.java:<span class="number">44</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">23</span> | INFO  | [main] | Parse Completed | com.huawei.streaming.cql.semanticanalyzer.parser.ApplicationParser (ApplicationParser.java:<span class="number">69</span>)</span><br><span class="line"><span class="number">2015</span>-<span class="number">11</span>-<span class="number">25</span> <span class="number">02</span>:<span class="number">32</span>:<span class="number">24</span> | INFO  | [main] | combine all split contexts | com.huawei.streaming.cql.builder.operatorsplitter.OperatorSplitter (OperatorCombiner.java:<span class="number">101</span>)</span><br></pre></td></tr></table></figure>
<h3 id="Task_&amp;_SemanticAnalyzer">Task &amp; SemanticAnalyzer</h3><p><img src="http://img.blog.csdn.net/20151125203206771" alt="stream-createTask"></p>
<blockquote>
<p>可以看到对于前面的输入流,输出流,insert语句,并没有对应的Task实现类,所以它们都使用LazyTask.  </p>
</blockquote>
<p>Driver.executeTask会根据ParseContext具体的实现类由TaskFactory创建对应的Task.  ParseContext抽象类除了创建Task,还会创建SemanticAnalyzer  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建对应语句的执行task</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Task <span class="title">createTask</span><span class="params">(DriverContext driverContext, List&lt;SemanticAnalyzeHook&gt; analyzeHooks)</span> <span class="keyword">throws</span> CQLException</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建语义分析执行解析器</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> SemanticAnalyzer <span class="title">createAnalyzer</span><span class="params">()</span> <span class="keyword">throws</span> SemanticAnalyzerException</span>;</span><br></pre></td></tr></table></figure>
<p>比如SubmitApplicationContext创建的分析器是SubmitApplicationAnalyzer. CreateStreamStatementContext也是个抽象类,<br>有三个子类CreateInputStatementContext,CreateOutputStatementContext,CreatePipeStatementContext,它们创建的分析器分别是:<br>CreateInputStreamAnalyzer, CreateOutputStreamAnalyzer, CreatePipeStreamAnalyzer 它们都继承了CreateStreamAnalyzer.  </p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">ParseContext</span> <span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">SubmitApplicationContext</span></span><br><span class="line">                   <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">CreateStreamStatementContext</span><span class="literal">-</span><span class="literal">-</span><span class="comment">|</span></span><br><span class="line">                                                    <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="comment">CreateInputStatementContext</span></span><br><span class="line">                                                    <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="comment">CreateOutputStatementContext</span></span><br><span class="line">                                                    <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="comment">CreatePipeStatementContext</span></span><br><span class="line">                                                            </span><br><span class="line"><span class="comment">SemanticAnalyzer</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">SubmitApplicationAnalyzer</span></span><br><span class="line">                   <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">CreateStreamAnalyzer</span> <span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="comment">|</span></span><br><span class="line">                                                    <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="comment">CreateInputStreamAnalyzer</span></span><br><span class="line">                                                    <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="comment">CreateOutputStreamAnalyzer</span></span><br><span class="line">                                                    <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="literal">-</span><span class="comment">CreatePipeStreamAnalyzer</span></span><br></pre></td></tr></table></figure>
<p>SemanticAnalyzer的创建方式和创建Task一样都是使用工厂类<code>SemanticAnalyzerFactory</code>. 在创建完之后都调用了init初始化.   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Task <span class="title">createTask</span><span class="params">(DriverContext driverContext, ParseContext parseContext, StreamingConfig config, List&lt;SemanticAnalyzeHook&gt; analyzeHooks)</span> </span>&#123;</span><br><span class="line">    Task task = parseContext.createTask(driverContext, analyzeHooks);   <span class="comment">// 创建Task ⬅️</span></span><br><span class="line">    task.init(driverContext, config, analyzeHooks);</span><br><span class="line">    <span class="keyword">return</span> task;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> SemanticAnalyzer <span class="title">createAnalyzer</span><span class="params">(ParseContext parseContext, List&lt;Schema&gt; schemas)</span> </span>&#123;</span><br><span class="line">    SemanticAnalyzer analyzer = parseContext.createAnalyzer();          <span class="comment">// 创建语义解析器 ⬅️</span></span><br><span class="line">    analyzer.init(schemas);</span><br><span class="line">    <span class="keyword">return</span> analyzer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="SubmitTask">SubmitTask</h3><h4 id="parseSubmit">parseSubmit</h4><p>SubmitTask执行应用程序提交的execute方法和前面的LazyTask有点复杂, 因为它要把前面创建的LazyTask都组合起来,组成一个完整的应用程序.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(ParseContext parseContext)</span> </span>&#123;   <span class="comment">//这里的parseContext是SubmitApplicationContext</span></span><br><span class="line">    parseSubmit(parseContext);  <span class="comment">//解析  ⬅️ </span></span><br><span class="line">    createApplication();        <span class="comment">//创建应用程序</span></span><br><span class="line">    dropApplicationIfAllow();   <span class="comment">//如果允许的话先删除应用程序</span></span><br><span class="line">    submitApplication();        <span class="comment">//提交应用程序</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//解析Submit,创建对应的语义解析器:SubmitApplicationAnalyzer</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">parseSubmit</span><span class="params">(ParseContext parseContext)</span> </span>&#123;</span><br><span class="line">    SemanticAnalyzer analyzer = SemanticAnalyzerFactory.createAnalyzer(parseContext, EMPTY_SCHEMAS);</span><br><span class="line">    submitContext = (SubmitApplicationAnalyzeContext)analyzer.analyze();    <span class="comment">// 创建完语义解析器, 就要进行语义解析 ⬅️</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里面几个对象的创建关系是(Parser语法解析器-&gt;Context-&gt;Analyzer语义分析器-&gt;AnalyzerContext):  </p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ApplicationParser.parse() <span class="comment">--&gt; SubmitApplicationContext : ParseContext</span></span><br><span class="line">                                    |<span class="comment">--createTask: SubmitTask</span></span><br><span class="line">                                    |<span class="comment">--createAnalyzer: SubmitApplicationAnalyzer</span></span><br><span class="line">                                                               |<span class="comment">--createAnalyzeContext: SubmitApplicationAnalyzeContext</span></span><br></pre></td></tr></table></figure>
<h4 id="createApplication">createApplication</h4><p>创建Application,如果有路径的话,直接加载物理执行计划,否则创建一个API用的Application并设置到DriverContext中.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Application <span class="title">createAPIApplication</span><span class="params">(String appName)</span> </span>&#123;</span><br><span class="line">    Application app = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (context.getApp() == <span class="keyword">null</span>) &#123;             <span class="comment">//创建Application</span></span><br><span class="line">        semanticAnalyzerLazyContexts();         <span class="comment">//准备analyzeContexts ⬅️</span></span><br><span class="line">        app = <span class="keyword">new</span> ApplicationBuilder().build(appName, analyzeContexts, context);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        app = context.getApp();</span><br><span class="line">    &#125;</span><br><span class="line">    app.setApplicationId(appName);</span><br><span class="line">    <span class="keyword">return</span> app;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>创建APIApplication, 记得前面的那些LazyTask吗, 都要用语义分析分析一遍,对应的AnalyzeContext会被ApplicationBuilder用到.<br>因为LazyTask的execute方法只是简单地把当前的ParseContext实现类加入到DriverContext中,所以下面的for循环能从DriverContext获取出所有ParseContext.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">semanticAnalyzerLazyContexts</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (ParseContext parseContext : context.getParseContexts()) &#123;</span><br><span class="line">        preAnalyze(context, parseContext);</span><br><span class="line">        SemanticAnalyzer analyzer = SemanticAnalyzerFactory.createAnalyzer(parseContext, context.getSchemas());</span><br><span class="line">        AnalyzeContext analyzeContext = analyzer.analyze();</span><br><span class="line">        postAnalyze(context, analyzeContext, parseContext);</span><br><span class="line">        analyzeContexts.add(analyzeContext);    <span class="comment">//将各自的分析器分析的结果AnalyzeContext实现类加入到analyzeContexts</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>像SubmitTask一样都要先创建SemanticAnalyzer,然后调用analyze方法, 这些没有调用的方法都要调用. 验证了那句话:<code>人在江湖漂,哪有不挨刀.该来的总是会来的</code>. </p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ApplicationParser.parse() --&gt; CreateInputStatementContext|<span class="string">CreateOutputStatementContext : ParseContext</span><br><span class="line">                                                         </span>|<span class="string">--createTask: LazyTask</span><br><span class="line">                                                         </span>|<span class="string">--createAnalyzer: CreateInputStreamAnalyzer</span>|CreateOutputStreamAnalyzer</span><br><span class="line">                                                                                                     |<span class="string">--createAnalyzeContext: CreateStreamAnalyzeContext</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意Input,Output的StatementContext,SemanticAnalyzer都有各自的实现类,并都继承了Stream的相关父类,但是AnalyzeContext没有各自的实现类,都是一样的了.   </p>
</blockquote>
<h3 id="ApplicationBuilder">ApplicationBuilder</h3><p>ApplicationBuilder的构建需要每个分析器分析的结果AnalyzeContext(parseContexts): 专门用来完成从多个解析内容到应用程序的转换<br>buildApplication()将整个应用程序的构建分成:  1、各个算子的构建;  2、将完成拆分的应用程序解析成为Application  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Application <span class="title">build</span><span class="params">(String appName, List&lt;AnalyzeContext&gt; parContexts, DriverContext driverContext)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.applicationName = appName;     <span class="comment">//应用程序名称</span></span><br><span class="line">    <span class="keyword">this</span>.parseContexts = parContexts;   <span class="comment">//一系列CQL语句的解析结果</span></span><br><span class="line">    </span><br><span class="line">    executeLogicOptimizer();            <span class="comment">//在构建应用程序之前，要先执行逻辑优化器. 目前貌似还没实现.</span></span><br><span class="line">    buildApplication();                 <span class="comment">//构建应用程序</span></span><br><span class="line">    executePhysicOptimizer();           <span class="comment">//在构建应用程序之后，要先执行物理优化器</span></span><br><span class="line">    parseDriverContext(driverContext);  <span class="comment">//将DriverContext的的值设置到Application中,比如UserConf,UserFile,UDF</span></span><br><span class="line">    <span class="keyword">return</span> app;                         <span class="comment">//构建完成的应用程序</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>逻辑计划包含的功能:<br>1、SQL语句的重写，比如将where中的聚合filter调整到having中等等<br>2、count(a+b),count(*),count(a) 的优化，全部改成count(1)<br>3、Join的调整，将不等值Join改为Innerjoin<br>4、将where条件中的等值表达式提升到On上面去。  </p>
<p>物理优化器的优化内容：<br>1、OrderBy优化，实现sorted-merge排序。<br>2、limit优化，上一个算子中加入limit。<br>3、算子替换，将功能比较简单的算子，替换为Filter算子或者functor算子<br>4、移除无意义的filter算子</p>
<p>逻辑计划和物理计划中间的步骤是构建Application. 在这里才开始new一个Application.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">buildApplication</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    app = <span class="keyword">new</span> Application(applicationName);                         <span class="comment">//创建Application</span></span><br><span class="line">    parseSchemas();                                                 <span class="comment">//设置所有AnalyzeContext的CreatedSchemas到app的schemas</span></span><br><span class="line">    List&lt;SplitContext&gt; splitContexts = splitOperators();            <span class="comment">//拆分算子</span></span><br><span class="line">    SplitContext splitContext = combineOperators(splitContexts);    <span class="comment">//合并算子</span></span><br><span class="line">    changeUnionOperators(splitContext);</span><br><span class="line">    changeSchemaAfterAggregate(splitContext);</span><br><span class="line">    app.setOperators(splitContext.getOperators());                  <span class="comment">//将拆分|合并算子的operators和transitions设置到Application里</span></span><br><span class="line">    app.setOpTransition(splitContext.getTransitions());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>预告: SemanticAnalyzer.analyze()和具体的CQL语法相关,下一篇我们就来看看CQL的语法和语义解析是怎么工作的.  </p>
</blockquote>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Source/">Source</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/storm/">storm</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-11-20-Cassandra-Stream" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/11/20/2015-11-20-Cassandra-Stream/" class="article-date">
  	<time datetime="2015-11-19T16:00:00.000Z" itemprop="datePublished">2015-11-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/20/2015-11-20-Cassandra-Stream/">Cassandra源码分析之Stream</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="sstableloader流程">sstableloader流程</h2><p>BulkLoader主方法创建SSTableLoader,调用stream开始流式传输:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Future操作,非阻塞的. 把长时间执行的任务封装在Future里, 程序主逻辑继续往下执行, 通过future.get获取结果</span></span><br><span class="line">StreamResultFuture future = loader.stream(options.ignores);</span><br><span class="line"><span class="comment">//如果没有获取到结果, 会一直阻塞下去, 直到任务完成, 才退出</span></span><br><span class="line">future.get();</span><br></pre></td></tr></table></figure>
<p>SSTableLoader.stream方法返回Future才能让调用者在Future上调用get.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> StreamResultFuture <span class="title">stream</span><span class="params">(Set&lt;InetAddress&gt; toIgnore, StreamEventHandler... listeners)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//初始化, 客户端为BulkLoader创建的ExternalClient. </span></span><br><span class="line">    client.init(keyspace);</span><br><span class="line">    <span class="comment">//构造一个流传输的执行计划. 流可以看做是流动,或者流式</span></span><br><span class="line">    StreamPlan plan = <span class="keyword">new</span> StreamPlan(<span class="string">"Bulk Load"</span>).connectionFactory(client.getConnectionFactory());</span><br><span class="line">    <span class="comment">//在初始化之后, 能得到目标集群每个节点对一个的TokenRange集合. 因为一个节点有256个vnodes,所以一个节点会有很多个Token!</span></span><br><span class="line">    Map&lt;InetAddress, Collection&lt;Range&lt;Token&gt;&gt;&gt; endpointToRanges = client.getEndpointToRangesMap();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//打开sstables... 会生成streamingDetails, 下面进一步处理streamingDetails, 转换结构</span></span><br><span class="line">    openSSTables(endpointToRanges);</span><br><span class="line">    <span class="comment">//上面的openSSTables是以一个个的SSTable为视觉, 现在回到stream主方法,需要以目标节点为视觉</span></span><br><span class="line">    <span class="keyword">for</span> (Map.Entry&lt;InetAddress, Collection&lt;Range&lt;Token&gt;&gt;&gt; entry : endpointToRanges.entrySet()) &#123;</span><br><span class="line">        InetAddress remote = entry.getKey();</span><br><span class="line">        List&lt;StreamSession.SSTableStreamingSections&gt; endpointDetails = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        <span class="comment">// transferSSTables assumes references have been acquired.  streamingDetails是从openSSTables中得到的</span></span><br><span class="line">        <span class="keyword">for</span> (StreamSession.SSTableStreamingSections details : streamingDetails.get(remote)) &#123;</span><br><span class="line">            endpointDetails.add(details);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//标记remote endpoint节点需要处理这些stream sections</span></span><br><span class="line">        plan.transferFiles(remote, endpointDetails);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//监听器,比如进度条ProgressIndicator</span></span><br><span class="line">    plan.listeners(<span class="keyword">this</span>, listeners);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//真正开始执行StreamPlan</span></span><br><span class="line">    <span class="keyword">return</span> plan.execute();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>先调用StreamPlan的transferFiles, 等所有endpoints都遍历完才开始execute. 在transferFiles会准备一些execute必备的数据比如sessions.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> StreamPlan <span class="title">transferFiles</span><span class="params">(InetAddress to, Collection&lt;StreamSession.SSTableStreamingSections&gt; sstableDetails)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//节点to对应的Session如果已经存在则直接获取,没有就创建</span></span><br><span class="line">    StreamSession session = getOrCreateSession(to, to);</span><br><span class="line">    <span class="comment">//为这个Session添加任务: 要传输的sstable文件</span></span><br><span class="line">    session.addTransferFiles(sstableDetails);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> StreamSession <span class="title">getOrCreateSession</span><span class="params">(InetAddress peer, InetAddress preferred)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//sessions是怎么来的? 只有在这个方法里put进去的. 所以调用该方法,如果不在sessions就会new一个并放进来</span></span><br><span class="line">    <span class="comment">//peer有可能是from节点,比如目标节点/接收数据的节点. peer也可能是to节点,目标节点,要传输到这个目标节点.</span></span><br><span class="line">    <span class="comment">//那么from和to就有可能是同一个节点.比如执行sstable命令的节点是源,则接收数据to/请求数据from的节点是目标节点</span></span><br><span class="line">    StreamSession session = sessions.get(peer);</span><br><span class="line">    <span class="keyword">if</span> (session == <span class="keyword">null</span>) &#123;</span><br><span class="line">        session = <span class="keyword">new</span> StreamSession(peer, preferred, connectionFactory);</span><br><span class="line">        sessions.put(peer, session);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> session;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StreamPlan.execute返回的是一个全局唯一的StreamResultFuture,基于Future.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> StreamResultFuture <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">//只有一个StreamPlan,但是有好多个StreamSession. 要开始一起开始吧</span></span><br><span class="line">    <span class="keyword">return</span> StreamResultFuture.init(planId, description, sessions.values(), handlers);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>初始化StreamResultFuture会创建StreamResultFuture并注册到StreamManager,然后把它传递给所有StreamSession的初始化方法, 最后启动每个StreamSession:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始化异步返回结果器. 一个StreamPlan只有一个StreamResultFuture,有多个StreamSessions, 所有的StreamSessions共用一个StreamResultFuture</span></span><br><span class="line"><span class="comment">//因为一次Stream只需要最后的一个结果来表示所有(节点)的StreamSession是否都已经完成. 一个StreamSession对应一个节点的传输.</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> StreamResultFuture <span class="title">init</span><span class="params">(UUID planId, String description, Collection&lt;StreamSession&gt; sessions, Collection&lt;StreamEventHandler&gt; listeners)</span> </span>&#123;</span><br><span class="line">    StreamResultFuture future = createAndRegister(planId, description, sessions);</span><br><span class="line">    <span class="keyword">if</span> (listeners != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">//给异步执行结果添加监听器</span></span><br><span class="line">        <span class="keyword">for</span> (StreamEventHandler listener : listeners)</span><br><span class="line">            future.addEventListener(listener);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    logger.info(<span class="string">"[Stream #&#123;&#125;] Executing streaming plan for &#123;&#125;"</span>, planId,  description);</span><br><span class="line">    <span class="comment">// start sessions</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">final</span> StreamSession session : sessions) &#123;</span><br><span class="line">        logger.info(<span class="string">"[Stream #&#123;&#125;] Beginning stream session with &#123;&#125;"</span>, planId, session.peer);</span><br><span class="line">        <span class="comment">//StreamPlan的execute会启动同一个StreamPlan的所有StreamSession. 最后实际执行的还是要交给StreamSession</span></span><br><span class="line">        session.init(future);</span><br><span class="line">        <span class="comment">//启动每一个节点的StreamSession任务</span></span><br><span class="line">        session.start();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//为什么要返回Future:对于异步执行的任务需要返回Future,这样调用者才能使用future.get来获得结果</span></span><br><span class="line">    <span class="keyword">return</span> future;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于在execute前已经transferFiles,所以每个StreamSession的transfers都是有数据的,当然也可能是requests. 然后用线程池启动任务 </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">//请求或者传输必选其一,否则说明这个Session已经完成了</span></span><br><span class="line">    <span class="keyword">if</span> (requests.isEmpty() &amp;&amp; transfers.isEmpty())&#123;</span><br><span class="line">        closeSession(State.COMPLETE);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    streamExecutor.execute(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            <span class="comment">//准备好ConnectionHandler</span></span><br><span class="line">            handler.initiate();</span><br><span class="line">            <span class="comment">//初始化完毕</span></span><br><span class="line">            onInitializationComplete();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>初始化ConnectionHandler创建输入和输出的消息处理器. handler管理这两个线程.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initiate</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    Socket incomingSocket = session.createConnection();</span><br><span class="line">    incoming.start(incomingSocket, StreamMessage.CURRENT_VERSION);</span><br><span class="line">    incoming.sendInitMessage(incomingSocket, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    Socket outgoingSocket = session.createConnection();</span><br><span class="line">    outgoing.start(outgoingSocket, StreamMessage.CURRENT_VERSION);</span><br><span class="line">    outgoing.sendInitMessage(outgoingSocket, <span class="keyword">false</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输入和输出MessageHandler都继承MessageHandler抽象线程类,初始化时都发送InitMessage:    </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">sendInitMessage</span><span class="params">(Socket socket, <span class="keyword">boolean</span> isForOutgoing)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    <span class="comment">//创建初始化消息, 并转化为ByteBuffer, 由WriteChannel发送出去(即写入到WriteChannel中)</span></span><br><span class="line">    <span class="comment">//WriteChannel是由Socket创建的, 表示要写入到Socket这个地址创建的写入通道中</span></span><br><span class="line">    StreamInitMessage message = <span class="keyword">new</span> StreamInitMessage(FBUtilities.getBroadcastAddress(), session.planId(), session.description(), isForOutgoing);</span><br><span class="line">    ByteBuffer messageBuf = message.createMessage(<span class="keyword">false</span>, protocolVersion);</span><br><span class="line">    <span class="keyword">while</span> (messageBuf.hasRemaining())</span><br><span class="line">        getWriteChannel(socket).write(messageBuf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>初始化完毕StreamSession.start开始发送PREPARE准备消息:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onInitializationComplete</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// send prepare message</span></span><br><span class="line">    state(State.PREPARING);</span><br><span class="line">    PrepareMessage prepare = <span class="keyword">new</span> PrepareMessage();</span><br><span class="line"></span><br><span class="line">    <span class="comment">//消息中附带了requests请求或者传输任务transfers(任务一开始只是Summary)</span></span><br><span class="line">    prepare.requests.addAll(requests);</span><br><span class="line">    <span class="keyword">for</span> (StreamTransferTask task : transfers.values())</span><br><span class="line">        prepare.summaries.add(task.getSummary());</span><br><span class="line"></span><br><span class="line">    <span class="comment">//发送消息</span></span><br><span class="line">    handler.sendMessage(prepare);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// if we don't need to prepare for receiving stream, start sending files immediately</span></span><br><span class="line">    <span class="keyword">if</span> (requests.isEmpty())</span><br><span class="line">        startStreamingFiles();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>发送消息放到OutgoingMessageHandler.messageQueue队列中. 与此同时输出线程从队列中获取消息并序列化消息到out写入通道中:<br>StreamMessage是消息的抽象类,各类消息需要有自己的序列化实现器,因为不同类型的消息里面的内容是不一样的.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//发送消息, 需要序列化消息</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">sendMessage</span><span class="params">(WritableByteChannel out, StreamMessage message)</span> </span>&#123;</span><br><span class="line">    StreamMessage.serialize(message, out, protocolVersion, session);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在假设往out发送了PrepareMessage消息, 与此同时ConnectionHandler的输入线程IncomingMessageHandler收到了这条消息进行反序列化:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    ReadableByteChannel in = getReadChannel(socket);</span><br><span class="line">    <span class="keyword">while</span> (!isClosed()) &#123;</span><br><span class="line">        <span class="comment">// receive message</span></span><br><span class="line">        StreamMessage message = StreamMessage.deserialize(in, protocolVersion, session);</span><br><span class="line">        <span class="comment">// Might be null if there is an error during streaming (see FileMessage.deserialize). It's ok to ignore here since we'll have asked for a retry.</span></span><br><span class="line">        <span class="keyword">if</span> (message != <span class="keyword">null</span>) &#123;</span><br><span class="line">            session.messageReceived(message);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StreamSession负责处理消息,如果是PrepareMessage,从中获取出附带的requests和transfers调用prepare方法:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">messageReceived</span><span class="params">(StreamMessage message)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (message.type) &#123;</span><br><span class="line">        <span class="keyword">case</span> PREPARE:</span><br><span class="line">            PrepareMessage msg = (PrepareMessage) message;</span><br><span class="line">            prepare(msg.requests, msg.summaries);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> FILE:</span><br><span class="line">            receive((IncomingFileMessage) message);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> RECEIVED:</span><br><span class="line">            ReceivedMessage received = (ReceivedMessage) message;</span><br><span class="line">            received(received.cfId, received.sequenceNumber);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>PREPARE后就上开始传输文件了:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">startStreamingFiles</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    streamResult.handleSessionPrepared(<span class="keyword">this</span>);</span><br><span class="line"></span><br><span class="line">    state(State.STREAMING);</span><br><span class="line">    <span class="keyword">for</span> (StreamTransferTask task : transfers.values()) &#123;</span><br><span class="line">        Collection&lt;OutgoingFileMessage&gt; messages = task.getFileMessages();</span><br><span class="line">        <span class="keyword">if</span> (messages.size() &gt; <span class="number">0</span>)</span><br><span class="line">            handler.sendMessages(messages);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            taskCompleted(task); <span class="comment">// there is no file to send</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在StreamPlan的transferFiless中会调用StreamSession.addTransferFiles将要传输的文件加入到StreamTransferTask:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//为Session添加要传输的文件列表, 添加到TransferTask中</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addTransferFiles</span><span class="params">(Collection&lt;SSTableStreamingSections&gt; sstableDetails)</span> </span>&#123;</span><br><span class="line">    Iterator&lt;SSTableStreamingSections&gt; iter = sstableDetails.iterator();</span><br><span class="line">    <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">        SSTableStreamingSections details = iter.next();</span><br><span class="line">        <span class="comment">//每张表应该都有一个唯一的ID, 而且ID是不会变的吧对于同一张表而言</span></span><br><span class="line">        UUID cfId = details.sstable.metadata.cfId;</span><br><span class="line">        StreamTransferTask task = transfers.get(cfId);</span><br><span class="line">        <span class="keyword">if</span> (task == <span class="keyword">null</span>) &#123;</span><br><span class="line">            task = <span class="keyword">new</span> StreamTransferTask(<span class="keyword">this</span>, cfId);</span><br><span class="line">            transfers.put(cfId, task);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//针对同一张表,只会有一个TransferTask.但是这个Task的tables文件会很多</span></span><br><span class="line">        task.addTransferFile(details.sstable, details.estimatedKeys, details.sections);</span><br><span class="line">        <span class="comment">//SSTableStreamingSections阅后即焚</span></span><br><span class="line">        iter.remove();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>传输文件的类型是OutgoingFileMessage, 所以上面startStreamingFiles开始传输的消息是Collection<outgoingfilemessage>,<br>因为一个Task可以调用多次addTransferFile就有多个要传输的文件(上面的cfId是CF表的编号,则sstableloader一次一个表就只有一个StreamTransferTask了):     </outgoingfilemessage></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">addTransferFile</span><span class="params">(SSTableReader sstable, <span class="keyword">long</span> estimatedKeys, List&lt;Pair&lt;Long, Long&gt;&gt; sections)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//每一个要传输的文件都包装成输出文件消息, 序列号可以表示文件编号,因为调用一次就增加1. 其他信息sstable,sections都是从一开始沿袭过来的.</span></span><br><span class="line">    OutgoingFileMessage message = <span class="keyword">new</span> OutgoingFileMessage(sstable, sequenceNumber.getAndIncrement(), estimatedKeys, sections);</span><br><span class="line">    files.put(message.header.sequenceNumber, message);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>OutgoingFileMessage的类型是FILE,对应messageReceived的会将消息转换为IncommingFileMessage并调用receive:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">OutgoingFileMessage</span><span class="params">(SSTableReader sstable, <span class="keyword">int</span> sequenceNumber, <span class="keyword">long</span> estimatedKeys, List&lt;Pair&lt;Long, Long&gt;&gt; sections)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>(Type.FILE);</span><br><span class="line">    <span class="keyword">this</span>.sstable = sstable;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>实际上IncommingFileMessage的StreamMessage也是FILE. 这样Incomming和Outgoing各司其职: Outgoing输出负责序列化,Incomming输入负责反序列化.  </p>
<ul>
<li>输出消息OutgoingFileMessage的sstable是SSTableReader, 通过封装成StreamWriter输出.  </li>
<li>读取消息IncomingFileMessage通过构造StreamReader读取输入流<code>reader.read(in)</code>最终形成SSTableWriter.  </li>
<li><code>SSTableReader</code>和<code>SSTableWriter</code>均继承<code>SSTable</code>,用于读写SSTable文件, 但是<code>StreamReader</code>和<code>StreamWriter</code>提供的是流的读写/传输. </li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对输出消息进行序列化(output, write, serialize)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Serializer&lt;OutgoingFileMessage&gt; serializer = <span class="keyword">new</span> Serializer&lt;OutgoingFileMessage&gt;() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">serialize</span><span class="params">(OutgoingFileMessage message, WritableByteChannel out, <span class="keyword">int</span> version, StreamSession session)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        DataOutput output = <span class="keyword">new</span> DataOutputStream(Channels.newOutputStream(out));</span><br><span class="line">        FileMessageHeader.serializer.serialize(message.header, output, version);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//包装在OutgoingFileMessage里的sstable是SSTableReader, 最早是在SSTableLoader:SSTableReader.openForBatch打开的.  </span></span><br><span class="line">        <span class="keyword">final</span> SSTableReader reader = message.sstable;</span><br><span class="line">        <span class="comment">//转换成StreamWriter, 为什么不直接是SSTableWriter? 因为要进行序列化和反序列化,用字节流形式即StreamWriter更快.  </span></span><br><span class="line">        StreamWriter writer = message.header.compressionInfo == <span class="keyword">null</span> ? <span class="keyword">new</span> StreamWriter(reader, message.header.sections, session) :</span><br><span class="line">                <span class="keyword">new</span> CompressedStreamWriter(reader, message.header.sections, message.header.compressionInfo, session);</span><br><span class="line">        writer.write(out);</span><br><span class="line">        session.fileSent(message.header);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//读取输入消息反序列化(input, read, deserialize)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Serializer&lt;IncomingFileMessage&gt; serializer = <span class="keyword">new</span> Serializer&lt;IncomingFileMessage&gt;() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> IncomingFileMessage <span class="title">deserialize</span><span class="params">(ReadableByteChannel in, <span class="keyword">int</span> version, StreamSession session)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        DataInputStream input = <span class="keyword">new</span> DataInputStream(Channels.newInputStream(in));</span><br><span class="line">        FileMessageHeader header = FileMessageHeader.serializer.deserialize(input, version);</span><br><span class="line">        StreamReader reader = header.compressionInfo == <span class="keyword">null</span> ? <span class="keyword">new</span> StreamReader(header, session) : <span class="keyword">new</span> CompressedStreamReader(header, session);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> IncomingFileMessage(reader.read(in), header);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>FileMessage</th>
<th>SSTable</th>
<th>Stream</th>
<th>IN/OUT</th>
<th>SER/DESER</th>
<th>read/write</th>
</tr>
</thead>
<tbody>
<tr>
<td>OutgoingFileMessage</td>
<td>SSTableReader</td>
<td>StreamWriter</td>
<td>Output</td>
<td>serialize</td>
<td>writer.write(out)</td>
</tr>
<tr>
<td>IncomingFileMessage</td>
<td>SSTableWriter</td>
<td>StreamReader</td>
<td>Input</td>
<td>deserialize</td>
<td>reader.read(in)</td>
</tr>
</tbody>
</table>
<p>StreamSession的receive将IncomingFileMessage转换为ReceivedMessage:   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">receive</span><span class="params">(IncomingFileMessage message)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// send back file received message</span></span><br><span class="line">    handler.sendMessage(<span class="keyword">new</span> ReceivedMessage(message.header.cfId, message.header.sequenceNumber));</span><br><span class="line">    receivers.get(message.header.cfId).received(message.sstable);   <span class="comment">//这里message里的sstable是SSTableWriter.  </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>receivers相关的StreamReceiveTask是在prepareReceiving创建并加入的(通过StreamSummary,即PREPARE附带的Summary信息).<br>接下来的流程交给了StreamReceiveTask.received方法, 而ReceivedMessage的处理是StreamTransferTask.complete发送方的工作接近完成了.<br>接收sstable文件的方式是用SSTableWriter关闭并打开SSTableReader, 加入到ColumnFamilyStore中,可能的话创建二级索引.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Pair&lt;String, String&gt; kscf = Schema.instance.getCF(task.cfId);</span><br><span class="line">    ColumnFamilyStore cfs = Keyspace.open(kscf.left).getColumnFamilyStore(kscf.right);</span><br><span class="line"></span><br><span class="line">    List&lt;SSTableReader&gt; readers = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (SSTableWriter writer : task.sstables)</span><br><span class="line">        readers.add(writer.closeAndOpenReader());</span><br><span class="line">    <span class="comment">// add sstables and build secondary indexes</span></span><br><span class="line">    cfs.addSSTables(readers);</span><br><span class="line">    cfs.indexManager.maybeBuildSecondaryIndexes(readers, cfs.indexManager.allIndexesNames());</span><br><span class="line"></span><br><span class="line">    task.session.taskCompleted(task);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Source/">Source</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cassandra/">cassandra</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-11-10-Cassandra-Client" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/11/10/2015-11-10-Cassandra-Client/" class="article-date">
  	<time datetime="2015-11-09T16:00:00.000Z" itemprop="datePublished">2015-11-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/11/10/2015-11-10-Cassandra-Client/">Cassandra Client查询优化</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="一次查询的流程">一次查询的流程</h2><p>通过Session执行查询等SQL语句:  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">session.<span class="function"><span class="title">execute</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<p>AbstractSession会异步地执行, 但是这种异步是Uninterruptibly不可中断的:  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">AbstractSession.<span class="function"><span class="title">execute</span><span class="params">(Statement)</span></span> : ResultSet</span><br><span class="line"><span class="function"><span class="title">executeAsync</span><span class="params">(statement)</span></span>.<span class="function"><span class="title">getUninterruptibly</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<p>根据Statement创建Message.Request, new Requests.Execute或者Query.  </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SessionManager: executeQuery(makeRequestMessage(statement, <span class="keyword">null</span>), statement)</span><br></pre></td></tr></table></figure>
<p>执行查询: executeQuery(Message.Request msg, Statement statement)  </p>
<figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DefaultResultSetFuture future = new DefaultResultSetFuture(this, , msg)<span class="comment">;</span></span><br><span class="line">execute(future, statement)<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>DefaultResultSetFuture结合了ResultSet返回结果集合和Future功能, 并实现了RequestHandler的Callback回调接口:  </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultResultSetFuture</span> <span class="keyword">extends</span> <span class="title">AbstractFuture</span>&lt;<span class="title">ResultSet</span>&gt; <span class="keyword">implements</span> <span class="title">ResultSetFuture</span>, <span class="title">RequestHandler</span>.<span class="title">Callback</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> RequestHandler <span class="keyword">handler</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">register</span><span class="params">(RequestHandler <span class="keyword">handler</span>)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.<span class="keyword">handler</span> = <span class="keyword">handler</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>execute发送请求, 传入的callback是DefaultResultSetFuture:  </p>
<figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> RequestHandler(<span class="keyword">this</span>, <span class="keyword">callback</span>, statement).sendRequest();</span><br></pre></td></tr></table></figure>
<p>创建的RequestHandler会将callback注册到自己: callback.register(this) 调用到DefaultResultSetFuture的register方法.<br>即设置DefaultResultSetFuture的handler为刚刚创建的RequestHandler.<br>在RequestHandler中, 还会负责调用callback即DefaultResultSetFuture的onSet方法, 用来设置返回值结果.  </p>
<p>一个请求包含了多个查询计划, 如果执行失败, 则回调callback.onException.  </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">sendRequest</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (queryPlan.hasNext() &amp;&amp; !isCanceled) &#123;</span><br><span class="line">        Host host = queryPlan.next();</span><br><span class="line">        <span class="keyword">if</span> (query(host))</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>查询计划实际上是当前请求会将请求依次转发到哪些节点上,所以是个Iterator<host><br>客户端连接到Cassandra, Driver提供了一个连接池. 查询时从连接池里borrow一个连接对象, 然后在这个Connection上发起调用请求:  </host></p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">boolean</span> <span class="title">query</span><span class="params">(Host host)</span> </span>&#123;</span><br><span class="line">    currentPool = manager.pools.get(host);</span><br><span class="line">    PooledConnection connection = currentPool.borrowConnection(manager.configuration().getPoolingOptions().getPoolTimeoutMillis(), TimeUnit.MILLISECONDS)</span><br><span class="line">    connectionHandler = connection.write(<span class="keyword">this</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>write会返回一个ResponseHandler. 发送读取请求, 收到响应请求. RequestHandler和ResponseHandler是相对应的.   </p>
<p>现在AbstractSession.executeAsync请求基本已经处理完成, 剩下的就是获取数据了.  </p>
<p>获取数据getUninterruptibly,通过future.get获取. future对象就是DefaultResultSetFuture.   </p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> future.<span class="keyword">get</span>();</span><br><span class="line">  &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">    interrupted = <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>只有在被中断抛出异常的时候, 数据获取线程才会停止. 这就存在一个问题,如果有数据,但是查询很慢,是不是要等到获取出数据才返回.<br>那么Cassandra本身设置的read time out是否会起作用?</p>
<h2 id="异步+不能中断">异步+不能中断</h2><p>CassandraClient查询: session.execute()</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ResultSet <span class="title">execute</span><span class="params">(Statement statement)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> executeAsync(statement).getUninterruptibly();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注: Statement包括了insert和select. </p>
<p>executeAsync是异步执行的(AbstractSession):  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ResultSetFuture <span class="title">executeAsync</span><span class="params">(Statement statement)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> executeQuery(makeRequestMessage(statement, <span class="keyword">null</span>), statement);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但是却是Uninterrupted不能中断的: 等待结果返回是不能中断的(DefaultResultSetFuture).   </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span><br><span class="line"> * Waits for the query to return and return its result.</span><br><span class="line"> *</span><br><span class="line"> * This method is usually more convenient than &#123;<span class="doctag">@link</span> #get&#125; because it:</span><br><span class="line"> * &lt;ul&gt;</span><br><span class="line"> *   &lt;li&gt;Waits for the result uninterruptibly, and so doesn't throw InterruptedException&#125;.</span><br><span class="line"> *   &lt;li&gt;Returns meaningful exceptions, instead of having to deal with ExecutionException.&lt;/li&gt;</span><br><span class="line"> * &lt;/ul&gt;</span><br><span class="line"> * As such, it is the preferred way to get the future result.  </span><br><span class="line"> *</span><br><span class="line"> * <span class="doctag">@throws</span> NoHostAvailableException if no host in the cluster can be contacted successfully to execute this query.</span><br><span class="line"> * <span class="doctag">@throws</span> QueryExecutionException if the query triggered an execution</span><br><span class="line"> * exception, that is an exception thrown by Cassandra when it cannot execute</span><br><span class="line"> * the query with the requested consistency level successfully.</span><br><span class="line"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ResultSet <span class="title">getUninterruptibly</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> Uninterruptibles.getUninterruptibly(<span class="keyword">this</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> extractCauseFromExecutionException(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Uninterrupted的实现:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> &lt;V&gt; <span class="function">V <span class="title">getUninterruptibly</span><span class="params">(Future&lt;V&gt; future)</span> <span class="keyword">throws</span> ExecutionException </span>&#123;</span><br><span class="line">  <span class="keyword">boolean</span> interrupted = <span class="keyword">false</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> future.get();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        interrupted = <span class="keyword">true</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (interrupted) &#123;</span><br><span class="line">      Thread.currentThread().interrupt();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这种对于Future而言是适合的, 而Future的特点是要等到执行完成才有返回值. 如果Future执行时间很长, 就会导致查询Cassandra超时.  </p>
<blockquote>
<p>疑问: Cassandra中设置的read time out=2s为什么没有起作用?  可能是driver客户端本身存在的问题. </p>
</blockquote>
<p>结论: 执行statement虽然使用executeAsync异步查询, 但是获取返回值是不能中断的: 包装上Future直到返回结果. </p>
<h2 id="带超时的Future">带超时的Future</h2><p>DefaultResultSetFuture还提供了一个带超时的Future:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Waits for the provided time for the query to return and return its result if available.</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ResultSet <span class="title">getUninterruptibly</span><span class="params">(<span class="keyword">long</span> timeout, TimeUnit unit)</span> <span class="keyword">throws</span> TimeoutException </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> Uninterruptibles.getUninterruptibly(<span class="keyword">this</span>, timeout, unit);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">        <span class="keyword">throw</span> extractCauseFromExecutionException(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>超时的控制通过将时间传入future, 如果到时间了还没有返回值, 则中断:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> &lt;V&gt; <span class="function">V <span class="title">getUninterruptibly</span><span class="params">(Future&lt;V&gt; future, <span class="keyword">long</span> timeout,  TimeUnit unit)</span> <span class="keyword">throws</span> ExecutionException, TimeoutException </span>&#123;</span><br><span class="line">  <span class="keyword">boolean</span> interrupted = <span class="keyword">false</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">long</span> remainingNanos = unit.toNanos(timeout);</span><br><span class="line">    <span class="keyword">long</span> end = System.nanoTime() + remainingNanos;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// Future treats negative timeouts just like zero.</span></span><br><span class="line">        <span class="keyword">return</span> future.get(remainingNanos, NANOSECONDS);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        interrupted = <span class="keyword">true</span>;</span><br><span class="line">        remainingNanos = end - System.nanoTime();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (interrupted) &#123;</span><br><span class="line">      Thread.currentThread().interrupt();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这种调用应该可以解决无法中断的问题.  但是AbstractSession的方法中没有提供带超时时间的execute方法.<br>一种方式是修改AbstractSession添加对应的方法. 并在CassandraClient中使用这种带超时的方式.<br>但这种方式需要修改源码, 并打包.  </p>
<h2 id="直接在CassandraClient控制">直接在CassandraClient控制</h2><p>DefaultResultSetFuture的cancel上说明了怎么使用Timeout:<br>但是这种方式只是取消客户端查询,并不会取消在Cassandra服务端已经开始的请求.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span><br><span class="line"> * Attempts to cancel the execution of the request corresponding to this</span><br><span class="line"> * future. This attempt will fail if the request has already returned.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * Please note that this only cancels the request driver side, but nothing</span><br><span class="line"> * is done to interrupt the execution of the request Cassandra side (and that even</span><br><span class="line"> * if &#123;<span class="doctag">@code</span> mayInterruptIfRunning&#125; is true) since  Cassandra does not</span><br><span class="line"> * support such interruption.</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * This method can be used to ensure no more work is performed driver side</span><br><span class="line"> * (which, while it doesn't include stopping a request already submitted</span><br><span class="line"> * to a Cassandra node, may include not retrying another Cassandra host on</span><br><span class="line"> * failure/timeout) if the ResultSet is not going to be retried. Typically,</span><br><span class="line"> * the code to wait for a request result for a maximum of 1 second could</span><br><span class="line"> * look like:</span><br><span class="line"> * &lt;pre&gt;</span><br><span class="line"> *   ResultSetFuture future = session.executeAsync(...some query...);</span><br><span class="line"> *   try &#123;</span><br><span class="line"> *       ResultSet result = future.get(1, TimeUnit.SECONDS);</span><br><span class="line"> *       ... process result ...</span><br><span class="line"> *   &#125; catch (TimeoutException e) &#123;</span><br><span class="line"> *       future.cancel(true); // Ensure any resource used by this query driver</span><br><span class="line"> *                            // side is released immediately</span><br><span class="line"> *       ... handle timeout ...</span><br><span class="line"> *   &#125;</span><br><span class="line"> * &lt;pre&gt;</span><br><span class="line"> *</span><br><span class="line"> * <span class="doctag">@param</span> mayInterruptIfRunning the value of this parameter is currently</span><br><span class="line"> * ignored.</span><br><span class="line"> * <span class="doctag">@return</span> &#123;<span class="doctag">@code</span> false&#125; if the future could not be cancelled (it has already</span><br><span class="line"> * completed normally); &#123;<span class="doctag">@code</span> true&#125; otherwise.</span><br><span class="line"> */</span></span><br><span class="line"><span class="annotation">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">cancel</span><span class="params">(<span class="keyword">boolean</span> mayInterruptIfRunning)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!<span class="keyword">super</span>.cancel(mayInterruptIfRunning))</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">    handler.cancel();</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以CassandraClient的查询可以改造成:  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//带超时的SQL</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ResultSet <span class="title">getResultSetTimeout</span><span class="params">(BoundStatement bstmt)</span> </span>&#123;</span><br><span class="line">    ResultSetFuture future = session.executeAsync(bstmt);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        ResultSet result = future.get(<span class="number">500</span>, TimeUnit.MILLISECONDS);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (TimeoutException e) &#123;</span><br><span class="line">        future.cancel(<span class="keyword">true</span>);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Row&gt; <span class="title">getRowTimeout</span><span class="params">(BoundStatement bstmt)</span></span>&#123;</span><br><span class="line">    ResultSet resultSet = getResultSetTimeout(bstmt);</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">null</span> == resultSet) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    List&lt;Row&gt; resultRows = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    Iterator&lt;Row&gt; it = resultSet.iterator();</span><br><span class="line">    <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">        resultRows.add(it.next());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> resultRows;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Source/">Source</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cassandra/">cassandra</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-07-26-drill-fragment-execute" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/26/2015-07-26-drill-fragment-execute/" class="article-date">
  	<time datetime="2015-07-25T16:00:00.000Z" itemprop="datePublished">2015-07-26</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/26/2015-07-26-drill-fragment-execute/">Apache Drill源码阅读(6) execute</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="设置根节点Fragment">设置根节点Fragment</h2><p>在前面根据物理计划生成的QueryWorkUnit, 会用于设置根节点和非根节点.<br>对于根节点, 因为现在是在Foreman里, 所以是运行在Foreman节点, 相对来说就是运行在本地了.<br>因为Root Fragment都是在Foreman节点执行的. </p>
<blockquote>
<p>RootFragment不是Major Fragment, 按照官方的文档Major Fragment并不会执行真正的任务.<br>那么Minor Fragment和Leaf Fragment又有什么区别呢?  </p>
</blockquote>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Set up the root fragment (which will run locally), and submit it for execution.</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">setupRootFragment</span><span class="params">(<span class="keyword">final</span> PlanFragment rootFragment, <span class="keyword">final</span> FragmentRoot rootOperator)</span> <span class="keyword">throws</span> ExecutionSetupException </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> FragmentContext rootContext = <span class="keyword">new</span> FragmentContext(drillbitContext, rootFragment, queryContext, initiatingClient, drillbitContext.getFunctionImplementationRegistry());</span><br><span class="line">  <span class="comment">//首先准备好IncomingBuffer</span></span><br><span class="line">  <span class="keyword">final</span> IncomingBuffers buffers = <span class="keyword">new</span> IncomingBuffers(rootFragment, rootContext);</span><br><span class="line">  rootContext.setBuffers(buffers);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//每个Fragment都要加入到QueryManager的监控中,当Fragment的状态发生更新,QM可以知道</span></span><br><span class="line">  queryManager.addFragmentStatusTracker(rootFragment, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//Fragment的Executor,是真正运行Fragment的线程</span></span><br><span class="line">  rootRunner = <span class="keyword">new</span> FragmentExecutor(rootContext, rootFragment, queryManager.newRootStatusHandler(rootContext, drillbitContext), rootOperator);</span><br><span class="line">  <span class="comment">//Fragment的Manager,除了线程,还有Buffer</span></span><br><span class="line">  <span class="keyword">final</span> RootFragmentManager fragmentManager = <span class="keyword">new</span> RootFragmentManager(rootFragment.getHandle(), buffers, rootRunner);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (buffers.isDone()) &#123;</span><br><span class="line">    <span class="comment">// if we don't have to wait for any incoming data, start the fragment runner. 不需要等待接收数据,启动线程</span></span><br><span class="line">    bee.addFragmentRunner(fragmentManager.getRunnable());</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// if we do, record the fragment manager in the workBus. 如果需要等待,则把管理Executor的Manager放到WorkBus中</span></span><br><span class="line">    <span class="comment">// TODO aren't we managing our own work? What does this do? It looks like this will never get run</span></span><br><span class="line">    drillbitContext.getWorkBus().addFragmentManager(fragmentManager);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在FragmentExecutor线程的run方法里会创建RootExec</p>
<figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root = ImplCreator.getExec(fragmentContext, rootOperator)<span class="comment">;</span></span><br><span class="line">//<span class="built_in">Run</span> the query <span class="keyword">until</span> root.<span class="keyword">next</span> returns <span class="literal">false</span> <span class="literal">OR</span> we no longer need <span class="keyword">to</span> continue.</span><br><span class="line"><span class="keyword">while</span> (shouldContinue() &amp;&amp; root.<span class="keyword">next</span>()) &#123;</span><br><span class="line">  // loop</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">null</span><span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>这里就要涉及到Drill底层读取数据的数据结构了.  对了,前面的IncomingBuffers到底是用来干嘛的?  </p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Source/">Source</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/drill/">drill</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-07-15-drill-fragments" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/15/2015-07-15-drill-fragments/" class="article-date">
  	<time datetime="2015-07-14T16:00:00.000Z" itemprop="datePublished">2015-07-15</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/15/2015-07-15-drill-fragments/">Apache Drill源码阅读(5) Fragment</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Foreman-runPhysicalPlan运行物理计划">Foreman.runPhysicalPlan运行物理计划</h2><p>在查询一节中说过: <code>有了物理计划,所有的统计信息,最优端点,Foreman中的Parallellizer会将物理计划转换为多个fragments</code><br>将物理计划转换为fragments是在Foreman中, 就是在runPhysicalPlan的第一步getQueryWorkUnit中  </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function">QueryWorkUnit <span class="title">getQueryWorkUnit</span><span class="params">(<span class="keyword">final</span> PhysicalPlan plan)</span> <span class="keyword">throws</span> ExecutionSetupException </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> PhysicalOperator rootOperator = plan.getSortedOperators(<span class="keyword">false</span>).iterator().next();   <span class="comment">// 物理计划的根节点物理操作符为Screen</span></span><br><span class="line">  <span class="keyword">final</span> Fragment rootFragment = rootOperator.accept(MakeFragmentsVisitor.INSTANCE, <span class="keyword">null</span>);   <span class="comment">// ①递归调用树入口,从上到下调用每个操作符的accept方法</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> SimpleParallelizer parallelizer = <span class="keyword">new</span> SimpleParallelizer(queryContext);   <span class="comment">// 并行, 用来设置fragments的并行度</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> QueryWorkUnit queryWorkUnit = parallelizer.getFragments(</span><br><span class="line">      queryContext.getOptions().getOptionList(), queryContext.getCurrentEndpoint(),</span><br><span class="line">      queryId, queryContext.getActiveEndpoints(), drillbitContext.getPlanReader(), rootFragment,</span><br><span class="line">      initiatingClient.getSession(), queryContext.getQueryContextInfo());</span><br><span class="line">  <span class="keyword">return</span> queryWorkUnit;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在debug一节,我们知道DrillRel drel, Prel prel, PhysicalOperator pop, PhysicalPlan plan各个变量的值.<br>上面通过PhysicalPlan获得的rootOperator就是PhysicalOperator pop根节点, 即<code>rootOperator=Screen</code>.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-pop.png" alt=""></p>
<p>rootOperator.accept后返回的是rootFragment.  然后通过root又开始递归遍历了(跟debug一节rootLOP.accept一样).<br>下面是MakeFragmentsVisitor访问器当访问到的是一个操作符时, 首先将当前操作符加入到Fragment中, 然后遍历其孩子节点.  </p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Fragment <span class="title">visitOp</span>(<span class="params">PhysicalOperator op, Fragment <span class="keyword">value</span></span>)  throws ForemanSetupException</span>&#123;</span><br><span class="line">  <span class="keyword">value</span> = ensureBuilder(<span class="keyword">value</span>);</span><br><span class="line">  <span class="keyword">value</span>.addOperator(op);</span><br><span class="line">  <span class="keyword">for</span> (PhysicalOperator child : op) &#123;</span><br><span class="line">    child.accept(<span class="keyword">this</span>, <span class="keyword">value</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">value</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面是Screen-&gt;..-&gt;Scan的递归调用示例树:  </p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">rootOperator</span><span class="string">.</span><span class="comment">accept(v</span><span class="string">,</span><span class="comment">null)</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">Screen</span><span class="string">.</span><span class="comment">accept(visitor</span><span class="string">,</span> <span class="comment">null)</span></span><br><span class="line">                          <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">MakeFragmentsVisitor</span><span class="string">.</span><span class="comment">visitOp</span></span><br><span class="line">                                        <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">value=new</span> <span class="comment">Fragment</span> </span><br><span class="line">                                        <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">value</span><span class="string">.</span><span class="comment">addOperator(Screen)</span></span><br><span class="line">                                        <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">for</span> <span class="comment">child</span> <span class="comment">:</span> <span class="title">[</span><span class="comment">EasyGroupScan</span><span class="title">]</span><span class="literal">-</span><span class="literal">-</span><span class="comment">child</span><span class="string">.</span><span class="comment">accept(visitor</span><span class="string">,</span> <span class="comment">value)</span>    <span class="comment">我们省略了中间的一些节点</span><span class="string">,</span><span class="comment">假设Screen的下一个节点是Scan</span></span><br><span class="line">                                                     <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">EasyGroupScan</span><span class="string">.</span><span class="comment">accept(visitor</span><span class="string">,</span> <span class="comment">value)</span></span><br><span class="line">                                                                    <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">MakeFragmentsVisitor</span><span class="string">.</span><span class="comment">visitOp</span></span><br><span class="line">                                                                    <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">value</span><span class="string">.</span><span class="comment">addOperator(EasyGroupScan)</span></span><br><span class="line">                                                                    <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">EasyGroupScan</span> <span class="comment">has</span> <span class="comment">no</span> <span class="comment">child</span></span><br><span class="line">                                                                    <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">return</span> <span class="comment">value</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>递归调用树有点类似于设计模式中的访问者模式(Visitor Pattern).  </p>
</blockquote>
<p>返回值<code>Fragment rootFragment = rootOperator.accept(MakeFragmentsVisitor.INSTANCE, null);</code><br>因为参数是null, 所以第一次调用rootOperator.accept的时候就创建了新的Fragment. 接下来child.accept,<br>因为把value : Fragment传入, 所以不会再构造Fragment了(除非出现Exchange的时候才会new一个新的Fragment).<br>注意在每次递归调用child.accept之前, 把当前的物理操作符加入到Fragment中.<br>也就是说物理计划组成的DAG图的每个物理操作符都会加入到Fragment中, 这个Fragment并不代表DAG图中的某个节点(比如根节点), 而是包含了所有的操作符.<br>这和前面的DrillRel, Prel, PhysicalOperator, PhysicalPlan不一样:它们的值是DAG图的第一个节点,然后通过input或者child嵌套包含其他节点.  </p>
<p>当然Fragment中要有根物理操作符, 这样把根拎出来, 其他所有的操作符也都能找到了.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Fragment</span> <span class="keyword">implements</span> <span class="title">Iterable</span>&lt;<span class="title">Fragment</span>.<span class="title">ExchangeFragmentPair</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> PhysicalOperator root;</span><br><span class="line">  <span class="keyword">private</span> ExchangeFragmentPair sendingExchange;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> List&lt;ExchangeFragmentPair&gt; receivingExchangePairs = Lists.newLinkedList();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Set the given operator as root operator of this fragment. If root operator is already set, then this method call is a no-op.</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addOperator</span><span class="params">(PhysicalOperator o)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">      root = o;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;  </span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addSendExchange</span><span class="params">(Exchange e, Fragment sendingToFragment)</span> <span class="keyword">throws</span> ForemanSetupException</span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (sendingExchange != <span class="keyword">null</span>) &#123;</span><br><span class="line">         <span class="keyword">throw</span> <span class="keyword">new</span> ForemanSetupException(<span class="string">"Fragment was trying to add a second SendExchange.  "</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      addOperator(e);</span><br><span class="line">      sendingExchange = <span class="keyword">new</span> ExchangeFragmentPair(e, sendingToFragment);</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addReceiveExchange</span><span class="params">(Exchange e, Fragment fragment)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">this</span>.receivingExchangePairs.add(<span class="keyword">new</span> ExchangeFragmentPair(e, fragment));</span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>MakeFragmentsVisitor如果访问到的是一个Exchange操作符, Exchange会和Fragment组成一个ExchangeFragmentPair.  </p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Fragment <span class="title">visitExchange</span>(<span class="params">Exchange exchange, Fragment <span class="keyword">value</span></span>) throws ForemanSetupException </span>&#123;</span><br><span class="line">  Fragment next = getNextBuilder();           <span class="comment">// 总是会新建一个新的Fragment:next</span></span><br><span class="line">  <span class="keyword">value</span>.addReceiveExchange(exchange, next);   <span class="comment">// 将Exchange操作符和新的Fragment组成一个ExchangeFragmentPair, 添加到原来Fragment的list中</span></span><br><span class="line">  next.addSendExchange(exchange, <span class="keyword">value</span>);      <span class="comment">// Exchange操作符和原来的Fragment也会组成一个ExchangeFragmentPair,不过用于发送</span></span><br><span class="line">  exchange.getChild().accept(<span class="keyword">this</span>, next);     <span class="comment">// Exchange下面的孩子节点, 用的Fragment是新的那一个</span></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">value</span>;                               <span class="comment">// 但是我们最后返回的, 仍然是第一次新建的那一个Fragment</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>调用这个方法时, value一定不为空: The simple fragmenter was called without a FragmentBuilder value.<br>This will only happen if the initial call to SimpleFragmenter  is by a Exchange node.<br>This should never happen since an Exchange node should never be the root node of a plan<br>一个Exchange节点永远不能是一个计划的根节点.  Exchange前是一个Major Fragment, Exchange后也是一个Major Fragment.  </p>
<blockquote>
<p>至于为什么先是Receiver,然后是Sender, 我们先看下官网中的概念,以及举个带有Exchange的例子: </p>
</blockquote>
<h2 id="fragments_theory">fragments theory</h2><blockquote>
<p><a href="http://drill.apache.org/docs/drill-query-execution/" target="_blank" rel="external">http://drill.apache.org/docs/drill-query-execution/</a><br>A parallelizer in the Foreman transforms the physical plan into multiple phases, called major and minor fragments.<br>These fragments create a multi-level execution tree that rewrites the query and executes it in<br>parallel against the configured data sources, sending the results back to the client or application.</p>
<p>并行化会将物理计划分成多个阶段. 什么时候需要并行? 任务是可以分解的时候, 任务之间没有关联, 比如Hadoop的MapReduce就是可并行化的.<br>这些阶段叫做major或者minor fragmens. 它们组成了一个多层的执行树, 重写查询, 并且能够并行地在数据源上执行.  </p>
<p>以传统DAG图的方式, 只有前面的节点处理完后,后面的节点才会继续运行. 而用并行化的方式,每个节点运行完一部分数据,后面的节点就可以接着这些数据进行计算.</p>
</blockquote>
<p><img src="http://drill.apache.org/docs/img/execution-tree.PNG" alt=""></p>
<h2 id="Major_Fragments">Major Fragments</h2><blockquote>
<p>Drill separates major fragments by an exchange operator. An exchange is a change in data location and/or parallelization of the physical plan.<br>An exchange is composed of a sender and a receiver to allow data to move between nodes  </p>
<p>Drill用交换操作符来分隔major fragments. 一个交换操作符是数据位置的交换, 或者物理计划并行度的变更.<br>一个交换操作符由一个发送器和一个接收器组成, 以运行数据在不同节点之间进行移动. </p>
<p>Major fragments do not actually perform any query tasks. Each major fragment is divided into one or multiple minor fragments<br>that actually execute the operations required to complete the query and return results back to the client. </p>
<p>Major Fragments不执行任何的查询任务, 每个major fragments会分成一个或多个minor fragments.<br>Minor Fragments会执行完成这个查询需要的操作, 并且返回结果给客户端.  </p>
</blockquote>
<p><img src="http://drill.apache.org/docs/img/ex-operator.png" alt=""></p>
<p>那么什么时候会产生Major Fragments: 读取的是HDFS上的文件时(count(*)无条件查询即使是hdfs文件也没有exchange).  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">- <span class="operator"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> hdfs.<span class="string">`/user/hive/warehouse/test.db/koudai`</span></span><br><span class="line">+ <span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> hdfs.<span class="string">`/user/hive/warehouse/test.db/koudai`</span> <span class="keyword">where</span> sequence_id <span class="keyword">like</span> <span class="string">'%12%'</span></span><br><span class="line">+ <span class="keyword">select</span> <span class="keyword">t</span>.event_result_map.<span class="keyword">map</span> <span class="keyword">from</span> hdfs.<span class="string">`/user/hive/warehouse/test.db/koudai`</span> <span class="keyword">t</span> <span class="keyword">where</span> <span class="keyword">t</span>.sequence_id=<span class="string">'1433300095954-25887486'</span></span><br><span class="line">+ <span class="keyword">select</span> <span class="keyword">t</span>.sequence_id <span class="keyword">from</span> hdfs.<span class="string">`/user/hive/warehouse/test.db/koudai`</span> <span class="keyword">t</span> <span class="keyword">limit</span> <span class="number">1</span></span><br><span class="line">+ <span class="keyword">select</span> * <span class="keyword">from</span> hdfs.<span class="string">`/user/hive/warehouse/test.db/koudai`</span> <span class="keyword">limit</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">- <span class="keyword">select</span> * <span class="keyword">from</span> cp.<span class="string">`employee.json`</span> <span class="keyword">limit</span> <span class="number">1</span></span><br><span class="line">- <span class="keyword">select</span> * <span class="keyword">from</span> dfs.<span class="string">`/Users/zhengqh/data/hive_alltypes.parquet`</span> <span class="keyword">limit</span> <span class="number">1</span></span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>为什么无条件的count(<em>)查询没有exchange. 观察Operator,发现count查询底层的scan是DIRECT_SUB_SCAN,<br>而parquet的其他查询(带条件的count,where,limit,</em>)用的是PARQUET_ROW_GROUP_SCAN. 后面的cp和本地查询则没有Exchange.    </p>
</blockquote>
<p>下图中白色的UnionExchange分隔了两个Major Fragments. totalFragments的个数指的是所有的minor framgnet.<br>对比DAG图和Operator Profiles. 可以看到Exchange对应的Operator是Receiver和Sender.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill24.png" alt=""> <img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill26.png" alt=""></p>
<blockquote>
<p>注意左侧的UnionExchange在右侧中被分成了Receiver和Sender.  </p>
</blockquote>
<p>第一个Major framgnet在UnionExchange的上方, 即Screen, 只有一个mior Fragment.<br>第二个Major framgnet包括了多个操作符, 有2个minor Fragments. </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill25.png" alt=""></p>
<h2 id="Receiver+Sender">Receiver+Sender</h2><p>以上图中的Screen-&gt;UnionExchange-&gt;Project-&gt;…-&gt;Scan的顺序分析下UnionExchange:  </p>
<p>在访问根操作符visitOp(Screen,null)时, MakeFragmentsVisitor会新建一个Fragment, 设置Fragment的root=Screen.<br>接着因为Screen的Child是UninonExchange,调用的是MakeFragmentsVisitor的visitExchange(UnionExchange,Fragment value).<br>第二个参数Fragment value是访问Screen时创建的第一个Fragment, 第一个Fragment value一定不为空, 因为不允许根节点是Exchange.  </p>
<p>因为Exchange是用来分隔Major Fragment的, 所以在Exchange之前和之后都要有一个Major Fragment,之前就是第一个Fragment了.<br>重点看下visitExchange的下面的逻辑, 理清到底第一个Fragment value和下一个Fragment next分别添加的是什么组件.  </p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">value.addReceiveExchange(exchange, <span class="keyword">next</span>);   // <span class="keyword">first</span> <span class="built_in">add</span> Receiver from <span class="keyword">next</span></span><br><span class="line"><span class="keyword">next</span>.addSendExchange(exchange, value);      // <span class="keyword">next</span> <span class="built_in">add</span> Sender <span class="keyword">to</span> <span class="keyword">first</span></span><br></pre></td></tr></table></figure>
<p>注意我们的DAG图从上到下,第一个节点是Screen,最下面的节点是Scan, 所以上面的是作为接收数据的一方,下面的是发送数据的一方.<br>而first Fragment即上面的value, 是在上方的,那么就是作为接收方Receiver的.<br>而且最后返回给客户端的也是上层的,客户端只需要知道和Screen相关的那个Fragment,即返回值是value.   </p>
<p>第一个Fragment value添加一个Receive Exchange, 只是把新建的ExchangeFragmentPair加入到value的List<exchangefragmentpair> receivingExchangePairs中.<br>而第二个Fragment next我们已经知道了在visitExchange时创建了一个新的Fragment. 对于每一个全新的Fragment, 都要设置root节点操作符.    </exchangefragmentpair></p>
<p>实际上观察前面的DAG图和Operator Profiles,你会发现UnionExchange的UNORDERED_RECEIVER的编号是00-xx-01,因此是属于第一个Fragment的.<br>而SINGLE_SENDER的编号是01-xx-00, 则是属于第二个Fragment. 因此第二个Fragment的root就是SINGLE_SENDER. </p>
<p>上面两个value和next互相添加对方, 实际上是为了在上下文中都能找到对方. 否则如果只是value添加了next. 则在next时就无法找到value的.  </p>
<table>
<thead>
<tr>
<th>Fragment</th>
<th>root</th>
<th>sendingExchange</th>
<th>receivingExchangePairs</th>
<th>Explain</th>
<th>Role</th>
</tr>
</thead>
<tbody>
<tr>
<td>value</td>
<td>Screen</td>
<td>×</td>
<td>ExchangeFragmentPair(e,next)</td>
<td>value的接收者是next</td>
<td>Reciever</td>
</tr>
<tr>
<td>next</td>
<td>SINGLE_SENDER</td>
<td>ExchangeFragmentPair(e,value)</td>
<td>×</td>
<td>next要发送给value</td>
<td>Sender</td>
</tr>
</tbody>
</table>
<p>看看UnionExchange的几个相关方法:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">UnionExchange</span> <span class="keyword">extends</span> <span class="title">AbstractExchange</span></span>&#123;</span><br><span class="line">  <span class="comment">// Ephemeral info for generating execution fragments. 这几个变量是AbstractExchange中的,为了阅读的方便放在这里</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">int</span> senderMajorFragmentId;</span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">int</span> receiverMajorFragmentId;</span><br><span class="line">  <span class="keyword">protected</span> List&lt;DrillbitEndpoint&gt; senderLocations;</span><br><span class="line">  <span class="keyword">protected</span> List&lt;DrillbitEndpoint&gt; receiverLocations;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setupSenders</span><span class="params">(List&lt;DrillbitEndpoint&gt; senderLocations)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.senderLocations = senderLocations;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setupReceivers</span><span class="params">(List&lt;DrillbitEndpoint&gt; receiverLocations)</span> <span class="keyword">throws</span> PhysicalOperatorSetupException </span>&#123;</span><br><span class="line">    Preconditions.checkArgument(receiverLocations.size() == <span class="number">1</span>, <span class="string">"Union Exchange only supports a single receiver endpoint."</span>);</span><br><span class="line">    <span class="keyword">super</span>.setupReceivers(receiverLocations);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Sender <span class="title">getSender</span><span class="params">(<span class="keyword">int</span> minorFragmentId, PhysicalOperator child)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> SingleSender(receiverMajorFragmentId, child, receiverLocations.get(<span class="number">0</span>));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Receiver <span class="title">getReceiver</span><span class="params">(<span class="keyword">int</span> minorFragmentId)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> UnorderedReceiver(senderMajorFragmentId, PhysicalOperatorUtil.getIndexOrderedEndpoints(senderLocations), <span class="keyword">false</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面getSender和getReceiver的第一个参数是minorFragmentId. new一个Sender或者Receiver都要知道对方的MajorFragmentId.<br>比如SingleSender要知道Receiver的MajorFragmentId,以及接收者的一个Location. UnorderReceiver要知道Sender的MajorId,以及所有发送者的Locations.     </p>
<p>SingleSender: <code>Sender that pushes all data to a single destination node.</code> 发送者会发送所有的数据到一个目标节点,那么当然要指定这个目标节点了.<br>这个目标节点应该是跟上表中的sendingExchange变量相关的, 可以看到这一行的root=SINGLE_SENDER. 当然目标节点指的应该是Drillbit级别,而不是Operator了.  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SingleSender</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">AbstractSender</span> &#123;</span></span><br><span class="line">  <span class="comment">/**</span><br><span class="line">   * Create a SingleSender which sends data to fragment identified by given MajorFragmentId and MinorFragmentId, and running at given endpoint</span><br><span class="line">   *</span><br><span class="line">   * @param oppositeMajorFragmentId MajorFragmentId of the receiver fragment.</span><br><span class="line">   * @param oppositeMinorFragmentId MinorFragmentId of the receiver fragment.</span><br><span class="line">   * @param child Child operator</span><br><span class="line">   * @param destination Drillbit endpoint where the receiver fragment is running.</span><br><span class="line">   */</span></span><br><span class="line">  <span class="annotation">@JsonCreator</span></span><br><span class="line">  public <span class="type">SingleSender</span>(<span class="annotation">@JsonProperty</span>(<span class="string">"receiver-major-fragment"</span>) int oppositeMajorFragmentId,</span><br><span class="line">                      <span class="annotation">@JsonProperty</span>(<span class="string">"receiver-minor-fragment"</span>) int oppositeMinorFragmentId,</span><br><span class="line">                      <span class="annotation">@JsonProperty</span>(<span class="string">"child"</span>) <span class="type">PhysicalOperator</span> child,</span><br><span class="line">                      <span class="annotation">@JsonProperty</span>(<span class="string">"destination"</span>) <span class="type">DrillbitEndpoint</span> destination) &#123;</span><br><span class="line">    <span class="keyword">super</span>(oppositeMajorFragmentId, child,</span><br><span class="line">        <span class="type">Collections</span>.singletonList(<span class="keyword">new</span> <span class="type">MinorFragmentEndpoint</span>(oppositeMinorFragmentId, destination)));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p><code>MinorFragmentEndpoint represents fragment&#39;s MinorFragmentId and Drillbit endpoint to which the fragment is assigned for execution.</code><br>DrillbitEndpoint是运行’Drillbit’服务的节点(集群的计算节点). MinorFragmentEndpoint是fragment要执行在哪个Drillbit节点,更细粒度(Container?).  </p>
<p>对于Reciever而言, 它可以有多个Sender. </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">UnorderedReceiver</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">AbstractReceiver</span>&#123;</span></span><br><span class="line">  <span class="annotation">@JsonCreator</span></span><br><span class="line">  public <span class="type">UnorderedReceiver</span>(<span class="annotation">@JsonProperty</span>(<span class="string">"sender-major-fragment"</span>) int oppositeMajorFragmentId,</span><br><span class="line">                           <span class="annotation">@JsonProperty</span>(<span class="string">"senders"</span>) <span class="type">List</span>&lt;<span class="type">MinorFragmentEndpoint</span>&gt; senders,</span><br><span class="line">                           <span class="annotation">@JsonProperty</span>(<span class="string">"spooling"</span>) boolean spooling) &#123;</span><br><span class="line">    <span class="keyword">super</span>(oppositeMajorFragmentId, senders, spooling);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>下面解释下FragmentLeaf这个接口下都有哪些实现类. </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill27.png" alt=""></p>
<p>FragmentLeaf是一个Fragment的叶子节点, Fragment和DAG图的叶子节点是有点差别呢的. 因为一个DAG图会包括多个Fragment.<br>1.接收者是一个Fragment的叶子, 因为Exchange会分隔Fragment. Fragment的上方是接收者,是上面一个Fragment的叶子节点.<br>2.整个DAG图的叶子节点通常是Scan,是组成DAG最下面的那个Fragment的叶子节点.   </p>
<p>Fragment的Root是一个Fragment的根节点<br>1.发送者是一个Fragment的根节点, 即Exchange分隔的下面一个Fragment的根节点,而Fragment下发是一个Sender.<br>2.整个DAG图的根节点通常是Screen.  </p>
<h2 id="QueryWorkUnit">QueryWorkUnit</h2><p>在运行物理计划的第一句是根据物理计划得到QueryWorkUnit:    </p>
<figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> QueryWorkUnit work = getQueryWorkUnit(plan);</span><br><span class="line"><span class="keyword">final</span> <span class="built_in">List</span>&lt;PlanFragment&gt; planFragments = work.getFragments();</span><br><span class="line"><span class="keyword">final</span> PlanFragment rootPlanFragment = work.getRootFragment();</span><br></pre></td></tr></table></figure>
<p>查询的工作单元包含了三个组件, 对于本地而言的根Fragment和根操作符.  这里的本地指的是Foreman.  </p>
<figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">QueryWorkUnit</span> &#123;</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> PlanFragment rootFragment; // <span class="keyword">for</span> <span class="keyword">local</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> FragmentRoot rootOperator; // <span class="keyword">for</span> <span class="keyword">local</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> List&lt;PlanFragment&gt; fragments;  // Major+Minor Fragments</span><br></pre></td></tr></table></figure>
<p>而PlanFragment既是Plan又是Fragment.  前面我们知道Fragment由Exchange分成了多个Major Fragment.<br>在遍历物理操作符时, 会将物理操作符加入到对应的Fragment中.  </p>
<h3 id="Protobuf">Protobuf</h3><p>必须上Protobuf这道菜了. 对于理解不同组件之间的关系是有作用的.  其实前面RPC部分也是用到了protobuf.<br>PlanFragment的protobuf定义在BitControl.proto中. FragmentHandle在ExecutionProtos.proto中  </p>
<figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">PlanFragment</span> </span>&#123;</span><br><span class="line">  <span class="keyword">optional</span> FragmentHandle handle = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">float</span> network_cost = <span class="number">4</span>;</span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">float</span> cpu_cost = <span class="number">5</span>;</span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">float</span> disk_cost = <span class="number">6</span>;</span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">float</span> memory_cost = <span class="number">7</span>;</span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">string</span> fragment_json = <span class="number">8</span>;</span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">bool</span> leaf_fragment = <span class="number">9</span>;</span><br><span class="line">  <span class="keyword">optional</span> DrillbitEndpoint assignment = <span class="number">10</span>;</span><br><span class="line">  <span class="keyword">optional</span> DrillbitEndpoint foreman = <span class="number">11</span>;</span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">int64</span> mem_initial = <span class="number">12</span> [default = <span class="number">20000000</span>]; <span class="comment">// 20 megs</span></span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">int64</span> mem_max = <span class="number">13</span> [default = <span class="number">2000000000</span>]; <span class="comment">// 20 gigs</span></span><br><span class="line">  <span class="keyword">optional</span> exec.shared.UserCredentials credentials = <span class="number">14</span>;</span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">string</span> options_json = <span class="number">15</span>;</span><br><span class="line">  <span class="keyword">optional</span> QueryContextInformation context = <span class="number">16</span>;</span><br><span class="line">  <span class="keyword">repeated</span> Collector collector = <span class="number">17</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">FragmentHandle</span> </span>&#123;</span><br><span class="line">	<span class="keyword">optional</span> exec.shared.QueryId query_id = <span class="number">1</span>;</span><br><span class="line">	<span class="keyword">optional</span> <span class="built_in">int32</span> major_fragment_id = <span class="number">2</span>;</span><br><span class="line">	<span class="keyword">optional</span> <span class="built_in">int32</span> minor_fragment_id = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在日志一节, 其中Root Fragment(rootFragment对象)打印的信息如下, 可以看到正好对应了上面的PlanFragment的协议格式:  </p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">handle &#123;				→ FragmentHandle</span><br><span class="line">  query_id &#123;</span><br><span class="line">    part1: <span class="number">3053657859282349058</span></span><br><span class="line">    part2: -<span class="number">8863752500417580646</span></span><br><span class="line">  &#125;</span><br><span class="line">  major_fragment_id: <span class="number">0</span></span><br><span class="line">  minor_fragment_id: <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line">fragment_json: <span class="string">"&#123;		→ fragment_json</span><br><span class="line">  ...</span><br><span class="line">&#125;"</span></span><br><span class="line">leaf_fragment: <span class="literal">true</span></span><br><span class="line">assignment &#123;			→ DrillbitEndpoint</span><br><span class="line">  address: <span class="string">"localhost"</span></span><br><span class="line">  user_port: <span class="number">31010</span></span><br><span class="line">  control_port: <span class="number">31011</span></span><br><span class="line">  data_port: <span class="number">31012</span></span><br><span class="line">&#125;</span><br><span class="line">foreman &#123;				→ DrillbitEndpoint</span><br><span class="line">  address: <span class="string">"localhost"</span></span><br><span class="line">  user_port: <span class="number">31010</span></span><br><span class="line">  control_port: <span class="number">31011</span></span><br><span class="line">  data_port: <span class="number">31012</span></span><br><span class="line">&#125;</span><br><span class="line">context &#123;</span><br><span class="line">  query_start_time: <span class="number">1436498522273</span></span><br><span class="line">  time_zone: <span class="number">299</span></span><br><span class="line">  default_schema_name: <span class="string">""</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="QueryContext_&amp;_DrillbitContext"><strong>QueryContext &amp; DrillbitContext</strong></h3><p>计算fragments要根据查询的上下文QueryContext,以及DrillbitContext.<br>queryContext.getCurrentEndpoint()表示Foreman节点, queryContext.getActiveEndpoints()表示参与计算的其他节点.<br>我们重点看下获取活动的Endpoints是怎么做得, 因为Drill是分布式的计算引擎,添加计算节点能够让计算能力提高.<br>那么它是怎么实现的, 通过ZK的Watcher机制, 如果有节点增加进来,获取可用的计算节点时就是动态实时的.   </p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">queryContext.getActiveEndpoints()</span><br><span class="line">            <span class="string">|</span></span><br><span class="line">            <span class="string">|--&gt;drillbitContext.getBits()</span></span><br><span class="line">                      <span class="string">|     </span></span><br><span class="line">                      <span class="string">|--&gt;ClusterCoordinator.getAvailableEndpoints()</span></span><br><span class="line">                                 <span class="string">|</span></span><br><span class="line">                                 <span class="string">|&lt;--ZKClusterCoordinator.endpoints</span></span><br><span class="line">                                               <span class="string">|</span></span><br><span class="line">                                               <span class="string">|&lt;--updateEndpoints()</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>我们知道了通过ZK实时获取动态的计算节点, 但是任务是怎么分配到计算节点上的. 我们能不能自定义转发规则??  </p>
</blockquote>
<h3 id="SimpleParallelizer">SimpleParallelizer</h3><p>由SimpleParallelizer获得Fragments, 参数activeEndpoints就是上面从上下文中得到的集群中可用的Drillbit计算节点.<br>rootFragment是rootOperator返回的Fragment, 物理计划的rootOperator一般是Screen. 这里的Fragment指的是由Exchange分割的Major Fragment.  </p>
<p>该方法根据提供的Fragment树生成分配好的Fragments集合, 就是PlanFragment Protobuf对象集合, 会被分配到单独的节点.<br>返回的Fragments(注意是复数形式), 则是Major+Minor级别的Fragment了.  而Minor Fragments可以有多个, 是可以并行处理的.  </p>
<blockquote>
<p>什么是Fragment树?  就是文档中提到的将物理计划转成多个Fragments,这些Fragments组成了一棵树.  </p>
</blockquote>
<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">final</span> QueryWorkUnit queryWorkUnit = parallelizer.getFragments(</span><br><span class="line">      queryContext.getOptions().getOptionList(), queryContext.getCurrentEndpoint(),</span><br><span class="line">      queryId, queryContext.getActiveEndpoints(), drillbitContext.getPlanReader(), rootFragment,</span><br><span class="line">      initiatingClient.getSession(), queryContext.getQueryContextInfo());</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span><br><span class="line"> * Generate a set of assigned fragments based on the provided fragment tree. Do not allow parallelization stages to go beyond the global max width.</span><br><span class="line"> * <span class="doctag">@param</span> foremanNode       The driving/foreman node for this query.  (this node) 本次查询的驱动节点/Foreman节点.</span><br><span class="line"> * <span class="doctag">@param</span> activeEndpoints   The list of endpoints to consider for inclusion in planning this query. 要计划本次查询, 需要考虑包括在内的计算节点</span><br><span class="line"> * <span class="doctag">@param</span> reader                  Tool used to read JSON plans 读取JSON格式的物理计划</span><br><span class="line"> * <span class="doctag">@param</span> rootFragment      The root node of the PhysicalPlan that we will be parallelizing. 物理计划的根节点(对应的Fragment), 会并行处理. </span><br><span class="line"> * <span class="doctag">@return</span> The list of generated PlanFragment protobuf objects to be assigned out to the individual nodes.</span><br><span class="line"> */</span></span><br><span class="line"><span class="keyword">public</span> QueryWorkUnit getFragments(OptionList options, DrillbitEndpoint foremanNode, QueryId queryId, Collection&lt;DrillbitEndpoint&gt; activeEndpoints, </span><br><span class="line">    PhysicalPlanReader reader, Fragment rootFragment, UserSession session, QueryContextInformation queryContextInfo)  &#123;</span><br><span class="line">  <span class="keyword">final</span> PlanningSet planningSet = <span class="keyword">new</span> PlanningSet();</span><br><span class="line">  initFragmentWrappers(rootFragment, planningSet);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> Set&lt;Wrapper&gt; leafFragments = constructFragmentDependencyGraph(planningSet);</span><br><span class="line">  <span class="comment">// Start parallelizing from leaf fragments</span></span><br><span class="line">  <span class="keyword">for</span> (Wrapper wrapper : leafFragments) &#123;</span><br><span class="line">    parallelizeFragment(wrapper, planningSet, activeEndpoints);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> generateWorkUnit(options, foremanNode, queryId, reader, rootFragment, planningSet, session, queryContextInfo);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们知道rootFragment只是代表了DAG图最顶上的那个Major Fragment, 在下面的迭代中,要给DAG图中的每个Major Fragment都添加到planningSet中.  </p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// For every fragment, create a Wrapper in PlanningSet.</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initFragmentWrappers</span>(<span class="params">Fragment rootFragment, PlanningSet planningSet</span>) </span>&#123;</span><br><span class="line">  planningSet.<span class="keyword">get</span>(rootFragment);</span><br><span class="line">  <span class="keyword">for</span>(ExchangeFragmentPair fragmentPair : rootFragment) &#123;</span><br><span class="line">    initFragmentWrappers(fragmentPair.getNode(), planningSet);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面我们再给出Fragment的迭代方法iterator.  for循环迭代的是receivingExchangePairs.<br>前面分析过上一个Fragment作为接收者接收下一个Fragment发送的数据: <code>value.addReceiveExchange(exchange, next);</code><br>那么ExchangeFragmentPair的Node就是next, 即下一个Major Fragment, 然后继续递归下去.  </p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public class <span class="type">Fragment</span> implements <span class="type">Iterable</span>&lt;<span class="type">Fragment</span>.<span class="type">ExchangeFragmentPair</span>&gt; &#123;</span><br><span class="line">  private final <span class="type">List</span>&lt;<span class="type">ExchangeFragmentPair</span>&gt; receivingExchangePairs = <span class="type">Lists</span>.newLinkedList();</span><br><span class="line"></span><br><span class="line">  public <span class="type">void</span> addReceiveExchange(<span class="type">Exchange</span> e, <span class="type">Fragment</span> fragment) &#123;</span><br><span class="line">    this.receivingExchangePairs.add(new <span class="type">ExchangeFragmentPair</span>(e, fragment));</span><br><span class="line">  &#125;</span><br><span class="line">  public <span class="type">Iterator</span>&lt;<span class="type">ExchangeFragmentPair</span>&gt; <span class="keyword">iterator</span>() &#123;</span><br><span class="line">    <span class="keyword">return</span> this.receivingExchangePairs.<span class="keyword">iterator</span>();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="PlanningSet+Wrapper">PlanningSet+Wrapper</h3><p>既然用到了ReceiveExchange, 下面马上就用到了SendingExchange.  添加是在: <code>next.addSendExchange(exchange, value);</code><br>下面用的不是ExchangeFragmentPair的Fragment了, 而是Fragment的Exchange. 这里要做到的是设置MajorFragmentId.<br>因为由Exchange分割的Major Fragment, 它们的ID分别是00,01,02等等.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PlanningSet</span> <span class="keyword">implements</span> <span class="title">Iterable</span>&lt;<span class="title">Wrapper</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Fragment, Wrapper&gt; fragmentMap = Maps.newHashMap();</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> majorFragmentIdIndex = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Wrapper <span class="title">get</span><span class="params">(Fragment node)</span> </span>&#123;</span><br><span class="line">    Wrapper wrapper = fragmentMap.get(node);</span><br><span class="line">    <span class="keyword">if</span> (wrapper == <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">int</span> majorFragmentId = <span class="number">0</span>;</span><br><span class="line">      <span class="comment">// If there is a sending exchange, we need to number other than zero.</span></span><br><span class="line">      <span class="keyword">if</span> (node.getSendingExchange() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// assign the upper 16 bits as the major fragment id.</span></span><br><span class="line">        majorFragmentId = node.getSendingExchange().getChild().getOperatorId() &gt;&gt; <span class="number">16</span>;</span><br><span class="line">        <span class="comment">// if they are not assigned, that means we mostly likely have an externally generated plan.  in this case, come up with a major fragmentid.</span></span><br><span class="line">        <span class="keyword">if</span> (majorFragmentId == <span class="number">0</span>)   majorFragmentId = majorFragmentIdIndex;</span><br><span class="line">      &#125;</span><br><span class="line">      wrapper = <span class="keyword">new</span> Wrapper(node, majorFragmentId);  <span class="comment">// Wrapper由Fragment和major编号组成</span></span><br><span class="line">      fragmentMap.put(node, wrapper);</span><br><span class="line">      majorFragmentIdIndex++;  <span class="comment">// 只有调用Major Fragment时, 每遇到新的Major, 索引编号+1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> wrapper;  <span class="comment">// planningSet.get并没有用返回值做什么事情. 其实主要是放到Map中, 由迭代器访问所有的Wrapper.  </span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Iterator&lt;Wrapper&gt; <span class="title">iterator</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.fragmentMap.values().iterator();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>Wrapper: <code>A wrapping class that allows us to add additional information to each fragment node for planning purposes</code><br>它的构造函数创建对象是由PlanningSet指定MajorFragment和MajorFragmentId. 它的其余属性需要在下面中设置进来.  </p>
<p>先来看下Exchange的并行依赖:  发送者和接收者是否相互依赖.  </p>
<figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Exchanges are <span class="keyword">fragment</span> boundaries <span class="keyword">in</span> physical operator tree. It is divided into two parts. First part is Sender</span><br><span class="line"> * which becomes part of the sending <span class="keyword">fragment</span>. Second part is Receiver which becomes part of the <span class="keyword">fragment</span> that receives the data.</span><br><span class="line"> * Exchange是物理操作符树的Fragment边界. 第一部分Sender,它是发送者Fragment的一部分, 第二部分Reciever是接收者Fragment的一部分. </span><br><span class="line"> * Assignment dependency describes whether sender fragments depend <span class="keyword">on</span> receiver <span class="keyword">fragment</span>'s endpoint assignment <span class="keyword">for</span></span><br><span class="line"> * determining its parallelization and endpoint assignment and vice versa.</span><br><span class="line"> * 分配依赖描述了发送者Fragment是否依赖于接收者Fragment的节点分配任务, 以便于决定并行度和如何分配工作到节点上. 反过来一样.   </span><br><span class="line"> */</span><br><span class="line">public enum ParallelizationDependency &#123;</span><br><span class="line">  SENDER_DEPENDS_ON_RECEIVER, // Sending <span class="keyword">fragment</span> depends <span class="keyword">on</span> receiving <span class="keyword">fragment</span> <span class="keyword">for</span> parallelization</span><br><span class="line">  RECEIVER_DEPENDS_ON_SENDER, // Receiving <span class="keyword">fragment</span> depends <span class="keyword">on</span> sending <span class="keyword">fragment</span> <span class="keyword">for</span> parallelization (<span class="keyword">default</span> value).</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>根据PlanningSet构造依赖图:  </p>
<figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">final</span> <span class="built_in">Set</span>&lt;Wrapper&gt; leafFragments = constructFragmentDependencyGraph(planningSet);</span><br><span class="line"></span><br><span class="line"><span class="comment"><span class="markdown">/** 根据Exchange的亲密程序分割两个fragments, 并且设置fragment的依赖关系.  </span><br><span class="line"> * Based on </span>the<span class="markdown"> affinity of </span>the<span class="markdown"> Exchange that separates two fragments, setup fragment dependencies.</span><br><span class="line"> * @return Returns </span>a<span class="markdown"> list of leaf fragments in fragment dependency graph. */</span></span></span><br><span class="line">private <span class="literal">static</span> <span class="built_in">Set</span>&lt;Wrapper&gt; constructFragmentDependencyGraph(PlanningSet planningSet) &#123;</span><br><span class="line">  <span class="comment">// Set up dependency of fragments based on the affinity of exchange that separates the fragments.</span></span><br><span class="line">  <span class="keyword">for</span>(Wrapper currentFragmentWrapper : planningSet) &#123;  <span class="comment">// PlanningSet包含了所有的Major Fragment组成的Wrapper,循环每一个Wrapper</span></span><br><span class="line">    ExchangeFragmentPair sendingExchange = currentFragmentWrapper.getNode().getSendingExchangePair();  <span class="comment">//每个MajorFragment要发送的目标</span></span><br><span class="line">    <span class="keyword">if</span> (sendingExchange != <span class="keyword">null</span>) &#123;  <span class="comment">// SendingExchange不为空的, 比如next, 而不是DAG图的第一个Fragment. 因为只有next才是发送者</span></span><br><span class="line">      ParallelizationDependency dependency = sendingExchange.getExchange().getParallelizationDependency();  <span class="comment">// 依赖关系记录在Exchange中, 而不是Fragment中</span></span><br><span class="line">      Wrapper receivingFragmentWrapper = planningSet.<span class="literal">get</span>(sendingExchange.getNode());  <span class="comment">// 目标节点, 实际上就是接收者了</span></span><br><span class="line">      <span class="comment">// 根据依赖关系, 判断要加到哪个Wrapper中, 实际上是哪个Fragment中. 因为Wrapper由MajorFragment组成.  </span></span><br><span class="line">      <span class="keyword">if</span> (dependency == ParallelizationDependency.RECEIVER_DEPENDS_ON_SENDER) &#123;     <span class="comment">// Receiver依赖Sender</span></span><br><span class="line">        receivingFragmentWrapper.addFragmentDependency(currentFragmentWrapper);   <span class="comment">// Receiver的依赖关系图中有当前Major Fragment</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dependency == ParallelizationDependency.SENDER_DEPENDS_ON_RECEIVER) &#123;  <span class="comment">// Sender依赖Reciever</span></span><br><span class="line">        currentFragmentWrapper.addFragmentDependency(receivingFragmentWrapper);   <span class="comment">// 当前节点刚好是Sender, 所以它依赖了接收者</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 上面的添加Fragment依赖图, 下面的Wrapper才可以获得依赖图, 来判断是否是叶子节点.  </span></span><br><span class="line">  <span class="comment">// Identify leaf fragments. Leaf fragments are fragments that have no other fragments depending on them for parallelization info. </span></span><br><span class="line">  <span class="comment">// First assume all fragments are leaf fragments. Go through the fragments one by one and  remove the fragment on which the current fragment depends on.</span></span><br><span class="line">  <span class="keyword">final</span> <span class="built_in">Set</span>&lt;Wrapper&gt; roots = Sets.newHashSet();</span><br><span class="line">  <span class="keyword">for</span>(Wrapper w : planningSet) &#123;</span><br><span class="line">    roots.add(w);  <span class="comment">// 所有的Major Fragment</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span>(Wrapper wrapper : planningSet) &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="built_in">List</span>&lt;Wrapper&gt; fragmentDependencies = wrapper.getFragmentDependencies();  <span class="comment">// 每个Major Fragment的依赖图</span></span><br><span class="line">    <span class="keyword">if</span> (fragmentDependencies != <span class="keyword">null</span> &amp;&amp; fragmentDependencies.size() &gt; <span class="number">0</span>) </span><br><span class="line">      <span class="keyword">for</span>(Wrapper dependency : fragmentDependencies)   <span class="comment">// 它的所有依赖者</span></span><br><span class="line">        <span class="keyword">if</span> (roots.contains(dependency)) </span><br><span class="line">          roots.remove(dependency);  <span class="comment">// 从roots中移除</span></span><br><span class="line">  &#125; </span><br><span class="line">  <span class="keyword">return</span> roots;  <span class="comment">// 返回值是leaf fragments. </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的方法roots返回的是leaf fragments. 在这之前首先对每个Major Fragments都设置了依赖图. 然后把非叶子节点从所有的Major中删除.<br>叶子节点的定义是: 没有依赖其他任何一个节点. 一旦一个节点有依赖某一个节点, 它就不是叶子节点了.  </p>
<p>获得叶子Fragment后, 对每一个叶子节点进行并行处理. 处理的时候先处理依赖的,然后才处理自己.所以也是递归的过程    </p>
<figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">  // Start parallelizing <span class="keyword">from</span> leaf fragments 从叶子节点开始并行处理</span><br><span class="line">  <span class="keyword">for</span> (Wrapper wrapper : leafFragments) &#123;</span><br><span class="line">    parallelizeFragment(wrapper, planningSet, activeEndpoints);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">// Helper method <span class="keyword">for</span> parallelizing a given <span class="keyword">fragment</span>. Dependent fragments are parallelized first before  parallelizing the given <span class="keyword">fragment</span>.</span><br><span class="line">private void parallelizeFragment(Wrapper <span class="keyword">fragment</span>Wrapper, PlanningSet planningSet, Collection<span class="variable">&lt;DrillbitEndpoint&gt;</span> activeEndpoints)  &#123;</span><br><span class="line">  // First parallelize fragments <span class="keyword">on</span> which this <span class="keyword">fragment</span> depends <span class="keyword">on</span>.</span><br><span class="line">  final List<span class="variable">&lt;Wrapper&gt;</span> <span class="keyword">fragment</span>Dependencies = <span class="keyword">fragment</span>Wrapper.getFragmentDependencies();</span><br><span class="line">  if (<span class="keyword">fragment</span>Dependencies != null &amp;&amp; <span class="keyword">fragment</span>Dependencies.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span>(Wrapper dependency : <span class="keyword">fragment</span>Dependencies) &#123;</span><br><span class="line">      parallelizeFragment(dependency, planningSet, activeEndpoints);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  Fragment <span class="keyword">fragment</span> = <span class="keyword">fragment</span>Wrapper.getNode();</span><br><span class="line"></span><br><span class="line">  // Step <span class="number">1</span>: Find stats. Stats include various factors including cost of physical operators, parallelizability of work <span class="keyword">in</span> physical operator and affinity of physical operator <span class="keyword">to</span> certain nodes.</span><br><span class="line">  <span class="keyword">fragment</span>.getRoot().accept(new StatsCollector(planningSet), <span class="keyword">fragment</span>Wrapper);</span><br><span class="line"></span><br><span class="line">  // Step <span class="number">2</span>: Find the parallelization width of <span class="keyword">fragment</span></span><br><span class="line">  </span><br><span class="line">  List<span class="variable">&lt;DrillbitEndpoint&gt;</span> assignedEndpoints = findEndpoints(activeEndpoints, parallelizationInfo.getEndpointAffinityMap(), <span class="keyword">fragment</span>Wrapper.getWidth());</span><br><span class="line">  <span class="keyword">fragment</span>Wrapper.assignEndpoints(assignedEndpoints);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>找到要分配的DrillBit后,就为Fragment分配计算节点 . 一个Fragment的Sending只有最多一个,可以有多个Receiver.   </p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public void assignEndpoints(List<span class="tag">&lt;DrillbitEndpoint&gt;</span> assignedEndpoints)  &#123;</span><br><span class="line">  endpoints.addAll(assignedEndpoints);</span><br><span class="line"></span><br><span class="line">  // Set scan <span class="operator">and</span> store endpoints.</span><br><span class="line">  AssignEndpointsToScanAndStore visitor = new AssignEndpointsToScanAndStore();</span><br><span class="line">  <span class="keyword">node</span>.<span class="identifier"></span><span class="title">getRoot</span>().accept(visitor, endpoints);</span><br><span class="line"></span><br><span class="line">  // Set the endpoints for this (one at most) sending exchange.</span><br><span class="line">  if (<span class="keyword">node</span>.<span class="identifier"></span><span class="title">getSendingExchange</span>() != null) &#123;</span><br><span class="line">    <span class="keyword">node</span>.<span class="identifier"></span><span class="title">getSendingExchange</span>().setupSenders(majorFragmentId, endpoints);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // Set the endpoints for each incoming exchange within this fragment.</span><br><span class="line">  for (ExchangeFragmentPair e : <span class="keyword">node</span>.<span class="identifier"></span><span class="title">getReceivingExchangePairs</span>()) &#123;</span><br><span class="line">    e.getExchange().setupReceivers(majorFragmentId, endpoints);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后基于上面的工作, 生成WorkUnit, QueryWorkUnit只是封装了rootOperator,rootFragment,fragments的对象.  注意下面是个双层循环,<br>外层的是对每个MajorFragment,内层则对每个MinorFragment. 如果不是根节点,则把创建的PlanFragment加入到fragments中.<br>PlanFragment一个重要的对象是FragmentHandle,顾名思义是Fragment的处理类, 它只封装了Major,Minor的FragmentID,以及查询ID.  </p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> QueryWorkUnit generateWorkUnit(OptionList <span class="keyword">options</span>, DrillbitEndpoint foremanNode, QueryId queryId,</span><br><span class="line">    PhysicalPlanReader reader, Fragment rootNode, PlanningSet planningSet, UserSession session, QueryContextInformation queryContextInfo) &#123;</span><br><span class="line">  List&lt;PlanFragment&gt; fragments = Lists.newArrayList();</span><br><span class="line">  PlanFragment rootFragment = <span class="keyword">null</span>;</span><br><span class="line">  FragmentRoot rootOperator = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// now we generate all the individual plan fragments and associated assignments. Note, we need all endpoints</span></span><br><span class="line">  <span class="comment">// assigned before we can materialize, so we start a new loop here rather than utilizing the previous one.</span></span><br><span class="line">  <span class="keyword">for</span> (Wrapper wrapper : planningSet) &#123;</span><br><span class="line">    Fragment node = wrapper.getNode();</span><br><span class="line">    <span class="keyword">final</span> PhysicalOperator physicalOperatorRoot = node.getRoot();</span><br><span class="line">    <span class="keyword">boolean</span> isRootNode = rootNode == node;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// a fragment is self driven if it doesn't rely on any other exchanges.</span></span><br><span class="line">    <span class="keyword">boolean</span> isLeafFragment = node.getReceivingExchangePairs().<span class="keyword">size</span>() == <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create a minorFragment for each major fragment.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> minorFragmentId = <span class="number">0</span>; minorFragmentId &lt; wrapper.getWidth(); minorFragmentId++) &#123;</span><br><span class="line">      IndexedFragmentNode iNode = <span class="keyword">new</span> IndexedFragmentNode(minorFragmentId, wrapper);</span><br><span class="line">      wrapper.resetAllocation();</span><br><span class="line">      PhysicalOperator op = physicalOperatorRoot.accept(Materializer.INSTANCE, iNode);</span><br><span class="line">      FragmentRoot root = (FragmentRoot) op;</span><br><span class="line">      FragmentHandle handle = FragmentHandle.newBuilder() <span class="comment">//</span></span><br><span class="line">          .setMajorFragmentId(wrapper.getMajorFragmentId()) <span class="comment">//</span></span><br><span class="line">          .setMinorFragmentId(minorFragmentId) <span class="comment">//</span></span><br><span class="line">          .setQueryId(queryId) <span class="comment">//</span></span><br><span class="line">          .build();</span><br><span class="line">      PlanFragment fragment = PlanFragment.newBuilder() <span class="comment">//</span></span><br><span class="line">          .setForeman(foremanNode) <span class="comment">//</span></span><br><span class="line">          .setFragmentJson(reader.writeJson(root)) <span class="comment">//</span></span><br><span class="line">          .setHandle(handle) <span class="comment">//</span></span><br><span class="line">          .setAssignment(wrapper.getAssignedEndpoint(minorFragmentId)) <span class="comment">//</span></span><br><span class="line">          .setLeafFragment(isLeafFragment) <span class="comment">//</span></span><br><span class="line">          .setContext(queryContextInfo)</span><br><span class="line">          .setMemInitial(wrapper.getInitialAllocation())<span class="comment">//</span></span><br><span class="line">          .setMemMax(wrapper.getMaxAllocation())</span><br><span class="line">          .setOptionsJson(reader.writeJson(<span class="keyword">options</span>))</span><br><span class="line">          .setCredentials(session.getCredentials())</span><br><span class="line">          .addAllCollector(CountRequiredFragments.getCollectors(root))</span><br><span class="line">          .build();</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (isRootNode) &#123;</span><br><span class="line">        logger.debug(<span class="string">"Root fragment:\n &#123;&#125;"</span>, DrillStringUtils.unescapeJava(fragment.toString()));</span><br><span class="line">        rootFragment = fragment;</span><br><span class="line">        rootOperator = root;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logger.debug(<span class="string">"Remote fragment:\n &#123;&#125;"</span>, DrillStringUtils.unescapeJava(fragment.toString()));</span><br><span class="line">        fragments.add(fragment);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> QueryWorkUnit(rootOperator, rootFragment, fragments);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="提交并执行Fragments">提交并执行Fragments</h2><p>现在主线回到Foreman的runPhysicalPlan, 在提交Fragments执行前, 先添加了两个监听器到DrillbitContext对应的WorkBus和集群协调器.<br>然后设置RootFragment和非RootFragment. 设置根节点需要QueryWorkUnit的rootFragment和rootOperator.  非根节点只需要planFragments.  </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">void</span> <span class="title">runPhysicalPlan</span><span class="params">(<span class="keyword">final</span> PhysicalPlan plan)</span> <span class="keyword">throws</span> ExecutionSetupException </span>&#123;</span><br><span class="line">  <span class="keyword">final</span> QueryWorkUnit work = getQueryWorkUnit(plan);</span><br><span class="line">  <span class="keyword">final</span> List&lt;PlanFragment&gt; planFragments = work.getFragments();</span><br><span class="line">  <span class="keyword">final</span> PlanFragment rootPlanFragment = work.getRootFragment();</span><br><span class="line"></span><br><span class="line">  drillbitContext.getWorkBus().addFragmentStatusListener(queryId, queryManager.getFragmentStatusListener());</span><br><span class="line">  drillbitContext.getClusterCoordinator().addDrillbitStatusListener(queryManager.getDrillbitStatusListener());</span><br><span class="line">  logger.debug(<span class="string">"Submitting fragments to run."</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// set up the root fragment first so we'll have incoming buffers available.</span></span><br><span class="line">  setupRootFragment(rootPlanFragment, work.getRootOperator());</span><br><span class="line">  setupNonRootFragments(planFragments);</span><br><span class="line">  drillbitContext.getAllocator().resetFragmentLimits(); <span class="comment">// TODO a global effect for this query?!?</span></span><br><span class="line"></span><br><span class="line">  moveToState(QueryState.RUNNING, <span class="keyword">null</span>);</span><br><span class="line">  logger.debug(<span class="string">"Fragments running."</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Source/">Source</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/drill/">drill</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-07-14-drill-logical" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/14/2015-07-14-drill-logical/" class="article-date">
  	<time datetime="2015-07-13T16:00:00.000Z" itemprop="datePublished">2015-07-14</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/14/2015-07-14-drill-logical/">Apache Drill源码阅读(4) 逻辑计划</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <p>在前面说过, Calcite的SQL节点转换为Drill的DrillRel节点,在DefaultSqlHandler.convertToDrel会包装上一个Screen: DrillScreenRel  </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function">DrillRel <span class="title">convertToDrel</span><span class="params">(RelNode relNode, RelDataType validatedRowType)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// Put a non-trivial topProject to ensure the final output field name is preserved, when necessary.</span></span><br><span class="line">      DrillRel topPreservedNameProj = addRenamedProject((DrillRel) convertedRelNode, validatedRowType);</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> DrillScreenRel(topPreservedNameProj.getCluster(), topPreservedNameProj.getTraitSet(), topPreservedNameProj);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>那么convertToDrel中的RelNode以及更早之前的SqlNode到底是个什么样的数据结构? 为了解开这个谜题,我们要debug下drill的源码才能知晓. </p>
<h2 id="sqlline">sqlline</h2><p>SQL语句: </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">select</span> department_id,<span class="keyword">count</span>(*) cnt  	→ <span class="keyword">Project</span></span><br><span class="line"><span class="keyword">from</span> cp.<span class="string">`employee.json`</span> 			→ <span class="keyword">Scan</span> 		</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> department_id 				→ HashAgg</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">count</span>(*) <span class="keyword">desc</span> 				→ <span class="keyword">Sort</span></span></span><br></pre></td></tr></table></figure>
<p>生成的物理计划:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">00</span>-<span class="number">00</span>    Screen : rowType = RecordType(ANY department_id, BIGINT cnt): rowcount = <span class="number">46.3</span>, cumulative cost = &#123;<span class="number">1023.2299999999999</span> rows, <span class="number">10798.630541406654</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">8889.6</span> memory&#125;, id = <span class="number">361</span></span><br><span class="line"><span class="number">00</span>-<span class="number">01</span>      Project(department_id=[$<span class="number">0</span>], cnt=[$<span class="number">1</span>]) : rowType = RecordType(ANY department_id, BIGINT cnt): rowcount = <span class="number">46.3</span>, cumulative cost = &#123;<span class="number">1018.5999999999999</span> rows, <span class="number">10794.000541406655</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">8889.6</span> memory&#125;, id = <span class="number">360</span></span><br><span class="line"><span class="number">00</span>-<span class="number">02</span>        SelectionVectorRemover : rowType = RecordType(ANY department_id, BIGINT cnt): rowcount = <span class="number">46.3</span>, cumulative cost = &#123;<span class="number">1018.5999999999999</span> rows, <span class="number">10794.000541406655</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">8889.6</span> memory&#125;, id = <span class="number">359</span></span><br><span class="line"><span class="number">00</span>-<span class="number">03</span>          Sort(sort0=[$<span class="number">1</span>], dir0=[DESC]) : rowType = RecordType(ANY department_id, BIGINT cnt): rowcount = <span class="number">46.3</span>, cumulative cost = &#123;<span class="number">972.3</span> rows, <span class="number">10747.700541406655</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">8889.6</span> memory&#125;, id = <span class="number">358</span></span><br><span class="line"><span class="number">00</span>-<span class="number">04</span>            HashAgg(group=[&#123;<span class="number">0</span>&#125;], cnt=[COUNT()]) : rowType = RecordType(ANY department_id, BIGINT cnt): rowcount = <span class="number">46.3</span>, cumulative cost = &#123;<span class="number">926.0</span> rows, <span class="number">9723.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">8148.800000000001</span> memory&#125;, id = <span class="number">357</span></span><br><span class="line"><span class="number">00</span>-<span class="number">05</span>              Scan(groupscan=[EasyGroupScan [selectionRoot=classpath:/employee.json, numFiles=<span class="number">1</span>, columns=[`department_id`], files=[classpath:/employee.json]]]) : rowType = RecordType(ANY department_id): rowcount = <span class="number">463.0</span>, cumulative cost = &#123;<span class="number">463.0</span> rows, <span class="number">463.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">356</span></span><br></pre></td></tr></table></figure>
<p>可视化的计划图:</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill21.png" alt=""></p>
<h2 id="Debug">Debug</h2><p>使用<code>mvn clean install -DskipTests</code>编译源码工程后, 在drill-jdbc的test工程下的DrillResultSetTest测试类下新建一个测试方法: </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="annotation">@Test</span></span><br><span class="line"><span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">testQueryCP</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">  Connection connection = <span class="keyword">new</span> Driver().connect( <span class="string">"jdbc:drill:zk=local"</span>, JdbcAssert.getDefaultProperties() );</span><br><span class="line">  Statement statement = connection.createStatement();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//SQL</span></span><br><span class="line">  ResultSet resultSet = statement.executeQuery( <span class="string">"select department_id,count(*) cnt from cp.`employee.json` group by department_id order by count(*) desc"</span> );</span><br><span class="line"></span><br><span class="line">  <span class="comment">//物理计划</span></span><br><span class="line">  <span class="comment">//ResultSet resultSet = statement.executeQuery( "explain plan for select department_id,count(*) cnt from cp.`employee.json` group by department_id order by count(*) desc" );</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//逻辑计划</span></span><br><span class="line">  <span class="comment">//ResultSet resultSet = statement.executeQuery( "explain plan without implementation for select department_id,count(*) cnt from cp.`employee.json` group by department_id order by count(*) desc" );</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后在DrillSqlWorker.getPlan的<code>sqlNode = planner.parse(sql)</code>打上断点: </p>
<h3 id="SqlNode">SqlNode</h3><p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-sqlnode.png" alt=""></p>
<p>这里的sqlNode是一个SqlOrderBy解析树节点.  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Parse tree node that represents an "ORDER BY" on a query other than a "SELECT" (e.g. "VALUES" or "UNION").</span></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SqlOrderBy</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">SqlCall</span> &#123;</span></span><br><span class="line">  public <span class="keyword">final</span> <span class="type">SqlNode</span> query;</span><br><span class="line">  public <span class="keyword">final</span> <span class="type">SqlNodeList</span> orderList;</span><br><span class="line">  public <span class="keyword">final</span> <span class="type">SqlNode</span> offset;</span><br><span class="line">  public <span class="keyword">final</span> <span class="type">SqlNode</span> fetch;</span><br></pre></td></tr></table></figure>
<p>其中query节点是SqlSelect. 一个完整的select语句有多个部分组成:  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A SqlSelect is a node of a parse tree which represents a select statement.</span></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">SqlSelect</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">SqlCall</span> &#123;</span></span><br><span class="line">  <span class="type">SqlNodeList</span> keywordList;</span><br><span class="line">  <span class="type">SqlNodeList</span> selectList;</span><br><span class="line">  <span class="type">SqlNode</span> from;</span><br><span class="line">  <span class="type">SqlNode</span> where;</span><br><span class="line">  <span class="type">SqlNodeList</span> groupBy;</span><br><span class="line">  <span class="type">SqlNode</span> having;</span><br><span class="line">  <span class="type">SqlNodeList</span> windowDecls;</span><br><span class="line">  <span class="type">SqlNodeList</span> orderBy;</span><br><span class="line">  <span class="type">SqlNode</span> offset;</span><br><span class="line">  <span class="type">SqlNode</span> fetch;</span><br></pre></td></tr></table></figure>
<p>注意上面的sqlNode有自己的orderList即count(*) desc, 所以SqlSelect中的order by为null.  </p>
<blockquote>
<p>注意到类名字后面跟的@数字吗, 这是一个对象的字符串值表示形式:Class@1234,<br>其中数字还代表了创建对象的顺序. 数字越大, 则对象创建的时间越晚.  </p>
</blockquote>
<h3 id="①RelNode">①RelNode</h3><p>RelNode是Calcite的关系表达式节点, 它比SqlNode还要更上层. 下面是rel在debug时候的value  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rel<span class="preprocessor">#<span class="number">13</span>:LogicalSort.NONE.ANY([]).[<span class="number">1</span> DESC]</span></span><br><span class="line">  (input=rel<span class="preprocessor">#<span class="number">11</span>:LogicalAggregate.NONE.ANY([]).[]</span></span><br><span class="line">  	(input=rel<span class="preprocessor">#<span class="number">9</span>:LogicalProject.NONE.ANY([]).[]</span></span><br><span class="line">  		(input=rel<span class="preprocessor">#<span class="number">4</span>:EnumerableTableScan.ENUMERABLE.ANY([]).[]</span></span><br><span class="line">  			(table=[cp, employee.json]),department_id=$<span class="number">1</span>),group=&#123;<span class="number">0</span>&#125;,cnt=COUNT()</span><br><span class="line">  	),</span><br><span class="line">  	sort0=$<span class="number">1</span>,</span><br><span class="line">  	dir0=DESC</span><br><span class="line">  )</span><br></pre></td></tr></table></figure>
<p>下图中rel的初始值是LogicalSort, 然后通过input指针, 不断地嵌套. 和下图的input嵌套一一对应.<br><code>LogicalSort &gt; LogicalAggregate &gt; LogicalProject &gt; EnumerableTableScan</code>  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-RelNode.png" alt=""></p>
<blockquote>
<p>从上面几个对象后面@代表的数字可以看出, 这几个对象, 依次创建的时间越来越晚.  </p>
</blockquote>
<h3 id="②DrillRel_drel">②DrillRel drel</h3><p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-DrillRel.png" alt=""></p>
<p>CalCite的RelNode转换为Drill的DrillRel, drel的值是DrillScreenRel. 它的input也是嵌套的:<br><code>DrillScreenRel &gt; DrillSortRel &gt; DrillAggregateRel &gt; DrillScanRel</code>  </p>
<p>虽然DrillRel drel的值是DrillScreenRel, 以及接下来的DrillRel, 它们都代表的是逻辑计划的表达式树.  </p>
<h3 id="③Prel">③Prel</h3><p>物理计划的input嵌套: <code>ScreenPrel &gt;&gt; ProjectPrel &gt;&gt; SelectionVectorRemoverPrel &gt; SortPrel &gt; HashAggPrel &gt; ScanPrel</code><br>可以看到物理计划会比逻辑计划多一些节点, 并且上面的嵌套和WEBUI上的物理计划和图是能够对应上来的.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-Prel.png" alt=""></p>
<h3 id="PhysicalOperator_pop">PhysicalOperator pop</h3><p>pop不再是input嵌套, 而是child嵌套了, 因为pop从plan过来,所以它和Prel类似  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-pop.png" alt=""></p>
<h3 id="④PhysicalPlan_plan">④PhysicalPlan plan</h3><p>最后的物理计划是一张DAG图, 即Drill Plan(注意和Drill PhysicalPlan不一样). </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-plan.png" alt=""></p>
<p>图中根节点roots和叶子节点leaves. 这里的根是Screen, 即DAG图的最上面一个屏幕输出节点, 叶子只有一个, 即DAG图的最底下扫描节点.  </p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">RelNode rel = convertToRel<span class="list">(<span class="keyword">validated</span>)</span><span class="comment">;</span></span><br><span class="line">rel = preprocessNode<span class="list">(<span class="keyword">rel</span>)</span><span class="comment">;</span></span><br><span class="line">log<span class="list">(<span class="string">"Optiq Logical"</span>, rel)</span><span class="comment">;					→ ①</span></span><br><span class="line"></span><br><span class="line">DrillRel drel = convertToDrel<span class="list">(<span class="keyword">rel</span>, validatedRowType)</span><span class="comment">;</span></span><br><span class="line">log<span class="list">(<span class="string">"Drill Logical"</span>, drel)</span><span class="comment">;					→ ②</span></span><br><span class="line"></span><br><span class="line">Prel prel = convertToPrel<span class="list">(<span class="keyword">drel</span>)</span><span class="comment">;			→ ③</span></span><br><span class="line">log<span class="list">(<span class="string">"Drill Physical"</span>, prel)</span><span class="comment">;				</span></span><br><span class="line"></span><br><span class="line">PhysicalOperator pop = convertToPop<span class="list">(<span class="keyword">prel</span>)</span><span class="comment">;</span></span><br><span class="line">PhysicalPlan plan = convertToPlan<span class="list">(<span class="keyword">pop</span>)</span><span class="comment">;    	→ ④</span></span><br><span class="line">log<span class="list">(<span class="string">"Drill Plan"</span>, plan)</span><span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>其实WebUI上的可视化Plan图就是这里的graph对象.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill23.png" alt=""></p>
<h2 id="从DrillScreenRel入手">从DrillScreenRel入手</h2><p>在 new DrillScreenRel添加前后打上断点, 验证添加Screen节点的变化:   </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-noscreen.png" alt=""></p>
<p>可以看到在还没有添加Screen时, convertedRelNode和topPreservedNameProj两个对象是一样的, 因为它们的对象编号都是: DrillSortRel@7614<br>添加Screen后, 即DrillRel dre变成了DrillScreenRel@7665, 而其input域仍然是DrillSortRel@7614. 说明确实封装了上面的节点对象.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-screen.png" alt=""></p>
<p>DrillRel drel是逻辑计划, 可以通过explain plan withou implementation for <query>得到查询的逻辑计划(和上图是对应的): </query></p>
<figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">+------+------+</span><br><span class="line">| <span class="atom">text</span> | <span class="atom">json</span> |</span><br><span class="line">+------+------+</span><br><span class="line">| <span class="name">DrillScreenRel</span></span><br><span class="line">  <span class="name">DrillSortRel</span>(<span class="atom">sort0</span>=[$<span class="number">1</span>], <span class="atom">dir0</span>=[<span class="name">DESC</span>])</span><br><span class="line">    <span class="name">DrillAggregateRel</span>(<span class="atom">group</span>=[&#123;<span class="number">0</span>&#125;], <span class="atom">cnt</span>=[<span class="name">COUNT</span>()])</span><br><span class="line">      <span class="name">DrillScanRel</span>(<span class="atom">table</span>=[[<span class="atom">cp</span>, <span class="atom">employee</span>.<span class="atom">json</span>]], <span class="atom">groupscan</span>=[<span class="name">EasyGroupScan</span> [<span class="atom">selectionRoot</span>=<span class="atom">classpath</span>:/<span class="atom">employee</span>.<span class="atom">json</span>, <span class="atom">numFiles</span>=<span class="number">1</span>, <span class="atom">columns</span>=[<span class="string">`department_id`</span>], <span class="atom">files</span>=[<span class="atom">classpath</span>:/<span class="atom">employee</span>.<span class="atom">json</span>]]])</span><br><span class="line"></span><br><span class="line">  <span class="string">"query"</span> : [ &#123;</span><br><span class="line">    <span class="string">"op"</span> : <span class="string">"scan"</span>,</span><br><span class="line">    <span class="string">"@id"</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="string">"storageengine"</span> : <span class="string">"cp"</span>,</span><br><span class="line">    <span class="string">"selection"</span> : &#123;</span><br><span class="line">      <span class="string">"format"</span> : &#123;</span><br><span class="line">        <span class="string">"type"</span> : <span class="string">"named"</span>,</span><br><span class="line">        <span class="string">"name"</span> : <span class="string">"json"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"files"</span> : [ <span class="string">"classpath:/employee.json"</span> ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;, &#123;</span><br><span class="line">    <span class="string">"op"</span> : <span class="string">"groupingaggregate"</span>,</span><br><span class="line">    <span class="string">"@id"</span> : <span class="number">2</span>,</span><br><span class="line">    <span class="string">"input"</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="string">"keys"</span> : [ &#123;</span><br><span class="line">      <span class="string">"ref"</span> : <span class="string">"`department_id`"</span>,</span><br><span class="line">      <span class="string">"expr"</span> : <span class="string">"`department_id`"</span></span><br><span class="line">    &#125; ],</span><br><span class="line">    <span class="string">"exprs"</span> : [ &#123;</span><br><span class="line">      <span class="string">"ref"</span> : <span class="string">"`cnt`"</span>,</span><br><span class="line">      <span class="string">"expr"</span> : <span class="string">"count(1) "</span></span><br><span class="line">    &#125; ]</span><br><span class="line">  &#125;, &#123;</span><br><span class="line">    <span class="string">"op"</span> : <span class="string">"order"</span>,</span><br><span class="line">    <span class="string">"@id"</span> : <span class="number">3</span>,</span><br><span class="line">    <span class="string">"input"</span> : <span class="number">2</span>,</span><br><span class="line">    <span class="string">"within"</span> : <span class="atom">null</span>,</span><br><span class="line">    <span class="string">"orderings"</span> : [ &#123;</span><br><span class="line">      <span class="string">"expr"</span> : <span class="string">"`cnt`"</span>,</span><br><span class="line">      <span class="string">"order"</span> : <span class="string">"DESC"</span>,</span><br><span class="line">      <span class="string">"nullDirection"</span> : <span class="string">"UNSPECIFIED"</span></span><br><span class="line">    &#125; ]</span><br><span class="line">  &#125;, &#123;</span><br><span class="line">    <span class="string">"op"</span> : <span class="string">"store"</span>,</span><br><span class="line">    <span class="string">"@id"</span> : <span class="number">4</span>,</span><br><span class="line">    <span class="string">"input"</span> : <span class="number">3</span>,</span><br><span class="line">    <span class="string">"target"</span> : <span class="atom">null</span>,</span><br><span class="line">    <span class="string">"storageEngine"</span> : <span class="string">"--SCREEN--"</span></span><br><span class="line">  &#125; ]</span><br></pre></td></tr></table></figure>
<p>上面我们debug出来RelNode rel是LogicalSort, 其input嵌套依次是: LogicalAggregate &gt; LogicalProject &gt; EnumerableTableScan<br>经过logicalPlanningVolcano()处理, 会将CalCite的LogicalSort操作符节点转换为Drill认识的DrillSortRel节点.<br>而且topPreservedNameProj=DrillSortRel@7614, 所以创建DrillScreenRel我们可以这么看:  </p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">return</span> new DrillScreenRel(<span class="keyword">cluster</span>, trait, DrillSortRel@7614)</span><br><span class="line"></span><br><span class="line">public DrillScreenRel(RelOptCluster <span class="keyword">cluster</span>, RelTraitSet traitSet, RelNode <span class="keyword">input</span>) &#123;</span><br><span class="line">  super(DRILL_LOGICAL, <span class="keyword">cluster</span>, traitSet, <span class="keyword">input</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于DrillScreenRel构造函数而言, 输入RelNode input=DrillSortRel对象. 其实从我们debug出来的变量也是这么一回事的.<br>即DrillScreenRel的input是DrillSortRel, DrillSortRel的input是DrillAggregateRel, DrillAggregateRel的input是DrillScanRel.<br><code>DrillScreenRel[4] &gt; DrillSortRel[1] &gt; DrillAggregateRel[2] &gt; DrillScanRel[3]</code>  序号表示创建时间顺序.  </p>
<p>这里只是创建对象, 由于DrillXXXRel都实现了DrillRel, 而DrillRel定义了implement接口方法, 我们找到了DrillImplementor.<br>比较重要的是入口方法是go, 它由ExplainHandler, 只有在逻辑计划的时候才会调用到(所以在debug时要用逻辑计划SQL语句才能进入DrillImplementor).  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-go.png" alt=""></p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title">LogicalExplain</span>&#123;</span><br><span class="line">  <span class="keyword">public</span> String text;</span><br><span class="line">  <span class="keyword">public</span> String json;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">LogicalExplain</span>(<span class="params">RelNode node, SqlExplainLevel level, QueryContext context</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.text = RelOptUtil.toString(node, level);</span><br><span class="line">    DrillImplementor implementor = <span class="keyword">new</span> DrillImplementor(<span class="keyword">new</span> DrillParseContext(context.getPlannerSettings()), ResultMode.LOGICAL);</span><br><span class="line">    implementor.go( (DrillRel) node);</span><br><span class="line">    LogicalPlan plan = implementor.getPlan();</span><br><span class="line">    <span class="keyword">this</span>.json = plan.unparse(context.getConfig());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">go</span>(<span class="params">DrillRel root</span>) </span>&#123;</span><br><span class="line">  LogicalOperator rootLOP = root.implement(<span class="keyword">this</span>);</span><br><span class="line">  System.<span class="keyword">out</span>.println(<span class="string">"ROOTLOP:"</span> + rootLOP);</span><br><span class="line">  rootLOP.accept(<span class="keyword">new</span> AddOpsVisitor(), <span class="keyword">null</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>参数root是逻辑计划节点=DrillScreenRel, 调用implement后返回的是逻辑操作符. 那么这个rootLOP是什么东东?  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-rootLOP.png" alt=""></p>
<p>调用root的implement, this为DrillImplementor. 看下DrillScreenRel的implement实现:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DrillScreenRel:</span><br><span class="line">  <span class="function"><span class="keyword">public</span> LogicalOperator <span class="title">implement</span><span class="params">(DrillImplementor implementor)</span> </span>&#123;</span><br><span class="line">    LogicalOperator childOp = implementor.visitChild(<span class="keyword">this</span>, <span class="number">0</span>, getInput());</span><br><span class="line">    <span class="keyword">return</span> Store.builder().setInput(childOp).storageEngine(<span class="string">"--SCREEN--"</span>).build();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>其中getInput()就是创建DrillScreenRel时的最后一个参数input, 那么这里就是DrillSortRel.  </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DrillImplementor:</span><br><span class="line">  <span class="keyword">public</span> LogicalOperator visitChild(DrillRel parent, <span class="keyword">int</span> ordinal, RelNode child) &#123;</span><br><span class="line">    <span class="keyword">return</span> ((DrillRel) child).implement(<span class="keyword">this</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>child就是DrillScreenRel的getInput()即DrillSortRel. 然后调用DrillSortRel的implement. 以此类推, 所以这是一个递归的过程. </p>
<figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">DrillImplementor</span><span class="string">.</span><span class="comment">go</span></span><br><span class="line">   <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">root</span><span class="string">.</span><span class="comment">implement</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">DrillScreenRel</span><span class="string">.</span><span class="comment">implement</span></span><br><span class="line">                           <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">implementor</span><span class="string">.</span><span class="comment">visitChild(</span><span class="string">.</span><span class="string">.</span><span class="string">,</span><span class="comment">DrillSortRel)</span></span><br><span class="line">                                  <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">child</span><span class="string">.</span><span class="comment">implement</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">DrillSortRel</span><span class="string">.</span><span class="comment">implement</span></span><br><span class="line">                                                            <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">implementor</span><span class="string">.</span><span class="comment">visitChild(</span><span class="string">.</span><span class="string">.</span><span class="string">,</span><span class="comment">DrillAggregateRel)</span></span><br><span class="line">                                                                   <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">child</span><span class="string">.</span><span class="comment">implement</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">DrillAggregateRel</span><span class="string">.</span><span class="comment">implement</span></span><br><span class="line">                                                                                            <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">implementor</span><span class="string">.</span><span class="comment">visitChild(</span><span class="string">.</span><span class="string">.</span><span class="string">,</span><span class="comment">DrillScanRel)</span></span><br><span class="line">                                                                                                  <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">child</span><span class="string">.</span><span class="comment">implement</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">DrillScanRel</span><span class="string">.</span><span class="comment">implement</span></span><br><span class="line">                                                                                                                         <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">不再visitChild了</span><span class="string">,</span><span class="comment">因为Scan是叶子节点</span><span class="string">,</span><span class="comment">NO</span><span class="literal">-</span><span class="comment">MORE</span><span class="literal">-</span><span class="comment">CHILD</span></span><br><span class="line">                                                                                                                         <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">implementor</span><span class="string">.</span><span class="comment">registerSource(table)</span></span><br><span class="line">                                                                                                                         <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">返回Scan</span> <span class="comment">LogicalOperator</span></span><br><span class="line">                                                                                            <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">GroupingAggregate</span><span class="string">.</span><span class="comment">setInput(implementor</span><span class="string">.</span><span class="comment">visitChild(</span><span class="string">.</span><span class="string">.</span><span class="comment">))=GroupingAggregate</span><span class="string">.</span><span class="comment">setInput(Scan)</span> </span><br><span class="line">                                                                                            <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">返回GroupingAggregate</span> <span class="comment">LogicalOperator</span> </span><br><span class="line">                                                            <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">Order</span><span class="string">.</span><span class="comment">setInput(implementor</span><span class="string">.</span><span class="comment">visitChild(</span><span class="string">.</span><span class="string">.</span><span class="comment">))=Order</span><span class="string">.</span><span class="comment">setInput(GroupingAggregate)</span></span><br><span class="line">                                                            <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">返回Order</span> <span class="comment">LogicalOperator</span> </span><br><span class="line">                            <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">Store</span><span class="string">.</span><span class="comment">setInput(implementor</span><span class="string">.</span><span class="comment">visitChild(</span><span class="string">.</span><span class="string">.</span><span class="comment">))=Store</span><span class="string">.</span><span class="comment">setInput(Order)</span> <span class="title">[</span><span class="comment">①</span><span class="title">]</span></span><br><span class="line">    <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">rootLOP</span><span class="string">.</span><span class="comment">accept</span><span class="literal">-</span><span class="literal">-</span>&gt;<span class="comment">Store</span><span class="string">.</span><span class="comment">accept</span></span><br><span class="line">    <span class="comment">						|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">planBuilder</span><span class="string">.</span><span class="comment">addLogicalOperator(Store)</span></span><br><span class="line">                            <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">for</span> <span class="comment">o</span> <span class="comment">:</span> <span class="comment">Store</span><span class="string">.</span><span class="comment">getInput=Order</span></span><br><span class="line">                                <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">o</span> <span class="comment">:</span> <span class="comment">=</span> <span class="comment">Order</span><span class="string">.</span><span class="comment">accept</span></span><br><span class="line">                                              <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">planBuilder</span><span class="string">.</span><span class="comment">addLogicalOperator(Order)</span></span><br><span class="line">                                              <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">for</span> <span class="comment">o</span> <span class="comment">:</span> <span class="comment">Order</span><span class="string">.</span><span class="comment">getInput=GroupingAggregate</span> </span><br><span class="line">                                                  <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">o</span> <span class="comment">:</span> <span class="comment">GroupingAggregate</span><span class="string">.</span><span class="comment">accept</span></span><br><span class="line">                                                             <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">planBuilder</span><span class="string">.</span><span class="comment">addLogicalOperator(GroupingAggregate)</span></span><br><span class="line">                                                             <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">for</span> <span class="comment">o</span> <span class="comment">:</span> <span class="comment">GroupingAggregate</span><span class="string">.</span><span class="comment">getInput=Scan</span></span><br><span class="line">                                                                 <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">o</span> <span class="comment">:</span> <span class="comment">Scan</span><span class="string">.</span><span class="comment">accept</span></span><br><span class="line">                                                                            <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">planBuilder</span><span class="string">.</span><span class="comment">addLogicalOperator(Scan)</span></span><br><span class="line">                                                                            <span class="comment">|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">Scan</span> <span class="comment">has</span> <span class="comment">not</span> <span class="comment">inputs</span><span class="string">.</span> <span class="comment">END</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">	|</span><span class="literal">-</span><span class="literal">-</span><span class="comment">END</span> <span class="comment">OF</span> <span class="comment">GO</span></span><br></pre></td></tr></table></figure>
<p>DONE with planBuilder which is builder of logical plan.<br>调用完DrillImplementor.go, 就把planBuilder构造完毕, 接着调用getPlan就可以获得planBuilder的输出了.  </p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Source/">Source</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/drill/">drill</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-07-13-drill-phyplan" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/13/2015-07-13-drill-phyplan/" class="article-date">
  	<time datetime="2015-07-12T16:00:00.000Z" itemprop="datePublished">2015-07-13</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/13/2015-07-13-drill-phyplan/">Apache Drill源码阅读(3) 物理计划</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <p>DrillBit各个角色:<br>UserServer处理RUN_QUERY_VALUE客户端的查询请求,会将任务分派给UserWorker处理, 由worker提交工作:<br>显然worker要在构造UserServer的时候也一起构造出来, 这样在收到任务的时候, 确保立即有工人接手这份工作.<br>UserServer的构造在ServiceEngine,而服务引擎是由DrillBit创建的.<br>UserWorker是由WorkerManager管理的, 而WorkerManager也是由DrillBit创建的.<br>所以启动DrillBit服务后,参与计算的角色都已经准备好了.  </p>
<table>
<thead>
<tr>
<th>Role</th>
<th>Explain</th>
</tr>
</thead>
<tbody>
<tr>
<td>WorkerBee</td>
<td>工蜂, 真正干活的</td>
</tr>
<tr>
<td>UserWorker</td>
<td>用户操作的(工人), 通过WorkerBee构成</td>
</tr>
<tr>
<td>WorkerManager</td>
<td>工人管理员,负责选择一个工人来工作</td>
</tr>
<tr>
<td>UserServer</td>
<td>用户操作的服务端,会将工作交给UserWorker,它需要一个UserWorker</td>
</tr>
<tr>
<td>Foreman</td>
<td>包工头,监工.由UserWorker创建出来. 因为UserWorker底层是WorkerBee,所以会将WorkerBee和Foreman关联起来</td>
</tr>
<tr>
<td>ServiceEngine</td>
<td>服务引擎,管理UserServer,Controller</td>
</tr>
<tr>
<td>DrillBit</td>
<td>Drill的服务端控制进程,管理ServiceEngine,WorkerManager</td>
</tr>
<tr>
<td>BootStrapContext</td>
<td>启动DrillBit的上下文,包括配置信息,度量注册</td>
</tr>
<tr>
<td>DrillbitContext</td>
<td>DrillBit工作时候的上下文</td>
</tr>
<tr>
<td>Controller</td>
<td>不同DrillBit节点的通信</td>
</tr>
<tr>
<td>ControllServer</td>
<td>不同节点间消息传输,连接等的RPC服务端</td>
</tr>
<tr>
<td>DataServer</td>
<td>负责数据交互的RPC服务端</td>
</tr>
</tbody>
</table>
<h2 id="工人和监工的那些事">工人和监工的那些事</h2><p>首先看下UserWorker是怎么提交一个任务的:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> UserWorker&#123;</span><br><span class="line">  <span class="keyword">private</span> final WorkerBee bee;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> QueryId <span class="title">submitWork</span><span class="params">(UserClientConnection connection, RunQuery query)</span> </span>&#123;</span><br><span class="line">    ThreadLocalRandom r = ThreadLocalRandom.current();</span><br><span class="line">    <span class="comment">// create a new queryid where the first four bytes are a growing time (each new value comes earlier in sequence).  Last 12 bytes are random.</span></span><br><span class="line">    <span class="keyword">long</span> time = (<span class="keyword">int</span>) (System.currentTimeMillis()/<span class="number">1000</span>);</span><br><span class="line">    <span class="keyword">long</span> p1 = ((Integer.MAX_VALUE - time) &lt;&lt; <span class="number">32</span>) + r.nextInt();</span><br><span class="line">    <span class="keyword">long</span> p2 = r.nextLong();</span><br><span class="line">    QueryId id = QueryId.newBuilder().setPart1(p1).setPart2(p2).build();</span><br><span class="line">    incrementer.increment(connection.getSession());</span><br><span class="line">    Foreman foreman = <span class="keyword">new</span> Foreman(bee, bee.getContext(), connection, id, query);</span><br><span class="line">    bee.addNewForeman(foreman);</span><br><span class="line">    <span class="keyword">return</span> id;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>返回的QueryId会由UserServer通过RPC发送给客户端, 表示客户端这一次的查询标识. 服务端已经接受了这次查询.<br>但是服务端还没有开始执行这个查询任务, 后续如果客户端需要查询结果, 可以凭这个QueryId, 就可以向服务端要数据结果.  </p>
<p>WorkerBee从名字上看是工作的蜜蜂, 工蜂一直默默无闻地工作. 它为母蜂Foreman服务.<br>现在我们由UserWorker创建了一个Foreman. 工蜂把它加进来.    </p>
<blockquote>
<p>问题:<br>1.为什么不是由Foreman管理WorkerBee,而是让WorkerBee(工蜂)主动把Foreman(监工)加进来?<br>2.为什么Foreman作为一个进程,不是自己启动,而是要由工人来启动?  </p>
</blockquote>
<p>Foreman负责管理一次查询的所有fragments, Foreman会作为根节点/驱动节点  </p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span><br><span class="line"> * Foreman manages all the fragments (local and remote) for a single query where this is the driving/root node.</span><br><span class="line"> * The flow is as follows:</span><br><span class="line"> * - Foreman is submitted as a runnable.  被提交为可执行的</span><br><span class="line"> * - Runnable does query planning. 做什么: 查询计划</span><br><span class="line"> * - state changes from PENDING to RUNNING 状态改变</span><br><span class="line"> * - Runnable sends out starting fragments 发射起始fragments</span><br><span class="line"> * - Status listener are activated 监听器被激活</span><br><span class="line"> * - The Runnable's run() completes, but the Foreman stays around 线程的run方法结束,而Foreman还停留...做什么, 看下面的</span><br><span class="line"> * - Foreman listens for state change messages. 监听状态改变的消息</span><br><span class="line"> * - state change messages can drive the state to FAILED or CANCELED, in which case 状态消息会驱动/更新Foreman的状态</span><br><span class="line"> *   messages are sent to running fragments to terminate 消息会使得正在运行的fragments终结</span><br><span class="line"> * - when all fragments complete, state change messages drive the state to COMPLETED 当所有的fragments完成后, 状态改变的消息更新Formeman的状态为已完成</span><br><span class="line"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Foreman</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> QueryId queryId; <span class="comment">//the id for the query</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> RunQuery queryRequest; <span class="comment">//the query to execute</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> QueryContext queryContext;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> QueryManager queryManager; <span class="comment">// handles lower-level details of query execution</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> WorkerBee bee; <span class="comment">// provides an interface to submit tasks, used to submit additional work</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> DrillbitContext drillbitContext;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> UserClientConnection initiatingClient; <span class="comment">// used to send responses</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Sets up the Foreman, but does not initiate any execution. 设置Foreman, 但是并没有初始化任何的执行</span></span><br><span class="line">  <span class="keyword">public</span> Foreman(<span class="keyword">final</span> WorkerBee bee, <span class="keyword">final</span> DrillbitContext drillbitContext, <span class="keyword">final</span> UserClientConnection connection, <span class="keyword">final</span> QueryId queryId, <span class="keyword">final</span> RunQuery queryRequest) &#123;</span><br><span class="line">    <span class="keyword">this</span>.bee = bee;</span><br><span class="line">    <span class="keyword">this</span>.queryId = queryId;</span><br><span class="line">    <span class="keyword">this</span>.queryRequest = queryRequest;</span><br><span class="line">    <span class="keyword">this</span>.drillbitContext = drillbitContext;</span><br><span class="line">    <span class="keyword">this</span>.initiatingClient = connection;</span><br><span class="line">    <span class="keyword">this</span>.closeFuture = initiatingClient.getChannel().closeFuture();</span><br><span class="line">    closeFuture.addListener(closeListener);</span><br><span class="line">    queryContext = <span class="keyword">new</span> QueryContext(connection.getSession(), drillbitContext);</span><br><span class="line">    queryManager = <span class="keyword">new</span> QueryManager(queryId, queryRequest, drillbitContext.getPersistentStoreProvider(), stateListener, <span class="keyword">this</span>);</span><br><span class="line">    recordNewState(QueryState.PENDING);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>Foreman的run方法根据RunQuery的类型执行不同的方法,比如SQL类型,则要负责将SQL语句通过Calcite解析成逻辑计划,生成物理计划,最后运行物理计划.  </p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> runSQL(<span class="keyword">final</span> <span class="keyword">String</span> sql) <span class="keyword">throws</span> ExecutionSetupException &#123;</span><br><span class="line">  <span class="keyword">final</span> DrillSqlWorker sqlWorker = <span class="keyword">new</span> DrillSqlWorker(queryContext);</span><br><span class="line">  <span class="keyword">final</span> Pointer&lt;<span class="keyword">String</span>&gt; textPlan = <span class="keyword">new</span> Pointer&lt;&gt;();</span><br><span class="line">  <span class="keyword">final</span> PhysicalPlan plan = sqlWorker.getPlan(sql, textPlan);</span><br><span class="line">  queryManager.setPlanText(textPlan.value);</span><br><span class="line">  runPhysicalPlan(plan);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="SQL_Parser">SQL Parser</h2><p>Calcite的planner对SQL进行parse解析, 生成SqlNode节点,  对于不同的SqlNode类型, 由不同的Handler进行进行解析.   </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DrillSqlWorker</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Planner planner;  <span class="comment">//这两个Planner都是Calcite的,负责解析成逻辑计划</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> HepPlanner hepPlanner;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> QueryContext context;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="function">PhysicalPlan <span class="title">getPlan</span><span class="params">(String sql, Pointer&lt;String&gt; textPlan)</span> <span class="keyword">throws</span> ForemanSetupException </span>&#123;</span><br><span class="line">    SqlNode sqlNode = planner.parse(sql);  <span class="comment">//将SQL语句解析成SqlNode解析树①</span></span><br><span class="line">    AbstractSqlHandler <span class="keyword">handler</span>;</span><br><span class="line">    SqlHandlerConfig config = <span class="keyword">new</span> SqlHandlerConfig(hepPlanner, planner, context);</span><br><span class="line">    <span class="keyword">switch</span>(sqlNode.getKind())&#123;</span><br><span class="line">    <span class="keyword">case</span> EXPLAIN:</span><br><span class="line">      <span class="keyword">handler</span> = <span class="keyword">new</span> ExplainHandler(config);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> SET_OPTION:</span><br><span class="line">      <span class="keyword">handler</span> = <span class="keyword">new</span> SetOptionHandler(context);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> OTHER:</span><br><span class="line">      <span class="keyword">if</span>(sqlNode <span class="keyword">instanceof</span> SqlCreateTable) &#123;</span><br><span class="line">        <span class="keyword">handler</span> = ((DrillSqlCall)sqlNode).getSqlHandler(config, textPlan);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (sqlNode <span class="keyword">instanceof</span> DrillSqlCall) &#123;</span><br><span class="line">        <span class="keyword">handler</span> = ((DrillSqlCall)sqlNode).getSqlHandler(config);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">      <span class="keyword">handler</span> = <span class="keyword">new</span> DefaultSqlHandler(config, textPlan);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">return</span> <span class="keyword">handler</span>.<span class="title">getPlan</span><span class="params">(sqlNode)</span></span>;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>The Drillbit that receives the query from a client or application becomes the Foreman for the query and drives the entire query.<br>A parser in the Foreman parses the SQL[①], applying custom rules[②] to convert specific SQL operators into a specific logical operator syntax that Drill understands.<br>This collection of logical operators forms a logical plan. The logical plan describes the work required to generate the query results and defines what data sources and operations to apply.  </p>
<p>Foreman中的parser解析SQL, 并运用定制的规则, 将SQL操作符(Calcite的节点)转换成Drill认识的逻辑操作符(Drill的节点DrillRel).<br>转换后的逻辑操作符集合会组成一个逻辑计划.  注意上面的sqlNode=planner.parse(sql)对应的是SQL操作符, 转换成DrillRelNode在Handler的getPlan中完成.  </p>
</blockquote>
<h2 id="SqlNode(Calcite_SQL操作符)">SqlNode(Calcite SQL操作符)</h2><p>Calcite的编程API主要包括了: Operator, Rule, RelationExpression, SqlNode.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/calcite1.png" alt=""></p>
<h3 id="What’s_Rule?">What’s Rule?</h3><p>Calcite的planner对SQL进行parse解析, 除了用到Calcite自身的一些规则外, Drill也会附加一些规则getRules给它. 定义在DrillSqlWorker的构造函数中.<br>规则包括物理计划, 逻辑计划, 转换规则.  其中逻辑计划包括基本规则,用户自定义规则. 物理计划包括物理规则,存储插件的规则. 比如hive插件有自己的SQL执行转换规则.  </p>
<figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="component">public DrillSqlWorker(QueryContext context) &#123;</span><br><span class="line">  FrameworkConfig config = Frameworks<span class="string">.newConfigBuilder()</span> ...</span><br><span class="line">      <span class="string">.ruleSets(getRules(context))...</span>  //Drill附加的规则②</span><br><span class="line">      <span class="string">.build()</span>;</span><br><span class="line">  this<span class="string">.planner</span> = Frameworks<span class="string">.getPlanner(config)</span>;    </span><br><span class="line">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="component">private RuleSet[] getRules(QueryContext context) &#123;</span><br><span class="line">  StoragePluginRegistry storagePluginRegistry = context<span class="string">.getStorage()</span>;</span><br><span class="line">  RuleSet drillLogicalRules = DrillRuleSets<span class="string">.mergedRuleSets(DrillRuleSets.getDrillBasicRules(context)</span>, DrillRuleSets<span class="string">.getJoinPermRules(context)</span>, DrillRuleSets<span class="string">.getDrillUserConfigurableLogicalRules(context))</span>;</span><br><span class="line">  RuleSet drillPhysicalMem = DrillRuleSets<span class="string">.mergedRuleSets(DrillRuleSets.getPhysicalRules(context)</span>, storagePluginRegistry<span class="string">.getStoragePluginRuleSet())</span>;</span><br><span class="line">  // Following is used in LOPT join OPT.</span><br><span class="line">  RuleSet logicalConvertRules = DrillRuleSets<span class="string">.mergedRuleSets(DrillRuleSets.getDrillBasicRules(context)</span>, DrillRuleSets<span class="string">.getDrillUserConfigurableLogicalRules(context))</span>;</span><br><span class="line">  RuleSet[] allRules = new RuleSet[] &#123;drillLogicalRules, drillPhysicalMem, logicalConvertRules&#125;</span>;</span><br><span class="line">  return allRules;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>逻辑计划的基本规则, 这些规则是通用的, 不需要在物理计划阶段完成, 通用的规则尽早做.  </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Get an immutable list of rules that will always be used when running logical planning.</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="function">RuleSet <span class="title">getDrillBasicRules</span><span class="params">(QueryContext context)</span> </span>&#123;</span><br><span class="line">    DRILL_BASIC_RULES = <span class="keyword">new</span> DrillRuleSet(ImmutableSet.&lt;RelOptRule&gt; builder().add( <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Add support for Distinct Union (by using Union-All followed by Distinct)</span></span><br><span class="line">    UnionToDistinctRule.INSTANCE,</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add support for WHERE style joins. 添加支持where类型的join</span></span><br><span class="line">    DrillFilterJoinRules.DRILL_FILTER_ON_JOIN,</span><br><span class="line">    DrillFilterJoinRules.DRILL_JOIN,</span><br></pre></td></tr></table></figure>
<p>举个where类型的join规则转换: <a href="http://blog.aliyun.com/733" target="_blank" rel="external">http://blog.aliyun.com/733</a><br>SELECT * FROM A JOIN B ON A.ID=B.ID WHERE A.AGE&gt;10 AND B.AGE&gt;5<br><code>Predict Push Down</code>: 在遇有JOIN运算时,用户很有可能还要在JOIN之后做WHERE运算,此时就要从代数逻辑上分析,<br>WHERE中计算的条件是否可以被提前到JOIN之前运算,以此来减少JOIN运算的数据量,提升效率  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill17.png" alt=""></p>
<p>那么Drill的FilterJoin规则是怎么样的呢?  </p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> DrillFilterJoinRules &#123;</span><br><span class="line">  <span class="comment">/** Predicate that always returns true for any filter in OUTER join, and only true for EQUAL or IS_DISTINCT_FROM over RexInputRef in INNER join.</span><br><span class="line">   * With this predicate, the filter expression that return true will be kept in the JOIN OP.</span><br><span class="line">   * Example:  INNER JOIN,   L.C1 = R.C2 and L.C3 + 100 = R.C4 + 100 will be kepted in JOIN.</span><br><span class="line">   *                         L.C5 &lt; R.C6 will be pulled up into Filter above JOIN. </span><br><span class="line">   *           OUTER JOIN,   Keep any filter in JOIN.</span><br><span class="line">  */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> FilterJoinRule.Predicate EQUAL_IS_DISTINCT_FROM =</span><br><span class="line">      <span class="keyword">new</span> FilterJoinRule.Predicate() &#123;</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">boolean</span> apply(<span class="keyword">Join</span> <span class="keyword">join</span>, JoinRelType joinType, RexNode exp) &#123;</span><br><span class="line">          <span class="comment">// In OUTER join, we could not pull-up the filter. All we can do is keep the filter with JOIN, and then decide whether the filter could be pushed down into LEFT/RIGHT.</span></span><br><span class="line">          <span class="keyword">if</span> (joinType != JoinRelType.INNER) <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">          List&lt;RexNode&gt; tmpLeftKeys = Lists.newArrayList();</span><br><span class="line">          List&lt;RexNode&gt; tmpRightKeys = Lists.newArrayList();</span><br><span class="line">          List&lt;RelDataTypeField&gt; sysFields = Lists.newArrayList();</span><br><span class="line">          RexNode remaining = RelOptUtil.splitJoinCondition(sysFields, <span class="keyword">join</span>.getLeft(), <span class="keyword">join</span>.getRight(), exp, tmpLeftKeys, tmpRightKeys, <span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">          <span class="keyword">if</span> (remaining.isAlwaysTrue())  <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Rule that pushes predicates from a Filter into the Join below them. */</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> FilterJoinRule DRILL_FILTER_ON_JOIN =</span><br><span class="line">      <span class="keyword">new</span> FilterJoinRule.FilterIntoJoinRule(<span class="keyword">true</span>, RelFactories.DEFAULT_FILTER_FACTORY,</span><br><span class="line">          RelFactories.DEFAULT_PROJECT_FACTORY, EQUAL_IS_DISTINCT_FROM);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里最好要理解下Calcite的一些概念, 要不然理解起来有一定困难.<br>参考<a href="http://blog.csdn.net/yunlong34574/article/details/46375733" target="_blank" rel="external">http://blog.csdn.net/yunlong34574/article/details/46375733</a>了解下optiq-javaben这个项目的源码.<br>然后参考这里了解下查询下推优化:<a href="https://datapsyche.wordpress.com/2014/08/06/optiq-query-push-down-concept" target="_blank" rel="external">https://datapsyche.wordpress.com/2014/08/06/optiq-query-push-down-concept</a>  </p>
</blockquote>
<h3 id="Calcite_FilterJoinRule">Calcite FilterJoinRule</h3><p>下面引用了Optiq作者的Apache Calcite Overview的一个示例:  </p>
<p>两张表进行join后有一个where过滤条件, 没有使用规则的话, 则要join完后才进行过滤:  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/calcite2.png" alt=""></p>
<p>使用FilterJoinRule后, 把Filter提前到Join之前, 扫描之后立刻进行, 这样减少了join的数据量:  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/calcite3.png" alt=""></p>
<p>那么怎么定义一个规则呢?  cal.rels是一个RelationExpression数组, 调用onMatch时, rels=[Join,Filter,Scan]<br>因此我们要获得call.rels中的Join和Filter. 使用数组索引rel(0)表示Join, rel(1)表示Filter.<br>最后调用call.transform(newJoin)将原始的RelationExpression转换成新的RelExp.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/calcite4.png" alt=""></p>
<blockquote>
<p>注意转换后的右图Join’,Filter’上面的引号表示new. 和原来的Join,Filter是不一样的变量了.  </p>
</blockquote>
<p>这里我们进入Calcite的源码看看它是怎么做的. 内部类FilterIntoJoinRule的构造函数参数:  </p>
<p>filterFactory和projectFactory分别是FilterFactoryImpl,ProjectFactoryImpl.<br>作为工厂类,它们的create方法会调用LogicalFilter,LogicalProject的create方法返回RelNode.  </p>
<p>那么问题是这里传入的为什么是Filter和Project呢? Filter显然需要,因为我们的规则就是针对Filter和Join进行优化的.<br>Project呢? Filter肯定是针对某个字段进行过滤的, 这里的过滤字段就可以认为是先Project把结果查出来,才有机会进行过滤.  </p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Rule that tries to push filter expressions into a join condition and into the inputs of the join. */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> FilterIntoJoinRule <span class="keyword">extends</span> FilterJoinRule &#123;</span><br><span class="line">  <span class="keyword">public</span> FilterIntoJoinRule(<span class="keyword">boolean</span> smart, RelFactories.FilterFactory filterFactory, RelFactories.ProjectFactory projectFactory, Predicate predicate) &#123;</span><br><span class="line">    <span class="keyword">super</span>(</span><br><span class="line">        operand(Filter.<span class="keyword">class</span>,</span><br><span class="line">            operand(<span class="keyword">Join</span>.<span class="keyword">class</span>, RelOptRule.<span class="keyword">any</span>())),</span><br><span class="line">        <span class="string">"FilterJoinRule:filter"</span>, smart, filterFactory, projectFactory, predicate);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  @Override <span class="keyword">public</span> <span class="keyword">void</span> onMatch(RelOptRuleCall <span class="keyword">call</span>) &#123;</span><br><span class="line">    Filter filter = <span class="keyword">call</span>.rel(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">Join</span> <span class="keyword">join</span> = <span class="keyword">call</span>.rel(<span class="number">1</span>);</span><br><span class="line">    perform(<span class="keyword">call</span>, filter, <span class="keyword">join</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>onMatch方法和上面的图是一样的. 而具体的call.transform则在FilterJoinRule的perform中完成.  我们先看下FilterIntoJoinRule类上面的注释:<br>尝试着把filter表达式push到一个join条件里面, 并且push到join的输入. 假设join的输入是Scan,则filer会push到Scan后面.  </p>
<p>再来看看FilterJoinRule类上面的注释: <code>Planner rule that pushes filters above and within a join node into the join node and/or its children nodes.</code> </p>
<p>向上提升filters(为什么是向上, 向上向下的方向是什么? Scan是输入源,则Scan在上, Scan-Join-Filter转换为Scan-Filter-Join,则Filter向上提升了一个等级),<br>within表示在一个join节点内部, 原先是Scan-Join-Filter, 第一步是把Filter合并到Join里面: Scan-Join(Filter)<br>或者join节点的子节点, 从Tree的角度来看, Join下面是两张数据源表,数据源就是Join的children节点.<br>我们可以把Filterwithin到Join的孩子节点即Scan中. 即第二步就是: Scan(Filter)-Join. DAG图就是: Scan-Filter-Join. Wow!!!</p>
<p>1.把left,right表解析出来即join.left,join.right. 以及leftFitlers,rightFilters.<br>2.根据leftFilters和left, rightFilters和right创建新的leftRel,rightRel节点<br>3.创建新的join节点,并且引用新的孩子节点(即上面的leftRel,rightRel)<br>4.调用call的transformTo,参数是最新的join节点.   </p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">// create FilterRels on top of the children if any filters were pushed to them</span></span><br><span class="line">  <span class="keyword">final</span> RexBuilder rexBuilder = <span class="keyword">join</span>.getCluster().getRexBuilder();</span><br><span class="line">  RelNode leftRel = RelOptUtil.createFilter(<span class="keyword">join</span>.getLeft(), leftFilters, filterFactory);</span><br><span class="line">  RelNode rightRel = RelOptUtil.createFilter(<span class="keyword">join</span>.getRight(), rightFilters, filterFactory);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// create the new join node referencing the new children and containing its new join filters (if there are any)</span></span><br><span class="line">  <span class="keyword">final</span> RexNode joinFilter = RexUtil.composeConjunction(rexBuilder, joinFilters, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">  RelNode newJoinRel = <span class="keyword">join</span>.<span class="keyword">copy</span>(<span class="keyword">join</span>.getTraitSet(), joinFilter, leftRel, rightRel, joinType, <span class="keyword">join</span>.isSemiJoinDone());</span><br><span class="line">  <span class="keyword">call</span>.getPlanner().onCopy(<span class="keyword">join</span>, newJoinRel);</span><br><span class="line">  <span class="keyword">if</span> (!leftFilters.isEmpty()) &#123;</span><br><span class="line">    <span class="keyword">call</span>.getPlanner().onCopy(filter, leftRel);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!rightFilters.isEmpty()) &#123;</span><br><span class="line">    <span class="keyword">call</span>.getPlanner().onCopy(filter, rightRel);</span><br><span class="line">  &#125;        </span><br><span class="line"></span><br><span class="line">  <span class="comment">// create a FilterRel on top of the join if needed</span></span><br><span class="line">  RelNode newRel = RelOptUtil.createFilter(newJoinRel,</span><br><span class="line">          RexUtil.fixUp(rexBuilder, aboveFilters, newJoinRel.getRowType()),</span><br><span class="line">          filterFactory);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">call</span>.transformTo(newRel);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面我们实现了自定义规则的onMatch方法, 那么谁来调用它呢:   </p>
<p>RelOptPlanner实现类VolcanoPlanner.fireRules-&gt;RelOptRule的实现类VolcanoRuleCall.match-&gt;matchRecurse-&gt;onMatch-&gt;getRule().onMatch(this);  </p>
<blockquote>
<p>Volcano的意思是火山似的,猛烈的. 由此说明规则很多的话, match调用会是很凶猛的. </p>
</blockquote>
<h2 id="Drill_FilterJoin_Example">Drill FilterJoin Example</h2><p>执行下面的SQL语句, 第一次不加where,第二次添加where过滤条件, 第三次where是字段比较</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">select * <span class="keyword">FROM</span> dfs.`<span class="regexp">/home/</span>hadoop<span class="regexp">/soft/</span>apache-drill-<span class="number">1.0</span>.<span class="number">0</span><span class="regexp">/sample-data/</span>nation.parquet` nations</span><br><span class="line"><span class="keyword">join</span> dfs.`<span class="regexp">/home/</span>hadoop<span class="regexp">/soft/</span>apache-drill-<span class="number">1.0</span>.<span class="number">0</span><span class="regexp">/sample-data/</span>region.parquet` regions</span><br><span class="line">on nations.N_REGIONKEY = regions.R_REGIONKEY</span><br><span class="line"></span><br><span class="line">select * <span class="keyword">FROM</span> dfs.`<span class="regexp">/home/</span>hadoop<span class="regexp">/soft/</span>apache-drill-<span class="number">1.0</span>.<span class="number">0</span><span class="regexp">/sample-data/</span>nation.parquet` nations</span><br><span class="line"><span class="keyword">join</span> dfs.`<span class="regexp">/home/</span>hadoop<span class="regexp">/soft/</span>apache-drill-<span class="number">1.0</span>.<span class="number">0</span><span class="regexp">/sample-data/</span>region.parquet` regions</span><br><span class="line">on nations.N_REGIONKEY = regions.R_REGIONKEY</span><br><span class="line">where nations.N_NATIONKEY&gt;<span class="number">10</span> and regions.R_NAME=<span class="string">'AMERICA'</span></span><br><span class="line"></span><br><span class="line">select * <span class="keyword">FROM</span> dfs.`<span class="regexp">/home/</span>hadoop<span class="regexp">/soft/</span>apache-drill-<span class="number">1.0</span>.<span class="number">0</span><span class="regexp">/sample-data/</span>nation.parquet` nations</span><br><span class="line"><span class="keyword">join</span> dfs.`<span class="regexp">/home/</span>hadoop<span class="regexp">/soft/</span>apache-drill-<span class="number">1.0</span>.<span class="number">0</span><span class="regexp">/sample-data/</span>region.parquet` regions</span><br><span class="line">on nations.N_REGIONKEY = regions.R_REGIONKEY</span><br><span class="line">where nations.N_NAME&lt;regions.R_NAME</span><br></pre></td></tr></table></figure>
<p>下面是对应物理计划可视化图, 图1在Scan和JOIN之间有Project:  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill18.png" alt="">   </p>
<p>图2虽然where过滤在join之后, 但是经过优化后, 会先于join执行的: 即filter之后才进行join  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill19.png" alt="">   </p>
<p>图3就没这么幸运了,要在join之后才能filter.</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill20.png" alt="">   </p>
<hr>
<h2 id="DrillRel(Drill逻辑操作符)">DrillRel(Drill逻辑操作符)</h2><p>getPlan的参数SqlNode在前面通过Calcite的解析, 结果是一颗SQL parse tree(不要以为Node就只有一个节点),<br>但它还只是Calcite认识的SQL操作符, 我们要将它转换为Drill能够认识的逻辑操作符即DrillRel.  </p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public PhysicalPlan getPlan(SqlNode sqlNode) throws ValidationException, RelConversionException, IOException, ForemanSetupException &#123;</span><br><span class="line">  SqlNode rewrittenSqlNode = rewrite(sqlNode)<span class="comment">;</span></span><br><span class="line">  TypedSqlNode validatedTypedSqlNode = validateNode(rewrittenSqlNode)<span class="comment">;</span></span><br><span class="line">  SqlNode validated = validatedTypedSqlNode.getSqlNode()<span class="comment">;</span></span><br><span class="line">  RelDataType validatedRowType = validatedTypedSqlNode.getType()<span class="comment">;</span></span><br><span class="line">  RelNode rel = convertToRel(validated)<span class="comment">;</span></span><br><span class="line">  rel = preprocessNode(rel)<span class="comment">;</span></span><br><span class="line">  log("Optiq Logical", rel)<span class="comment">;</span></span><br><span class="line"></span><br><span class="line">  DrillRel drel = convertToDrel(rel, validatedRowType)<span class="comment">;</span></span><br><span class="line">  log("Drill Logical", drel)<span class="comment">;</span></span><br><span class="line"></span><br><span class="line">  Prel prel = convertToPrel(drel)<span class="comment">;</span></span><br><span class="line">  log("Drill Physical", prel)<span class="comment">;</span></span><br><span class="line"></span><br><span class="line">  PhysicalOperator pop = convertToPop(prel)<span class="comment">;</span></span><br><span class="line">  PhysicalPlan plan = convertToPlan(pop)<span class="comment">;</span></span><br><span class="line">  log("Drill Plan", plan)<span class="comment">;</span></span><br><span class="line">  return plan<span class="comment">;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Relational Expression(Rel)</strong></p>
<p>在查询过程中也说了: <code>执行计划总是包含一个Screen Operator,用来阻塞并且等待返回的数据. 返回的DrillRel就是逻辑计划.</code><br>SqlNode,RelNode是Calcite的节点, DrillRel是Drill的关系表达式节点,在最外层包装了一个Screen用于屏幕输出.    </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function">DrillRel <span class="title">convertToDrel</span><span class="params">(RelNode relNode, RelDataType validatedRowType)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// Put a non-trivial topProject to ensure the final output field name is preserved, when necessary.</span></span><br><span class="line">      DrillRel topPreservedNameProj = addRenamedProject((DrillRel) convertedRelNode, validatedRowType);</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> DrillScreenRel(topPreservedNameProj.getCluster(), topPreservedNameProj.getTraitSet(), topPreservedNameProj);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>Screen Node和其他一些DrillRel的构造函数, 其中input指定了Screen的输入,表示用Screen节点包装上原先的节点, 使其成为一个新的节点.  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">DrillScreenRel</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">DrillScreenRelBase</span> <span class="title">implements</span> <span class="title">DrillRel</span> &#123;</span></span><br><span class="line">  public <span class="type">DrillScreenRel</span>(<span class="type">RelOptCluster</span> cluster, <span class="type">RelTraitSet</span> traitSet, <span class="type">RelNode</span> input) &#123;</span><br><span class="line">    <span class="keyword">super</span>(<span class="type">DRILL_LOGICAL</span>, cluster, traitSet, input);</span><br><span class="line">  &#125;</span><br><span class="line">  public <span class="type">LogicalOperator</span> implement(<span class="type">DrillImplementor</span> implementor) &#123;</span><br><span class="line">    <span class="type">LogicalOperator</span> childOp = implementor.visitChild(<span class="keyword">this</span>, <span class="number">0</span>, getInput());</span><br><span class="line">    <span class="keyword">return</span> <span class="type">Store</span>.builder().setInput(childOp).storageEngine(<span class="string">"--SCREEN--"</span>).build();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>类继承关系: DrillScreenRel &gt;&gt; DrillRel &gt;&gt; DrillRelNode &gt;&gt; RelNode<br>其中DrillRel是逻辑计划的关系表达式. 子类要实现implement方法, 返回逻辑操作符.  </p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Relational expression that is implemented in Drill.</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">DrillRel</span> <span class="keyword">extends</span> <span class="title">DrillRelNode</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Calling convention for relational expressions that are "implemented" by generating Drill logical plans</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> Convention DRILL_LOGICAL = <span class="keyword">new</span> Convention.Impl(<span class="string">"LOGICAL"</span>, DrillRel.<span class="keyword">class</span>);</span><br><span class="line"></span><br><span class="line">  LogicalOperator implement(DrillImplementor implementor);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill13-2.png" alt="">  <img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill13-1.png" alt=""></p>
<h2 id="DrillRel_Nodes_Tree_→_Drill_LogicalPlan">DrillRel Nodes Tree → Drill LogicalPlan</h2><p>DrillImplementor: <code>Context for converting a tree of DrillRel nodes into a Drill logical plan</code>  </p>
<h2 id="物理计划Prel">物理计划Prel</h2><p>然后将逻辑计划转换为物理计划, 将DrillRel转换为Prel. 最后才是Drill的Plan. 注意Drill的物理计划和最终的Plan是有点差别的.  </p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">protected <span class="type">Prel</span> convertToPrel(<span class="type">RelNode</span> drel) &#123;</span><br><span class="line">  <span class="type">Prel</span> phyRelNode = (<span class="type">Prel</span>) planner.transform(<span class="type">DrillSqlWorker</span>.<span class="type">PHYSICAL_MEM_RULES</span>, traits, drel);</span><br><span class="line"></span><br><span class="line">  /*  <span class="type">The</span> order <span class="keyword">of</span> the following transformation <span class="keyword">is</span> important */</span><br><span class="line"></span><br><span class="line">  /*</span><br><span class="line">   * <span class="number">0</span>.) <span class="type">For</span> select * <span class="keyword">from</span> join query, we need insert project on top <span class="keyword">of</span> scan <span class="keyword">and</span> a top project just</span><br><span class="line">   * under screen operator. <span class="type">The</span> project on top <span class="keyword">of</span> scan will rename <span class="keyword">from</span> * to <span class="type">T1</span>*, <span class="keyword">while</span> the top project</span><br><span class="line">   * will rename <span class="type">T1</span>* to *, before it output the final <span class="literal">result</span>. <span class="type">Only</span> the top project will allow</span><br><span class="line">   * duplicate columns, since user could <span class="string">"explicitly"</span> ask <span class="keyword">for</span> duplicate columns ( select *, col, *).</span><br><span class="line">   * <span class="type">The</span> rest <span class="keyword">of</span> projects will remove the duplicate column <span class="keyword">when</span> we generate <span class="type">POP</span> <span class="keyword">in</span> json format.</span><br><span class="line">   */</span><br><span class="line">  phyRelNode = <span class="type">StarColumnConverter</span>.insertRenameProject(phyRelNode);  //* <span class="keyword">is</span> star, <span class="keyword">and</span> this column should convert</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>转换的过程比较复杂, 而且转换的顺序也很重要. 先看第一个, 在select * from join这种情况下, 要插入两个Project.<br>一个是scan(bottom)之上, 一个是screen(top)之下. 比如下面的SQL语句:    </p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select <span class="keyword">*</span> from dfs.`/usr/install/apache-drill-1.1.0/sample-data/nation.parquet` nations</span><br><span class="line">join dfs.`/usr/install/apache-drill-1.1.0/sample-data/region.parquet` regions</span><br><span class="line">on nations.N_REGIONKEY = regions.R_REGIONKEY;</span><br><span class="line">+--------------+-----------------+--------------+-----------------------+--------------+--------------+-----------------------+</span><br><span class="line">|<span class="string"> N_NATIONKEY  </span>|<span class="string">     N_NAME      </span>|<span class="string"> N_REGIONKEY  </span>|<span class="string">       N_COMMENT       </span>|<span class="string"> R_REGIONKEY  </span>|<span class="string">    R_NAME    </span>|<span class="string">       R_COMMENT       </span>|</span><br><span class="line">+--------------+-----------------+--------------+-----------------------+--------------+--------------+-----------------------+</span><br><span class="line">|<span class="string"> 0            </span>|<span class="string"> ALGERIA         </span>|<span class="string"> 0            </span>|<span class="string">  haggle. carefully f  </span>|<span class="string"> 0            </span>|<span class="string"> AFRICA       </span>|<span class="string"> lar deposits. blithe  </span>|</span><br><span class="line">|<span class="string"> 1            </span>|<span class="string"> ARGENTINA       </span>|<span class="string"> 1            </span>|<span class="string"> al foxes promise sly  </span>|<span class="string"> 1            </span>|<span class="string"> AMERICA      </span>|<span class="string"> hs use ironic, even   </span>|</span><br></pre></td></tr></table></figure>
<p>物理计划: </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">00</span>-<span class="number">00</span>    Screen : rowType = RecordType(ANY *, ANY *<span class="number">0</span>): rowcount = <span class="number">25.0</span>, cumulative cost = &#123;<span class="number">62.5</span> rows, <span class="number">402.5</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">88.0</span> memory&#125;, id = <span class="number">2432</span></span><br><span class="line"><span class="number">00</span>-<span class="number">01</span>   ⑤ ProjectAllowDup(*=[$<span class="number">0</span>], *<span class="number">0</span>=[$<span class="number">1</span>]) : rowType = RecordType(ANY *, ANY *<span class="number">0</span>): rowcount = <span class="number">25.0</span>, cumulative cost = &#123;<span class="number">60.0</span> rows, <span class="number">400.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">88.0</span> memory&#125;, id = <span class="number">2431</span></span><br><span class="line"><span class="number">00</span>-<span class="number">02</span>   ④   Project(T0¦¦*=[$<span class="number">0</span>], T1¦¦*=[$<span class="number">2</span>]) : rowType = RecordType(ANY T0¦¦*, ANY T1¦¦*): rowcount = <span class="number">25.0</span>, cumulative cost = &#123;<span class="number">60.0</span> rows, <span class="number">400.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">88.0</span> memory&#125;, id = <span class="number">2430</span></span><br><span class="line"><span class="number">00</span>-<span class="number">03</span>   ③     HashJoin(condition=[=($<span class="number">1</span>, $<span class="number">3</span>)], joinType=[inner]) : rowType = RecordType(ANY T0¦¦*, ANY N_REGIONKEY, ANY T1¦¦*, ANY R_REGIONKEY): rowcount = <span class="number">25.0</span>, cumulative cost = &#123;<span class="number">60.0</span> rows, <span class="number">400.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">88.0</span> memory&#125;, id = <span class="number">2429</span></span><br><span class="line"><span class="number">00</span>-<span class="number">05</span>   ①       Project(T0¦¦*=[$<span class="number">0</span>], N_REGIONKEY=[$<span class="number">1</span>]) : rowType = RecordType(ANY T0¦¦*, ANY N_REGIONKEY): rowcount = <span class="number">25.0</span>, cumulative cost = &#123;<span class="number">25.0</span> rows, <span class="number">50.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">2426</span></span><br><span class="line"><span class="number">00</span>-<span class="number">07</span>              Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=file:/usr/install/apache-drill-<span class="number">1.1</span><span class="number">.0</span>/sample-data/nation.parquet]], selectionRoot=file:/usr/install/apache-drill-<span class="number">1.1</span><span class="number">.0</span>/sample-data/nation.parquet, numFiles=<span class="number">1</span>, columns=[`*`]]]) : rowType = (DrillRecordRow[*, N_REGIONKEY]): rowcount = <span class="number">25.0</span>, cumulative cost = &#123;<span class="number">25.0</span> rows, <span class="number">50.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">2425</span></span><br><span class="line"><span class="number">00</span>-<span class="number">04</span>   ②       Project(T1¦¦*=[$<span class="number">0</span>], R_REGIONKEY=[$<span class="number">1</span>]) : rowType = RecordType(ANY T1¦¦*, ANY R_REGIONKEY): rowcount = <span class="number">5.0</span>, cumulative cost = &#123;<span class="number">5.0</span> rows, <span class="number">10.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">2428</span></span><br><span class="line"><span class="number">00</span>-<span class="number">06</span>              Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=file:/usr/install/apache-drill-<span class="number">1.1</span><span class="number">.0</span>/sample-data/region.parquet]], selectionRoot=file:/usr/install/apache-drill-<span class="number">1.1</span><span class="number">.0</span>/sample-data/region.parquet, numFiles=<span class="number">1</span>, columns=[`*`]]]) : rowType = (DrillRecordRow[*, R_REGIONKEY]): rowcount = <span class="number">5.0</span>, cumulative cost = &#123;<span class="number">5.0</span> rows, <span class="number">10.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">2427</span></span><br></pre></td></tr></table></figure>
<p>对应的可视化图:  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill14.png" alt=""></p>
<p>物理计划中的$0, $1…这些数字代表的是as后的变量,如果是join有可能列名相同,所以也要添加project重命名防止名称冲突:    </p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">① select T<span class="number">0</span>.* <span class="keyword">as</span> <span class="variable">$0</span>, T<span class="number">0</span>.N_REGIONKEY <span class="keyword">as</span> <span class="variable">$1</span> from nations T<span class="number">0</span> </span><br><span class="line">② select T1.* <span class="keyword">as</span> <span class="variable">$0</span>, T1.R_REGIONKEY <span class="keyword">as</span> <span class="variable">$1</span> from regions T1</span><br><span class="line">③ select T<span class="number">0</span>.<span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$0</span>, T<span class="number">0</span>.<span class="variable">$1</span> <span class="keyword">as</span> <span class="variable">$1</span>, T1.<span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$2</span>, T1.<span class="variable">$1</span> <span class="keyword">as</span> <span class="variable">$3</span> </span><br><span class="line">   from (select T<span class="number">0</span>.<span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$0</span>, T<span class="number">0</span>.<span class="variable">$1</span> <span class="keyword">as</span> <span class="variable">$1</span> from nations) T<span class="number">0</span></span><br><span class="line">   join (select T1.<span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$2</span>, T1.<span class="variable">$1</span> <span class="keyword">as</span> <span class="variable">$3</span> from regions) T1</span><br><span class="line">   on T<span class="number">0</span>.<span class="variable">$1</span> = T1.<span class="variable">$3</span> </span><br><span class="line">④ select <span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$0</span>,<span class="variable">$2</span> <span class="keyword">as</span> <span class="variable">$1</span> from ( </span><br><span class="line">     select T<span class="number">0</span>.<span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$0</span>, T<span class="number">0</span>.<span class="variable">$1</span> <span class="keyword">as</span> <span class="variable">$1</span>, T1.<span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$2</span>, T1.<span class="variable">$1</span> <span class="keyword">as</span> <span class="variable">$3</span> </span><br><span class="line">     from (select T<span class="number">0</span>.<span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$0</span>, T<span class="number">0</span>.<span class="variable">$1</span> <span class="keyword">as</span> <span class="variable">$1</span> from nations) T<span class="number">0</span></span><br><span class="line">     join (select T1.<span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$2</span>, T1.<span class="variable">$1</span> <span class="keyword">as</span> <span class="variable">$3</span> from regions) T1</span><br><span class="line">     on T<span class="number">0</span>.<span class="variable">$1</span> = T1.<span class="variable">$3</span></span><br><span class="line">   )</span><br><span class="line">⑤ select <span class="variable">$0</span> <span class="keyword">as</span> *, <span class="variable">$1</span> <span class="keyword">as</span> *<span class="number">0</span> from(  </span><br><span class="line">     select <span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$0</span>,<span class="variable">$2</span> <span class="keyword">as</span> <span class="variable">$1</span> from ( </span><br><span class="line">       select T<span class="number">0</span>.<span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$0</span>, T<span class="number">0</span>.<span class="variable">$1</span> <span class="keyword">as</span> <span class="variable">$1</span>, T1.<span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$2</span>, T1.<span class="variable">$1</span> <span class="keyword">as</span> <span class="variable">$3</span> </span><br><span class="line">       from (select T<span class="number">0</span>.<span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$0</span>, T<span class="number">0</span>.<span class="variable">$1</span> <span class="keyword">as</span> <span class="variable">$1</span> from nations) T<span class="number">0</span></span><br><span class="line">       join (select T1.<span class="variable">$0</span> <span class="keyword">as</span> <span class="variable">$2</span>, T1.<span class="variable">$1</span> <span class="keyword">as</span> <span class="variable">$3</span> from regions) T1</span><br><span class="line">       on T<span class="number">0</span>.<span class="variable">$1</span> = T1.<span class="variable">$3</span></span><br><span class="line">     )</span><br><span class="line">   )</span><br><span class="line">   select *,*<span class="number">0</span> from ...</span><br></pre></td></tr></table></figure>
<p>上面的StarColumn规则有点复杂, 我们看下Join列冲突的规则. 对应上面的③JOIN. 将所有的列都重命名了($0,$1,$2,$3, 然后以$1,$3进行join).  </p>
<figure class="highlight vbscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * <span class="number">1.</span>)</span><br><span class="line"> * <span class="built_in">Join</span> might cause naming conflicts from its <span class="built_in">left</span> <span class="keyword">and</span> <span class="built_in">right</span> child.</span><br><span class="line"> * <span class="keyword">In</span> such <span class="keyword">case</span>, we have <span class="keyword">to</span> insert Project <span class="keyword">to</span> rename the conflicting names.</span><br><span class="line"> */</span><br><span class="line">phyRelNode = JoinPrelRenameVisitor.insertRenameProject(phyRelNode);</span><br></pre></td></tr></table></figure>
<p>根据注释中说的join有left或者right child. 注意child这个词的含义. join作为根, 而left和right表分别是根的左右子节点.    </p>
<blockquote>
<p>为了防止名称冲突, 添加project, 这样就和上面我们看到的可视化Plan图是一一对应的了.<br>那么思考下: 这里的join插入的Project是在①和②,还是④??<br>我觉得是在④这里, 因为①和②已经在上面第一个转换规则StarColumnConverter中运用过了.  </p>
</blockquote>
<p>insert操作让传入的phyRelNode节点调用它的accept方法, 并接收JoinPrelRenameVisitor实例对象.  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">JoinPrelRenameVisitor</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">BasePrelVisitor&lt;Prel</span>, <span class="title">Void</span>, <span class="title">RuntimeException&gt;</span>&#123;</span></span><br><span class="line">  <span class="keyword">private</span> static <span class="type">JoinPrelRenameVisitor</span> <span class="type">INSTANCE</span> = <span class="keyword">new</span> <span class="type">JoinPrelRenameVisitor</span>();</span><br><span class="line"></span><br><span class="line">  public static <span class="type">Prel</span> insertRenameProject(<span class="type">Prel</span> prel)&#123;</span><br><span class="line">    <span class="keyword">return</span> prel.accept(<span class="type">INSTANCE</span>, <span class="literal">null</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>这里的Prel通过层层的规则嵌套, 最终返回的还是一个Prel, 也就是说,每次运用一个规则,都要把当前最新值传进来. Prel也实现了DrillRelNode接口.<br>DrillRelNode再结合上Visitor, 有种层层嵌套的感觉.首先注册操作符的规则,从而构成一张图,最后根据DAG图访问每个操作符的时候,再运用上规则. </p>
<p>假设上面JoinPrelRenameVisitor的insertRenameProject的Prel是JoinPrel</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill15.png" alt=""></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinPrel</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">DrillJoinRelBase</span> <span class="title">implements</span> <span class="title">Prel</span>&#123;</span></span><br><span class="line">  public &lt;<span class="type">T</span>, <span class="type">X</span>, <span class="type">E</span> <span class="keyword">extends</span> <span class="type">Throwable</span>&gt; <span class="type">T</span> accept(<span class="type">PrelVisitor</span>&lt;<span class="type">T</span>, <span class="type">X</span>, <span class="type">E</span>&gt; logicalVisitor, <span class="type">X</span> value) <span class="keyword">throws</span> <span class="type">E</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> logicalVisitor.visitJoin(<span class="keyword">this</span>, value);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public <span class="type">Iterator</span>&lt;<span class="type">Prel</span>&gt; iterator() &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="type">PrelUtil</span>.iter(getLeft(), getRight());</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>accept()的参数logicalVisitor显然就是JoinPrelRenameVisitor了. this是当前对象即JoinPrel.<br>那么就要调用JoinPrelRenameVisitor的visitJoin方法. 你看又回到Visitor来了.  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">JoinPrelRenameVisitor</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">BasePrelVisitor&lt;Prel</span>, <span class="title">Void</span>, <span class="title">RuntimeException&gt;</span>&#123;</span></span><br><span class="line"></span><br><span class="line">  public <span class="type">Prel</span> visitJoin(<span class="type">JoinPrel</span> prel, <span class="type">Void</span> value) <span class="keyword">throws</span> <span class="type">RuntimeException</span> &#123;</span><br><span class="line">    <span class="type">List</span>&lt;<span class="type">RelNode</span>&gt; children = <span class="type">Lists</span>.newArrayList();</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">Prel</span> child : prel)&#123;</span><br><span class="line">      child = child.accept(<span class="keyword">this</span>, <span class="literal">null</span>);</span><br><span class="line">      children.add(child);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">final</span> int leftCount = children.get(<span class="number">0</span>).getRowType().getFieldCount();</span><br><span class="line">    <span class="type">List</span>&lt;<span class="type">RelNode</span>&gt; reNamedChildren = <span class="type">Lists</span>.newArrayList();</span><br><span class="line"></span><br><span class="line">    <span class="type">RelNode</span> left = prel.getJoinInput(<span class="number">0</span>, children.get(<span class="number">0</span>));</span><br><span class="line">    <span class="type">RelNode</span> right = prel.getJoinInput(leftCount, children.get(<span class="number">1</span>));</span><br><span class="line">    reNamedChildren.add(left);</span><br><span class="line">    reNamedChildren.add(right);</span><br><span class="line">    <span class="keyword">return</span> (<span class="type">Prel</span>) prel.copy(prel.getTraitSet(), reNamedChildren);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>JoinPrel是个迭代器, 因此用for-loop方式可以遍历它的节点: 即参与join的left和right表(实现了iterator方法).<br>JoinPrel的getJoinInput方法参数是offset和RelNode. offset表示join之后列的索引(两张表join后的所有列). </p>
<p>假设我们用两张一样的表进行join,可以看到相同的列, 右边的表会被重命名:  </p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select * from dfs.<span class="code">`/usr/install/apache-drill-1.1.0/sample-data/region.parquet`</span> region1</span><br><span class="line">join dfs.<span class="code">`/usr/install/apache-drill-1.1.0/sample-data/region.parquet`</span> regions</span><br><span class="line"><span class="header">on region1.R_REGIONKEY = regions.R_REGIONKEY;</span><br><span class="line">+--------------+--------------+-----------------------+---------------+--------------+-----------------------+</span></span><br><span class="line"><span class="header">| R_REGIONKEY  |    R_NAME    |       R_COMMENT       | R_REGIONKEY0  |   R_NAME0    |      R_COMMENT0       |</span><br><span class="line">+--------------+--------------+-----------------------+---------------+--------------+-----------------------+</span></span><br><span class="line">| 0            | AFRICA       | lar deposits. blithe  | 0             | AFRICA       | lar deposits. blithe  |</span><br></pre></td></tr></table></figure>
<p>分别调用两次getJoinInput,传入不同的offset和input, 这两个结果一定是不同的.   </p>
<figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Check to make sure that the fields of the inputs are the same as the output field names.  If not, insert a project renaming them.</span></span><br><span class="line">public RelNode getJoinInput(<span class="built_in">int</span> offset, RelNode input) &#123;</span><br><span class="line">  <span class="keyword">final</span> <span class="built_in">List</span>&lt;<span class="built_in">String</span>&gt; fields = getRowType().getFieldNames();</span><br><span class="line">  <span class="keyword">final</span> <span class="built_in">List</span>&lt;<span class="built_in">String</span>&gt; inputFields = input.getRowType().getFieldNames();</span><br><span class="line">  <span class="keyword">final</span> <span class="built_in">List</span>&lt;<span class="built_in">String</span>&gt; outputFields = fields.subList(offset, offset + inputFields.size());</span><br><span class="line">  <span class="keyword">if</span> (!outputFields.equals(inputFields)) &#123;</span><br><span class="line">    <span class="comment">// Ensure that input field names are the same as output field names.</span></span><br><span class="line">    <span class="comment">// If there are duplicate field names on left and right, fields will get lost.</span></span><br><span class="line">    <span class="comment">// In such case, we need insert a rename Project on top of the input.</span></span><br><span class="line">    <span class="keyword">return</span> rename(input, input.getRowType().getFieldList(), outputFields);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> input;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>上面的处理不知道什么情况下会进入if部分.  假设有两张表都是A,B,C三列.<br>left表不可能有重复的列名, right表相对于left而言,三个列都是重复的. 调用getJoinInput(3, rightNode){}<br>inputFields=[A,B,C], fields=[A,B,C,A,B,C]. outputFields=[A,B,C],不是相等的吗??</p>
</blockquote>
<p>看下相同表的join的可视化树, 对比一下就知道了, 在00-04中加了Project:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">00</span>-<span class="number">00</span>    Screen : rowType = RecordType(ANY *, ANY *<span class="number">0</span>): rowcount = <span class="number">5.0</span>, cumulative cost = &#123;<span class="number">20.5</span> rows, <span class="number">120.5</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">88.0</span> memory&#125;, id = <span class="number">1299</span></span><br><span class="line"><span class="number">00</span>-<span class="number">01</span>      ProjectAllowDup(*=[$<span class="number">0</span>], *<span class="number">0</span>=[$<span class="number">1</span>]) : rowType = RecordType(ANY *, ANY *<span class="number">0</span>): rowcount = <span class="number">5.0</span>, cumulative cost = &#123;<span class="number">20.0</span> rows, <span class="number">120.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">88.0</span> memory&#125;, id = <span class="number">1298</span></span><br><span class="line"><span class="number">00</span>-<span class="number">02</span>        Project(T0¦¦*=[$<span class="number">0</span>], T1¦¦*=[$<span class="number">2</span>]) : rowType = RecordType(ANY T0¦¦*, ANY T1¦¦*): rowcount = <span class="number">5.0</span>, cumulative cost = &#123;<span class="number">20.0</span> rows, <span class="number">120.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">88.0</span> memory&#125;, id = <span class="number">1297</span></span><br><span class="line"><span class="number">00</span>-<span class="number">03</span>          HashJoin(condition=[=($<span class="number">1</span>, $<span class="number">3</span>)], joinType=[inner]) : rowType = RecordType(ANY T0¦¦*, ANY R_REGIONKEY, ANY T1¦¦*, ANY R_REGIONKEY0): rowcount = <span class="number">5.0</span>, cumulative cost = &#123;<span class="number">20.0</span> rows, <span class="number">120.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">88.0</span> memory&#125;, id = <span class="number">1296</span></span><br><span class="line"><span class="number">00</span>-<span class="number">04</span>            Project(T1¦¦*=[$<span class="number">0</span>], R_REGIONKEY0=[$<span class="number">1</span>]) : rowType = RecordType(ANY T1¦¦*, ANY R_REGIONKEY0): rowcount = <span class="number">5.0</span>, cumulative cost = &#123;<span class="number">5.0</span> rows, <span class="number">10.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">1295</span></span><br><span class="line"><span class="number">00</span>-<span class="number">06</span>              Project(T1¦¦*=[$<span class="number">0</span>], R_REGIONKEY=[$<span class="number">1</span>]) : rowType = RecordType(ANY T1¦¦*, ANY R_REGIONKEY): rowcount = <span class="number">5.0</span>, cumulative cost = &#123;<span class="number">5.0</span> rows, <span class="number">10.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">1294</span></span><br><span class="line"><span class="number">00</span>-<span class="number">08</span>                Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=file:/usr/install/apache-drill-<span class="number">1.1</span><span class="number">.0</span>/sample-data/region.parquet]], selectionRoot=file:/usr/install/apache-drill-<span class="number">1.1</span><span class="number">.0</span>/sample-data/region.parquet, numFiles=<span class="number">1</span>, columns=[`*`]]]) : rowType = (DrillRecordRow[*, R_REGIONKEY]): rowcount = <span class="number">5.0</span>, cumulative cost = &#123;<span class="number">5.0</span> rows, <span class="number">10.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">1293</span></span><br><span class="line"><span class="number">00</span>-<span class="number">05</span>            Project(T0¦¦*=[$<span class="number">0</span>], R_REGIONKEY=[$<span class="number">1</span>]) : rowType = RecordType(ANY T0¦¦*, ANY R_REGIONKEY): rowcount = <span class="number">5.0</span>, cumulative cost = &#123;<span class="number">5.0</span> rows, <span class="number">10.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">1292</span></span><br><span class="line"><span class="number">00</span>-<span class="number">07</span>              Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=file:/usr/install/apache-drill-<span class="number">1.1</span><span class="number">.0</span>/sample-data/region.parquet]], selectionRoot=file:/usr/install/apache-drill-<span class="number">1.1</span><span class="number">.0</span>/sample-data/region.parquet, numFiles=<span class="number">1</span>, columns=[`*`]]]) : rowType = (DrillRecordRow[*, R_REGIONKEY]): rowcount = <span class="number">5.0</span>, cumulative cost = &#123;<span class="number">5.0</span> rows, <span class="number">10.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">1291</span></span><br></pre></td></tr></table></figure>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill16.png" alt=""></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Source/">Source</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/drill/">drill</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-07-12-drill-rpc" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/12/2015-07-12-drill-rpc/" class="article-date">
  	<time datetime="2015-07-11T16:00:00.000Z" itemprop="datePublished">2015-07-12</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/12/2015-07-12-drill-rpc/">Apache Drill源码阅读(2) 分析一次查询过程以及RPC</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="一次Query的生命周期">一次Query的生命周期</h2><p>Foreman线程的run方法中的queryRequest是org.apache.drill.exec.proto.UserProtos的RunQuery<br>可以认为就是用户输入的查询语句,只不过由于是分布式,客户端输入的查询,会通过RPC在Foreman上执行<br>protobuffer文件的定义在drill-protocol/src/main/protobuf下,比如User.proto对应了UserProtos</p>
<p>关键看下run()上面的注释. 什么时候被调用: 在查询建立起来的时候<br>以什么样的方式调用: 执行线程池. 功能是什么: 完成远程执行<br>注意这个方法的结束并不代表查询生命周期的Foreman角色的结束.  </p>
<blockquote>
<p>Called by execution pool to do query setup, and kick off remote execution.<br>Note that completion of this function is not the end of the Foreman’s role in the query’s lifecycle.</p>
</blockquote>
<p><a href="https://tnachen.wordpress.com/2013/11/05/lifetime-of-a-query-in-drill-alpha-release/" target="_blank" rel="external">https://tnachen.wordpress.com/2013/11/05/lifetime-of-a-query-in-drill-alpha-release/</a><br><a href="http://yangyoupeng-cn-fujitsu-com.iteye.com/blog/1974556" target="_blank" rel="external">http://yangyoupeng-cn-fujitsu-com.iteye.com/blog/1974556</a></p>
<h3 id="Client">Client</h3><p>The SELECT statement in particular is querying a HDFS data file, with a specific WHERE clause filtering based on the expression clause.</p>
<p>From the client, it submits this statement into Sqlline, which is a simple Java-based console that is able to talk to a JDBC driver, passes the SELECT statement into Optiq.</p>
<p>Optiq is a library that Drill utilizes for query parsing and planning, which it provides pluggable transformation rules that can map a SQL token into any specific object you want. Optiq also embeds a cost-based query optimizer, which we utilize for it for choosing the best order of SQL operators in the statement without any other statistics. We implemented custom Optiq rules that maps specific SQL operators (WHERE, LIMIT, etc) and each rule converts its operator into our specific Logical Operator syntax that Drill understands.</p>
<p>This collection of Logical operators with its own configurations forms a Drill Logical plan. Drill’s logical plan sole purpose is to describe logically what work Drill needs to perform to produce the Query results, without any optimizations or distribution.</p>
<p>Once the client generates this logical plan, it looks up one of the DrillBit host/port information and passes the logical plan to that DrillBit.</p>
<blockquote>
<p>Drill logical plan的唯一的目标就是Drill的数据流的工作流程，而没有做任何的优化，和分布式计算的分发等工作<br>一旦client产生了logical plan，那么他会查询其中一个已经配置好的DrillBit的host/port的信息，<br>然后将logical plan传递给DrillBit(这个接收查询的DrillBit就是Foreman)  </p>
</blockquote>
<h3 id="Running_Logical_Plan">Running Logical Plan</h3><p>Any DrillBit in the cluster can initiate a query, and that DrillBit becomes the Drill process that is responsible for sending back the results back to the client.</p>
<blockquote>
<p>在集群中任何一个DrillBit都能运行一个查询，而执行查询的DrillBit要负责将查询结果返回给client  </p>
</blockquote>
<p>Once the UserServer that is listening to the User submission gets the logical plan, it passes it through Foreman that is responsible for turning this plan into an actual execution plan and submits the plan information for execution.</p>
<blockquote>
<p>UserServer会监听客户端提交的查询任务,一旦获取到逻辑计划,它会把逻辑计划传给Foreman.<br>Foreman会调优该plan，并且转换为实际执行的计划，并提交该计划的信息用于后面的执行.</p>
</blockquote>
<p>Inside of Foreman, the first operation it does is to transform the logical plan into a physical plan via Drill’s Optimizer. Drill’s current version of Optimizer is very basic, which simply transforms each logical operator directly into one or more phyiscal operators without much optimization rules looking into the association of other operators.</p>
<p>The physical plan is simple a DAG of physical operators, and each child/parent relationship implies how data flows through the graph. As we are inspired by Google’s Dremel paper, the take away we saw that implemented which is a MPP style multi-level execution tree, where in this execution tree each node represents a different DrillBit process and they each depend on each other results for computation.</p>
<blockquote>
<p>物理计划是physical operators的有向无环图.每一子节点或者父节点之间的关系都指明了数据如何在DAG图中流动<br>在这个执行树中，每一个节点都代表一个不同的DrillBit计算过程，他们相互依赖彼此的计算结果</p>
</blockquote>
<p>As we want to break this physical plan into a multi-level execution tree that involves multiple DrillBits, we first need to collect information about each physical operator. Each physical operator with the given operator configuration can return estimated stats about Network/Cpu/Memory/Disk and Row size and count. It also can return a list of preferred DrillBits that it wants to get executed on, which we call endpoint affinities.</p>
<blockquote>
<p>将物理计划分解成多个DrillBits参与的多层级的执行树，首先要搜集每一个physical operators的信息<br>根据给定的操作符的配置信息,每个physical operators会返回预估的统计信息,RowSize行的大小,Count数量<br><strong>它也能够返回一个将要执行该operator的DrilBit列表,称作距离最近/最优的端点</strong></p>
<p>类似HDFS中的读操作,读取HDFS块时,NN会返回这个块的DN列表,客户端读取离自己最近的DN的数据块</p>
</blockquote>
<p>One example Endpoint affinity is where a Parquet Scan opreator will want to initiate this query closet to where the Parquet file is stored, and it can look up the Parquet file’s Metadata information that stores the HDFS data node host and return that as a preferred endpoint if we have a DrillBit running there.</p>
<blockquote>
<p>比如一个Parquet扫描操作符会在离保存着Parquet文件最近的DrillBit上面发起查询<br>他可以查询Parquet文件的元数据信息: 元数据保存了HDFS的DN节点，并返回一个最优的endpoint  </p>
<p>Parquet文件是类似JSON那样有者self-describe格式的文件,即文件本身含有schema,尽管schema是free的.<br>由于Parquet保存在HDFS上,HDFS上的文件是有副本的. 而Scan操作符是要访问文件的,<br>所以Scan操作符会选择离自己这个操作符最近的DN上的Parquet文件副本时,是最优的情况.<br>当然对于最优的端点的前提是这个节点安装了DrillBit服务. 因为Drill是操作符的载体.  </p>
<p>也就是说,集群的DrillBit服务可以执行一个物理计划分解出来的physical operators<br>physical operators可以被集群的多个Drillbit执行.<br>通常DrillBit计算节点上也运行着DN这样的数据存储节点,而操作符需要存储的数据资源<br>所以操作符会选择离存储资源最近的Drillbit,这样的Drillbit是最优的endpoint.  </p>
</blockquote>
<p>With the physical plan, all the stats and endpoint affinities, the Parallellizer in Foreman transforms this plan into a number of fragments. Each fragment is simply a self contained Physical plan that is designed to run on each DrillBit node. In any given Physical plan, there will be only one Root Fragment that is going to run in the initiating DrillBit, possibly one or more Leaf fragments and possibly one or more intermediate fragments.</p>
<blockquote>
<p>有了物理计划,所有的统计信息,最优端点,Foreman中的Parallellizer会将物理计划转换为多个fragments.<br>每一个Fragment自身也是一个物理计划, 它们同样会被分配到DrillBit节点上面运行.<br>任何一个物理计划(经过Foreman转换后的每一个Fragment)只有一个RootFragment,多个中间或Leaf Fragment.</p>
</blockquote>
<h3 id="Running_Fragments">Running Fragments</h3><p>The Root fragment will be submitted to the Worker manager in its current DrillBit, the intermediate fragments will be stored in the HazelCast distributed cache, and all the leaf fragments will be sent directly to the DrillBits assigned via our BitCom through our RPC layer with Protobuf messages.</p>
<blockquote>
<p>Rootfragment会被提交给它所在的当前DrillBit上的WorkerManager.中间fragment保存在Hazelcast分布式缓存,<br>所有的leaf fragment会直接通过BitCom(RPC层次的东西，协议是Protobuf)发送给其他DrillBits  </p>
<p>在WEB页面可以看到的是Major和MinorFragment.那么这里的Root,Intermediate,Leaf Fragment是怎么YY出来的?</p>
</blockquote>
<p>The Worker Manager once receives this Root Fragment starts running this plan, which always contains a Screen Operator that blocks and wait for Records to be returned. If a plan also has multiple Drillbits involved, then it will also contain a exchange operator that sets up a Receiver that waits for incoming Records from the wire.</p>
<blockquote>
<p>Worker Manager一旦接受到Root Fragment就会运行这个plan,并且总是会包含一个Screen Operator,用来阻塞并且等待返回的数据.<br>如果该plan需要另外多个DrillBit参与,这些DrillBit组成一个wire，Worker Manager也同时会包含一个exchange operator，该exchange operator启动了一个Receiver用以等待wire中的数据</p>
<p>wire类似HDFS中DN组成的pipeline.当客户端写数据时,参与存储的DN形成一个管道,数据包在管道中流动.<br>只有所有的DN节点返回ack应答给客户端时,才认为数据写入成功. 这里参与计算的DrillBit节点也一样.<br>Exchange操作符类似于客户端,只有wire中的DrillBit数据获取完毕,返回给Receiver,才认为计算完成.  </p>
</blockquote>
<p>The Leaf fragments that are sent via the wire will be executed as soon as they arrive. The fragment will be parsed into a DAG with Physical operators, and setups the execution/data flow. Each Physical operator imposes a Pull style messaging, where starting from the bottom of the tree it pulls it’s parent for Records and the parent will return an Outcome status. The operators is designed to handle each possible outcome status, which can be STOP, OK, OK_WITH_NEW_SCHEMA, NONE. Since Drill supports flexible schema, which in other words can tolerate schema changes in the same dataset, the operators needs to handle what happens when there is a new schema for this set of records. Seeing the benefits of columnar storage:<a href="http://the-paper-trail.org/blog/columnar-storage/" target="_blank" rel="external">http://the-paper-trail.org/blog/columnar-storage/</a>. Drill also implemented its own in memory columnar format which we called ValueVectors. A ValueVector is simply a collection of bytes that represent a run of values within the same column. Each pull of messages in each Physical operator returns a RecordBatch that can contain one or more ValueVectors per column with schema.</p>
<blockquote>
<p>通过wire被发送的叶子Fragment一旦到达目的DrillBit就会被立即执行.叶子Fragment会被解析为由物理操作符组成的DAG.<br>每一个物理操作符都会利用一个Pull类型的消息机制，从树的底部开始，operator会从他的parent operator中拉取Records，<br>而他的parent operator则返回一个Outcome status消息. operators被设计为能处理每一种可能的结果状态.<br>因为Drill支持动态schema，也就是说Drill允许在同一个数据集中schema发生变化，所以Drill要能够处理当schema发生变化的情况  </p>
<p>Drill同时实现了他自己的列式内存数据结构:ValueVector，它是一个bytes集合，代表了一个column内的数据.<br>在每一个Physical operator pull的消息中会返回一个RecordBatch: 对于每个列都会包含一个或者多个ValueVector</p>
</blockquote>
<p>At the very tip of the Leaf fragment in my slideshow example is the Scan operator, which is configured to look up a Parquet file and run it through the Parquet Storage Engine. The Storage engine is simply responsible for pulling in data from the data source, and converting them into ValueVectors and passes that back to its child as a RecordBatch.</p>
<blockquote>
<p>从数据源中拉取数据，把数据转换为ValueVectors，然后将这些ValueVector作为RecordBatch传递回他的child</p>
<p>FragmentTree从底到上, 底部是Parent, 越往上就是Child. child会拉取parent的记录.<br>而从上到下来看,Fragment分解为RootFragment-&gt;Intermediate-&gt;LeafFragment.<br>这似乎有点矛盾,leaf是parent,往上则是child. 而最上面又是root fragment.  </p>
<p>扫描操作符的步骤:<br>1.Leaf Fragment拉取数据源的数据<br>2.将数据转换为ValueVectors<br>3.组装成RecordBatch<br>4.传递给它的孩子,即上一层</p>
</blockquote>
<p>Eventually the Leaf fragment will take this batch, and through the Sender operator send it to the Intermediate DrillBit.</p>
<blockquote>
<p>最终，所有的Leaf fragment将会接管这些batch数据，通过Sender operator发送给中间DrillBit<br>对于数据源,它们只暴露给物理计划形成的所有Leaf Fragment.这些Leaf Fragment负责读取数据.  </p>
</blockquote>
<p>The Intermediate DrillBit once receives a RecordBatch for the first time, will lookup the fragment from HazelCast by its fragment id from RecordBatch, and setup the Receiver and Physical opreators necessary for processing in this DrillBit.</p>
<blockquote>
<p>中间fragment一旦第一次接受到一个RecordBatch，会从HazleCast中通过RecordBatch中保留的fragment id<br>查询相应的fragment，并且设置Receiver以及必要的physical operator来继续在DrillBit中进行处理计算</p>
</blockquote>
<p>The intermediate fragment contains a Filtering operator, and inside this operator once it receives a RecordBatch it looks up the new schema and passes that to our CodeGeneration with the specified filter expression and type information, and generate a specific code that does the filtering. This is designed to avoid casting, run tight loops and leverage prefetching as we can eliminate function calls. This is a very common approach looking at the Hive’s new vectorized query engine via Stinger initiative or in Impala.</p>
<blockquote>
<p><strong>中间Fragment包含一个Filtering operator，在这个Filtering operator内部，一旦他接收到一个RecordBatch，他就会查找新的schema，并且将schema传递给CodeGeneration,同时还会传递一个特殊定义的filter expression，type information，借此产生一段特殊的code来完成filter 操作。通过设计成避免casting，运行轻量级的loop，以及进行prefetching，来减少方法的调用，这种方式在Hive的新vectoried query engine（通过Stinger initiative）以及impala中很普遍</strong></p>
</blockquote>
<p>The intermediate fragment eventually streams a batch at a time as soon as it is available to the Root DrillBit, which the Screen operator receives and returns it to the Client.</p>
<blockquote>
<p>中间fragment最终会议batch为单元，一次发送一个batch给Root DrillBit(就是Foreman),<br>在Root DrillBit中会由Screen operator来接收相关数据，并且返回给client(接收查询的也负责返回查询结果    )</p>
</blockquote>
<p>DrillClient that receives the RecordBatch, simply transforms our own columnar ValueVectors into Rows and shows the result to the client.</p>
<blockquote>
<p>DrillClient接收RecordBatch，简单将ValueVector转换成Rows并且显示给client</p>
</blockquote>
<p>This is overall what the flow looks like in our Drill alpha release, and we will be continuing to validate this architecture through diagnostic and benchmarking, and also provide more operators and storage engine so we can do much more interesting data analysis.</p>
<h2 id="Physical_Operator">Physical Operator</h2><p>前面有逻辑操作符LogicalOperator接口,同样也有物理操作符PhysicalOperator接口  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill6.png" alt=""></p>
<p>我们先看下HasAffinity有最优节点,方法getOperatorAffinity返回最优的EndPoint列表<br>描述了一个物理操作符对一些特定的DrillBit节点有亲和性的, 用于分配决策.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Describes a physical operator that has affinity to particular nodes. Used for assignment decisions.</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">HasAffinity</span> <span class="keyword">extends</span> <span class="title">PhysicalOperator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> List&lt;EndpointAffinity&gt; <span class="title">getOperatorAffinity</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EndpointAffinity</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> DrillbitEndpoint endpoint;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">double</span> affinity = <span class="number">0.0</span>d;</span><br></pre></td></tr></table></figure>
<p>EndpointAffinity captures affinity value for a given single Drillbit endpoint.<br>EndpointAffinity有DrillbitEndpoint的引用, 注释中提到affinity value,所以是不是够亲和是可以计算出来的.<br>初始时这个值是0,调用addAffinity()可以给Drillbit endpoint添加一个亲和力的值.  </p>
<p>DrillBit Endpoint对象被定义为protobuf,在Coordination.proto中:  </p>
<figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">DrillbitEndpoint</span></span>&#123;</span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">string</span> address = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">int32</span> user_port = <span class="number">2</span>;</span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">int32</span> control_port = <span class="number">3</span>;</span><br><span class="line">  <span class="keyword">optional</span> <span class="built_in">int32</span> data_port = <span class="number">4</span>;</span><br><span class="line">  <span class="keyword">optional</span> Roles roles = <span class="number">5</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Drillbit可以认为是Drill的计算节点. 在bin下的drillbit.sh start启动一个Drill服务.  </p>
<p>LogicalPlan有一定的格式:head,storageengine,query. 同样PyhsicalPlan也有,它们的head是相同的.<br>PhysicalPlan的构造函数是一系列的物理操作符,而LogicalPlan的构造函数是逻辑操作符集合.目的都是构造一张DAG图.  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="annotation">@JsonPropertyOrder</span>(&#123; <span class="string">"head"</span>, <span class="string">"graph"</span> &#125;)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhysicalPlan</span> &#123;</span></span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">final</span> org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(PhysicalPlan.<span class="keyword">class</span>);</span><br><span class="line"></span><br><span class="line">  PlanProperties properties;</span><br><span class="line">  Graph&lt;PhysicalOperator, Root, Leaf&gt; graph;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@JsonCreator</span></span><br><span class="line">  <span class="keyword">public</span> PhysicalPlan(<span class="annotation">@JsonProperty</span>(<span class="string">"head"</span>) PlanProperties properties, <span class="annotation">@JsonProperty</span>(<span class="string">"graph"</span>) List&lt;PhysicalOperator&gt; operators)&#123;</span><br><span class="line">    <span class="keyword">this</span>.properties = properties;</span><br><span class="line">    <span class="keyword">this</span>.graph = Graph.newGraph(operators, Root.<span class="keyword">class</span>, Leaf.<span class="keyword">class</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h2 id="查询过程分析">查询过程分析</h2><p>在官方的设计文档中<a href="http://drill.apache.org/docs/rpc-overview/" target="_blank" rel="external">http://drill.apache.org/docs/rpc-overview/</a>对RPC只是寥寥数语,还有待补充.  </p>
<h3 id="客户端提交查询">客户端提交查询</h3><p>我们根据上面的Query流程一步步分析, 首先是客户端提交一个查询, 经过Optiq生成逻辑计划后会交给DrillClient运行:  </p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * <span class="type">Submits</span> a <span class="type">Logical</span> plan <span class="keyword">for</span> direct execution (bypasses parsing) 提交一个逻辑计划,直接执行</span><br><span class="line"> * @param plan the plan to execute</span><br><span class="line"> * @<span class="keyword">return</span> a handle <span class="keyword">for</span> the query <span class="literal">result</span></span><br><span class="line"> */</span><br><span class="line">public <span class="type">List</span>&lt;<span class="type">QueryDataBatch</span>&gt; runQuery(<span class="type">QueryType</span> <span class="keyword">type</span>, <span class="type">String</span> plan) throws <span class="type">RpcException</span> &#123;</span><br><span class="line">  <span class="type">UserProtos</span>.<span class="type">RunQuery</span> query = newBuilder().setResultsMode(<span class="type">STREAM_FULL</span>).setType(<span class="keyword">type</span>).setPlan(plan).build();</span><br><span class="line">  <span class="type">ListHoldingResultsListener</span> listener = new <span class="type">ListHoldingResultsListener</span>(query);</span><br><span class="line">  client.submitQuery(listener, query);  //这个client是<span class="type">UserClient</span>,而不是<span class="type">DrillClient</span>了</span><br><span class="line">  <span class="keyword">return</span> listener.getResults();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>逻辑计划封装成RunQuery协议,监听器ListHoldingResultsListener用于当获取到结果后通知客户端可以获取数据了.<br>监听器用Future来封装查询的结果集合,如果结果还没有出来,调用future.get()会被阻塞,这是Java的Future语法本身的特性.  </p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">private class <span class="type">ListHoldingResultsListener</span> implements <span class="type">UserResultsListener</span> &#123;</span><br><span class="line">  private <span class="type">Vector</span>&lt;<span class="type">QueryDataBatch</span>&gt; results = new <span class="type">Vector</span>&lt;&gt;();</span><br><span class="line">  private <span class="type">SettableFuture</span>&lt;<span class="type">List</span>&lt;<span class="type">QueryDataBatch</span>&gt;&gt; future = <span class="type">SettableFuture</span>.create();</span><br><span class="line">  private <span class="type">UserProtos</span>.<span class="type">RunQuery</span> query ;  //提交失败时, 在重新连接后, 由客户端重新连接</span><br><span class="line"></span><br><span class="line">  public <span class="type">void</span> queryCompleted(<span class="type">QueryState</span> state) &#123;</span><br><span class="line">    future.<span class="type">set</span>(results);</span><br><span class="line">  &#125;</span><br><span class="line">  public <span class="type">void</span> dataArrived(<span class="type">QueryDataBatch</span> <span class="literal">result</span>, <span class="type">ConnectionThrottle</span> throttle) &#123;</span><br><span class="line">    results.add(<span class="literal">result</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  public <span class="type">List</span>&lt;<span class="type">QueryDataBatch</span>&gt; getResults() throws <span class="type">RpcException</span>&#123;</span><br><span class="line">      <span class="keyword">return</span> future.get();</span><br><span class="line">  &#125;   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>UserClient作为用户的客户端,会向DrillBit发送一个RUN_QUERY的请求, 发送内容在RunQuery query对象里.  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">UserClient</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">BasicClientWithConnection&lt;RpcType</span>, <span class="title">UserToBitHandshake</span>, <span class="title">BitToUserHandshake&gt;</span> &#123;</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">QueryResultHandler</span> queryResultHandler = <span class="keyword">new</span> <span class="type">QueryResultHandler</span>();</span><br><span class="line"></span><br><span class="line">  public void submitQuery(<span class="type">UserResultsListener</span> resultsListener, <span class="type">RunQuery</span> query) &#123;</span><br><span class="line">    send(queryResultHandler.getWrappedListener(connection, resultsListener), <span class="type">RpcType</span>.<span class="type">RUN_QUERY</span>, query, <span class="type">QueryId</span>.<span class="keyword">class</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>connection对象是客户端建立的到服务器端的连接, 在UserClient的父类BasicClient的构造方法中创建的.<br>这里用的是Netty, 客户端在创建过程还绑定了多个Handler: 协议的编解码,消息的编解码,<code>InboundHandler到达处理器</code>,客户端握手Handler.   </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">BasicClient&lt;T</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">EnumLite</span>, <span class="title">R</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">RemoteConnection</span>, <span class="title">HANDSHAKE_SEND</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">MessageLite</span>, <span class="title">HANDSHAKE_RESPONSE</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">MessageLite&gt;</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">RpcBus&lt;T</span>, <span class="title">R&gt;</span> &#123;</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Bootstrap</span> b;</span><br><span class="line">  <span class="keyword">protected</span> <span class="type">R</span> connection;</span><br><span class="line">  public <span class="type">BasicClient</span>(...) &#123;  </span><br><span class="line">    b = <span class="keyword">new</span> <span class="type">Bootstrap</span>()  </span><br><span class="line">        .handler(<span class="keyword">new</span> <span class="type">ChannelInitializer</span>&lt;<span class="type">SocketChannel</span>&gt;() &#123;</span><br><span class="line">          <span class="keyword">protected</span> void initChannel(<span class="type">SocketChannel</span> ch) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">            connection = initRemoteConnection(ch);</span><br><span class="line">            ch.closeFuture().addListener(getCloseHandler(ch, connection));</span><br><span class="line">            <span class="keyword">final</span> <span class="type">ChannelPipeline</span> pipe = ch.pipeline();</span><br><span class="line">            pipe.addLast(<span class="string">"protocol-decoder"</span>, getDecoder(connection.getAllocator()));</span><br><span class="line">            pipe.addLast(<span class="string">"message-decoder"</span>, <span class="keyword">new</span> <span class="type">RpcDecoder</span>(<span class="string">"c-"</span> + rpcConfig.getName()));</span><br><span class="line">            pipe.addLast(<span class="string">"protocol-encoder"</span>, <span class="keyword">new</span> <span class="type">RpcEncoder</span>(<span class="string">"c-"</span> + rpcConfig.getName()));</span><br><span class="line">            pipe.addLast(<span class="string">"handshake-handler"</span>, <span class="keyword">new</span> <span class="type">ClientHandshakeHandler</span>());</span><br><span class="line">            pipe.addLast(<span class="string">"message-handler"</span>, <span class="keyword">new</span> <span class="type">InboundHandler</span>(connection));</span><br><span class="line">            pipe.addLast(<span class="string">"exception-handler"</span>, <span class="keyword">new</span> <span class="type">RpcExceptionHandler</span>(connection));</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;);    </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>send()调用最终会调用RpcBus的同名send方法, 第一个参数listener是Rpc的输出监听器(相对Income到达)<br>其中发送内容RunQuery query是protobufBody, 最后一个参数dataBodies是可选的.   </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The Rpc Bus deals with incoming and outgoing communication and is used on both the server and the client side of a system.</span></span><br><span class="line">public <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RpcBus&lt;T</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">EnumLite</span>, <span class="title">C</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">RemoteConnection&gt;</span> <span class="title">implements</span> <span class="title">Closeable</span> &#123;</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">final</span> <span class="type">CoordinationQueue</span> queue = <span class="keyword">new</span> <span class="type">CoordinationQueue</span>(<span class="number">16</span>, <span class="number">16</span>);</span><br><span class="line"></span><br><span class="line">  public &lt;<span class="type">SEND</span> <span class="keyword">extends</span> <span class="type">MessageLite</span>, <span class="type">RECEIVE</span> <span class="keyword">extends</span> <span class="type">MessageLite</span>&gt; void send(<span class="type">RpcOutcomeListener</span>&lt;<span class="type">RECEIVE</span>&gt; listener, <span class="type">C</span> connection, <span class="type">T</span> rpcType,</span><br><span class="line">      <span class="type">SEND</span> protobufBody, <span class="type">Class</span>&lt;<span class="type">RECEIVE</span>&gt; clazz, boolean allowInEventLoop, <span class="type">ByteBuf</span>... dataBodies) &#123;</span><br><span class="line">      <span class="type">ChannelListenerWithCoordinationId</span> futureListener = queue.get(listener, clazz, connection);</span><br><span class="line">      <span class="type">OutboundRpcMessage</span> m = <span class="keyword">new</span> <span class="type">OutboundRpcMessage</span>(<span class="type">RpcMode</span>.<span class="type">REQUEST</span>, rpcType, futureListener.getCoordinationId(), protobufBody, dataBodies);</span><br><span class="line">      <span class="type">ChannelFuture</span> channelFuture = connection.getChannel().writeAndFlush(m);</span><br><span class="line">      channelFuture.addListener(futureListener);</span><br><span class="line">      channelFuture.addListener(<span class="type">ChannelFutureListener</span>.<span class="type">FIRE_EXCEPTION_ON_FAILURE</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>客户端发起的一次查询是怎么提交到服务端上执行: <code>connection.getChannel().writeAndFlush(m)</code><br>客户端通过connection的通道往服务端写入一个Rpc消息,  Rpc消息分为到达Inboud和输出Outbound.<br>OutboundRpcMessage含有protobuf协议体,以及数据部分. 协议本身只是定义了数据的格式.  真正的数据也要一起发送出去.<br>虽然这里是客户端的查询请求, dataBodies本身是没有值的,因为在一开始调用的时候就没有传入这个参数.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill10.png" alt=""></p>
<p>到此为止, 客户端发起的一次查询请求已经完成了, 接下去的流程交给服务端处理这个请求了.<br>这里用到一个futureListener, 是为了能够监听服务器端是否已经把数据准备好了.<br>这里的queue会将CoordinationId和RpcListener放到map中, 并在接收到数据后从map中移除.  </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;V&gt; <span class="function">ChannelListenerWithCoordinationId <span class="title">get</span><span class="params">(RpcOutcomeListener&lt;V&gt; <span class="keyword">handler</span>, Class&lt;V&gt; clazz, RemoteConnection connection)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i = circularInt.getNext();</span><br><span class="line">  RpcListener&lt;V&gt; future = <span class="keyword">new</span> RpcListener&lt;V&gt;(<span class="keyword">handler</span>, clazz, i, connection);</span><br><span class="line">  Object old = map.put(i, future);</span><br><span class="line">  <span class="keyword">return</span> future;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="服务端处理Query请求">服务端处理Query请求</h3><p>服务端的启动方式和客户端一样都是通过Netty. 并且都注册了一个InboundHandler.<br>因为在上一步客户端发送REQUET请求, 所以服务器的InboundHandler能够接收到这个请求</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="class"><span class="keyword">class</span> <span class="title">InboundHandler</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">MessageToMessageDecoder&lt;InboundRpcMessage&gt;</span> &#123;</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">C</span> connection;</span><br><span class="line">  public <span class="type">InboundHandler</span>(<span class="type">C</span> connection) &#123;</span><br><span class="line">    <span class="keyword">this</span>.connection = connection;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> void decode(<span class="keyword">final</span> <span class="type">ChannelHandlerContext</span> ctx, <span class="keyword">final</span> <span class="type">InboundRpcMessage</span> msg, <span class="keyword">final</span> <span class="type">List</span>&lt;<span class="type">Object</span>&gt; output) <span class="keyword">throws</span> <span class="type">Exception</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Channel</span> channel = connection.getChannel();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Stopwatch</span> watch = <span class="keyword">new</span> <span class="type">Stopwatch</span>().start();</span><br><span class="line"></span><br><span class="line">      switch (msg.mode) &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">REQUEST</span>: &#123;</span><br><span class="line">          <span class="comment">// handle message and ack.</span></span><br><span class="line">          <span class="type">ResponseSender</span> sender = <span class="keyword">new</span> <span class="type">ResponseSenderImpl</span>(connection, msg.coordinationId);</span><br><span class="line">          handle(connection, msg.rpcType, msg.pBody, msg.dBody, sender);</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>
<p>这里InboundHandler继承的是Netty的MessageToMessageDecoder抽象类,所以要在decode中重写接收的逻辑<br>如果是继承Netty的ChannelInboundHandlerAdapter, 则重写的方法是channelRead, 后面这种在netty的helloworld中比较常见.<br>为什么需要ResponseSender, 因为服务端接收客户端的查询请求, 在获取到结果后, 要负责将结果response发送send给客户端才算完成.  </p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">void</span> <span class="title">handle</span><span class="params">(C connection, <span class="keyword">int</span> rpcType, ByteBuf pBody, ByteBuf dBody, ResponseSender sender)</span> <span class="keyword">throws</span> RpcException</span>&#123;</span><br><span class="line">  sender.send(handle(connection, rpcType, pBody, dBody));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="function">Response <span class="title">handle</span><span class="params">(C connection, <span class="keyword">int</span> rpcType, ByteBuf pBody, ByteBuf dBody)</span> <span class="keyword">throws</span> RpcException</span>;</span><br></pre></td></tr></table></figure>
<p>handle是个抽象方法,  这里因为在Server中了, 所以选择UserServer的实现方法. 客户端传入的rpcType=RUN_QUERY等于下面的RUN_QUERY_VALUE</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill11.png" alt=""></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">UserServer</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">BasicServer&lt;RpcType</span>, <span class="title">UserServer</span>.<span class="title">UserClientConnection&gt;</span> &#123;</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">UserWorker</span> worker;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="type">Response</span> handle(<span class="type">UserClientConnection</span> connection, int rpcType, <span class="type">ByteBuf</span> pBody, <span class="type">ByteBuf</span> dBody)&#123;</span><br><span class="line">    switch (rpcType) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">RpcType</span>.<span class="type">RUN_QUERY_VALUE</span>:</span><br><span class="line">        <span class="keyword">final</span> <span class="type">RunQuery</span> query = <span class="type">RunQuery</span>.<span class="type">PARSER</span>.parseFrom(<span class="keyword">new</span> <span class="type">ByteBufInputStream</span>(pBody));</span><br><span class="line">        <span class="keyword">final</span> <span class="type">QueryId</span> queryId = worker.submitWork(connection, query);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">Response</span>(<span class="type">RpcType</span>.<span class="type">QUERY_HANDLE</span>, queryId);</span><br></pre></td></tr></table></figure>
<p>DrillClient将查询交给UserClient, UserServer则将具体的查询工作交给了UserWorker, 它的返回值也是一个QueryId协议.<br>最终的返回值是Response, 对应了RpcBus的sender.send(handle(…)) –&gt; sender.send(Response)  </p>
<p>注意服务端接受到查询请求RUN_QUERY后, 交给worker处理, worker会立即返回这个查询对应的QueryId. 因此也不是立即返回结果的<br>看下服务端的ResponseSender的实现类, 定义在RpcBus中.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">ResponseSenderImpl</span> <span class="keyword">implements</span> <span class="title">ResponseSender</span> </span>&#123;</span><br><span class="line">  RemoteConnection connection;</span><br><span class="line">  <span class="keyword">int</span> coordinationId;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">ResponseSenderImpl</span><span class="params">(RemoteConnection connection, <span class="keyword">int</span> coordinationId)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">super</span>();</span><br><span class="line">    <span class="keyword">this</span>.connection = connection;</span><br><span class="line">    <span class="keyword">this</span>.coordinationId = coordinationId;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">(Response r)</span> </span>&#123;</span><br><span class="line">    OutboundRpcMessage outMessage = <span class="keyword">new</span> OutboundRpcMessage(RpcMode.RESPONSE, r.rpcType, coordinationId, r.pBody, r.dBodies);</span><br><span class="line">    connection.getChannel().writeAndFlush(outMessage);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意这里的coordinationId是客户端放入queue队列中futureListener的一个编号.  而服务端返回的QueryId是r.pBody.<br>服务端也将OutboundRpcMessage通过connection通道写回去. 即写到客户端去, 因为服务端并没有将查询结果立即计算出来,<br>所以需要将QueryId返回给客户端, 并在适当的时候如果服务端获取到结果会通知客户端.  </p>
<p>服务端发送的是Response, 所以现在服务端的流程已经走完了(虽然worker还没有完成,但是对于一次RPC来说是完成了), 轮到客户端接收响应.  </p>
<h3 id="客户端获取查询结果">客户端获取查询结果</h3><p>还是在RpcBus的InboundHandler中.  只不过这次是客户端接受到服务端发送的响应请求:  </p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> RESPONSE:</span><br><span class="line">    MessageLite m = getResponseDefaultInstance(msg.rpcType);  <span class="comment">//这里是QueryId</span></span><br><span class="line">    RpcOutcome&lt;?&gt; rpcFuture = queue.getFuture(msg.rpcType, msg.coordinationId, m.getClass());  <span class="comment">//对应一开始的queue.get(...)</span></span><br><span class="line">    Parser&lt;?&gt; parser = m.getParserForType();</span><br><span class="line">    Object <span class="keyword">value</span> = parser.parseFrom(<span class="keyword">new</span> ByteBufInputStream(msg.pBody, msg.pBody.readableBytes()));  <span class="comment">//pBody只是协议格式</span></span><br><span class="line">    rpcFuture.<span class="keyword">set</span>(<span class="keyword">value</span>, msg.dBody);  <span class="comment">//dBody才是数据内容</span></span><br><span class="line">  <span class="keyword">break</span>;</span><br></pre></td></tr></table></figure>
<p>从队列中获取Future, 最后调用future的set方法, 将数据设置到value中, 这样future.get()就能获取到value.  </p>
<figure class="highlight zephir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> &lt;V&gt; RpcOutcome&lt;V&gt; getFuture(<span class="keyword">int</span> rpcType, <span class="keyword">int</span> coordinationId, <span class="class"><span class="keyword">Class</span>&lt;<span class="title">V</span>&gt; <span class="title">clazz</span>) </span>&#123;</span><br><span class="line">  RpcOutcome&lt;?&gt; rpc = map.remove(coordinationId);</span><br><span class="line">  <span class="keyword">return</span> (RpcOutcome&lt;V&gt;) rpc;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Listener_&amp;_Handler">Listener &amp; Handler</h3><table>
<thead>
<tr>
<th>Step</th>
<th>Operation</th>
</tr>
</thead>
<tbody>
<tr>
<td>DrillClient.runQuery:</td>
<td>ListHoldingResultsListener</td>
</tr>
<tr>
<td>UserClient.submitQuery:</td>
<td>通过QueryResultHandler将ListHoldingResultsListener&gt;&gt;UserResultsListener封装为SubmissionListener&gt;&gt;RpcOutcomeListener</td>
</tr>
<tr>
<td>RpcBus.send:</td>
<td>将RpcOutcomeListener封装为ChannelListenerWithCoordinationId&lt;<rpclistener>&gt;RpcOutcome</rpclistener></td>
</tr>
<tr>
<td>InboundHandler.RESPONSE:</td>
<td>从队列中获取RpcOutcome rpcFuture, 设置数据到value</td>
</tr>
<tr>
<td>RpcListener.set:</td>
<td>调用RpcOutcomeListener.success, 即SubmissionListener.success: 添加UserResultsListener负责监听结果</td>
</tr>
<tr>
<td>ListHoldingResultsListener.getResults:</td>
<td>从Future中获取结果</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Object</th>
<th>Why</th>
</tr>
</thead>
<tbody>
<tr>
<td>ListHoldingResultsListener</td>
<td>持有结果集的监听器, 所以负责最终结果的获取,当然获取操作不在这里, 它只管取数据</td>
</tr>
<tr>
<td>SubmissionListener</td>
<td>提交监听器, 作业提交后, 我负责执行, 但是执行的动作并不在这里哦</td>
</tr>
<tr>
<td>UserResultsListener</td>
<td>用户的结果集监听器, 这里应该是负责结果的产生, 不过它是个接口</td>
</tr>
<tr>
<td>RpcListener</td>
<td>RPC监听器,要和具体本次查询的coordinationId关联起来</td>
</tr>
</tbody>
</table>
<p>用户的结果集监听器的方法表示了查询的一个过程:<br>1.QueryId到达, 由服务端产生QueryId<br>2.数据到达, 并被监听器成功接收到<br>3.查询完毕, 监听器不会再收到任何数据</p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public <span class="keyword">interface</span> <span class="type">UserResultsListener</span> &#123;</span><br><span class="line">  /**</span><br><span class="line">   * <span class="type">QueryId</span> <span class="keyword">is</span> available. <span class="type">Called</span> <span class="keyword">when</span> a query <span class="keyword">is</span> successfully submitted to the server.</span><br><span class="line">   * @param queryId sent by the server along &#123;@link org.apache.drill.exec.rpc.<span class="type">Acks</span>.<span class="type">OK</span> <span class="type">Acks</span>.<span class="type">OK</span>&#125;</span><br><span class="line">   */</span><br><span class="line">  <span class="type">void</span> queryIdArrived(<span class="type">QueryId</span> queryId);</span><br><span class="line"></span><br><span class="line">  <span class="type">void</span> submissionFailed(<span class="type">UserException</span> ex);</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * A &#123;@link org.apache.drill.exec.proto.beans.<span class="type">QueryData</span> <span class="type">QueryData</span>&#125; message was received</span><br><span class="line">   * @param <span class="literal">result</span> data batch received</span><br><span class="line">   */</span><br><span class="line">  <span class="type">void</span> dataArrived(<span class="type">QueryDataBatch</span> <span class="literal">result</span>, <span class="type">ConnectionThrottle</span> throttle);</span><br><span class="line"></span><br><span class="line">  /**</span><br><span class="line">   * <span class="type">The</span> query has completed (successsful completion <span class="keyword">or</span> cancellation). <span class="type">The</span> listener will <span class="keyword">not</span> receive <span class="type">any</span> other</span><br><span class="line">   * data <span class="keyword">or</span> <span class="literal">result</span> message. <span class="type">Called</span> <span class="keyword">when</span> the server returns a terminal-non failing- state (<span class="type">COMPLETED</span> <span class="keyword">or</span> <span class="type">CANCELLED</span>)</span><br><span class="line">   */</span><br><span class="line">  <span class="type">void</span> queryCompleted(<span class="type">QueryState</span> state);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Source/">Source</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/drill/">drill</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-07-10-drill-log" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/10/2015-07-10-drill-log/" class="article-date">
  	<time datetime="2015-07-09T16:00:00.000Z" itemprop="datePublished">2015-07-10</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/10/2015-07-10-drill-log/">Apache Drill源码阅读(1) 环境准备和查看日志</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="准备工作">准备工作</h2><p>修改logback.xml的日志级别为debug</p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;logger name=<span class="string">"org.apache.drill"</span> additivity=<span class="string">"false"</span>&gt;</span><br><span class="line">  &lt;level value=<span class="string">"debug"</span> /&gt;</span><br><span class="line">  &lt;appender-<span class="keyword">ref</span> <span class="keyword">ref</span>=<span class="string">"FILE"</span> /&gt;</span><br><span class="line">&lt;/logger&gt;</span><br><span class="line"></span><br><span class="line">&lt;logger name=<span class="string">"query.logger"</span> additivity=<span class="string">"false"</span>&gt;</span><br><span class="line">  &lt;level value=<span class="string">"debug"</span> /&gt;</span><br><span class="line">  &lt;appender-<span class="keyword">ref</span> <span class="keyword">ref</span>=<span class="string">"QUERY"</span> /&gt;</span><br><span class="line">&lt;/logger&gt;</span><br><span class="line"></span><br><span class="line">&lt;root&gt;</span><br><span class="line">  &lt;level value=<span class="string">"debug"</span> /&gt;</span><br><span class="line">  &lt;appender-<span class="keyword">ref</span> <span class="keyword">ref</span>=<span class="string">"STDOUT"</span> /&gt;</span><br><span class="line">&lt;/root&gt;</span><br></pre></td></tr></table></figure>
<p>使用单机模式,而不是集群模式. 启动<code>drill-embedded</code>  </p>
<p>除了在上面的drill-embedded观察输出, 还要观察log目录下的sqlline.log文件</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">41</span>,<span class="number">636</span> [main] DEBUG o<span class="class">.apache</span><span class="class">.drill</span><span class="class">.exec</span><span class="class">.server</span><span class="class">.Drillbit</span> - Construction started.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">42</span>,<span class="number">481</span> [main] INFO  o<span class="class">.apache</span><span class="class">.drill</span><span class="class">.exec</span><span class="class">.server</span><span class="class">.Drillbit</span> - Construction completed (<span class="number">845</span> ms).</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">42</span>,<span class="number">481</span> [main] DEBUG o<span class="class">.apache</span><span class="class">.drill</span><span class="class">.exec</span><span class="class">.server</span><span class="class">.Drillbit</span> - Startup begun.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">42</span>,<span class="number">481</span> [main] DEBUG o<span class="class">.a</span><span class="class">.d</span><span class="class">.e</span><span class="class">.c</span><span class="class">.l</span><span class="class">.LocalClusterCoordinator</span> - Local Cluster Coordinator started.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">42</span>,<span class="number">607</span> [main] DEBUG o<span class="class">.a</span><span class="class">.drill</span><span class="class">.exec</span><span class="class">.rpc</span><span class="class">.user</span><span class="class">.UserServer</span> - Server of type UserServer started on port <span class="number">31010</span>.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">42</span>,<span class="number">650</span> [main] DEBUG o<span class="class">.a</span><span class="class">.d</span><span class="class">.exec</span><span class="class">.rpc</span><span class="class">.control</span><span class="class">.ControlServer</span> - Server of type ControlServer started on port <span class="number">31011</span>.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">42</span>,<span class="number">688</span> [main] DEBUG o<span class="class">.a</span><span class="class">.drill</span><span class="class">.exec</span><span class="class">.rpc</span><span class="class">.data</span><span class="class">.DataServer</span> - Server of type DataServer started on port <span class="number">31012</span>.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">42</span>,<span class="number">924</span> [main] DEBUG o<span class="class">.a</span><span class="class">.drill</span><span class="class">.common</span><span class="class">.util</span><span class="class">.PathScanner</span> - Classpath scanning took <span class="number">60ms</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">42</span>,<span class="number">924</span> [main] DEBUG o<span class="class">.a</span><span class="class">.d</span><span class="class">.e</span><span class="class">.p</span><span class="class">.base</span><span class="class">.PhysicalOperatorUtil</span> - Adding Physical Operator sub types: .................</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">43</span>,<span class="number">047</span> [main] DEBUG org<span class="class">.apache</span><span class="class">.drill</span><span class="class">.common</span><span class="class">.JSONOptions</span> - Creating Deserializer.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">43</span>,<span class="number">146</span> [main] DEBUG o<span class="class">.a</span><span class="class">.d</span><span class="class">.e</span><span class="class">.p</span><span class="class">.i</span><span class="class">.OperatorCreatorRegistry</span> - Adding Operator Creator map:..............</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">43</span>,<span class="number">385</span> [main] DEBUG o<span class="class">.a</span><span class="class">.d</span><span class="class">.e</span><span class="class">.e</span><span class="class">.f</span><span class="class">.FunctionImplementationRegistry</span> - Generating function registry.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">48</span>,<span class="number">643</span> [main] DEBUG o<span class="class">.a</span><span class="class">.d</span><span class="class">.e</span><span class="class">.s</span><span class="class">.h</span><span class="class">.HBaseStoragePluginConfig</span> - Initializing HBase StoragePlugin configuration with zookeeper quorum <span class="string">'localhost'</span>, port <span class="string">'2181'</span>.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">48</span>,<span class="number">977</span> [main] DEBUG o<span class="class">.a</span><span class="class">.d</span><span class="class">.e</span><span class="class">.c</span><span class="class">.l</span><span class="class">.LocalClusterCoordinator</span> - Endpoint registered <span class="tag">address</span>: <span class="string">"localhost"</span></span><br><span class="line">user_port: <span class="number">31010</span></span><br><span class="line">control_port: <span class="number">31011</span></span><br><span class="line">data_port: <span class="number">31012</span></span><br><span class="line">.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">51</span>,<span class="number">700</span> [main] INFO  o<span class="class">.apache</span><span class="class">.drill</span><span class="class">.exec</span><span class="class">.server</span><span class="class">.Drillbit</span> - Startup completed (<span class="number">9218</span> ms).</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">51</span>,<span class="number">741</span> [main] DEBUG o<span class="class">.a</span><span class="class">.drill</span><span class="class">.exec</span><span class="class">.client</span><span class="class">.DrillClient</span> - Connecting to server localhost:<span class="number">31010</span></span><br></pre></td></tr></table></figure>
<p>可以看到命令行执行drill-embedded, 会连接到本地的Drill Server上.</p>
<p>在sqlline上执行一条SQL命令, 可以看到最终调用的是FragmentExecutor线程的run方法:</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span>: <span class="string">jdbc:</span><span class="string">drill:</span>zk=local&gt; select count(*) from cp.`employee.json`;</span><br><span class="line"><span class="number">11</span>:<span class="number">22</span>:<span class="number">02.300</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-<span class="string">a01023a1319a:</span>foreman] DEBUG o.a.h.security.UserGroupInformation - PrivilegedAction <span class="string">as:</span>zhengqh (<span class="string">auth:</span>SIMPLE) <span class="string">from:</span>org.apache.drill.exec.util.ImpersonationUtil.createFileSystem(ImpersonationUtil.<span class="string">java:</span><span class="number">141</span>)</span><br><span class="line"><span class="number">11</span>:<span class="number">22</span>:<span class="number">02.390</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-<span class="string">a01023a1319a:</span><span class="string">frag:</span><span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.h.security.UserGroupInformation - PrivilegedAction <span class="string">as:</span>zhengqh (<span class="string">auth:</span>SIMPLE) <span class="string">from:</span>org.apache.drill.exec.work.fragment.FragmentExecutor.run(FragmentExecutor.<span class="string">java:</span><span class="number">255</span>)</span><br><span class="line">+---------+</span><br><span class="line">| EXPR$<span class="number">0</span>  |</span><br><span class="line">+---------+</span><br><span class="line">| <span class="number">1155</span>    |</span><br><span class="line">+---------+</span><br><span class="line"><span class="number">1</span> row selected (<span class="number">0.221</span> seconds)</span><br></pre></td></tr></table></figure>
<h2 id="日志分析">日志分析</h2><p>sqlline.log日志我们一段一段地分析</p>
<p>首先注册查询语句, 即在sqlline输入的sql语句, 会分配一个query-id. 访问<a href="http://localhost:8047/profiles" target="_blank" rel="external">http://localhost:8047/profiles</a>可以找到这个Query Job.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">257</span> [main] DEBUG o.a.d.j.impl.DrillStatementRegistry - Adding to open-statements registry: org.apache.drill.jdbc.impl.DrillStatementImpl@<span class="number">4</span>eb2bb3d</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">258</span> [main] DEBUG o.a.d.j.i.DrillResultSetImpl$ResultsListener - [<span class="preprocessor">#<span class="number">3</span>] Query listener created.</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">259</span> [UserServer-<span class="number">1</span>] DEBUG o.a.drill.exec.rpc.user.UserServer - Received query to run.  Returning query handle.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">273</span> [UserServer-<span class="number">1</span>] DEBUG o.a.d.exec.memory.TopLevelAllocator - New child allocator with initial reservation <span class="number">1048576</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">274</span> [UserServer-<span class="number">1</span>] DEBUG o.a.drill.exec.rpc.user.UserServer - Sending response with Sender <span class="number">575856913</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">275</span> [Client-<span class="number">1</span>] DEBUG o.a.d.j.i.DrillResultSetImpl$ResultsListener - [<span class="preprocessor">#<span class="number">3</span>] Received query ID: <span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a.</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">276</span> [Client-<span class="number">1</span>] DEBUG o.a.d.e.rpc.user.QueryResultHandler - Received QueryId <span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a successfully. Adding results listener org.apache.drill.jdbc.impl.DrillResultSetImpl$ResultsListener@<span class="number">6f</span>3634b1.</span><br></pre></td></tr></table></figure>
<p>使用了Optiq生成Logical逻辑计划, 查看SQL语句对应的逻辑计划/物理计划是从下到上的, 比如下面的TableScan-&gt;Project-&gt;Aggregate<br>对于<code>SELECT COUNT(COLUMN) FOME TABLE</code>, 实际上逻辑计划的顺序是:<br>FROM TABLE(TableScan扫描) -&gt; SELECT COLUMN(Project映射) -&gt; COUNT(Aggregate聚合计算)  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">315</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:foreman] DEBUG o.a.d.e.p.s.h.DefaultSqlHandler - Optiq Logical :</span><br><span class="line">LogicalAggregate(group=[&#123;&#125;], EXPR$<span class="number">0</span>=[COUNT()]): rowcount = <span class="number">10.0</span>, cumulative cost = &#123;<span class="number">211.25</span> rows, <span class="number">201.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">211</span></span><br><span class="line">  LogicalProject($f0=[<span class="number">0</span>]): rowcount = <span class="number">100.0</span>, cumulative cost = &#123;<span class="number">200.0</span> rows, <span class="number">201.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">209</span></span><br><span class="line">    EnumerableTableScan(table=[[cp, employee.json]]): rowcount = <span class="number">100.0</span>, cumulative cost = &#123;<span class="number">100.0</span> rows, <span class="number">101.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> memory&#125;, id = <span class="number">205</span></span><br></pre></td></tr></table></figure>
<p>Optiq的逻辑计划最终会形成Drill的逻辑计划,再到Drill的物理计划: Drill Logial -&gt; Drill Physical的过程</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">2015-07-10 11:22:02,320 [2a60c5a4-e01a-ac02-84fd-a01023a1319a:foreman] DEBUG o.a.d.e.s.schedule.BlockMapBuilder - Took 0 ms to build endpoint map</span><br><span class="line">2015-07-10 11:22:02,323 [2a60c5a4-e01a-ac02-84fd-a01023a1319a:foreman] DEBUG o.a.d.e.s.schedule.BlockMapBuilder - FileWork group (classpath:/employee.json,0) max bytes 474631</span><br><span class="line">2015-07-10 11:22:02,323 [2a60c5a4-e01a-ac02-84fd-a01023a1319a:foreman] DEBUG o.a.d.e.s.schedule.BlockMapBuilder - Took 2 ms to <span class="operator"><span class="keyword">set</span> endpoint <span class="keyword">bytes</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">323</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-a01023a1319a:foreman] INFO  o.a.<span class="keyword">d</span>.<span class="keyword">e</span>.s.schedule.BlockMapBuilder - <span class="keyword">Get</span> <span class="keyword">block</span> maps: Executed <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">1</span> <span class="keyword">using</span> <span class="number">1</span> threads. <span class="keyword">Time</span>: <span class="number">2</span>ms total, <span class="number">2.240000</span>ms <span class="keyword">avg</span>, <span class="number">2</span>ms <span class="keyword">max</span>.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">323</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-a01023a1319a:foreman] INFO  o.a.<span class="keyword">d</span>.<span class="keyword">e</span>.s.schedule.BlockMapBuilder - <span class="keyword">Get</span> <span class="keyword">block</span> maps: Executed <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">1</span> <span class="keyword">using</span> <span class="number">1</span> threads. Earliest <span class="keyword">start</span>: <span class="number">1.000000</span> μs, Latest <span class="keyword">start</span>: <span class="number">1.000000</span> μs, Average <span class="keyword">start</span>: <span class="number">1.000000</span> μs .</span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">324</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-a01023a1319a:foreman] DEBUG o.a.<span class="keyword">d</span>.<span class="keyword">e</span>.s.schedule.AffinityCreator - <span class="keyword">Work</span>: [<span class="keyword">File</span>: classpath:/employee.<span class="keyword">json</span> <span class="keyword">start</span>: <span class="number">0</span> <span class="keyword">length</span>: <span class="number">474630</span>] Endpoint: localhost <span class="keyword">Bytes</span>: <span class="number">474630</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">324</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-a01023a1319a:foreman] DEBUG o.a.<span class="keyword">d</span>.<span class="keyword">e</span>.s.schedule.AffinityCreator - Endpoint localhost has affinity <span class="number">1.0</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">324</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-a01023a1319a:foreman] DEBUG o.a.<span class="keyword">d</span>.<span class="keyword">e</span>.s.schedule.AffinityCreator - Took <span class="number">0</span> ms <span class="keyword">to</span> <span class="keyword">get</span> <span class="keyword">operator</span> affinity</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">329</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-a01023a1319a:foreman] DEBUG o.a.<span class="keyword">d</span>.<span class="keyword">e</span>.<span class="keyword">p</span>.s.h.DefaultSqlHandler - VolCalciteRel :</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">331</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-a01023a1319a:foreman] DEBUG o.a.<span class="keyword">d</span>.<span class="keyword">e</span>.<span class="keyword">p</span>.s.h.DefaultSqlHandler - HepCalciteRel :</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">333</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-a01023a1319a:foreman] DEBUG o.a.<span class="keyword">d</span>.<span class="keyword">e</span>.<span class="keyword">p</span>.s.h.DefaultSqlHandler - Drill <span class="keyword">Logical</span> :</span><br><span class="line">DrillScreenRel: rowcount = <span class="number">1.0</span>, cumulative <span class="keyword">cost</span> = &#123;<span class="number">927.1</span> <span class="keyword">rows</span>, <span class="number">1853.1</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> <span class="keyword">memory</span>&#125;, <span class="keyword">id</span> = <span class="number">239</span></span><br><span class="line">  DrillAggregateRel(<span class="keyword">group</span>=[&#123;&#125;], EXPR$<span class="number">0</span>=[<span class="keyword">COUNT</span>()]): rowcount = <span class="number">1.0</span>, cumulative <span class="keyword">cost</span> = &#123;<span class="number">927.0</span> <span class="keyword">rows</span>, <span class="number">1853.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> <span class="keyword">memory</span>&#125;, <span class="keyword">id</span> = <span class="number">236</span></span><br><span class="line">    DrillProjectRel($f0=[<span class="number">0</span>]): rowcount = <span class="number">463.0</span>, cumulative <span class="keyword">cost</span> = &#123;<span class="number">926.0</span> <span class="keyword">rows</span>, <span class="number">1852.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> <span class="keyword">memory</span>&#125;, <span class="keyword">id</span> = <span class="number">234</span></span><br><span class="line">      DrillScanRel(<span class="keyword">table</span>=[[cp, employee.<span class="keyword">json</span>]], groupscan=[EasyGroupScan [selectionRoot=classpath:/employee.<span class="keyword">json</span>, numFiles=<span class="number">1</span>, <span class="keyword">columns</span>=[<span class="string">`*`</span>], files=[classpath:/employee.<span class="keyword">json</span>]]]): rowcount = <span class="number">463.0</span>, cumulative <span class="keyword">cost</span> = &#123;<span class="number">463.0</span> <span class="keyword">rows</span>, <span class="number">0.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> <span class="keyword">memory</span>&#125;, <span class="keyword">id</span> = <span class="number">222</span></span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">368</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-a01023a1319a:foreman] DEBUG o.a.<span class="keyword">d</span>.<span class="keyword">e</span>.<span class="keyword">p</span>.s.h.DefaultSqlHandler - Drill <span class="keyword">Physical</span> :</span><br><span class="line"><span class="number">00</span>-<span class="number">00</span>    Screen : rowType = RecordType(<span class="built_in">BIGINT</span> EXPR$<span class="number">0</span>): rowcount = <span class="number">1.0</span>, cumulative <span class="keyword">cost</span> = &#123;<span class="number">1389.1</span> <span class="keyword">rows</span>, <span class="number">7408.1</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> <span class="keyword">memory</span>&#125;, <span class="keyword">id</span> = <span class="number">317</span></span><br><span class="line"><span class="number">00</span>-<span class="number">01</span>      StreamAgg(<span class="keyword">group</span>=[&#123;&#125;], EXPR$<span class="number">0</span>=[<span class="keyword">COUNT</span>()]) : rowType = RecordType(<span class="built_in">BIGINT</span> EXPR$<span class="number">0</span>): rowcount = <span class="number">1.0</span>, cumulative <span class="keyword">cost</span> = &#123;<span class="number">1389.0</span> <span class="keyword">rows</span>, <span class="number">7408.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> <span class="keyword">memory</span>&#125;, <span class="keyword">id</span> = <span class="number">316</span></span><br><span class="line"><span class="number">00</span>-<span class="number">02</span>        <span class="keyword">Project</span>($f0=[<span class="number">0</span>]) : rowType = RecordType(<span class="built_in">INTEGER</span> $f0): rowcount = <span class="number">463.0</span>, cumulative <span class="keyword">cost</span> = &#123;<span class="number">926.0</span> <span class="keyword">rows</span>, <span class="number">1852.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> <span class="keyword">memory</span>&#125;, <span class="keyword">id</span> = <span class="number">315</span></span><br><span class="line"><span class="number">00</span>-<span class="number">03</span>          <span class="keyword">Scan</span>(groupscan=[EasyGroupScan [selectionRoot=classpath:/employee.<span class="keyword">json</span>, numFiles=<span class="number">1</span>, <span class="keyword">columns</span>=[<span class="string">`*`</span>], files=[classpath:/employee.<span class="keyword">json</span>]]]) : rowType = RecordType(): rowcount = <span class="number">463.0</span>, cumulative <span class="keyword">cost</span> = &#123;<span class="number">463.0</span> <span class="keyword">rows</span>, <span class="number">0.0</span> cpu, <span class="number">0.0</span> io, <span class="number">0.0</span> network, <span class="number">0.0</span> <span class="keyword">memory</span>&#125;, <span class="keyword">id</span> = <span class="number">314</span></span></span><br></pre></td></tr></table></figure>
<p>访问<a href="http://localhost:8047/profiles/2a60c5a4-e01a-ac02-84fd-a01023a1319a" target="_blank" rel="external">http://localhost:8047/profiles/2a60c5a4-e01a-ac02-84fd-a01023a1319a</a>查看这个作业的物理计划<br>可以看到物理计划从下到上的顺序是: Scan-&gt;Project-&gt;StreamAgg. 因为物理计划实际上是从逻辑计划计算出来的.    </p>
<p>然后会输出详细的json格式的计划.  graph有几个字段pop代表操作类型,@id是编号,child是VisualizedPlan树从上到下第几层.<br>graph域的fs-scan代表扫描文件系统,扫描所有列*; project映射字段,$f0实际上就是columns中的第一个字段;<br>streaming-aggregate的计算表达式count(1), 最后通过screen输出    </p>
<p>我们先看一下Web页面的可视化计划数, 先来个比较直观的映象<br>以Scan 00-03为例,00是major id,03是fs-scan的@id=3</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill1.png" alt=""></p>
<p>下面不同的pop,对应的metadata也不一样, 比如project映射需要表达式,因为选择一个列,是可以在列上做计算的  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">370</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:foreman] DEBUG o.a.d.e.p.s.h.DefaultSqlHandler - Drill Plan :</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"head"</span> : &#123;</span><br><span class="line">    <span class="string">"version"</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="string">"generator"</span> : &#123;</span><br><span class="line">      <span class="string">"type"</span> : <span class="string">"DefaultSqlHandler"</span>,</span><br><span class="line">      <span class="string">"info"</span> : <span class="string">""</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"type"</span> : <span class="string">"APACHE_DRILL_PHYSICAL"</span>,</span><br><span class="line">    <span class="string">"options"</span> : [ ],</span><br><span class="line">    <span class="string">"queue"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="string">"resultMode"</span> : <span class="string">"EXEC"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"graph"</span> : [ &#123;</span><br><span class="line">    <span class="string">"pop"</span> : <span class="string">"fs-scan"</span>,</span><br><span class="line">    <span class="string">"@id"</span> : <span class="number">3</span>,</span><br><span class="line">    <span class="string">"userName"</span> : <span class="string">"zhengqh"</span>,</span><br><span class="line">    <span class="string">"files"</span> : [ <span class="string">"classpath:/employee.json"</span> ],</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">"columns"</span> : [ <span class="string">"`*`"</span> ],</span><br><span class="line">    <span class="string">"selectionRoot"</span> : <span class="string">"classpath:/employee.json"</span>,</span><br><span class="line">  &#125;, &#123;</span><br><span class="line">    <span class="string">"pop"</span> : <span class="string">"project"</span>,</span><br><span class="line">    <span class="string">"@id"</span> : <span class="number">2</span>,</span><br><span class="line">    <span class="string">"exprs"</span> : [ &#123;</span><br><span class="line">      <span class="string">"ref"</span> : <span class="string">"`$f0`"</span>,</span><br><span class="line">      <span class="string">"expr"</span> : <span class="string">"0"</span></span><br><span class="line">    &#125; ],</span><br><span class="line">    <span class="string">"child"</span> : <span class="number">3</span></span><br><span class="line">  &#125;, &#123;</span><br><span class="line">    <span class="string">"pop"</span> : <span class="string">"streaming-aggregate"</span>,</span><br><span class="line">    <span class="string">"@id"</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="string">"child"</span> : <span class="number">2</span>,</span><br><span class="line">    <span class="string">"keys"</span> : [ ],</span><br><span class="line">    <span class="string">"exprs"</span> : [ &#123;</span><br><span class="line">      <span class="string">"ref"</span> : <span class="string">"`EXPR$0`"</span>,</span><br><span class="line">      <span class="string">"expr"</span> : <span class="string">"count(1) "</span></span><br><span class="line">    &#125; ]</span><br><span class="line">  &#125;, &#123;</span><br><span class="line">    <span class="string">"pop"</span> : <span class="string">"screen"</span>,</span><br><span class="line">    <span class="string">"@id"</span> : <span class="number">0</span>,</span><br><span class="line">    <span class="string">"child"</span> : <span class="number">1</span></span><br><span class="line">  &#125; ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Drill的执行引擎会将逻辑计划组成一个Fragment树. 下面是Root Fragment.<br>对照WebUI, 这个Query在本地测试时,只生成了一个Fragment.<br>Root Fragment的major和minor id都是0.  下面的fragment_json和上面graph不同的是它是嵌套的.  </p>
<blockquote>
<p>为什么可视化的Plan对应的graph是扁平的,而Root Fragment是嵌套的?<br>可以这么理解: 如果图的结构是嵌套的,那么就要在Screen这个组件里画上streaming-aggregate<br>并在streaming-aggregate里再画上project,以此类推,就不叫图了,图是一个DAG有向无环图.<br>而树如果是扁平的,则只能像上面的图一样一直下去,没有分支. 使用嵌套,就有了分支的概念了.  </p>
</blockquote>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">372</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-a01023a1319a:foreman] DEBUG o.a.d.e.s.schedule.AssignmentCreator - Took <span class="number">0</span> ms <span class="keyword">to</span> assign <span class="number">1</span> work units <span class="keyword">to</span> <span class="number">1</span> fragments</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">378</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84</span>fd-a01023a1319a:foreman] DEBUG o.a.d.e.p.f.SimpleParallelizer - Root fragment:</span><br><span class="line"> handle &#123;</span><br><span class="line">  query_id &#123;</span><br><span class="line">    part1: <span class="number">3053657859282349058</span></span><br><span class="line">    part2: -<span class="number">8863752500417580646</span></span><br><span class="line">  &#125;</span><br><span class="line">  major_fragment_id: <span class="number">0</span></span><br><span class="line">  minor_fragment_id: <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line">fragment_json: <span class="string">"&#123;</span><br><span class="line">  "</span>pop<span class="string">" : "</span>screen<span class="string">",</span><br><span class="line">  "</span>@id<span class="string">" : 0,</span><br><span class="line">  "</span>child<span class="string">" : &#123;</span><br><span class="line">    "</span>pop<span class="string">" : "</span>streaming-aggregate<span class="string">",</span><br><span class="line">    "</span>@id<span class="string">" : 1,</span><br><span class="line">    "</span>child<span class="string">" : &#123;</span><br><span class="line">      "</span>pop<span class="string">" : "</span>project<span class="string">",</span><br><span class="line">      "</span>@id<span class="string">" : 2,</span><br><span class="line">      "</span>exprs<span class="string">" : [ &#123;</span><br><span class="line">        "</span>ref<span class="string">" : "</span>`<span class="variable">$f0</span>`<span class="string">",</span><br><span class="line">        "</span>expr<span class="string">" : "</span><span class="number">0</span><span class="string">"</span><br><span class="line">      &#125; ],</span><br><span class="line">      "</span>child<span class="string">" : &#123;</span><br><span class="line">        "</span>pop<span class="string">" : "</span>fs-sub-scan<span class="string">",</span><br><span class="line">        "</span>@id<span class="string">" : 3,</span><br><span class="line">        "</span>userName<span class="string">" : "</span>zhengqh<span class="string">",</span><br><span class="line">        "</span>files<span class="string">" : [ &#123;</span><br><span class="line">          "</span><span class="keyword">start</span><span class="string">" : 0,</span><br><span class="line">          "</span>length<span class="string">" : 474630,</span><br><span class="line">          "</span>path<span class="string">" : "</span>classpath:/employee.json<span class="string">"</span><br><span class="line">        &#125; ],</span><br><span class="line">        "</span>columns<span class="string">" : [ "</span>`*`<span class="string">" ],</span><br><span class="line">        "</span>selectionRoot<span class="string">" : "</span>classpath:/employee.json<span class="string">"</span><br><span class="line">      &#125;,</span><br><span class="line">      "</span>initialAllocation<span class="string">" : 1000000,</span><br><span class="line">      "</span>maxAllocation<span class="string">" : 10000000000,</span><br><span class="line">      "</span>cost<span class="string">" : 463.0</span><br><span class="line">    &#125;,</span><br><span class="line">    "</span>keys<span class="string">" : [ ],</span><br><span class="line">    "</span>exprs<span class="string">" : [ &#123;</span><br><span class="line">      "</span>ref<span class="string">" : "</span>`EXPR<span class="variable">$0</span>`<span class="string">",</span><br><span class="line">      "</span>expr<span class="string">" : "</span>count(<span class="number">1</span>) <span class="string">"</span><br><span class="line">    &#125; ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;"</span></span><br><span class="line">leaf_fragment: <span class="literal">true</span></span><br><span class="line">assignment &#123;</span><br><span class="line">  address: <span class="string">"localhost"</span></span><br><span class="line">  user_port: <span class="number">31010</span></span><br><span class="line">  control_port: <span class="number">31011</span></span><br><span class="line">  data_port: <span class="number">31012</span></span><br><span class="line">&#125;</span><br><span class="line">foreman &#123;</span><br><span class="line">  address: <span class="string">"localhost"</span></span><br><span class="line">  user_port: <span class="number">31010</span></span><br><span class="line">  control_port: <span class="number">31011</span></span><br><span class="line">  data_port: <span class="number">31012</span></span><br><span class="line">&#125;</span><br><span class="line">mem_initial: <span class="number">3000000</span></span><br><span class="line">mem_max: <span class="number">30000000000</span></span><br><span class="line">credentials &#123;</span><br><span class="line">  user_name: <span class="string">"anonymous"</span></span><br><span class="line">&#125;</span><br><span class="line">options_json: <span class="string">"[ ]"</span></span><br><span class="line">context &#123;</span><br><span class="line">  query_start_time: <span class="number">1436498522273</span></span><br><span class="line">  time_zone: <span class="number">299</span></span><br><span class="line">  default_schema_name: <span class="string">""</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面和fragment_json同级的还有foreman,表示接受客户端查询作业的节点, 因为是本地模式,所以是localhost.<br>初始化内存mem_initial和最大内存mem_max在FragmentContext中.  </p>
<p>Fragment提交到Foreman后, 查询开始运行, Foreman的状态从PENDING到RUNNING, FragmentExecutor从AWAITING_ALLOCATION到RUNNING.<br>这里有个比较重要的概念是通过ImplCreator创建的RecordBatch Tree.       </p>
<blockquote>
<p>上面我们已经有了Root Fragment形成的Tree的概念. 这里批记录还有树.<br>什么是RecordBatch,顾名思义是批记录. 那为什么又有树的概念?  </p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">378</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:foreman] DEBUG o.a.d.exec.rpc.control.WorkEventBus - Adding fragment status listener <span class="keyword">for</span> queryId <span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">378</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:foreman] DEBUG o.a.drill.exec.work.foreman.Foreman - Submitting fragments to run.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">378</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:foreman] DEBUG o.a.drill.exec.ops.FragmentContext - Getting initial memory allocation of <span class="number">3000000</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">378</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:foreman] DEBUG o.a.drill.exec.ops.FragmentContext - Fragment max allocation: <span class="number">30000000000</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">378</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:foreman] DEBUG o.a.d.exec.memory.TopLevelAllocator - New child allocator with initial reservation <span class="number">3000000</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">379</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:foreman] DEBUG o.a.d.e.work.batch.IncomingBuffers - Came up with a <span class="built_in">list</span> of <span class="number">0</span> required fragments.  Fragments &#123;&#125;</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">380</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:foreman] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  PENDING --&gt; RUNNING</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">381</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:foreman] DEBUG o.a.drill.exec.work.foreman.Foreman - Fragments running.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">381</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.exec.memory.BufferAllocator - New child allocator with initial reservation <span class="number">1000000</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">387</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.exec.memory.BufferAllocator - New child allocator with initial reservation <span class="number">1000000</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">387</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.exec.memory.BufferAllocator - New child allocator with initial reservation <span class="number">1000000</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">388</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.exec.memory.BufferAllocator - New child allocator with initial reservation <span class="number">1000000</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">388</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.exec.physical.impl.ImplCreator - Took <span class="number">7</span> ms to create RecordBatch tree</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">388</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] INFO  o.a.d.e.w.fragment.FragmentExecutor - <span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:<span class="number">0</span>:<span class="number">0</span>: State change requested from AWAITING_ALLOCATION --&gt; RUNNING <span class="keyword">for</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">388</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] INFO  o.a.d.e.w.f.AbstractStatusReporter - State changed <span class="keyword">for</span> <span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:<span class="number">0</span>:<span class="number">0.</span> New state: RUNNING</span><br></pre></td></tr></table></figure>
<p>Fragment的状态会被QueryManager管理, 其中operator_profile是操作算子的选项, 包括了一些字段input_profile, 操作算子id, 操作类型等等.<br>什么是profile, 其实WEB页面<a href="http://localhost:8047/profiles" target="_blank" rel="external">http://localhost:8047/profiles</a>就是Drill查询作业运行时的profile收集页面.<br>包括了Query Profile, Fragment Profiles,Operator Profiles,Full JSON Profile.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">389</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.e.w.f.NonRootStatusReporter - Sending status change message message to remote node: profile &#123;</span><br><span class="line">  state: RUNNING</span><br><span class="line">  minor_fragment_id: <span class="number">0</span></span><br><span class="line">  operator_profile &#123;</span><br><span class="line">    input_profile &#123;</span><br><span class="line">      records: <span class="number">0</span></span><br><span class="line">      batches: <span class="number">0</span></span><br><span class="line">      schemas: <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line">    operator_id: <span class="number">3</span></span><br><span class="line">    operator_type: <span class="number">29</span></span><br><span class="line">    setup_nanos: <span class="number">0</span></span><br><span class="line">    process_nanos: <span class="number">4651000</span></span><br><span class="line">    peak_local_memory_allocated: <span class="number">0</span></span><br><span class="line">    wait_nanos: <span class="number">3000</span></span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">390</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.e.w.fragment.FragmentExecutor - Starting fragment <span class="number">0</span>:<span class="number">0</span> on localhost:<span class="number">31010</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">392</span> [BitServer-<span class="number">4</span>] DEBUG o.a.d.exec.work.foreman.QueryManager - New fragment status was provided to QueryManager of profile &#123;</span><br></pre></td></tr></table></figure>
<p>上面提到的RecordBatch在下面有几个实现类: ProjectRecordBatch, StreamingAggBatch.</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill2.png" alt="">  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">392</span> [BitServer-<span class="number">4</span>] DEBUG o.a.d.exec.rpc.control.ControlServer - Sending response with Sender <span class="number">762133699</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">408</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.e.p.i.p.ProjectRecordBatch - Added eval <span class="keyword">for</span> project expression.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">410</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.e.p.i.a.StreamingAggBatch - Creating <span class="keyword">new</span> aggregator.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">413</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.e.p.i.a.StreamingAggBatch - Next outcome of OK_NEW_SCHEMA</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">413</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.e.p.i.a.StreamingAggBatch - Creating <span class="keyword">new</span> aggregator.</span><br><span class="line"></span><br><span class="line">+++++++++++++batch1+++++++++++++</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">414</span> [Client-<span class="number">1</span>] DEBUG o.a.d.e.rpc.user.QueryResultHandler - batchArrived: queryId = part1: <span class="number">3053657859282349058</span> part2: -<span class="number">8863752500417580646</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">415</span> [Client-<span class="number">1</span>] DEBUG o.a.d.j.i.DrillResultSetImpl$ResultsListener - [<span class="preprocessor">#<span class="number">3</span>] Received query data batch #<span class="number">1</span>: QueryResultBatch [header=query_id &#123;</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">415</span> [Client-<span class="number">1</span>] DEBUG o.a.drill.exec.rpc.user.UserClient - Sending response with Sender <span class="number">955795882</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">416</span> [main] DEBUG o.a.d.j.i.DrillResultSetImpl$ResultsListener - [<span class="preprocessor">#<span class="number">3</span>] Dequeued query data batch #<span class="number">1</span>: QueryResultBatch [header=query_id &#123;</span></span><br></pre></td></tr></table></figure>
<p>上面的batchArrived表示批记录到来, 那么接下去就是处理到来的数据了:<br>对于batch, 总是先<code>Received query data batch, 然后</code>Sending response with Sender,<br>最后<code>Dequeued query data batch</code>. 很显然query data batch会在队列中进进出出.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+++++++++++++batch2+++++++++++++</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">419</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.e.p.i.a.StreamingAggBatch - Aggregator response RETURN_OUTCOME, records <span class="number">1</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">419</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.e.p.i.a.StreamingAggBatch - Aggregator response CLEANUP_AND_RETURN, records <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">421</span> [Client-<span class="number">1</span>] DEBUG o.a.d.e.rpc.user.QueryResultHandler - batchArrived: queryId = part1: <span class="number">3053657859282349058</span> part2: -<span class="number">8863752500417580646</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">421</span> [Client-<span class="number">1</span>] DEBUG o.a.d.j.i.DrillResultSetImpl$ResultsListener - [<span class="preprocessor">#<span class="number">3</span>] Received query data batch #<span class="number">2</span>: QueryResultBatch [header=query_id &#123;</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">421</span> [Client-<span class="number">1</span>] DEBUG o.a.drill.exec.rpc.user.UserClient - Sending response with Sender <span class="number">1000578767</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">422</span> [main] DEBUG o.a.d.j.i.DrillResultSetImpl$ResultsListener - [<span class="preprocessor">#<span class="number">3</span>] Dequeued query data batch #<span class="number">2</span>: QueryResultBatch [header=query_id &#123;</span></span><br></pre></td></tr></table></figure>
<p>计算完成, 关闭上下文, FragmentExecutor的状态从RUNNING到FINISHED.  同样也会打印profile.<br>这里我们终于看到了1155这个Query计算出来的结果了.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">423</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.exec.ops.OperatorContextImpl - Closing context <span class="keyword">for</span> org.apache.drill.exec.store.dfs.easy.EasySubScan</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">423</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.exec.ops.OperatorContextImpl - Closing context <span class="keyword">for</span> org.apache.drill.exec.physical.config.Project</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">423</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.exec.ops.OperatorContextImpl - Closing context <span class="keyword">for</span> org.apache.drill.exec.physical.config.StreamingAggregate</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">423</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.exec.ops.OperatorContextImpl - Closing context <span class="keyword">for</span> org.apache.drill.exec.physical.config.Screen</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">423</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.apache.drill.exec.memory.Accountor - Fragment <span class="number">0</span>:<span class="number">0</span>  accountor being closed</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">423</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] INFO  o.a.d.e.w.fragment.FragmentExecutor - <span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:<span class="number">0</span>:<span class="number">0</span>: State change requested from RUNNING --&gt; FINISHED <span class="keyword">for</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">423</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] INFO  o.a.d.e.w.f.AbstractStatusReporter - State changed <span class="keyword">for</span> <span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:<span class="number">0</span>:<span class="number">0.</span> New state: FINISHED</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">424</span> [<span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a:frag:<span class="number">0</span>:<span class="number">0</span>] DEBUG o.a.d.e.w.f.NonRootStatusReporter - Sending status change message message to remote node: profile &#123;</span><br><span class="line">  state: FINISHED</span><br><span class="line">  minor_fragment_id: <span class="number">0</span></span><br><span class="line">  operator_profile &#123;</span><br><span class="line">    input_profile &#123;</span><br><span class="line">      records: <span class="number">1155</span></span><br><span class="line">      batches: <span class="number">1</span></span><br><span class="line">      schemas: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    operator_id: <span class="number">3</span></span><br><span class="line">    operator_type: <span class="number">29</span></span><br><span class="line">    setup_nanos: <span class="number">0</span></span><br><span class="line">    process_nanos: <span class="number">22301000</span></span><br><span class="line">    peak_local_memory_allocated: <span class="number">4608</span></span><br><span class="line">    wait_nanos: <span class="number">133000</span></span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>最后Forman也会关闭, Foreman的状态从RUNNING到COMPLETED.  打印resultArrived的时候其实结果已经在sqlline上输出了.<br>剩下就是一些资源移除,注销之类的工作了. 其实和最开始的资源申请,注册是对应的.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">427</span> [BitServer-<span class="number">4</span>] INFO  o.a.drill.exec.work.foreman.Foreman - State change requested.  RUNNING --&gt; COMPLETED</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">427</span> [BitServer-<span class="number">4</span>] INFO  o.a.drill.exec.work.foreman.Foreman - foreman cleaning up.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">429</span> [BitServer-<span class="number">4</span>] DEBUG o.a.d.exec.rpc.control.WorkEventBus - Removing fragment status listener <span class="keyword">for</span> queryId <span class="number">2</span>a60c5a4-e01a-ac02-<span class="number">84f</span>d-a01023a1319a.</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">430</span> [BitServer-<span class="number">4</span>] DEBUG o.apache.drill.exec.memory.Accountor - Fragment <span class="number">0</span>:<span class="number">0</span>  accountor being closed</span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">446</span> [BitServer-<span class="number">4</span>] DEBUG o.a.d.exec.rpc.control.ControlServer - Sending response with Sender <span class="number">739019148</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">447</span> [Client-<span class="number">1</span>] DEBUG o.a.d.e.rpc.user.QueryResultHandler - resultArrived: queryState: COMPLETED, queryId = part1: <span class="number">3053657859282349058</span></span><br><span class="line">part2: -<span class="number">8863752500417580646</span></span><br><span class="line"></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">448</span> [Client-<span class="number">1</span>] DEBUG o.a.d.j.i.DrillResultSetImpl$ResultsListener - [<span class="preprocessor">#<span class="number">3</span>] Received query completion: COMPLETED.</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">448</span> [Client-<span class="number">1</span>] DEBUG o.a.drill.exec.rpc.user.UserClient - Sending response with Sender <span class="number">264929084</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">479</span> [main] DEBUG o.a.d.j.i.DrillResultSetImpl$ResultsListener - [<span class="preprocessor">#<span class="number">3</span>] Query listener closing.</span></span><br><span class="line"><span class="number">2015</span>-<span class="number">07</span>-<span class="number">10</span> <span class="number">11</span>:<span class="number">22</span>:<span class="number">02</span>,<span class="number">479</span> [main] DEBUG o.a.d.j.impl.DrillStatementRegistry - Removing from open-statements registry: org.apache.drill.jdbc.impl.DrillStatementImpl@<span class="number">4</span>eb2bb3d</span><br></pre></td></tr></table></figure>
<h2 id="Logical_Plan逻辑计划">Logical Plan逻辑计划</h2><p><a href="http://drill.apache.org/docs/drill-plan-syntax/" target="_blank" rel="external">http://drill.apache.org/docs/drill-plan-syntax/</a><br><a href="https://docs.google.com/document/d/1QTL8warUYS2KjldQrGUse7zp8eA72VKtLOHwfXy6c7I/edit" target="_blank" rel="external">https://docs.google.com/document/d/1QTL8warUYS2KjldQrGUse7zp8eA72VKtLOHwfXy6c7I/edit</a><br><a href="http://yangyoupeng-cn-fujitsu-com.iteye.com/blog/1971728" target="_blank" rel="external">http://yangyoupeng-cn-fujitsu-com.iteye.com/blog/1971728</a></p>
<p>在Architecture中我们见到这张图了</p>
<p><img src="http://drill.apache.org/docs/img/client-phys-plan.png" alt=""></p>
<p>在DesignDoc中是一张比较粗略的图</p>
<p><img src="http://drill.apache.org/docs/img/slide-15-638.png" alt=""></p>
<p>总的来说过程就是: 查询语句–解析器–逻辑计划–优化器–物理计划–执行引擎</p>
<p>关于逻辑计划比较详细的文档也给出了(上面第二个链接,请自行fq).</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill3.png" alt=""></p>
<p>以Logical Plan Operators的Scan为例</p>
<blockquote>
<p>The Scan operator outputs a stream of records. The “storageengine” argument must refer by name to a storage engine defined in the engines clause of the logical plan. The “selection” argument accepts a JSON object that is used by the data source itself to limit the amount of data actually retrieved. The format and content of this object is specific to the actual input source being used. Examples might include an HBase table name, a MongoDB collection, an HDFS path or a partition of a Hive table. Data sources will use the the selection argument in an implementation-specific way.  The provided “ref” argument ensures that all records within the scanned source are held in the provided namespace.<br>{ @id†: &lt; opref &gt;, op: “scan”,<br>   storageengine: &lt; string &gt;,<br>   selection*: &lt; json &gt;,<br>   ref: &lt; name &gt;<br> }</p>
</blockquote>
<p>Scan操作算子会输出记录流. 参数storageengine必须引用逻辑计划中定义的engine声明.<br>selection参数接收JSON对象,会被数据源使用,用于限制接收到的数据的数量.<br>这个JSON对象的格式和内容和实际的数据源有关.比如HBase的表名,MongoDB的集合,HDFS的路径,或者Hive表的一个分区.  </p>
<blockquote>
<p>Talk is cheap, Show me the Code. </p>
</blockquote>
<p>在<code>org.apache.drill.common.logical.data</code>有很多上文提到的操作符比如Scan,Join,Project,Union等.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill5.png" alt=""></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="annotation">@JsonTypeName</span>(<span class="string">"scan"</span>)</span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">Scan</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">SourceOperator</span> &#123;</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">String</span> storageEngine;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">JSONOptions</span> selection;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@JsonCreator</span></span><br><span class="line">  public <span class="type">Scan</span>(<span class="annotation">@JsonProperty</span>(<span class="string">"storageengine"</span>) <span class="type">String</span> storageEngine, <span class="annotation">@JsonProperty</span>(<span class="string">"selection"</span>) <span class="type">JSONOptions</span> selection) &#123;</span><br><span class="line">    <span class="keyword">super</span>();</span><br><span class="line">    <span class="keyword">this</span>.storageEngine = storageEngine;</span><br><span class="line">    <span class="keyword">this</span>.selection = selection;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@JsonProperty</span>(<span class="string">"storageengine"</span>)</span><br><span class="line">  public <span class="type">String</span> getStorageEngine() &#123; <span class="keyword">return</span> storageEngine; &#125;</span><br><span class="line"></span><br><span class="line">  public <span class="type">JSONOptions</span> getSelection() &#123;  <span class="keyword">return</span> selection; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  public &lt;<span class="type">T</span>, <span class="type">X</span>, <span class="type">E</span> <span class="keyword">extends</span> <span class="type">Throwable</span>&gt; <span class="type">T</span> accept(<span class="type">LogicalVisitor</span>&lt;<span class="type">T</span>, <span class="type">X</span>, <span class="type">E</span>&gt; logicalVisitor, <span class="type">X</span> value) <span class="keyword">throws</span> <span class="type">E</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> logicalVisitor.visitScan(<span class="keyword">this</span>, value);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>LogicalVisitor</code>是逻辑(操作符)的访问器. Visitor class designed to traversal of a operator tree.  遍历一颗操作符树<br>Basis for a number of operator manipulations including fragmentation and materialization. 对算子的维护包括分片,序列化</p>
<p>Scan操作比较简单, 它继承的是SourceOperator:An operator that produces data without any parents.  (zero input operator)</p>
<blockquote>
<p>Operator分成若干类，每一个operator都标示了它的类型，目前operator类包括：<br>0：可以产生不依赖其他operator的数据，类似于源数据. 比如Scan,<br>1：该operator可以处理一个单独input source, 比如Project,Order,Limit等<br>M：可以处理多个input source数据<br>K：该operator不会产生输出。 </p>
</blockquote>
<p>我们知道Project包括字段和表达式, 比如count(<em>), 其中</em>是ref引用,count是expr表达式</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="annotation">@JsonTypeName</span>(<span class="string">"project"</span>)</span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">Project</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">SingleInputOperator</span> &#123;</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">NamedExpression</span>[] selections;</span><br><span class="line"></span><br><span class="line"><span class="annotation">@JsonPropertyOrder</span>(&#123;<span class="string">"ref"</span>, <span class="string">"expr"</span>&#125;)</span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">NamedExpression</span> &#123;</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">LogicalExpression</span> expr;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FieldReference</span> ref;</span><br></pre></td></tr></table></figure>
<p>在org.apache.drill.common.logical这个包下有个比较重要的类LogicalPlan,先来看看Plan的属性有哪些</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">PlanProperties</span> &#123;</span></span><br><span class="line">  public static <span class="class"><span class="keyword">enum</span> <span class="title">PlanType</span> &#123;<span class="title">APACHE_DRILL_LOGICAL</span>, <span class="title">APACHE_DRILL_PHYSICAL</span>&#125;</span></span><br><span class="line"></span><br><span class="line">  public <span class="constant">PlanType</span> <span class="keyword">type</span>;</span><br><span class="line">  public int version;</span><br><span class="line">  public <span class="constant">Generator</span> generator;</span><br><span class="line">  public <span class="constant">ResultMode</span> resultMode;</span><br><span class="line">  public <span class="constant">JSONOptions</span> options;</span><br><span class="line">  public int queue;</span><br><span class="line"></span><br><span class="line">  public static <span class="class"><span class="keyword">class</span> <span class="title">Generator</span> &#123;</span></span><br><span class="line">    public <span class="constant">String</span> <span class="keyword">type</span>;</span><br><span class="line">    public <span class="constant">String</span> info;</span><br><span class="line"></span><br><span class="line">    public static <span class="class"><span class="keyword">enum</span> <span class="title">ResultMode</span> &#123;</span></span><br><span class="line">      <span class="constant">EXEC</span>, <span class="constant">LOGICAL</span>, <span class="constant">PHYSICAL</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="constant">Generator</span>(<span class="variable">@JsonProperty</span>(<span class="string">"type"</span>) <span class="constant">String</span> <span class="keyword">type</span>, <span class="variable">@JsonProperty</span>(<span class="string">"info"</span>) <span class="constant">String</span> info) &#123;</span><br><span class="line">      this.<span class="keyword">type</span> = <span class="keyword">type</span>;</span><br><span class="line">      this.info = info;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>对应了前面的Drill Plan中head部分的输出(虽然前面我们看到的Drill Plan应该是物理计划,而不是逻辑计划,但是head部分是一样的)</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "<span class="attribute">head</span>" : <span class="value">&#123;</span><br><span class="line">    "<span class="attribute">version</span>" : <span class="value"><span class="number">1</span></span>,</span><br><span class="line">    "<span class="attribute">generator</span>" : <span class="value">&#123;</span><br><span class="line">      "<span class="attribute">type</span>" : <span class="value"><span class="string">"DefaultSqlHandler"</span></span>,</span><br><span class="line">      "<span class="attribute">info</span>" : <span class="value"><span class="string">""</span></span><br><span class="line">    </span>&#125;</span>,</span><br><span class="line">    "<span class="attribute">type</span>" : <span class="value"><span class="string">"APACHE_DRILL_PHYSICAL"</span></span>,</span><br><span class="line">    "<span class="attribute">options</span>" : <span class="value">[ ]</span>,</span><br><span class="line">    "<span class="attribute">queue</span>" : <span class="value"><span class="number">0</span></span>,</span><br><span class="line">    "<span class="attribute">resultMode</span>" : <span class="value"><span class="string">"EXEC"</span></span><br><span class="line">  </span>&#125;</span>,</span><br></pre></td></tr></table></figure>
<p>逻辑计划LogicalPlan包含了三个部分: head,storage,query.  </p>
<p><a href="http://www.confusedcoders.com/bigdata/apache-drill/understanding-apache-drill-logical-plan" target="_blank" rel="external">http://www.confusedcoders.com/bigdata/apache-drill/understanding-apache-drill-logical-plan</a></p>
<blockquote>
<p>The query node is the actual query that we want to execute on Drill. The query itself is a collection of operations on the data.</p>
</blockquote>
<figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="annotation">@JsonPropertyOrder</span>(&#123; <span class="string">"head"</span>, <span class="string">"storage"</span>, <span class="string">"query"</span> &#125;)</span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">LogicalPlan</span> </span>&#123;</span><br><span class="line">  <span class="literal">static</span> <span class="keyword">final</span> Logger logger = LoggerFactory.getLogger(LogicalPlan.<span class="keyword">class</span>);</span><br><span class="line"></span><br><span class="line">  private <span class="keyword">final</span> PlanProperties properties;</span><br><span class="line">  private <span class="keyword">final</span> <span class="built_in">Map</span>&lt;<span class="built_in">String</span>, StoragePluginConfig&gt; storageEngineMap;</span><br><span class="line">  private <span class="keyword">final</span> Graph&lt;LogicalOperator, SinkOperator, SourceOperator&gt; graph;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="annotation">@JsonCreator</span></span><br><span class="line">  public LogicalPlan(<span class="annotation">@JsonProperty</span>(<span class="string">"head"</span>) PlanProperties head,</span><br><span class="line">      <span class="annotation">@JsonProperty</span>(<span class="string">"storage"</span>) <span class="built_in">Map</span>&lt;<span class="built_in">String</span>, StoragePluginConfig&gt; storageEngineMap,</span><br><span class="line">      <span class="annotation">@JsonProperty</span>(<span class="string">"query"</span>) <span class="built_in">List</span>&lt;LogicalOperator&gt; operators) &#123;</span><br><span class="line">    <span class="keyword">this</span>.storageEngineMap = storageEngineMap != <span class="keyword">null</span> ? storageEngineMap : <span class="keyword">new</span> HashMap&lt;<span class="built_in">String</span>, StoragePluginConfig&gt;();</span><br><span class="line">    <span class="keyword">this</span>.properties = head;</span><br><span class="line">    <span class="keyword">this</span>.graph = Graph.newGraph(operators, SinkOperator.<span class="keyword">class</span>, SourceOperator.<span class="keyword">class</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>query是逻辑操作符集合, 它们和Sink,Source操作符一起构成了一张逻辑计划数据流的执行图graph.<br>LogicalPlan的构建器的build()会创建LogicalPlan对象. 这个Builder对象提供了逻辑计划的编程接口.  </p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">LogicalPlanBuilder</span> &#123;</span><br><span class="line">  <span class="keyword">private</span> PlanProperties planProperties;</span><br><span class="line">  <span class="keyword">private</span> ImmutableMap.Builder&lt;String, StoragePluginConfig&gt; storageEngines = ImmutableMap.builder();</span><br><span class="line">  <span class="keyword">private</span> ImmutableList.Builder&lt;LogicalOperator&gt; operators = ImmutableList.builder();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> LogicalPlanBuilder <span class="title">addLogicalOperator</span>(<span class="params">LogicalOperator <span class="keyword">operator</span></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.operators.add(<span class="keyword">operator</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> LogicalPlan <span class="title">build</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> LogicalPlan(<span class="keyword">this</span>.planProperties, <span class="keyword">this</span>.storageEngines.build(), <span class="keyword">this</span>.operators.build());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在build之前,要构建一个完整的图,要调用相应的addXXX()方法,因为方法返回this,所以调用者可以链式调用.<br>build就是构建者模式,一旦调用了build方法,返回的对象就是不可修改的.因此要在build前填充所有的数据.  </p>
<h2 id="PhysicalPlan物理计划">PhysicalPlan物理计划</h2><p>怎么知道前面日志中打印的Drill Plan是物理计划,而不是逻辑计划, 首先可以从调用的类DefaultSqlHandler,搜索Drill Plan</p>
<figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> PhysicalPlan getPlan(SqlNode sqlNode) &#123;</span><br><span class="line">  <span class="keyword">final</span> ConvertedRelNode convertedRelNode = validateAndConvert(sqlNode);</span><br><span class="line">  <span class="keyword">final</span> RelDataType validatedRowType = convertedRelNode.getValidatedRowType();</span><br><span class="line">  <span class="keyword">final</span> RelNode queryRelNode = convertedRelNode.getConvertedNode();</span><br><span class="line"></span><br><span class="line">  <span class="built_in">log</span>(<span class="string">"Optiq Logical"</span>, queryRelNode, logger);</span><br><span class="line">  DrillRel drel = convertToDrel(queryRelNode, validatedRowType);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">log</span>(<span class="string">"Drill Logical"</span>, drel, logger);</span><br><span class="line">  Prel prel = convertToPrel(drel);</span><br><span class="line">  <span class="built_in">log</span>(<span class="string">"Drill Physical"</span>, prel, logger);</span><br><span class="line">  PhysicalOperator pop = convertToPop(prel);</span><br><span class="line">  PhysicalPlan plan = convertToPlan(pop);</span><br><span class="line">  <span class="built_in">log</span>(<span class="string">"Drill Plan"</span>, plan, logger);</span><br><span class="line">  <span class="keyword">return</span> plan;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>1.可以看到在上面的getPlan方法中, 依次生成的计划是: Optiq逻辑计划–&gt;Drill逻辑计划–&gt;Drill物理计划<br>2.物理计划和逻辑计划一样也有很多operator, 都在`org.apache.drill.exec.physical包下<br>3.Optiq现在变成Apache的Calcite, 入门教程: <a href="http://blog.csdn.net/yunlong34574/article/details/46375733" target="_blank" rel="external">http://blog.csdn.net/yunlong34574/article/details/46375733</a></p>
<p>getPlan调用树是被Foreman线程运行,由Foreman创建的DrillSqlWorker调用执行的.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill4.png" alt=""></p>
<p>真正运行物理计划,还是在Foreman的runPhysicalPlan中</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> runSQL(<span class="keyword">final</span> <span class="keyword">String</span> sql) <span class="keyword">throws</span> ExecutionSetupException &#123;</span><br><span class="line">  <span class="keyword">final</span> DrillSqlWorker sqlWorker = <span class="keyword">new</span> DrillSqlWorker(queryContext);</span><br><span class="line">  <span class="keyword">final</span> Pointer&lt;<span class="keyword">String</span>&gt; textPlan = <span class="keyword">new</span> Pointer&lt;&gt;();</span><br><span class="line">  <span class="keyword">final</span> PhysicalPlan plan = sqlWorker.getPlan(sql, textPlan);</span><br><span class="line">  queryManager.setPlanText(textPlan.value);</span><br><span class="line">  runPhysicalPlan(plan);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/Source/">Source</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/drill/">drill</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-07-07-Spark-HA-YARN" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/07/2015-07-07-Spark-HA-YARN/" class="article-date">
  	<time datetime="2015-07-06T16:00:00.000Z" itemprop="datePublished">2015-07-07</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/07/2015-07-07-Spark-HA-YARN/">Spark HA和YARN模式</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SparkStandalone-HA">SparkStandalone-HA</h2><p><a href="http://taoistwar.gitbooks.io/spark-operationand-maintenance-management/content/spark_install/spark_standalone_with_zookeeper_ha.html" target="_blank" rel="external">Spark Standalone基于ZooKeeper的HA</a></p>
<table>
<thead>
<tr>
<th>node</th>
<th>host</th>
</tr>
</thead>
<tbody>
<tr>
<td>master</td>
<td>dp0652,dp0653</td>
</tr>
<tr>
<td>slaves</td>
<td>dp0655,dp0656,dp0657</td>
</tr>
</tbody>
</table>
<p>vi spark-env.sh</p>
<figure class="highlight objectivec"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> SPARK_MASTER_IP=dp0652</span><br><span class="line"><span class="keyword">export</span> SPARK_DAEMON_J<span class="built_in">AVA_OPTS</span>=<span class="string">"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=dp0655:2181,dp0656:2181,dp0657:2181 -Dspark.deploy.zookeeper.dir=/spark"</span></span><br></pre></td></tr></table></figure>
<p>vi slaves</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">dp0655</span></span><br><span class="line">dp0656</span><br><span class="line">dp0657</span><br></pre></td></tr></table></figure>
<p>将配置文件分发到集群所有节点</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scp spark-env.sh dp0653:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp spark-env.sh dp0655:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp spark-env.sh dp0656:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp spark-env.sh dp0657:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp slaves dp0653:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp slaves dp0655:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp slaves dp0656:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp slaves dp0657:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br></pre></td></tr></table></figure>
<p>在dp0653上修改spark-env.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_MASTER_IP=dp0653</span><br></pre></td></tr></table></figure>
<p>在dp0652上启动集群: <code>sbin/start-all.sh</code></p>
<p>在dp0653上启动Master: <code>sbin/start-master.sh</code></p>
<p>可以看到dp0652是active, dp0653是standby</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/sha1.png" alt=""></p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/sha2.png" alt=""></p>
<p>关闭dp0652的master: <code>sbin/stop-master.sh</code></p>
<p>观察dp0653是否成为master:</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/sha3.png" alt=""></p>
<p>执行应用程序. 注意–master现在有多个了</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark:<span class="comment">//dp0652:7077,dp0653:7077 \</span></span><br><span class="line">  --<span class="keyword">class</span> org.apache.spark.examples.SparkPi \</span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">2</span> \</span><br><span class="line">  lib/spark-examples-<span class="number">1.4</span><span class="number">.0</span>-hadoop2<span class="number">.6</span><span class="number">.0</span>.jar <span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>dp0652的master日志:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">41</span>:<span class="number">50</span> INFO Master: Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">58543</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">41</span>:<span class="number">50</span> INFO Master: Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">37859</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">41</span>:<span class="number">50</span> INFO Master: Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">34379</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">01</span> ERROR Master: RECEIVED SIGNAL <span class="number">15</span>: SIGTERM</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">01</span> INFO Utils: Shutdown hook called</span><br></pre></td></tr></table></figure>
<p>dp0653的master日志:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">42</span>:<span class="number">03</span> INFO ConnectionStateManager: State change: CONNECTED</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">35</span> INFO ZooKeeperLeaderElectionAgent: We have gained leadership</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: I have been elected leader! New state: RECOVERING</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Trying to recover worker: worker-<span class="number">20150707144149</span>-<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>-<span class="number">37859</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Trying to recover worker: worker-<span class="number">20150707144149</span>-<span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>-<span class="number">58543</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Trying to recover worker: worker-<span class="number">20150707144149</span>-<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>-<span class="number">34379</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Worker has been re-registered: worker-<span class="number">20150707144149</span>-<span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>-<span class="number">58543</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Worker has been re-registered: worker-<span class="number">20150707144149</span>-<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>-<span class="number">37859</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Worker has been re-registered: worker-<span class="number">20150707144149</span>-<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>-<span class="number">34379</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Recovery complete - resuming operations!</span><br></pre></td></tr></table></figure>
<h2 id="SparkHA-Streamming">SparkHA-Streamming</h2><p>如果线上的SparkStreamming程序一直在运行, 而Master发生了切换, 验证下SparkStreamming还能不能正常运行.<br>这个实验的基础是在Spark-Streamming这一节的基础上.  </p>
<p>还是先启动KafkaProducer[在52上]</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ bin/spark-submit --<span class="keyword">class</span> org.apache.spark.examples.streaming.KafkaWordCountProducer /home/qihuang.zheng/spark-intro-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">9092</span> kafka-spark-test <span class="number">2000</span> <span class="number">70</span></span><br></pre></td></tr></table></figure>
<p>然后启动Spark-Streamming程序[也在52上], 注意–master的值  </p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-1.4.0-bin-hadoop2.6]$ bin/spark-submit --master spark://dp<span class="number">0652:7077</span>,dp<span class="number">0653:7077</span> --class org.apache.spark.examples.streaming.KafkaWordCount --jars /home/qihuang.zheng/spark-streaming-kafka_<span class="number">2.10-1.4</span>.0.jar /home/qihuang.zheng/spark-intro-1.0-SNAPSHOT-jar-with-dependencies.jar <span class="number">192.168.6.55</span>:<span class="number">2181,192.168</span>.<span class="number">6.56:2181</span>,<span class="number">192.168.6.57</span>:2181 my-consumer-group kafka-spark-test 5</span><br></pre></td></tr></table></figure>
<p>当前的master是53,我们把53的停掉: <code>sbin/stop-master.sh</code></p>
<p>可以发现Spark-Streamming的程序并没有受到影响.  也就是说master的变化并不会影响driver程序.<br><strong>只要你的driver指定–master为HA,而不是单点!</strong>  </p>
<blockquote>
<p>如果Driver程序比如spark-streamming程序挂掉了怎么办(比如要升级应用程序),那么再起一个streamming程序,这个程序会一直被阻塞住<br>直到旧的程序关掉了, 新的streamming程序就会立马接管过来.  这在<a href="http://zqhxuyuan.github.io/2015/06/28/Spark-Streamming/" target="_blank" rel="external">http://zqhxuyuan.github.io/2015/06/28/Spark-Streamming/</a>中已经做过了  </p>
</blockquote>
<p>注意: 不管是dp0652还是dp0653,都无法看到正在运行中的SparkStreamming程序.<br>SparkStreamming程序类似于Driver, 你在那台机器执行,就再这台机器的4040端口查看.<br>比如上面的实验,我们在dp0652上运行了Spark-Streamming程序, 所以在<a href="http://192.168.6.52:4040/streaming/" target="_blank" rel="external">http://192.168.6.52:4040/streaming/</a>查看StreamingUI<br>至于Master的8082端口,则只有主master才可以看到worker, 但是都看不到Running Applications</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark9.png" alt="spark-master"></p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark10.png" alt="spark-stream"></p>
<h2 id="Spark_on_YARN">Spark on YARN</h2><p>[qihuang.zheng@dp0652 lib]$ /usr/install/hadoop/bin/hadoop fs -mkdir /lib<br>[qihuang.zheng@dp0652 lib]$ /usr/install/hadoop/bin/hadoop fs -put spark-assembly-1.4.0-hadoop2.6.0.jar /lib</p>
<h3 id="YARN_Cluster">YARN Cluster</h3><p>其实在按照好Hadoop后,Spark on YARN不需要任何配置,只不过spark-submit的时候–master发生了变化</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SPARK_JAR=hdfs://tdhdfs/lib/spark-assembly-1.4.0-hadoop2.6.0.jar \</span><br><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    -<span class="ruby">-master yarn-cluster \</span><br><span class="line"></span>    -<span class="ruby">-num-executors <span class="number">3</span> \</span><br><span class="line"></span>    -<span class="ruby">-driver-memory <span class="number">4</span>g \</span><br><span class="line"></span>    -<span class="ruby">-executor-memory <span class="number">2</span>g \</span><br><span class="line"></span>    -<span class="ruby">-executor-cores <span class="number">1</span> \</span><br><span class="line"></span>    lib/spark-examples*.jar \</span><br><span class="line">    10</span><br></pre></td></tr></table></figure>
<p>注意: 因为现在是Spark on YARN了,所以spark-submit的Job不会显示在Spark的UI上,而是HadoopYARN的UI上<br>而且, 因为是yarn-cluster模式, 终端并不会输出Pi的值! 相反,下面yarn-client模式会在终端输出Pi的值.</p>
<h3 id="YARN_Client模式">YARN Client模式</h3><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="constant">SPARK_JAR</span>=<span class="symbol">hdfs:</span>/<span class="regexp">/tdhdfs/lib</span><span class="regexp">/spark-assembly-1.4.0-hadoop2.6.0.jar \</span><br><span class="line">bin/spark</span>-submit --<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">examples</span>.<span class="title">SparkPi</span> \</span></span><br><span class="line">    --master yarn-client \</span><br><span class="line">    --num-executors <span class="number">3</span> \</span><br><span class="line">    --driver-memory <span class="number">4</span>g \</span><br><span class="line">    --executor-memory <span class="number">2</span>g \</span><br><span class="line">    --executor-cores <span class="number">1</span> \</span><br><span class="line">    <span class="class"><span class="keyword">lib</span>/<span class="title">spark</span>-<span class="title">examples</span>-1.4.0-<span class="title">hadoop2</span>.6.0.<span class="title">jar</span> \</span></span><br><span class="line">    <span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>yarn-client的日志输出: client模式也可以在sparkui上查看作业<br>实际上, 虽然4040能访问, 但是并不会在8082(默认8081)上看到这个作业.  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">07</span> INFO <span class="string">SparkUI:</span> Started SparkUI at <span class="string">http:</span><span class="comment">//192.168.6.52:4040</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">07</span> INFO <span class="string">SparkContext:</span> Added JAR <span class="string">file:</span><span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/spark-examples-1.4.0-hadoop2.6.0.jar at http:/</span><span class="regexp">/192.168.6.52:42366/</span>jars/spark-examples-<span class="number">1.4</span><span class="number">.0</span>-hadoop2<span class="number">.6</span><span class="number">.0</span>.jar with timestamp <span class="number">1436254447750</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">08</span> INFO <span class="string">ConfiguredRMFailoverProxyProvider:</span> Failing over to rm2</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">08</span> INFO <span class="string">Client:</span> Requesting a <span class="keyword">new</span> application from cluster with <span class="number">2</span> NodeManagers</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">08</span> INFO <span class="string">Client:</span> Verifying our application has not requested more than the maximum memory capability of the cluster (<span class="number">8192</span> MB per container)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">08</span> INFO <span class="string">Client:</span> Will allocate AM container, with <span class="number">896</span> MB memory including <span class="number">384</span> MB overhead</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">08</span> INFO <span class="string">Client:</span> Setting up container launch context <span class="keyword">for</span> our AM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">08</span> INFO <span class="string">Client:</span> Preparing resources <span class="keyword">for</span> our AM container</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">08</span> WARN <span class="string">Client:</span> SPARK_JAR detected <span class="keyword">in</span> the system environment. This variable has been deprecated <span class="keyword">in</span> favor of the spark.yarn.jar configuration variable.</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">08</span> INFO <span class="string">Client:</span> Source and destination file systems are the same. Not copying <span class="string">hdfs:</span><span class="comment">//tdhdfs/lib/spark-assembly-1.4.0-hadoop2.6.0.jar</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">09</span> INFO <span class="string">Client:</span> Uploading resource <span class="string">file:</span><span class="regexp">/tmp/</span>spark-c564fff3-dd94-<span class="number">4</span>b52-<span class="number">81</span>c9-<span class="number">6e6</span>f7e4d4ceb<span class="regexp">/__hadoop_conf__2813971044505707826.zip -&gt; hdfs:/</span><span class="regexp">/tdhdfs/</span>user<span class="regexp">/qihuang.zheng/</span>.sparkStaging<span class="regexp">/application_1436175086022_0003/</span>__hadoop_conf__2813971044505707826.zip</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">09</span> INFO <span class="string">Client:</span> Setting up the launch environment <span class="keyword">for</span> our AM container</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">09</span> WARN <span class="string">Client:</span> SPARK_JAR detected <span class="keyword">in</span> the system environment. This variable has been deprecated <span class="keyword">in</span> favor of the spark.yarn.jar configuration variable.</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">09</span> INFO <span class="string">Client:</span> Submitting application <span class="number">3</span> to ResourceManager</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">09</span> INFO <span class="string">YarnClientImpl:</span> Submitted application application_1436175086022_0003</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">10</span> INFO <span class="string">Client:</span> Application report <span class="keyword">for</span> application_1436175086022_0003 (<span class="string">state:</span> ACCEPTED)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">11</span> INFO <span class="string">Client:</span> Application report <span class="keyword">for</span> application_1436175086022_0003 (<span class="string">state:</span> ACCEPTED)</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">38</span> INFO <span class="string">SparkContext:</span> Starting <span class="string">job:</span> reduce at SparkPi.<span class="string">scala:</span><span class="number">35</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">38</span> INFO <span class="string">SparkContext:</span> Created broadcast <span class="number">0</span> from broadcast at DAGScheduler.<span class="string">scala:</span><span class="number">874</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">38</span> INFO <span class="string">DAGScheduler:</span> Submitting <span class="number">10</span> missing tasks from ResultStage <span class="number">0</span> (MapPartitionsRDD[<span class="number">1</span>] at map at SparkPi.<span class="string">scala:</span><span class="number">31</span>)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">38</span> INFO <span class="string">YarnScheduler:</span> Adding task set <span class="number">0.0</span> with <span class="number">10</span> tasks</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">38</span> INFO <span class="string">TaskSetManager:</span> Starting task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">0</span>, dp0653, PROCESS_LOCAL, <span class="number">1446</span> bytes)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">38</span> INFO <span class="string">TaskSetManager:</span> Starting task <span class="number">1.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">1</span>, dp0652, PROCESS_LOCAL, <span class="number">1446</span> bytes)</span><br><span class="line">...</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">48</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_0_piece0 <span class="keyword">in</span> memory on <span class="string">dp0653:</span><span class="number">43216</span> (<span class="string">size:</span> <span class="number">1202.0</span> B, <span class="string">free:</span> <span class="number">1060.3</span> MB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">49</span> INFO <span class="string">TaskSetManager:</span> Finished task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">0</span>) <span class="keyword">in</span> <span class="number">10414</span> ms on dp0653 (<span class="number">10</span>/<span class="number">10</span>)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">49</span> INFO <span class="string">DAGScheduler:</span> ResultStage <span class="number">0</span> (reduce at SparkPi.<span class="string">scala:</span><span class="number">35</span>) finished <span class="keyword">in</span> <span class="number">10.420</span> s</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">49</span> INFO <span class="string">YarnScheduler:</span> Removed TaskSet <span class="number">0.0</span>, whose tasks have all completed, from pool</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">07</span> <span class="number">15</span>:<span class="number">34</span>:<span class="number">49</span> INFO <span class="string">DAGScheduler:</span> Job <span class="number">0</span> <span class="string">finished:</span> reduce at SparkPi.<span class="string">scala:</span><span class="number">35</span>, took <span class="number">10.776165</span> s</span><br><span class="line">Pi is roughly <span class="number">3.144008</span></span><br></pre></td></tr></table></figure>
<p>下图是YARN的应用程序, 第一个是yarn-client, 第二个是yarn-cluster</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/syarn1.png" alt=""></p>
<h3 id="YARN客户端和集群模式比较">YARN客户端和集群模式比较</h3><p>关于为什么yarn-client能看到结果,而yarn-cluster无法看到结果,可以参考:<br><a href="http://blog.csdn.net/book_mmicky/article/details/25714287" target="_blank" rel="external">http://blog.csdn.net/book_mmicky/article/details/25714287</a></p>
<blockquote>
<p>在YARN上部署Spark应用程序的时候，不需要象Standalone、Mesos一样提供URL作为master参数的值，<br>因为Spark应用程序可以在hadoop的配置文件里面获取相关的信息，<br>所以只需要简单以yarn-cluster或yarn-client指定给master就可以了  </p>
<p>YARN Client:  Spark driver在客户机上运行，然后向YARN申请运行exeutor以运行Task<br>YARN Cluster: Spark driver将作为一个ApplicationMaster在YARN集群中先启动，<br>              然后再由ApplicationMaster向RM申请资源启动executor以运行Task</p>
<p>采用yarn-client方式，因为driver在客户端，所以程序的运行结果可以在客户端显示<br>采用yarn-cluster方式，因为driver在YARN中运行，所以程序的运行结果不能在客户端显示，<br>   所以最好将结果保存在hdfs上，客户端的终端显示的是作为YARN的job的运行情况</p>
</blockquote>
<h2 id="其他参考文档">其他参考文档</h2><p><a href="http://www.cnblogs.com/byrhuangqiang/p/3937654.html" target="_blank" rel="external">Spark:Master High Availability（HA）高可用配置的2种实现</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-07-06-Spark-Streamming" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/06/2015-07-06-Spark-Streamming/" class="article-date">
  	<time datetime="2015-07-05T16:00:00.000Z" itemprop="datePublished">2015-07-06</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/06/2015-07-06-Spark-Streamming/">Spark Stramming入门</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NetworkWordCount">NetworkWordCount</h2><p>1.本机运行时,修改spark-env.sh(这一步不是必须的)</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1<span class="number">.7</span><span class="number">.0</span>_79.jdk/Contents/Home</span><br><span class="line"><span class="keyword">export</span> SCALA_HOME=/Users/zhengqh/Soft/scala-<span class="number">2.10</span><span class="number">.5</span></span><br><span class="line"><span class="keyword">export</span> HADOOP_HOME=/Users/zhengqh/Soft/cdh542/hadoop-<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.4</span><span class="number">.2</span></span><br><span class="line"><span class="keyword">export</span> HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_IP=localhost</span><br><span class="line"><span class="preprocessor">#export MASTER=spark:<span class="comment">//localhost:7077</span></span></span><br><span class="line"><span class="keyword">export</span> SPARK_LOCAL_IP=localhost</span><br></pre></td></tr></table></figure>
<p>2.开启netcat数据服务器</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~  nc -lk <span class="number">9999</span></span><br><span class="line">hello world hello spark hello spark hello world</span><br></pre></td></tr></table></figure>
<p>3.运行spark-streamming示例,当在nc终端输入text时,spark-streamming会实时统计过去一秒的wordcount</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>  bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost <span class="number">9999</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">36</span> INFO dstream.SocketReceiver: Connected to localhost:<span class="number">9999</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">37</span> INFO scheduler.ReceiverTracker: Registered receiver <span class="keyword">for</span> stream <span class="number">0</span> from <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">50018</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">37</span> INFO scheduler.JobScheduler: Added jobs <span class="keyword">for</span> time <span class="number">1436143117000</span> ms</span><br><span class="line">...</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">37</span> INFO scheduler.DAGScheduler: Job <span class="number">2</span> finished: print at NetworkWordCount.scala:<span class="number">55</span>, took <span class="number">0.066998</span> s</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">1436143116000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">37</span> INFO scheduler.JobScheduler: Finished job streaming job <span class="number">1436143116000</span> ms<span class="number">.0</span> from job <span class="built_in">set</span> of time <span class="number">1436143116000</span> ms</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">37</span> INFO scheduler.JobScheduler: Total delay: <span class="number">1.204</span> s <span class="keyword">for</span> time <span class="number">1436143116000</span> ms (execution: <span class="number">1.118</span> s)</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">40</span> INFO scheduler.DAGScheduler: Job <span class="number">10</span> finished: print at NetworkWordCount.scala:<span class="number">55</span>, took <span class="number">0.048230</span> s</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">1436143120000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(spark,<span class="number">2</span>)</span><br><span class="line">(hello,<span class="number">4</span>)</span><br><span class="line">(world,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">40</span> INFO scheduler.JobScheduler: Finished job streaming job <span class="number">1436143120000</span> ms<span class="number">.0</span> from job <span class="built_in">set</span> of time <span class="number">1436143120000</span> ms</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">40</span> INFO rdd.ShuffledRDD: Removing RDD <span class="number">16</span> from persistence <span class="built_in">list</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">40</span> INFO scheduler.JobScheduler: Total delay: <span class="number">0.356</span> s <span class="keyword">for</span> time <span class="number">1436143120000</span> ms (execution: <span class="number">0.335</span> s)</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">40</span> INFO rdd.MapPartitionsRDD: Removing RDD <span class="number">15</span> from persistence <span class="built_in">list</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">06</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">40</span> INFO storage.BlockManager: Removing RDD <span class="number">16</span></span><br></pre></td></tr></table></figure>
<p>4.在<localhost:4040 streaming="">可以观察streaming的统计信息, 其中在InputRate向上凸出的是产生数据的速度  </localhost:4040></p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss1.png" alt=""></p>
<p>5.IDEA本地运行</p>
<p>给SparkConf添加.setMaster(), 在<code>Program Arguments</code>中添加<code>localhost 9999 local[2]</code>, 然后运行</p>
<p>否则会报错:<code>org.apache.spark.SparkException: A master URL must be set in your configuration</code>  </p>
<p>example-code: spark-streamming实时读取socker流,统计过去5秒的wordcount</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create the context with a 1 second batch size</span></span><br><span class="line">val sparkConf = new <span class="function"><span class="title">SparkConf</span><span class="params">()</span></span>.<span class="function"><span class="title">setAppName</span><span class="params">(<span class="string">"NetworkWordCount"</span>)</span></span>.<span class="function"><span class="title">setMaster</span><span class="params">(args(<span class="number">2</span>)</span></span>)</span><br><span class="line">val ssc = new <span class="function"><span class="title">StreamingContext</span><span class="params">(sparkConf, Seconds(<span class="number">5</span>)</span></span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a socket stream on target ip:port and count the</span></span><br><span class="line"><span class="comment">// words in input stream of \n delimited text (eg. generated by 'nc')</span></span><br><span class="line"><span class="comment">// Note that no duplication in storage level only for running locally.</span></span><br><span class="line"><span class="comment">// Replication necessary in distributed scenario for fault tolerance.</span></span><br><span class="line">val lines = ssc.<span class="function"><span class="title">socketTextStream</span><span class="params">(args(<span class="number">0</span>)</span></span>, <span class="function"><span class="title">args</span><span class="params">(<span class="number">1</span>)</span></span><span class="class">.toInt</span>, StorageLevel.MEMORY_AND_DISK_SER)</span><br><span class="line">val words = lines.<span class="function"><span class="title">flatMap</span><span class="params">(_.split(<span class="string">" "</span>)</span></span>)</span><br><span class="line">val wordCounts = words.<span class="function"><span class="title">map</span><span class="params">(x =&gt; (x, <span class="number">1</span>)</span></span>).<span class="function"><span class="title">reduceByKey</span><span class="params">(_ + _)</span></span></span><br><span class="line">wordCounts.<span class="function"><span class="title">print</span><span class="params">()</span></span></span><br><span class="line">ssc.<span class="function"><span class="title">start</span><span class="params">()</span></span></span><br><span class="line">ssc.<span class="function"><span class="title">awaitTermination</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<p>6.打包本地运行</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>  bin/spark-submit --class com<span class="class">.tongdun</span><span class="class">.bigdata</span><span class="class">.spark</span><span class="class">.intro</span><span class="class">.NetworkWordCount</span> --jars /Users/zhengqh/IdeaProjects/bigdata/out/artifacts/spark_intro/spark-intro<span class="class">.jar</span> /Users/zhengqh/IdeaProjects/bigdata/out/artifacts/spark_intro/spark-intro<span class="class">.jar</span> localhost <span class="number">9999</span> <span class="string">"local[2]"</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>PS: –jars xx.jar可以省略</p>
</blockquote>
<p>7.打包集群运行. 程序的最后一个参数可以指定为local[2]或者spark://dp0652:7077</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>  bin/spark-submit --class com<span class="class">.tongdun</span><span class="class">.bigdata</span><span class="class">.spark</span><span class="class">.intro</span><span class="class">.NetworkWordCount</span> /home/qihuang.zheng/spark-intro<span class="class">.jar</span> localhost <span class="number">9999</span> <span class="string">"local[2]"</span></span><br><span class="line"></span><br><span class="line">➜  spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>  bin/spark-submit --class com<span class="class">.tongdun</span><span class="class">.bigdata</span><span class="class">.spark</span><span class="class">.intro</span><span class="class">.NetworkWordCount</span> /home/qihuang.zheng/spark-intro<span class="class">.jar</span> localhost <span class="number">9999</span> spark:<span class="comment">//dp0652:7077</span></span><br></pre></td></tr></table></figure>
<p>8.一般在集群中运行,我们使用–master而不是在代码中设置setMaster(),所以把代码中的setMaster去掉重新打包</p>
<p>先在本地实验, 👌 👉 不指定–master可以成功运行</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --class com<span class="class">.tongdun</span><span class="class">.bigdata</span><span class="class">.spark</span><span class="class">.intro</span><span class="class">.NetworkWordCount</span> /Users/zhengqh/spark-intro<span class="class">.jar</span> localhost <span class="number">9999</span></span><br></pre></td></tr></table></figure>
<p>然后集群实验, 👌 👉 指定–master或不指定都可以成功运行</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --class com<span class="class">.tongdun</span><span class="class">.bigdata</span><span class="class">.spark</span><span class="class">.intro</span><span class="class">.NetworkWordCount</span> /home/qihuang.zheng/spark-intro<span class="class">.jar</span> <span class="number">192.168</span>.<span class="number">6.52</span> <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">bin/spark-submit --master spark:<span class="comment">//dp0652:7077 --class com.tongdun.bigdata.spark.intro.NetworkWordCount /home/qihuang.zheng/spark-intro.jar 192.168.6.52 9999</span></span><br></pre></td></tr></table></figure>
<p><strong>关于本地和集群的运行方式</strong></p>
<p>下面是在本地和集群中运行NetworkWordCount几种方式的结果, ✅表示能正常统计,🙅表示没显示结果.</p>
<p>localhost(没有更改spark-env.sh)</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">✅ bin/run-example org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">🙅 bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master <span class="string">"local[2]"</span> --jars lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">🙅 bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master local\[<span class="number">2</span>\] --jars lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">✅ bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master <span class="string">"local[*]"</span> --jars lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> localhost <span class="number">9999</span></span><br></pre></td></tr></table></figure>
<p>cluster:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">🙅 bin/run-example org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">✅ bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master <span class="string">"local[2]"</span>  lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">✅ bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master local\[<span class="number">2</span>\] lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">✅ bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master <span class="string">"local[*]"</span> lib/spark-examples-<span class="number">1.4</span>.<span class="number">0</span>-hadoop2.<span class="number">6.0</span><span class="class">.jar</span> localhost <span class="number">9999</span></span><br><span class="line"></span><br><span class="line">🙅 bin/spark-submit --class org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.NetworkWordCount</span> --master spark:<span class="comment">//dp0652:7077 lib/spark-examples-1.4.0-hadoop2.6.0.jar localhost 9999</span></span><br></pre></td></tr></table></figure>
<h2 id="Kafka-SparkStreamming">Kafka-SparkStreamming</h2><p>如果是本地IDEA运行, 分别启动zookeeper,kafka,然后运行KafkaWordCountProducer,KafkaWordCount.<br>下面是在集群中的运行步骤:  </p>
<p>1.首先在kafka中创建一个topic:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0656 kafka]$ bin/kafka-topics.sh --zookeeper localhost:<span class="number">2181</span> --create --topic kafka-spark-test --replication-factor <span class="number">2</span> --partitions <span class="number">2</span></span><br><span class="line">Created topic <span class="string">"kafka-spark-test"</span>.</span><br><span class="line">[qihuang.zheng@dp0656 kafka]$ bin/kafka-topics.sh --zookeeper localhost:<span class="number">2181</span> --<span class="built_in">list</span></span><br><span class="line">kafka-spark-test</span><br><span class="line">[qihuang.zheng@dp0656 kafka]$ bin/kafka-topics.sh --zookeeper localhost:<span class="number">2181</span> --describe kafka-spark-test</span><br><span class="line">Topic:kafka-spark-test	PartitionCount:<span class="number">2</span>	ReplicationFactor:<span class="number">2</span>	Configs:</span><br><span class="line">	Topic: kafka-spark-test	Partition: <span class="number">0</span>	Leader: <span class="number">0</span>	Replicas: <span class="number">0</span>,<span class="number">1</span>	Isr: <span class="number">0</span>,<span class="number">1</span></span><br><span class="line">	Topic: kafka-spark-test	Partition: <span class="number">1</span>	Leader: <span class="number">1</span>	Replicas: <span class="number">1</span>,<span class="number">2</span>	Isr: <span class="number">1</span>,<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>2.运行kafka生产者模拟程序: 下面模拟了往kafka-spark-test队列中每秒发送2000个消息,每条消息的长度是70个数字.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ bin/spark-submit --<span class="keyword">class</span> org.apache.spark.examples.streaming.KafkaWordCountProducer /home/qihuang.zheng/spark-intro.jar <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">9092</span> kafka-spark-test <span class="number">2000</span> <span class="number">70</span></span><br><span class="line">Exception in thread <span class="string">"main"</span> java.lang.NoClassDefFoundError: org/apache/kafka/clients/producer/KafkaProducer</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>这是因为我们打包的方式没有把依赖包放进来. 用依赖包的方式就可以正常地在控制台输出kafka的模拟消息:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ bin/spark-submit --<span class="keyword">class</span> org.apache.spark.examples.streaming.KafkaWordCountProducer /home/qihuang.zheng/spark-intro-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">9092</span> kafka-spark-test <span class="number">2000</span> <span class="number">70</span></span><br></pre></td></tr></table></figure>
<p>3.运行KafkaWordCount实时流分析程序</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ bin/spark-submit --<span class="keyword">class</span> org.apache.spark.examples.streaming.KafkaWordCount /home/qihuang.zheng/spark-intro-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">2181</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">2181</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">2181</span> my-consumer-group kafka-spark-test <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>报错: 没有找到Kafka-SparkStreamming的相关jar包,因为我们在pom.xml中把<code>spark-streaming-kafka_2.10-1.4.0</code>也设置为了provided  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java<span class="class">.lang</span><span class="class">.NoClassDefFoundError</span>: org/apache/spark/streaming/kafka/KafkaUtils$</span><br><span class="line">	at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.KafkaWordCount</span>$.<span class="function"><span class="title">main</span><span class="params">(KafkaWordCount.scala:<span class="number">59</span>)</span></span></span><br><span class="line">	at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.examples</span><span class="class">.streaming</span><span class="class">.KafkaWordCount</span><span class="class">.main</span>(KafkaWordCount.scala)</span><br></pre></td></tr></table></figure>
<p>当然也可以把<code>spark-streaming-kafka_2.10-1.4.0</code>的scope去掉,然后重新编译. 不过还有一种办法:<br>下载<code>spark-streaming-kafka_2.10-1.4.0.jar</code>,在spark-submit中添加–jars选项把<code>spark-streaming-kafka_2.10-1.4.0.jar</code>加进来  </p>
<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>]$ bin/spark-submit --<span class="keyword">master</span> <span class="title">spark</span>://dp0652:<span class="number">7077</span> --class org.apache.spark.examples.streaming.KafkaWordCount --jars /home/qihuang.zheng/spark-streaming-kafka_2.<span class="number">10</span>-<span class="number">1.4</span>.<span class="number">0</span>.jar /home/qihuang.zheng/spark-intro-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar <span class="number">192.168</span>.<span class="number">6.55</span>:<span class="number">2181</span>,<span class="number">192.168</span>.<span class="number">6.56</span>:<span class="number">2181</span>,<span class="number">192.168</span>.<span class="number">6.57</span>:<span class="number">2181</span> my-consumer-<span class="keyword">group</span> <span class="title">kafka-spark-test</span> <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>4.KafkaWordCount的输出日志:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">09</span>:<span class="number">40</span>:<span class="number">11</span> WARN BlockManager: Block input-<span class="number">0</span>-<span class="number">1436233210800</span> replicated to only <span class="number">0</span> peer(s) instead of <span class="number">1</span> peers</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">09</span>:<span class="number">40</span>:<span class="number">11</span> WARN BlockManager: Block input-<span class="number">0</span>-<span class="number">1436233211000</span> replicated to only <span class="number">0</span> peer(s) instead of <span class="number">1</span> peers</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">09</span>:<span class="number">40</span>:<span class="number">12</span> WARN BlockManager: Block input-<span class="number">0</span>-<span class="number">1436233212000</span> replicated to only <span class="number">0</span> peer(s) instead of <span class="number">1</span> peers</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">1436233212000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(<span class="number">4</span>,<span class="number">139354</span>)</span><br><span class="line">(<span class="number">8</span>,<span class="number">140225</span>)</span><br><span class="line">(<span class="number">6</span>,<span class="number">140124</span>)</span><br><span class="line">(<span class="number">0</span>,<span class="number">140515</span>)</span><br><span class="line">(<span class="number">2</span>,<span class="number">139827</span>)</span><br><span class="line">(<span class="number">7</span>,<span class="number">140151</span>)</span><br><span class="line">(<span class="number">5</span>,<span class="number">139425</span>)</span><br><span class="line">(<span class="number">9</span>,<span class="number">140219</span>)</span><br><span class="line">(<span class="number">3</span>,<span class="number">140248</span>)</span><br><span class="line">(<span class="number">1</span>,<span class="number">139912</span>)</span><br></pre></td></tr></table></figure>
<p>5.Streaming-WebUI</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss2.png" alt=""></p>
<p>因为每秒2000条消息,每隔2秒统计一次,所以每次job的输入大小InputSize大概为2000*2=4000</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss3.png" alt=""></p>
<h2 id="关于打包">关于打包</h2><h3 id="IDEA打包">IDEA打包</h3><p>Project Structures | Artifacts | + | Jar | Empty | 填写jar包名称: spark-intro |<br>在Available Elements中选择项目名称下的’spark-intro’ compile output | 双击 | 就会到左侧的jar包下<br>Build | Build Artifacts | 选择刚刚填写的Artifact: spark-intro | Rebuild</p>
<h3 id="Maven打包">Maven打包</h3><p>因为是maven工程, 所以可以在工程下直接mvn package. 但是注意如果不配置maven的插件.则不会把依赖包打进去.<br>如果要把依赖包添加进去, 则要添加<code>maven-assembly-plugin</code>插件:  </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="title">descriptorRef</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">descriptorRefs</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">archive</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">manifest</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">mainClass</span>&gt;</span><span class="tag">&lt;/<span class="title">mainClass</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">manifest</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">archive</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="title">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="title">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="title">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="title">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="title">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="title">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="title">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>注意: 对于spark的相关jar依赖设置scope=provided</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-core_$&#123;scala.bin.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>spark-streaming_$&#123;scala.bin.version&#125;<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="title">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>然后在工程的根目录执行<code>mvn package</code>, 编译成功后会在target下生成2个文件.一个是有依赖的,一个是没有任何依赖的.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-intro git:(master) ✗ ll target</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  staff    <span class="number">16</span>M  <span class="number">7</span>  <span class="number">7</span> <span class="number">09</span>:<span class="number">22</span> spark-intro-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line">-rw-r--r--   <span class="number">1</span> zhengqh  staff   <span class="number">466</span>K  <span class="number">7</span>  <span class="number">7</span> <span class="number">09</span>:<span class="number">22</span> spark-intro-<span class="number">1.0</span>-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>
<p>我们把有依赖的jar包拷贝到spark集群中运行</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  <span class="tag">target</span> <span class="tag">git</span><span class="pseudo">:(master)</span> ✗ <span class="tag">scp</span> <span class="tag">spark-intro-1</span><span class="class">.0-SNAPSHOT-jar-with-dependencies</span><span class="class">.jar</span> <span class="tag">qihuang</span><span class="class">.zheng</span><span class="at_rule">@<span class="keyword">192.168.6.52:~/</span></span></span><br></pre></td></tr></table></figure>
<h2 id="Kafka-SparkStreamming-Redis">Kafka-SparkStreamming-Redis</h2><p>在192.168.6.52上启动redis, 注意要以后台方式启动, 并且用admin用户, 否则shutdown时会报错:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/<span class="operator"><span class="keyword">install</span>/redis-<span class="number">3.0</span><span class="number">.2</span></span><br><span class="line">sudo -u <span class="keyword">admin</span> src/redis-<span class="keyword">server</span> &amp;</span></span><br></pre></td></tr></table></figure>
<p>启动kafka生产者模拟程序</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --<span class="keyword">class</span> com.tongdun.bigdata.spark.intro.KafkaEventProducer /home/qihuang.zheng/spark-intro-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">9092</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">9092</span></span><br></pre></td></tr></table></figure>
<p>启动用户点击次数实时流统计,最终写到Redis中</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark://dp<span class="number">0652:7077</span> --class com.tongdun.bigdata.spark.intro.UserClickCountAnalytics --jars /home/qihuang.zheng/spark-streaming-kafka_<span class="number">2.10-1.4</span>.0.jar /home/qihuang.zheng/spark-intro-1.0-SNAPSHOT-jar-with-dependencies.jar spark://dp<span class="number">0652:7077</span> <span class="number">192.168.6.55</span>:<span class="number">9092,192.168</span>.<span class="number">6.56:9092</span>,<span class="number">192.168.6.57</span>:9092</span><br><span class="line"></span><br><span class="line">bin/spark-submit --master spark://dp<span class="number">0652:7077</span> --class com.tongdun.bigdata.spark.intro.UserClickCountAnalytics2 --jars /home/qihuang.zheng/spark-streaming-kafka_<span class="number">2.10-1.4</span>.0.jar /home/qihuang.zheng/spark-intro-1.0-SNAPSHOT-jar-with-dependencies.jar spark://dp<span class="number">0652:7077</span> user_events <span class="number">192.168.6.55</span>:<span class="number">9092,192.168</span>.<span class="number">6.56:9092</span>,<span class="number">192.168.6.57</span>:<span class="number">9092 192.168</span>.6.52</span><br></pre></td></tr></table></figure>
<p>使用客户端在本机中验证数据:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">src/redis-cli -h <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span></span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">6379</span>&gt; select <span class="number">1</span></span><br><span class="line"><span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">6379</span>[<span class="number">1</span>]&gt; HGETALL app::users::click</span><br><span class="line"> <span class="number">1</span>) <span class="string">"d7f141563005d1b5d0d3dd30138f3f62"</span></span><br><span class="line"> <span class="number">2</span>) <span class="string">"1891"</span></span><br><span class="line"> <span class="number">3</span>) <span class="string">"97edfc08311c70143401745a03a50706"</span></span><br><span class="line"> <span class="number">4</span>) <span class="string">"1814"</span></span><br><span class="line"> <span class="number">5</span>) <span class="string">"4A4D769EB9679C054DE81B973ED5D768"</span></span><br><span class="line"> <span class="number">6</span>) <span class="string">"1845"</span></span><br><span class="line"> <span class="number">7</span>) <span class="string">"a95f22eabc4fd4b580c011a3161a9d9d"</span></span><br><span class="line"> <span class="number">8</span>) <span class="string">"1835"</span></span><br><span class="line"> <span class="number">9</span>) <span class="string">"8dfeb5aaafc027d89349ac9a20b3930f"</span></span><br><span class="line"><span class="number">10</span>) <span class="string">"1821"</span></span><br><span class="line"><span class="number">11</span>) <span class="string">"6b67c8c700427dee7552f81f3228c927"</span></span><br><span class="line"><span class="number">12</span>) <span class="string">"1842"</span></span><br><span class="line"><span class="number">13</span>) <span class="string">"011BBF43B89BFBF266C865DF0397AA71"</span></span><br><span class="line"><span class="number">14</span>) <span class="string">"1819"</span></span><br><span class="line"><span class="number">15</span>) <span class="string">"f2a8474bf7bd94f0aabbd4cdd2c06dcf"</span></span><br><span class="line"><span class="number">16</span>) <span class="string">"1909"</span></span><br><span class="line"><span class="number">17</span>) <span class="string">"c8ee90aade1671a21336c721512b817a"</span></span><br><span class="line"><span class="number">18</span>) <span class="string">"1843"</span></span><br><span class="line"><span class="number">19</span>) <span class="string">"068b746ed4620d25e26055a9f804385f"</span></span><br><span class="line"><span class="number">20</span>) <span class="string">"1955"</span></span><br></pre></td></tr></table></figure>
<p>假设我们同时启动了两个UserClickCountAnalytics进程, 则第二个会进入等待状态. 只有当第一个进程杀掉后, 第二个进程才会开始运行.  </p>
<p>下图是处于等待的进程,除了InputRate有图像,其他三个都没有</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss4.png" alt=""></p>
<p>可以看到status为queued</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss5.png" alt=""></p>
<p>在等待进程的那个终端也可以看到如下输出</p>
<figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">[</span>Stage 0:&gt;                                                         <span class="function"> (</span>0 + 0<span class="function">)</span> / 2]15/07/07 12:05:25 WARN TaskSchedulerImpl: Initial job has<span class="instruction"> not </span>accepted any resources;<span class="instruction"> check </span>your cluster UI to ensure that workers are registered<span class="instruction"> and </span>have sufficient resources</span><br><span class="line">15/07/07 12:05:40 WARN TaskSchedulerImpl: Initial job has<span class="instruction"> not </span>accepted any resources;<span class="instruction"> check </span>your cluster UI to ensure that workers are registered<span class="instruction"> and </span>have sufficient resources</span><br><span class="line">15/07/07 12:05:55 WARN TaskSchedulerImpl: Initial job has<span class="instruction"> not </span>accepted any resources;<span class="instruction"> check </span>your cluster UI to ensure that workers are registered<span class="instruction"> and </span>have sufficient resources</span><br></pre></td></tr></table></figure>
<p>第一个正在运行的进程</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss6.png" alt=""></p>
<p>当我们把正在运行的杀掉, 这时候等待运行的就会接着执行</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/ss7.png" alt=""></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-07-05-Spark-SQL" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/05/2015-07-05-Spark-SQL/" class="article-date">
  	<time datetime="2015-07-04T16:00:00.000Z" itemprop="datePublished">2015-07-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/05/2015-07-05-Spark-SQL/">Spark SQL入门</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SparkSQL_Table_Operation_with_ParquetFile">SparkSQL Table Operation with ParquetFile</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sc is an existing SparkContext.</span></span><br><span class="line">val sqlContext = new org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.SQLContext</span>(sc)</span><br><span class="line"><span class="comment">// this is used to implicitly convert an RDD to a DataFrame.</span></span><br><span class="line">import sqlContext<span class="class">.implicits</span>._</span><br><span class="line"></span><br><span class="line"><span class="comment">// Define the schema using a case class.</span></span><br><span class="line"><span class="comment">// <span class="doctag">Note:</span> Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,</span></span><br><span class="line"><span class="comment">// you can use custom classes that implement the Product interface.</span></span><br><span class="line">case class <span class="function"><span class="title">Person</span><span class="params">(name: String, age: Int)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an RDD of Person objects and register it as a table.</span></span><br><span class="line">val people = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"/user/qihuang.zheng/sparktest/people.txt"</span>)</span></span>.<span class="function"><span class="title">map</span><span class="params">(_.split(<span class="string">","</span>)</span></span>).<span class="function"><span class="title">map</span><span class="params">(p =&gt; Person(p(<span class="number">0</span>)</span></span>, <span class="function"><span class="title">p</span><span class="params">(<span class="number">1</span>)</span></span><span class="class">.trim</span><span class="class">.toInt</span>)).<span class="function"><span class="title">toDF</span><span class="params">()</span></span></span><br><span class="line">people.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"people"</span>)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL statements can be run by using the sql methods provided by sqlContext.</span></span><br><span class="line">val teenagers = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// The results of SQL queries are DataFrames and support all the normal RDD operations.</span></span><br><span class="line"><span class="comment">// The columns of a row in the result can be accessed by field index:</span></span><br><span class="line">teenagers.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// or by field name:</span></span><br><span class="line">teenagers.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t.getAs[String](<span class="string">"name"</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]</span></span><br><span class="line">teenagers.<span class="function"><span class="title">map</span><span class="params">(_.getValuesMap[Any](List(<span class="string">"name"</span>, <span class="string">"age"</span>)</span></span>)).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span></span><br><span class="line"><span class="comment">// Map("name" -&gt; "Justin", "age" -&gt; 19)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// The RDD is implicitly converted to a DataFrame by implicits, allowing it to be stored using Parquet.</span></span><br><span class="line">people.<span class="function"><span class="title">saveAsParquetFile</span><span class="params">(<span class="string">"/user/qihuang.zheng/sparktest/people.parquet"</span>)</span></span></span><br><span class="line">people<span class="class">.write</span><span class="class">.parquet</span>(<span class="string">"/user/qihuang.zheng/sparktest/people2.parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read in the parquet file created above.  Parquet files are self-describing so the schema is preserved.</span></span><br><span class="line"><span class="comment">// The result of loading a Parquet file is also a DataFrame.</span></span><br><span class="line">val parquetFile = sqlContext<span class="class">.read</span><span class="class">.parquet</span>(<span class="string">"/user/qihuang.zheng/sparktest/people.parquet"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Parquet files can also be registered as tables and then used in SQL statements.</span></span><br><span class="line">parquetFile.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"parquetFile"</span>)</span></span></span><br><span class="line">val teenagers = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span></span></span><br><span class="line">teenagers.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// JOIN TABLE</span></span><br><span class="line">val jointbls = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"SELECT people.name FROM people join parquetFile where people.name=parquetFile.name"</span>)</span></span></span><br><span class="line">jointbls.<span class="function"><span class="title">map</span><span class="params">(t =&gt; <span class="string">"Name: "</span> + t(<span class="number">0</span>)</span></span>).<span class="function"><span class="title">collect</span><span class="params">()</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span></span><br></pre></td></tr></table></figure>
<h2 id="Action:_activity_table">Action: activity table</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val activityTestRDD = sqlContext<span class="class">.read</span><span class="class">.parquet</span>(<span class="string">"/user/lab/activity"</span>)</span><br><span class="line">scala&gt; activityTestRDD.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"activity_test"</span>)</span></span></span><br><span class="line">scala&gt; sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select count(*) from activity_test"</span>)</span></span>.<span class="function"><span class="title">take</span><span class="params">(<span class="number">1</span>)</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span></span><br><span class="line">[<span class="number">14543461</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select * from activity_test"</span>)</span></span>.<span class="function"><span class="title">take</span><span class="params">(<span class="number">1</span>)</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span></span><br><span class="line">[<span class="number">1420549464102</span>,<span class="number">1420549464331</span>,<span class="function"><span class="title">Map</span><span class="params">(eventType -&gt; Other, appName -&gt; nonobank, ...,Map( -&gt; )</span></span>,forseti-<span class="number">20150105</span>]</span><br><span class="line"></span><br><span class="line">scala&gt; sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select event_result_map from activity_test"</span>)</span></span>.<span class="function"><span class="title">take</span><span class="params">(<span class="number">1</span>)</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span></span><br><span class="line">[<span class="function"><span class="title">Map</span><span class="params">(eventType -&gt; Other, appType -&gt; web, timestamp -&gt; <span class="number">1420549464331</span>, ...)</span></span>]</span><br><span class="line"></span><br><span class="line">scala&gt; sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select event_result_map.policyList from activity_test"</span>)</span></span>.<span class="function"><span class="title">take</span><span class="params">(<span class="number">1</span>)</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span></span><br><span class="line">[[<span class="string">"反短信炸弹模型"</span>,<span class="string">"虚假号码"</span>]]</span><br><span class="line"></span><br><span class="line">scala&gt; sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select event_result_map.appName,event_result_map.eventType,event_result_map.riskStatus from activity_test"</span>)</span></span>.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"activity_event_result_map"</span>)</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select * from activity_event_result_map"</span>)</span></span>.<span class="function"><span class="title">take</span><span class="params">(<span class="number">10</span>)</span></span>.<span class="function"><span class="title">foreach</span><span class="params">(println)</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select appName,riskStatus,eventType,count(*) from activity_event_result_map group by appName,riskStatus,eventType order by appName,riskStatus,eventType"</span>)</span></span><span class="class">.collect</span><span class="class">.foreach</span>(println)</span><br><span class="line">[aixuedai,Accept,Loan,<span class="number">6594</span>]</span><br><span class="line"></span><br><span class="line">sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select appName,riskStatus,eventType,count(*) from activity_event_result_map group by appName,riskStatus,eventType order by appName,riskStatus,eventType"</span>)</span></span><span class="class">.collect</span><span class="class">.foreach</span>(println)</span><br><span class="line">val cacheResult = sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select appName,riskStatus,eventType,count(*) from activity_event_result_map group by appName,riskStatus,eventType order by appName,riskStatus,eventType"</span>)</span></span><span class="class">.cache</span></span><br><span class="line">cacheResult<span class="class">.collect</span><span class="class">.foreach</span>(println)</span><br></pre></td></tr></table></figure>
<p>如果在执行cache时内存不足,会退出当前shell,解决办法是在spark-shell命令前添加<code>SPARK_SUBMIT_OPTS=&quot;-XX:MaxPermSize=1g&quot;</code></p>
<h2 id="DataFrame_API">DataFrame API</h2><p>将parquet文件加载出来后,可以打印出它的schema,注意最后一个indice是partition-key</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">val activityTestRDD = sqlContext.read.parquet(<span class="string">"/user/lab/activity"</span>)</span><br><span class="line">scala&gt; activityTestRDD.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- <span class="string">sequence_id:</span> string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- <span class="string">occur_time:</span> <span class="typename">long</span> (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- <span class="string">activity_map:</span> map (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- <span class="string">key:</span> string</span><br><span class="line"> |    |-- <span class="string">value:</span> string (valueContainsNull = <span class="literal">true</span>)</span><br><span class="line"> |-- <span class="string">browser_map:</span> map (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- <span class="string">key:</span> string</span><br><span class="line"> |    |-- <span class="string">value:</span> string (valueContainsNull = <span class="literal">true</span>)</span><br><span class="line"> |-- <span class="string">device_map:</span> map (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- <span class="string">key:</span> string</span><br><span class="line"> |    |-- <span class="string">value:</span> string (valueContainsNull = <span class="literal">true</span>)</span><br><span class="line"> |-- <span class="string">event_result_map:</span> map (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- <span class="string">key:</span> string</span><br><span class="line"> |    |-- <span class="string">value:</span> string (valueContainsNull = <span class="literal">true</span>)</span><br><span class="line"> |-- <span class="string">geo_map:</span> map (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- <span class="string">key:</span> string</span><br><span class="line"> |    |-- <span class="string">value:</span> string (valueContainsNull = <span class="literal">true</span>)</span><br><span class="line"> |-- <span class="string">policy_map:</span> map (nullable = <span class="literal">true</span>)</span><br><span class="line"> |    |-- <span class="string">key:</span> string</span><br><span class="line"> |    |-- <span class="string">value:</span> string (valueContainsNull = <span class="literal">true</span>)</span><br><span class="line"> |-- <span class="string">indice:</span> string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
<p>DataFrame支持解析复杂的Map结构,用map.field就可以获取到key=field的value</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val event_result_map = activityTestRDD.<span class="function"><span class="title">select</span><span class="params">(<span class="string">"event_result_map.appName"</span>,<span class="string">"event_result_map.riskStatus"</span>,<span class="string">"event_result_map.eventType"</span>)</span></span></span><br><span class="line">event_result_map.<span class="function"><span class="title">take</span><span class="params">(<span class="number">10</span>)</span></span></span><br><span class="line">event_result_map.<span class="function"><span class="title">select</span><span class="params">(<span class="string">"appName"</span>)</span></span>.<span class="function"><span class="title">take</span><span class="params">(<span class="number">10</span>)</span></span></span><br><span class="line">event_result_map.<span class="function"><span class="title">select</span><span class="params">(<span class="string">"appName"</span>,<span class="string">"riskStatus"</span>,<span class="string">"eventType"</span>)</span></span>.<span class="function"><span class="title">take</span><span class="params">(<span class="number">10</span>)</span></span></span><br><span class="line">event_result_map.<span class="function"><span class="title">select</span><span class="params">(<span class="string">"appName"</span>,<span class="string">"riskStatus"</span>,<span class="string">"eventType"</span>)</span></span>.<span class="function"><span class="title">groupBy</span><span class="params">(<span class="string">"appName"</span>,<span class="string">"riskStatus"</span>,<span class="string">"eventType"</span>)</span></span><span class="class">.count</span><span class="class">.show</span></span><br></pre></td></tr></table></figure>
<p>可以使用缓存在第二次查询时, 速度会加快.  </p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val res = event<span class="emphasis">_result_</span>map.select("appName","riskStatus","eventType").groupBy("appName","riskStatus","eventType").count.cache</span><br><span class="line">res.show</span><br><span class="line"><span class="header">res.show</span><br><span class="line">+---------------+----------+---------+-------+</span></span><br><span class="line"><span class="header">|        appName|riskStatus|eventType|  count|</span><br><span class="line">+---------------+----------+---------+-------+</span></span><br><span class="line"><span class="header">|    niwodai_ios|    Review| Register|     77|</span><br><span class="line">+---------------+----------+---------+-------+</span></span><br></pre></td></tr></table></figure>
<p>但是上面只显示部分结果(只有20条). 完整的记录应该有174条</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; res</span><br><span class="line">res53: org.apache.spark.sql.DataFrame = [appName: string, riskStatus: string, eventType: string, count: bigint]</span><br><span class="line">scala&gt; res.count</span><br><span class="line">res54: Long = 174</span><br><span class="line"></span><br><span class="line"><span class="header">scala&gt; res.filter("appName = 'ppdai'").show</span><br><span class="line">+-------+----------+---------+-----+</span></span><br><span class="line"><span class="header">|appName|riskStatus|eventType|count|</span><br><span class="line">+-------+----------+---------+-----+</span></span><br><span class="line"><span class="header">|  ppdai|    Review|   Modify|  505|</span><br><span class="line">+-------+----------+---------+-----+</span></span><br></pre></td></tr></table></figure>
<p>那么<code>为什么DataFrame.show只返回部分结果呢?</code> TODO  </p>
<p>为了返回全部数据, 要用collect收集所有结果(collect会把所有Worker的结果都汇聚到Driver上返回给客户端)</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; res.<span class="function"><span class="title">filter</span><span class="params">(<span class="string">"appName = 'ppdai'"</span>)</span></span><span class="class">.collect</span><span class="class">.foreach</span>(println)</span><br><span class="line">[ppdai,Review,Modify,<span class="number">505</span>]</span><br></pre></td></tr></table></figure>
<p>上面只是返回一个合作方的所有统计指标, 时间只需要0.4秒左右. 要统计所有合作方,需要去掉前面的filter:  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res<span class="class">.collect</span><span class="class">.foreach</span>(println)</span><br></pre></td></tr></table></figure>
<p>不过速度会慢很多, 大概需要30秒. 而且多次查询都是这样的: <code>res缓存后使用collect貌似没有起作用??</code><br>如果使用orderBy,则也会加快查询速度,大概只需要0.9秒  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; res.<span class="function"><span class="title">orderBy</span><span class="params">(<span class="string">"appName"</span>,<span class="string">"riskStatus"</span>,<span class="string">"eventType"</span>)</span></span><span class="class">.collect</span><span class="class">.foreach</span>(println)</span><br></pre></td></tr></table></figure>
<p>DataFrame可以注册为Table然后在Table上查询, 查询所有记录也只需要0.4秒. SO, That’s Great!</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; res.<span class="function"><span class="title">registerTempTable</span><span class="params">(<span class="string">"activity_group"</span>)</span></span></span><br><span class="line">scala&gt; sqlContext.<span class="function"><span class="title">sql</span><span class="params">(<span class="string">"select * from activity_group"</span>)</span></span><span class="class">.collect</span><span class="class">.foreach</span>(println)</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">16</span>:<span class="number">39</span>:<span class="number">38</span> INFO scheduler<span class="class">.DAGScheduler</span>: Job <span class="number">62</span> finished: collect at &lt;console&gt;:<span class="number">20</span>, took <span class="number">0.390465</span> s</span><br><span class="line">[niwodai_ios,Review,Register,<span class="number">77</span>]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>DataFrame可以保存到HDFS上, 然后可以再从hdfs上加载上来查询,不过这样有点多余了.直接用上面的临时表|orderBy|filter会更快</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res.<span class="function"><span class="title">save</span><span class="params">(<span class="string">"/user/qihuang.zheng/sparktest/group3"</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>默认保存的是parquet文件, 但是<code>为什么小文件这么多?</code>  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 ~]$ /usr/install/hadoop/bin/hadoop fs -ls /user/qihuang.zheng/sparktest/group3</span><br><span class="line">Found <span class="number">203</span> items</span><br><span class="line">-rw-r--r--   <span class="number">3</span> qihuang.zheng supergroup          <span class="number">0</span> <span class="number">2015</span>-<span class="number">07</span>-<span class="number">02</span> <span class="number">16</span>:<span class="number">35</span> /user/qihuang.zheng/sparktest/group3/_SUCCESS</span><br><span class="line">-rw-r--r--   <span class="number">3</span> qihuang.zheng supergroup        <span class="number">462</span> <span class="number">2015</span>-<span class="number">07</span>-<span class="number">02</span> <span class="number">16</span>:<span class="number">35</span> /user/qihuang.zheng/sparktest/group3/_common_metadata</span><br><span class="line">-rw-r--r--   <span class="number">3</span> qihuang.zheng supergroup      <span class="number">44724</span> <span class="number">2015</span>-<span class="number">07</span>-<span class="number">02</span> <span class="number">16</span>:<span class="number">35</span> /user/qihuang.zheng/sparktest/group3/_metadata</span><br><span class="line">-rw-r--r--   <span class="number">3</span> qihuang.zheng supergroup       <span class="number">1014</span> <span class="number">2015</span>-<span class="number">07</span>-<span class="number">02</span> <span class="number">16</span>:<span class="number">35</span> /user/qihuang.zheng/sparktest/group3/part-r-<span class="number">00001.</span>gz.parquet</span><br><span class="line">-rw-r--r--   <span class="number">3</span> qihuang.zheng supergroup        <span class="number">999</span> <span class="number">2015</span>-<span class="number">07</span>-<span class="number">02</span> <span class="number">16</span>:<span class="number">35</span> /user/qihuang.zheng/sparktest/group3/part-r-<span class="number">00002.</span>gz.parquet</span><br><span class="line">-rw-r--r--   <span class="number">3</span> qihuang.zheng supergroup       <span class="number">1034</span> <span class="number">2015</span>-<span class="number">07</span>-<span class="number">02</span> <span class="number">16</span>:<span class="number">35</span> /user/qihuang.zheng/sparktest/group3/part-r-<span class="number">00003.</span>gz.parquet</span><br><span class="line">-rw-r--r--   <span class="number">3</span> qihuang.zheng supergroup        <span class="number">989</span> <span class="number">2015</span>-<span class="number">07</span>-<span class="number">02</span> <span class="number">16</span>:<span class="number">35</span> /user/qihuang.zheng/sparktest/group3/part-r-<span class="number">00004.</span>gz.parquet</span><br><span class="line">-rw-r--r--   <span class="number">3</span> qihuang.zheng supergroup       <span class="number">1041</span> <span class="number">2015</span>-<span class="number">07</span>-<span class="number">02</span> <span class="number">16</span>:<span class="number">35</span> /user/qihuang.zheng/sparktest/group3/part-r-<span class="number">00005.</span>gz.parquet</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>下面的三条语句分别生成的parquet文件的个数为: 203,5,5</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; res<span class="class">.write</span><span class="class">.save</span>(<span class="string">"/user/qihuang.zheng/sparktest/group0.parquet"</span>)</span><br><span class="line">scala&gt; res.<span class="function"><span class="title">orderBy</span><span class="params">(<span class="string">"appName"</span>)</span></span><span class="class">.write</span><span class="class">.save</span>(<span class="string">"/user/qihuang.zheng/sparktest/group1.parquet"</span>)</span><br><span class="line">scala&gt; res.<span class="function"><span class="title">orderBy</span><span class="params">(<span class="string">"appName"</span>,<span class="string">"riskStatus"</span>,<span class="string">"eventType"</span>)</span></span><span class="class">.write</span><span class="class">.save</span>(<span class="string">"/user/qihuang.zheng/sparktest/group3.parquet"</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Spark_&amp;_Hive">Spark &amp; Hive</h2><p>编译支持hive的spark</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn -Pyarn -Dyarn.version=<span class="number">2.6</span><span class="number">.0</span> -Phadoop-<span class="number">2.6</span> -Dhadoop.version=<span class="number">2.6</span><span class="number">.0</span> -Phive -Phive-<span class="number">0.13</span><span class="number">.1</span> -Phive-thriftserver -DskipTests clean package</span><br></pre></td></tr></table></figure>
<p>如果没有编译hive on spark,而是直接把hive-site.xml分发到spark集群的conf目录下,直接启动spark-sql会报错:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>]$ bin/spark-sql</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java<span class="class">.lang</span><span class="class">.RuntimeException</span>: java<span class="class">.io</span><span class="class">.IOException</span>: 权限不够</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.hive</span><span class="class">.ql</span><span class="class">.session</span><span class="class">.SessionState</span><span class="class">.start</span>(SessionState<span class="class">.java</span>:<span class="number">330</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.hive</span><span class="class">.thriftserver</span><span class="class">.SparkSQLCLIDriver</span>$.<span class="function"><span class="title">main</span><span class="params">(SparkSQLCLIDriver.scala:<span class="number">109</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.hive</span><span class="class">.thriftserver</span><span class="class">.SparkSQLCLIDriver</span><span class="class">.main</span>(SparkSQLCLIDriver.scala)</span><br><span class="line">    at sun<span class="class">.reflect</span><span class="class">.NativeMethodAccessorImpl</span><span class="class">.invoke0</span>(Native Method)</span><br><span class="line">    at sun<span class="class">.reflect</span><span class="class">.NativeMethodAccessorImpl</span><span class="class">.invoke</span>(NativeMethodAccessorImpl<span class="class">.java</span>:<span class="number">57</span>)</span><br><span class="line">    at sun<span class="class">.reflect</span><span class="class">.DelegatingMethodAccessorImpl</span><span class="class">.invoke</span>(DelegatingMethodAccessorImpl<span class="class">.java</span>:<span class="number">43</span>)</span><br><span class="line">    at java<span class="class">.lang</span><span class="class">.reflect</span><span class="class">.Method</span><span class="class">.invoke</span>(Method<span class="class">.java</span>:<span class="number">606</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.org<span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$deploy</span><span class="variable">$SparkSubmit</span>$<span class="variable">$runMain</span>(SparkSubmit<span class="class">.scala</span>:<span class="number">664</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.doRunMain$<span class="number">1</span>(SparkSubmit<span class="class">.scala</span>:<span class="number">169</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.<span class="function"><span class="title">submit</span><span class="params">(SparkSubmit.scala:<span class="number">192</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.<span class="function"><span class="title">main</span><span class="params">(SparkSubmit.scala:<span class="number">111</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span><span class="class">.main</span>(SparkSubmit.scala)</span><br><span class="line">Caused by: java<span class="class">.io</span><span class="class">.IOException</span>: 权限不够</span><br><span class="line">    at java<span class="class">.io</span><span class="class">.UnixFileSystem</span><span class="class">.createFileExclusively</span>(Native Method)</span><br><span class="line">    at java<span class="class">.io</span><span class="class">.File</span><span class="class">.createNewFile</span>(File<span class="class">.java</span>:<span class="number">1006</span>)</span><br><span class="line">    at java<span class="class">.io</span><span class="class">.File</span><span class="class">.createTempFile</span>(File<span class="class">.java</span>:<span class="number">1989</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.hive</span><span class="class">.ql</span><span class="class">.session</span><span class="class">.SessionState</span><span class="class">.createTempFile</span>(SessionState<span class="class">.java</span>:<span class="number">432</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.hive</span><span class="class">.ql</span><span class="class">.session</span><span class="class">.SessionState</span><span class="class">.start</span>(SessionState<span class="class">.java</span>:<span class="number">328</span>)</span><br><span class="line">    ... <span class="number">11</span> more</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">03</span> <span class="number">08</span>:<span class="number">42</span>:<span class="number">33</span> INFO util<span class="class">.Utils</span>: Shutdown hook called</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">03</span> <span class="number">08</span>:<span class="number">42</span>:<span class="number">33</span> INFO util<span class="class">.Utils</span>: Deleting directory /tmp/spark-<span class="number">831</span>ff199-cf80-<span class="number">4</span>d49-a22f-<span class="number">824736065289</span></span><br></pre></td></tr></table></figure>
<p>这是因为Spark集群的每个Worker都需要Hive的支持,而Worker节点并没有都安装了hive. 而且spark需要编译支持hive的包.<br>但是重新编译hive on spark要花很多时间,可不可以直接使用集群中已经安装好的hive呢?  YES!!<br><a href="http://lxw1234.com/archives/2015/06/294.htm" target="_blank" rel="external">http://lxw1234.com/archives/2015/06/294.htm</a><br><a href="http://shiyanjun.cn/archives/1113.html" target="_blank" rel="external">http://shiyanjun.cn/archives/1113.html</a><br><a href="http://www.cnblogs.com/hseagle/p/3758922.html" target="_blank" rel="external">http://www.cnblogs.com/hseagle/p/3758922.html</a>  </p>
<p>1.在spark-env.sh中添加</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/usr/install/apache-hive-<span class="number">0</span>.<span class="number">13.1</span>-bin</span><br><span class="line">export SPARK_CLASSPATH=<span class="variable">$HIVE</span>_HOME/lib/mysql-connector-java-<span class="number">5.1</span>.<span class="number">34</span>.jar:<span class="variable">$SPARK</span>_CLASSPATH</span><br></pre></td></tr></table></figure>
<p>2.将<code>apache-hive-0.13.1-bin</code>分发到集群中的每个节点(SparkWorker所在的节点)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd <span class="operator"><span class="keyword">install</span></span><br><span class="line">scp -r apache-hive-<span class="number">0.13</span><span class="number">.1</span>-<span class="keyword">bin</span> dp0653:/usr/<span class="keyword">install</span>/</span></span><br></pre></td></tr></table></figure>
<p>3.拷贝apache-hive-0.13.1-bin/conf/hive-site.xml到$SPARK_HOME/conf下</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp apache-hive-<span class="number">0.13</span><span class="number">.1</span>-bin/conf/hive-site.xml dp0653:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br></pre></td></tr></table></figure>
<p>4.重启spark集群</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sbin/<span class="keyword">stop</span>-<span class="keyword">all</span>.<span class="keyword">sh</span></span><br><span class="line">sbin/start-<span class="keyword">all</span>.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>
<p>5.测试spark-sql</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">03</span> <span class="number">10</span>:<span class="number">00</span>:<span class="number">56</span> WARN spark.<span class="string">SparkConf:</span> Setting <span class="string">'spark.executor.extraClassPath'</span> to <span class="string">'/usr/install/apache-hive-0.13.1-bin/lib/mysql-connector-java-5.1.34.jar:'</span> <span class="keyword">as</span> a work-around.</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">03</span> <span class="number">10</span>:<span class="number">00</span>:<span class="number">56</span> WARN spark.<span class="string">SparkConf:</span> Setting <span class="string">'spark.driver.extraClassPath'</span> to <span class="string">'/usr/install/apache-hive-0.13.1-bin/lib/mysql-connector-java-5.1.34.jar:'</span> <span class="keyword">as</span> a work-around.</span><br><span class="line"></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">03</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span> INFO hive.<span class="string">metastore:</span> Trying to connect to metastore with URI <span class="string">thrift:</span><span class="comment">//192.168.6.53:9083</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">03</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span> INFO hive.<span class="string">metastore:</span> Connected to metastore.</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">03</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span> INFO session.<span class="string">SessionState:</span> No Tez session required at <span class="keyword">this</span> point. hive.execution.engine=mr.</span><br><span class="line">SET spark.sql.hive.version=<span class="number">0.13</span><span class="number">.1</span></span><br><span class="line">SET spark.sql.hive.version=<span class="number">0.13</span><span class="number">.1</span></span><br><span class="line"></span><br><span class="line">spark-sql&gt; show databases;</span><br><span class="line"><span class="keyword">default</span></span><br><span class="line">test</span><br><span class="line">spark-sql&gt; use test;</span><br><span class="line">spark-sql&gt; show tables;</span><br><span class="line">koudai  <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">spark-sql&gt; select count(*) from koudai;</span><br><span class="line"><span class="number">311839</span></span><br></pre></td></tr></table></figure>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark-perf.png" alt=""></p>
<h2 id="SparkSQL_&amp;_thrift">SparkSQL &amp; thrift</h2><p>直接用bin/spark-sql启动:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 ~]$ jps -lm</span><br><span class="line"><span class="number">35146</span> org.apache.spark.deploy.SparkSubmit --master spark:<span class="comment">//192.168.6.52:7078 --class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver spark-internal</span></span><br><span class="line"><span class="number">35668</span> sun.tools.jps.Jps -lm</span><br><span class="line">[qihuang.zheng@dp0653 ~]$ ps -ef | grep SparkSQL</span><br><span class="line"><span class="number">506</span>      <span class="number">35146</span> <span class="number">35011</span> <span class="number">26</span> <span class="number">09</span>:<span class="number">16</span> pts/<span class="number">15</span>   <span class="number">00</span>:<span class="number">00</span>:<span class="number">19</span> /usr/java/jdk1<span class="number">.7</span><span class="number">.0</span>_51/bin/java ... org.apache.spark.deploy.SparkSubmit --master spark:<span class="comment">//192.168.6.52:7078 --class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver spark-internal</span></span><br></pre></td></tr></table></figure>
<p>hive在53上, 查看thrift服务:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 ~]$ ps -ef | grep thrift</span><br><span class="line">admin    <span class="number">28541</span>     <span class="number">1</span>  <span class="number">0</span> Aug19 ?        <span class="number">00</span>:<span class="number">02</span>:<span class="number">47</span> /usr/java/jdk1<span class="number">.7</span><span class="number">.0</span>_51/bin/java ... org.apache.hadoop.util.RunJar /usr/install/apache-hive-<span class="number">1.2</span><span class="number">.0</span>-bin/lib/hive-service-<span class="number">1.2</span><span class="number">.0</span>.jar org.apache.hive.service.server.HiveServer2 --hiveconf hive.metastore.uris=thrift:<span class="comment">//192.168.6.53:9083 --hiveconf hive.metastore.local=false --hiveconf hive.server2.thrift.bind.host=192.168.6.53 --hiveconf hive.server2.thrift.port=10001</span></span><br></pre></td></tr></table></figure>
<p>启动spark的thrift server:  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo sbin/start-thriftserver<span class="class">.sh</span> --master spark:<span class="comment">//192.168.6.52:7078</span></span><br><span class="line">starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.hive</span><span class="class">.thriftserver</span><span class="class">.HiveThriftServer2</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">1</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.hive</span><span class="class">.thriftserver</span><span class="class">.HiveThriftServer2-1-dp0652</span><span class="class">.out</span></span><br></pre></td></tr></table></figure>
<p>在启动thrift-server的时候, 指定master, 会在master的web ui上看到app. 但是启动完成后, app就结束了.<br>根据日志信息, 由于没有正确指定端口,导致无法连接   </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> INFO hive.metastore: Trying to connect to metastore with URI thrift:<span class="comment">//192.168.6.53:9083</span></span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> INFO hive.metastore: Connected to metastore.</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> INFO service.AbstractService: Service:ThriftBinaryCLIService is started.</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> INFO service.AbstractService: Service:HiveServer2 is started.</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> INFO thriftserver.HiveThriftServer2: HiveThriftServer2 started</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> WARN conf.HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  Use hive.hmshandler.retry.* instead</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> ERROR thrift.ThriftCLIService: Error:</span><br><span class="line">org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address /<span class="number">192.168</span><span class="number">.47</span><span class="number">.213</span>:<span class="number">10000.</span></span><br><span class="line">        at org.apache.thrift.transport.TServerSocket.&lt;init&gt;(TServerSocket.java:<span class="number">93</span>)</span><br><span class="line">        at org.apache.thrift.transport.TServerSocket.&lt;init&gt;(TServerSocket.java:<span class="number">79</span>)</span><br><span class="line">        at org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(HiveAuthFactory.java:<span class="number">236</span>)</span><br><span class="line">        at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:<span class="number">69</span>)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:<span class="number">744</span>)</span><br></pre></td></tr></table></figure>
<p>指定hive的端口:   </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sbin/start-thriftserver<span class="class">.sh</span> \</span><br><span class="line">  --hiveconf hive<span class="class">.server2</span><span class="class">.thrift</span><span class="class">.port</span>=<span class="number">10001</span> \</span><br><span class="line">  --hiveconf hive<span class="class">.server2</span><span class="class">.thrift</span><span class="class">.bind</span><span class="class">.host</span>=<span class="number">192.168</span>.<span class="number">6.53</span> \</span><br><span class="line">  --master spark:<span class="comment">//192.168.6.52:7078</span></span><br></pre></td></tr></table></figure>
<p>查看thrift进程:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span>]$ ps -ef | grep thrift</span><br><span class="line">root     <span class="number">24997</span>     <span class="number">1</span> <span class="number">99</span> <span class="number">09</span>:<span class="number">55</span> pts/<span class="number">0</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">12</span> /usr/java/jdk1<span class="number">.7</span><span class="number">.0</span>_51/bin/java .. org.apache.spark.deploy.SparkSubmit --master spark:<span class="comment">//192.168.6.52:7078 --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 spark-internal --hiveconf hive.server2.thrift.port=10001 --hiveconf hive.server2.thrift.bind.host=192.168.6.53</span></span><br></pre></td></tr></table></figure>
<p>但是日志还是报错:<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">55</span>:<span class="number">42</span> ERROR thrift.ThriftCLIService: Error:</span><br><span class="line">org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address /<span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">10001.</span></span><br></pre></td></tr></table></figure></p>
<p>过了几秒,再次查看thrift进程, 找不到HiveThriftServer2了!</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">qihuang.zheng<span class="annotation">@dp</span>0652 spark-<span class="number">1.4</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span>]$ bin<span class="regexp">/beeline -u jdbc:hive2:/</span>/<span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">10001</span></span><br><span class="line">scan complete <span class="keyword">in</span> <span class="number">3</span>ms</span><br><span class="line">Connecting to <span class="string">jdbc:</span><span class="string">hive2:</span><span class="comment">//192.168.6.53:10001</span></span><br><span class="line">Connected <span class="string">to:</span> Apache Hive (version <span class="number">1.2</span><span class="number">.0</span>)</span><br><span class="line"><span class="string">Driver:</span> Spark Project Core (version <span class="number">1.4</span><span class="number">.1</span>)</span><br><span class="line">Transaction <span class="string">isolation:</span> TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version <span class="number">1.4</span><span class="number">.1</span> by Apache Hive</span><br><span class="line"><span class="number">0</span>: <span class="string">jdbc:</span><span class="string">hive2:</span><span class="comment">//192.168.6.53:10001&gt; show tables;</span></span><br></pre></td></tr></table></figure>
<h2 id="spark-sql后台运行">spark-sql后台运行</h2><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">nohup /usr/install/spark-<span class="number">1</span><span class="number">.4</span><span class="number">.1</span>-bin-hadoop<span class="number">2</span><span class="number">.4</span>/bin/spark-sql \</span><br><span class="line">--master spark:<span class="comment">//192.168.47.213:7077,192.168.47.216:7077 \</span><br><span class="line">--executor-memory 8g --total-executor-cores 20 --driver-memory 8g \</span><br><span class="line">-e "</span><br><span class="line">SET spark.kryoserializer.buffer.mb=1024;</span></span><br><span class="line"></span><br><span class="line">cache table consumer_loan as</span><br><span class="line"><span class="keyword">SELECT</span> activity_map[<span class="string">'eventOccurTime'</span>] as eventOccurTime,</span><br><span class="line">                      activity_map[<span class="string">'accountLogin'</span>] as accountLogin,</span><br><span class="line">                      activity_map[<span class="string">'accountMobile'</span>] as accountMobile,</span><br><span class="line">                      activity_map[<span class="string">'idNumber'</span>] as idNumber,</span><br><span class="line">                      activity_map[<span class="string">'accountEmail'</span>] as accountEmail,</span><br><span class="line">                      activity_map[<span class="string">'accountPhone'</span>] as accountPhone,</span><br><span class="line">                      activity_map[<span class="string">'qqNumber'</span>] as qqNumber,</span><br><span class="line">                      activity_map[<span class="string">'cardNumber'</span>] as cardNumber,</span><br><span class="line">                      device_map[<span class="string">'deviceId'</span>] as deviceId,</span><br><span class="line">                      geo_map[<span class="string">'ipAddress'</span>] as ipAddress,</span><br><span class="line">                      geo_map[<span class="string">'trueIpAddress'</span>] as trueIpAddress,</span><br><span class="line">                      activity_map[<span class="string">'eventType'</span>] as eventType,</span><br><span class="line">                      event_result_map[<span class="string">'ruleList'</span>] as ruleList</span><br><span class="line">FROM activity</span><br><span class="line">WHERE activity_map[<span class="string">'partnerCode'</span>] in (<span class="string">'aixuedai'</span>,<span class="string">'fenqile'</span>) and activity_map[<span class="string">'eventType'</span>] = <span class="string">'Loan'</span>;</span><br><span class="line"></span><br><span class="line">cache table consumer_register as </span><br><span class="line"><span class="keyword">SELECT</span> activity_map[<span class="string">'eventOccurTime'</span>] as eventOccurTime,</span><br><span class="line">                      activity_map[<span class="string">'accountLogin'</span>] as accountLogin,</span><br><span class="line">                      activity_map[<span class="string">'accountMobile'</span>] as accountMobile,</span><br><span class="line">                      activity_map[<span class="string">'idNumber'</span>] as idNumber,</span><br><span class="line">                      activity_map[<span class="string">'accountEmail'</span>] as accountEmail,</span><br><span class="line">                      activity_map[<span class="string">'accountPhone'</span>] as accountPhone,</span><br><span class="line">                      activity_map[<span class="string">'qqNumber'</span>] as qqNumber,</span><br><span class="line">                      activity_map[<span class="string">'cardNumber'</span>] as cardNumber,</span><br><span class="line">                      device_map[<span class="string">'deviceId'</span>] as deviceId,</span><br><span class="line">                      geo_map[<span class="string">'ipAddress'</span>] as ipAddress,</span><br><span class="line">                      geo_map[<span class="string">'trueIpAddress'</span>] as trueIpAddress,</span><br><span class="line">                      activity_map[<span class="string">'eventType'</span>] as eventType,</span><br><span class="line">                      event_result_map[<span class="string">'ruleList'</span>] as ruleList</span><br><span class="line">   FROM activity</span><br><span class="line">   WHERE activity_map[<span class="string">'partnerCode'</span>] in (<span class="string">'aixuedai'</span>,<span class="string">'fenqile'</span>) and activity_map[<span class="string">'eventType'</span>] = <span class="string">'Register'</span>;</span><br><span class="line"></span><br><span class="line">select * from consumer_loan</span><br><span class="line"></span><br><span class="line">UNION ALL</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> a.* FROM consumer_register a</span><br><span class="line">JOIN (<span class="keyword">SELECT</span> deviceId FROM consumer_loan) b </span><br><span class="line"><span class="constant">ON</span> a.deviceId = b.deviceId;</span><br><span class="line"></span><br><span class="line"><span class="string">" &gt; ./consume_loan/consumer_loan.csv 2&gt; ./consume_loan/log_loan &amp;</span></span><br></pre></td></tr></table></figure>
<h2 id="Spark-SQL配置参数">Spark-SQL配置参数</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/usr/<span class="operator"><span class="keyword">install</span>/spark-<span class="number">1.4</span><span class="number">.1</span>-<span class="keyword">bin</span>-hadoop2<span class="number">.4</span>/<span class="keyword">bin</span>/spark-shell <span class="comment">--conf spark.driver.maxResultSize=2g --conf spark.executor.memory=512m                              </span></span><br><span class="line"></span><br><span class="line">/usr/<span class="keyword">install</span>/spark-<span class="number">1.4</span><span class="number">.1</span>-<span class="keyword">bin</span>-hadoop2<span class="number">.4</span>/<span class="keyword">bin</span>/spark-shell <span class="comment">--driver-java-options -Dspark.driver.maxResultSize=2g</span></span><br><span class="line">scala&gt; sc.getConf.<span class="keyword">get</span>(<span class="string">"spark.driver.maxResultSize"</span>)</span><br><span class="line">res1: <span class="keyword">String</span> = <span class="number">2</span><span class="keyword">g</span></span><br><span class="line"></span><br><span class="line">/usr/<span class="keyword">install</span>/spark-<span class="number">1.4</span><span class="number">.1</span>-<span class="keyword">bin</span>-hadoop2<span class="number">.4</span>/<span class="keyword">bin</span>/spark-<span class="keyword">sql</span> <span class="comment">--driver-java-options -Dspark.driver.maxResultSize=2g</span></span><br><span class="line">/usr/<span class="keyword">install</span>/spark-<span class="number">1.4</span><span class="number">.1</span>-<span class="keyword">bin</span>-hadoop2<span class="number">.4</span>/<span class="keyword">bin</span>/spark-<span class="keyword">sql</span> <span class="comment">--conf spark.driver.maxResultSize=2g</span></span><br><span class="line"></span><br><span class="line">/usr/<span class="keyword">install</span>/spark-<span class="number">1.4</span><span class="number">.1</span>-<span class="keyword">bin</span>-hadoop2<span class="number">.4</span>/<span class="keyword">bin</span>/spark-<span class="keyword">sql</span> -h</span><br><span class="line"><span class="keyword">Usage</span>: ./<span class="keyword">bin</span>/spark-<span class="keyword">sql</span> [options] [cli <span class="keyword">option</span>]</span><br><span class="line">  <span class="comment">--driver-java-options       Extra Java options to pass to the driver.</span></span><br><span class="line">  <span class="comment">--conf PROP=VALUE           Arbitrary Spark configuration property.</span></span><br><span class="line">  <span class="comment">--properties-file FILE      Path to a file from which to load extra properties. If not</span></span><br><span class="line">                              specified, this will look <span class="keyword">for</span> conf/spark-<span class="keyword">defaults</span>.conf.</span></span><br></pre></td></tr></table></figure>
<h2 id="Tips_&amp;_TODO">Tips &amp; TODO</h2><ol>
<li>在做指标分析时,如果是每天或者每周这样的统计间隔,可以将分析后的结果保存成Persistent Table或者save到HDFS上供别人使用.  </li>
<li>线上的数据一般都比较多,查询时可以使用partition分区</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-07-05-Apache-Spark" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/07/05/2015-07-05-Apache-Spark/" class="article-date">
  	<time datetime="2015-07-04T16:00:00.000Z" itemprop="datePublished">2015-07-05</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/07/05/2015-07-05-Apache-Spark/">Apache Spark入门</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <p><a href="http://spark.apache.org/docs/latest/quick-start.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/quick-start.html</a></p>
<h2 id="Spark_Shell">Spark Shell</h2><p>➜  spark-1.4.0-bin-hadoop2.6  <code>bin/spark-shell</code></p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  '_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   <span class="keyword">version</span> 1.4.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using <span class="keyword">Scala</span> <span class="keyword">version</span> 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_25)</span><br><span class="line">15/06/28 10:36:07 INFO ui.SparkUI: Started SparkUI at http:<span class="comment">//127.0.0.1:4040</span></span><br><span class="line">15/06/28 10:36:07 INFO repl.SparkILoop: Created spark context..</span><br><span class="line">Spark context available <span class="keyword">as</span> <span class="keyword">sc</span>.</span><br><span class="line">15/06/28 10:36:08 INFO hive.HiveContext: Initializing execution hive, <span class="keyword">version</span> 0.13.1</span><br><span class="line">15/06/28 10:36:23 INFO repl.SparkILoop: Created sql context (with Hive support)..</span><br><span class="line">SQL context available <span class="keyword">as</span> sqlContext.</span><br></pre></td></tr></table></figure>
<h2 id="Basic_RDD_Operation">Basic RDD Operation</h2><p>第一个例子: 统计一个文本文件的单词数量.<br>调用sc的textFile(fileName)会生成一个MapPartitionsRDD  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textFile = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"README.md"</span>)</span></span></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: <span class="function"><span class="title">ensureFreeSpace</span><span class="params">(<span class="number">63424</span>)</span></span> called with curMem=<span class="number">0</span>, maxMem=<span class="number">278019440</span></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: Block broadcast_0 stored as values <span class="keyword">in</span> memory (estimated size <span class="number">61.9</span> KB, free <span class="number">265.1</span> MB)</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: <span class="function"><span class="title">ensureFreeSpace</span><span class="params">(<span class="number">20061</span>)</span></span> called with curMem=<span class="number">63424</span>, maxMem=<span class="number">278019440</span></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: Block broadcast_0_piece0 stored as bytes <span class="keyword">in</span> memory (estimated size <span class="number">19.6</span> KB, free <span class="number">265.1</span> MB)</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.BlockManagerInfo</span>: Added broadcast_0_piece0 <span class="keyword">in</span> memory on localhost:<span class="number">58638</span> (size: <span class="number">19.6</span> KB, free: <span class="number">265.1</span> MB)</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO spark<span class="class">.SparkContext</span>: Created broadcast <span class="number">0</span> from textFile at &lt;console&gt;:<span class="number">21</span></span><br><span class="line">textFile: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br></pre></td></tr></table></figure>
<p>调用上面生成的textFile RDD的count()会触发一个Action.  </p>
<figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.count()</span><br><span class="line">java.net.ConnectException: Call <span class="keyword">From</span> hadoop/<span class="number">127.0</span>.<span class="number">0.1</span> <span class="keyword">to</span> localhost:<span class="number">9000</span> failed <span class="keyword">on</span> connection exception: java.net.ConnectException: 拒绝连接; <span class="keyword">For</span> more details see:  http:<span class="comment">//wiki.apache.org/hadoop/ConnectionRefused</span></span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native <span class="function"><span class="keyword">Method</span>)</span><br><span class="line">    ...</span><br><span class="line"><span class="title">Caused</span> <span class="title">by</span>:</span> java.net.ConnectException: 拒绝连接</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.checkConnect(Native <span class="function"><span class="keyword">Method</span>)</span><br><span class="line">	...</span></span><br></pre></td></tr></table></figure>
<p>由于本机已经安装了Hadoop,使用的是伪分布式模式,所以Spark会读取Hadoop的配置信息.<br>我们这里先不启动Hadoop,使用本地模式,要手动添加file:///并使用绝对路径读取文本文件  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile</span><br><span class="line">res1: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br></pre></td></tr></table></figure>
<p>重新构造读取本地文本文件的textFile RDD</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textFile = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"file:///home/hadoop/soft/spark-1.4.0-bin-hadoop2.6/README.md"</span>)</span></span></span><br><span class="line">textFile: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">3</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br></pre></td></tr></table></figure>
<p>触发RDD的Action: count  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.count()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">44</span>:<span class="number">07</span> INFO scheduler.DAGScheduler: Job <span class="number">0</span> finished: count at &lt;console&gt;:<span class="number">24</span>, took <span class="number">0.275609</span> s</span><br><span class="line">res2: Long = <span class="number">98</span></span><br></pre></td></tr></table></figure>
<p>又一个Action RDD : 输出文本文件的第一行  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.first()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">44</span>:<span class="number">27</span> INFO scheduler.DAGScheduler: Job <span class="number">1</span> finished: first at &lt;console&gt;:<span class="number">24</span>, took <span class="number">0.017917</span> s</span><br><span class="line">res3: String = <span class="preprocessor"># Apache Spark</span></span><br></pre></td></tr></table></figure>
<h2 id="More_RDD_Operations">More RDD Operations</h2><p>1.统计包含了Spark这个单词一共有几行</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val linesWithSpark = textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>)</span><br><span class="line">linesWithSpark: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">4</span>] at <span class="attribute">filter</span> at &lt;console&gt;:<span class="number">23</span></span><br><span class="line">scala&gt; textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>).<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<p>2.文本文件中长度最长的那一行,它一共有多少个单词</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.<span class="function"><span class="title">map</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>.size).<span class="function"><span class="title">reduce</span><span class="params">((a, b)</span></span> =&gt; <span class="keyword">if</span> (<span class="tag">a</span> &gt; b) <span class="tag">a</span> <span class="keyword">else</span> b)</span><br></pre></td></tr></table></figure>
<p>3.MapReduce WordCount</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val wordCounts = textFile.<span class="function"><span class="title">flatMap</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>).<span class="function"><span class="title">map</span><span class="params">(word =&gt; (word, <span class="number">1</span>)</span></span>).<span class="function"><span class="title">reduceByKey</span><span class="params">((a, b)</span></span> =&gt; <span class="tag">a</span> + b)</span><br><span class="line">wordCounts: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[(String, Int)] = ShuffledRDD[<span class="number">9</span>] at reduceByKey at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt;  wordCounts.<span class="function"><span class="title">collect</span><span class="params">()</span></span></span><br><span class="line">res6: Array[(String, Int)] = <span class="function"><span class="title">Array</span><span class="params">((package,<span class="number">1</span>)</span></span>, (this,<span class="number">1</span>), (Version<span class="string">"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (cluster.,1), (its,1), ([run,1), (general,2), (have,1), (pre-built,1), (locally.,1), (locally,2), (changed,1), (sc.parallelize(1,1), (only,1), (several,1), (This,2), (basic,1), (Configuration,1), (learning,,1), (documentation,3), (YARN,,1), (graph,1), (Hive,2), (first,1), (["</span>Specifying,<span class="number">1</span>), (<span class="string">"yarn-client"</span>,<span class="number">1</span>), (page](http:<span class="comment">//spark.apache.org/documentation.html),1), ([params]`.,1), (application,1), ([project,2), (prefer,1), (SparkPi,2), (&lt;http://spark.apache.org/&gt;,1), (engine,1), (version,1), (file,1), (documentation,,1), (MASTER,1), (example,3), (distribution.,1), (are,1), (params,1), (scala&gt;,1), (systems.,1...</span></span><br></pre></td></tr></table></figure>
<p>4.Cache</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  linesWithSpark.cache()</span><br><span class="line">res7: linesWithSpark.type = MapPartitionsRDD[<span class="number">4</span>] at filter at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">47</span>:<span class="number">11</span> INFO scheduler.DAGScheduler: Job <span class="number">5</span> finished: count at &lt;console&gt;:<span class="number">26</span>, took <span class="number">0.054036</span> s</span><br><span class="line">res8: Long = <span class="number">19</span></span><br><span class="line"></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">47</span>:<span class="number">14</span> INFO scheduler.DAGScheduler: Job <span class="number">6</span> finished: count at &lt;console&gt;:<span class="number">26</span>, took <span class="number">0.016638</span> s</span><br><span class="line">res9: Long = <span class="number">19</span></span><br></pre></td></tr></table></figure>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">val textFile = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"file:///home/hadoop/soft/spark-1.4.0-bin-hadoop2.6/README.md"</span>)</span></span></span><br><span class="line">〇 textFile.<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br><span class="line">① textFile.<span class="function"><span class="title">first</span><span class="params">()</span></span></span><br><span class="line">val linesWithSpark = textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>)</span><br><span class="line">② textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>).<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br><span class="line">textFile.<span class="function"><span class="title">map</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>.size).<span class="function"><span class="title">reduce</span><span class="params">((a, b)</span></span> =&gt; <span class="keyword">if</span> (<span class="tag">a</span> &gt; b) <span class="tag">a</span> <span class="keyword">else</span> b)</span><br><span class="line">③ val wordCounts = textFile.<span class="function"><span class="title">flatMap</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>).<span class="function"><span class="title">map</span><span class="params">(word =&gt; (word, <span class="number">1</span>)</span></span>).<span class="function"><span class="title">reduceByKey</span><span class="params">((a, b)</span></span> =&gt; <span class="tag">a</span> + b)</span><br><span class="line">④ wordCounts.<span class="function"><span class="title">collect</span><span class="params">()</span></span></span><br><span class="line">linesWithSpark.<span class="function"><span class="title">cache</span><span class="params">()</span></span></span><br><span class="line">⑤ linesWithSpark.<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br><span class="line">⑥ linesWithSpark.<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br><span class="line">⑦ linesWithSpark.<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<h2 id="Spark_Shell_UI">Spark Shell UI</h2><p><a href="http://127.0.0.1:4040" target="_blank" rel="external">http://127.0.0.1:4040</a></p>
<h3 id="Jobs,_Stages,_Storage">Jobs, Stages, Storage</h3><p>Jobs: 上面每个Action RDD编号对应了下图中的Job Id.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark1.png" alt=""></p>
<p>Stages: 上面有8个Job, 但是Stages多了一个. 其实是④的<code>collect</code>有两个stage  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark2.png" alt=""></p>
<p>Storage: 在Cache的时候才有</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark3.png" alt=""></p>
<h3 id="查看Stage">查看Stage</h3><p>在Jobs中点击Job Id=4的collect RDD(输出WordCount的结果). 在下方的列表中可以看到有2个Stages<br>仔细观察列表的最后面两列, 分别是Shuffle Read和Shuffle Write.<br>其中map会进行Shuffle Write, collect会进行Shuffle Read</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark4.png" alt=""></p>
<p>点击Stage Id=4的map. 它的DAG可视化图和上面的概览图的左侧是一样的</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark5.png" alt=""></p>
<p>Spark的WebUI还提供了一个EventTime,可以很清楚地看到每个阶段消耗的时间</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark7.png" alt=""></p>
<p>回退,点击Stage Id=5的collect</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark6.png" alt=""></p>
<hr>
<h2 id="Spark_Standalone_集群安装">Spark Standalone 集群安装</h2><p>准备工作:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>master无密码ssh到slaves(将master的pub追加到所有slaves的authorized_keys)</span><br><span class="line"><span class="number">2.</span>关闭所有节点的防火墙(chkconfig iptables off)</span><br><span class="line"><span class="number">3.</span>安装scala-<span class="number">2.10</span>,并设置~/.bashrc</span><br></pre></td></tr></table></figure>
<p>cd $SPARK_HOME<br>vi conf/spark-env.sh  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> JAVA_HOME=/usr/java/jdk1<span class="number">.7</span><span class="number">.0</span>_51</span><br><span class="line"><span class="keyword">export</span> SCALA_HOME=/usr/install/scala-<span class="number">2.10</span><span class="number">.5</span></span><br><span class="line"><span class="keyword">export</span> HADOOP_HOME=/usr/install/hadoop</span><br><span class="line"><span class="keyword">export</span> HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_IP=dp0652</span><br><span class="line"><span class="keyword">export</span> MASTER=spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="preprocessor">#export SPARK_LOCAL_IP=dp0652</span></span><br><span class="line"><span class="keyword">export</span> SPARK_LOCAL_DIRS=/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_WEBUI_PORT=<span class="number">8082</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_PORT=<span class="number">7077</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_CORES=<span class="number">1</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_INSTANCES=<span class="number">1</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_MEMORY=<span class="number">8</span>g</span><br></pre></td></tr></table></figure>
<p>vi conf/slaves</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">dp0652</span></span><br><span class="line">dp0653</span><br><span class="line">dp0655</span><br><span class="line">dp0656</span><br><span class="line">dp0657</span><br></pre></td></tr></table></figure>
<p>将spark目录分发到集群的其他节点</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0653:/usr/install</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0655:/usr/install</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0656:/usr/install</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0657:/usr/install</span><br></pre></td></tr></table></figure>
<p>由于集群中dp0652和dp0653的内存比较大, 我们修改了这两个节点的spark-env.sh  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> SPARK_WORKER_INSTANCES=<span class="number">2</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_MEMORY=<span class="number">20</span>g</span><br></pre></td></tr></table></figure>
<p>启动集群, 在master上启动即可.  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>]$ sbin/start-all<span class="class">.sh</span></span><br><span class="line">starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.master</span><span class="class">.Master</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.master</span><span class="class">.Master-1-dp0652</span><span class="class">.out</span></span><br><span class="line">dp0656: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0656</span><span class="class">.out</span></span><br><span class="line">dp0655: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0655</span><span class="class">.out</span></span><br><span class="line">dp0657: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0657</span><span class="class">.out</span></span><br><span class="line">dp0652: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0652</span><span class="class">.out</span></span><br><span class="line">dp0653: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0653</span><span class="class">.out</span></span><br><span class="line">dp0652: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-2-dp0652</span><span class="class">.out</span></span><br><span class="line">dp0653: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-2-dp0653</span><span class="class">.out</span></span><br></pre></td></tr></table></figure>
<p>在master和slaves上查看Spark进程</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ jps -lm</span><br><span class="line"><span class="number">40708</span> org.apache.spark.deploy.master.Master --ip dp0652 --port <span class="number">7077</span> --webui-port <span class="number">8082</span></span><br><span class="line"><span class="number">41095</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8082</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="number">40926</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>]$ ssh dp0653</span><br><span class="line">Last login: Thu Jul  <span class="number">2</span> <span class="number">09</span>:<span class="number">07</span>:<span class="number">17</span> <span class="number">2015</span> from <span class="number">192.168</span><span class="number">.6</span><span class="number">.140</span></span><br><span class="line">[qihuang.zheng@dp0653 ~]$ jps -lm</span><br><span class="line"><span class="number">27153</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8082</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="number">27029</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line">[qihuang.zheng@dp0653 ~]$ <span class="built_in">exit</span></span><br><span class="line">logout</span><br><span class="line">Connection to dp0653 closed.</span><br><span class="line">[qihuang.zheng@dp0652 logs]$ ssh dp0655</span><br><span class="line">Last login: Thu Jul  <span class="number">2</span> <span class="number">08</span>:<span class="number">55</span>:<span class="number">05</span> <span class="number">2015</span> from <span class="number">192.168</span><span class="number">.6</span><span class="number">.140</span></span><br><span class="line">[qihuang.zheng@dp0655 ~]$ jps -lm</span><br><span class="line"><span class="number">8766</span> org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line">[qihuang.zheng@dp0655 ~]$</span><br></pre></td></tr></table></figure>
<p>在master上查看web ui: <a href="http://dp0652:8082/" target="_blank" rel="external">http://dp0652:8082/</a></p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark8.png" alt=""></p>
<h3 id="遇到一些问题">遇到一些问题</h3><p><strong>1.如果配置了SPARK_LOCAL_IP, 但是并没有在slaves上修改为自己的IP,则会报错:</strong>  </p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">15/07/02 09:04:08 <span class="keyword">ERROR</span> netty.NettyTransport: failed to bind to /192.168.6.52:0, shutting down Netty transport</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.<span class="keyword">net</span>.BindException: Failed to bind to: /192.168.6.52:0: Service 'sparkWorker' failed after 16 retries!</span><br><span class="line">        at org.jboss.netty.<span class="keyword">bootstrap</span>.ServerBootstrap.bind(ServerBootstrap.java:272)</span><br><span class="line">        at akka.remote.transport.netty.NettyTransport$<span class="label">$anonfun</span><span class="label">$listen</span><span class="label">$1</span>.apply(NettyTransport.<span class="keyword">scala</span>:393)</span><br><span class="line">        at akka.remote.transport.netty.NettyTransport$<span class="label">$anonfun</span><span class="label">$listen</span><span class="label">$1</span>.apply(NettyTransport.<span class="keyword">scala</span>:389)</span><br><span class="line">        at <span class="keyword">scala</span>.util.Success$<span class="label">$anonfun</span><span class="label">$map</span><span class="label">$1</span>.apply(Try.<span class="keyword">scala</span>:206)</span><br><span class="line">        at <span class="keyword">scala</span>.util.Try$.apply(Try.<span class="keyword">scala</span>:161)</span><br><span class="line">        at <span class="keyword">scala</span>.util.Success.map(Try.<span class="keyword">scala</span>:206)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.Future$<span class="label">$anonfun</span><span class="label">$map</span><span class="label">$1</span>.apply(Future.<span class="keyword">scala</span>:235)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.Future$<span class="label">$anonfun</span><span class="label">$map</span><span class="label">$1</span>.apply(Future.<span class="keyword">scala</span>:235)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.impl.CallbackRunnable.<span class="keyword">run</span>(Promise.<span class="keyword">scala</span>:32)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.processBatch<span class="label">$1</span>(BatchingExecutor.<span class="keyword">scala</span>:67)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.apply<span class="label">$mcV</span><span class="label">$sp</span>(BatchingExecutor.<span class="keyword">scala</span>:82)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.apply(BatchingExecutor.<span class="keyword">scala</span>:59)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.apply(BatchingExecutor.<span class="keyword">scala</span>:59)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.BlockContext$.withBlockContext(BlockContext.<span class="keyword">scala</span>:72)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>.<span class="keyword">run</span>(BatchingExecutor.<span class="keyword">scala</span>:58)</span><br><span class="line">        at akka.dispatch.TaskInvocation.<span class="keyword">run</span>(AbstractDispatcher.<span class="keyword">scala</span>:41)</span><br><span class="line">        at akka.dispatch.ForkJoinExecutorConfigurator<span class="label">$AkkaForkJoinTask</span>.exec(AbstractDispatcher.<span class="keyword">scala</span>:393)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinPool<span class="label">$WorkQueue</span>.runTask(ForkJoinPool.java:1339)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinWorkerThread.<span class="keyword">run</span>(ForkJoinWorkerThread.java:107)</span><br><span class="line">15/07/02 09:04:09 INFO remote.RemoteActorRefProvider<span class="label">$RemotingTerminator</span>: Shutting down remote daemon.</span><br><span class="line">15/07/02 09:04:09 INFO remote.RemoteActorRefProvider<span class="label">$RemotingTerminator</span>: Remote daemon shut down; proceeding with flushing remote transports.</span><br><span class="line">15/07/02 09:04:09 INFO util.Utils: Shutdown hook called</span><br></pre></td></tr></table></figure>
<p>原因分析: SPARK_LOCAL_IP指的是本机IP地址,因此分发到集群的不同节点上,都要到各自的节点修改为自己的IP地址.<br>如果集群节点比较多,则比较麻烦, 可以用SPARK_LOCAL_DIRS代替.</p>
<p><strong>2.如果没有配置export MASTER, 在worker上会报错:</strong>  </p>
<figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">40</span>:<span class="number">51</span> INFO worker.Worker: Retrying connection <span class="keyword">to</span> master (attempt <span class="preprocessor"># 12)</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">40</span>:<span class="number">51</span> INFO worker.Worker: Connecting <span class="keyword">to</span> master akka.tcp://sparkMaster<span class="constant">@dp0652</span>:<span class="number">7077</span>/user/Master...</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">40</span>:<span class="number">51</span> WARN Remoting: Tried <span class="keyword">to</span> associate <span class="keyword">with</span> unreachable remote address [akka.tcp://sparkMaster<span class="constant">@dp0652</span>:<span class="number">7077</span>].</span><br><span class="line">Address is <span class="built_in">now</span> gated <span class="keyword">for</span> <span class="number">5000</span> ms, all messages <span class="keyword">to</span> this address will be delivered <span class="keyword">to</span> dead letters.</span><br><span class="line">Reason: 拒绝连接: dp0652/<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">7077</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">41</span>:<span class="number">23</span> ERROR worker.Worker: RECEIVED SIGNAL <span class="number">15</span>: SIGTERM</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">41</span>:<span class="number">23</span> INFO util.Utils: <span class="built_in">Shutdown</span> hook called</span><br></pre></td></tr></table></figure>
<p>导致的后果是虽然slaves上都启动了Worker进程(使用jps查看),但是在Master上并没有看到workers. 这时候应该查看Master上的日志.<br>master上启动成功显示的日志是spark@dp0652:7077. 而上面却显示的是sparkMaster@dp0652:7077. 所以应该手动export MASTER  </p>
<p><strong>3.最后成功启动集群, 在Master上的日志:</strong>  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Spark <span class="string">Command:</span> <span class="regexp">/usr/</span>java<span class="regexp">/jdk1.7.0_51/</span>bin<span class="regexp">/java -cp /</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/sbin/</span>..<span class="regexp">/conf/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/spark-assembly-1.4.0-hadoop2.6.0.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-rdbms-<span class="number">3.2</span><span class="number">.9</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/datanucleus-core-3.2.10.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-api-jdo-<span class="number">3.2</span><span class="number">.6</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span> -Xms512m -Xmx512m -<span class="string">XX:</span>MaxPermSize=<span class="number">128</span>m org.apache.spark.deploy.master.Master --ip dp0652 --port <span class="number">7077</span> --webui-port <span class="number">8082</span></span><br><span class="line">========================================</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">49</span> INFO master.<span class="string">Master:</span> Registered signal handlers <span class="keyword">for</span> [TERM, HUP, INT]</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> WARN util.<span class="string">NativeCodeLoader:</span> Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> INFO spark.<span class="string">SecurityManager:</span> Changing view acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> INFO spark.<span class="string">SecurityManager:</span> Changing modify acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> INFO spark.<span class="string">SecurityManager:</span> <span class="string">SecurityManager:</span> authentication disabled; ui acls disabled; users with view <span class="string">permissions:</span> Set(qihuang.zheng); users with modify <span class="string">permissions:</span> Set(qihuang.zheng)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO slf4j.<span class="string">Slf4jLogger:</span> Slf4jLogger started</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO <span class="string">Remoting:</span> Starting remoting</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO <span class="string">Remoting:</span> Remoting started; listening on <span class="string">addresses :</span>[akka.<span class="string">tcp:</span><span class="comment">//sparkMaster@dp0652:7077]</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'sparkMaster'</span> on port <span class="number">7077.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">Server:</span> jetty-<span class="number">8.</span>y.z-SNAPSHOT</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">AbstractConnector:</span> Started SelectChannelConnector<span class="annotation">@dp</span><span class="number">0652:</span><span class="number">6066</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO util.<span class="string">Utils:</span> Successfully started service on port <span class="number">6066.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO rest.<span class="string">StandaloneRestServer:</span> Started REST server <span class="keyword">for</span> submitting applications on port <span class="number">6066</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO master.<span class="string">Master:</span> Starting Spark master at <span class="string">spark:</span><span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO master.<span class="string">Master:</span> Running Spark version <span class="number">1.4</span><span class="number">.0</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">Server:</span> jetty-<span class="number">8.</span>y.z-SNAPSHOT</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">AbstractConnector:</span> Started SelectChannelConnector@<span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">8082</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'MasterUI'</span> on port <span class="number">8082.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO ui.<span class="string">MasterWebUI:</span> Started MasterWebUI at <span class="string">http:</span><span class="comment">//192.168.6.52:8082</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO master.<span class="string">Master:</span> I have been elected leader! New <span class="string">state:</span> ALIVE</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">35398</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">60106</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">50995</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">55994</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">34020</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">56</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">55912</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">56</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">35846</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br></pre></td></tr></table></figure>
<p><strong>在53的其中一个Worker上的日志:</strong>  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Spark <span class="string">Command:</span> <span class="regexp">/usr/</span>java<span class="regexp">/jdk1.7.0_51/</span>bin<span class="regexp">/java -cp /</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/sbin/</span>..<span class="regexp">/conf/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/spark-assembly-1.4.0-hadoop2.6.0.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-api-jdo-<span class="number">3.2</span><span class="number">.6</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/datanucleus-core-3.2.10.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-rdbms-<span class="number">3.2</span><span class="number">.9</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span> -Xms512m -Xmx512m -<span class="string">XX:</span>MaxPermSize=<span class="number">128</span>m org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> <span class="string">spark:</span><span class="comment">//dp0652:7077</span></span><br><span class="line">========================================</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO worker.<span class="string">Worker:</span> Registered signal handlers <span class="keyword">for</span> [TERM, HUP, INT]</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> WARN util.<span class="string">NativeCodeLoader:</span> Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO spark.<span class="string">SecurityManager:</span> Changing view acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO spark.<span class="string">SecurityManager:</span> Changing modify acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO spark.<span class="string">SecurityManager:</span> <span class="string">SecurityManager:</span> authentication disabled; ui acls disabled; users with view <span class="string">permissions:</span> Set(qihuang.zheng); users with modify <span class="string">permissions:</span> Set(qihuang.zheng)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">53</span> INFO slf4j.<span class="string">Slf4jLogger:</span> Slf4jLogger started</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">53</span> INFO <span class="string">Remoting:</span> Starting remoting</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO <span class="string">Remoting:</span> Remoting started; listening on <span class="string">addresses :</span>[akka.<span class="string">tcp:</span><span class="comment">//sparkWorker@192.168.6.53:55994]</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'sparkWorker'</span> on port <span class="number">55994.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Starting Spark worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">55994</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Running Spark version <span class="number">1.4</span><span class="number">.0</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Spark <span class="string">home:</span> <span class="regexp">/usr/</span>install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO server.<span class="string">Server:</span> jetty-<span class="number">8.</span>y.z-SNAPSHOT</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO server.<span class="string">AbstractConnector:</span> Started SelectChannelConnector@<span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">8081</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'WorkerUI'</span> on port <span class="number">8081.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO ui.<span class="string">WorkerWebUI:</span> Started WorkerWebUI at <span class="string">http:</span><span class="comment">//192.168.6.53:8081</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Connecting to master akka.<span class="string">tcp:</span><span class="comment">//sparkMaster@dp0652:7077/user/Master...</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Successfully registered with master <span class="string">spark:</span><span class="comment">//dp0652:7077</span></span><br></pre></td></tr></table></figure>
<h3 id="spark-shell_&amp;_spark-submit">spark-shell &amp; spark-submit</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --master spark:<span class="comment">//dp0652:7077 --executor-memory 4g</span></span><br><span class="line"></span><br><span class="line">bin/spark-submit --master spark:<span class="comment">//dp0652:7077 \</span></span><br><span class="line">  --<span class="keyword">class</span> org.apache.spark.examples.SparkPi \</span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">2</span> \</span><br><span class="line">  lib/spark-examples-<span class="number">1.4</span><span class="number">.0</span>-hadoop2<span class="number">.6</span><span class="number">.0</span>.jar <span class="number">1000</span></span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-06-25-ES2HDFS" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/06/25/2015-06-25-ES2HDFS/" class="article-date">
  	<time datetime="2015-06-24T16:00:00.000Z" itemprop="datePublished">2015-06-25</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/25/2015-06-25-ES2HDFS/">ES导入数据到HDFS</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ES2HDFS_in_Action">ES2HDFS in Action</h2><p>elasticsearch-hadoop 支持Hadoop和Spark，利用elasticsearch-hadoop项目，spark可以较为轻松的从elasticsearch中读取数据。</p>
<h3 id="项目要求">项目要求</h3><p>存储于elasticsearch中的数据是以json格式存储，由于历史的原因，其格式不统一。<br>现在要求将这些json串组织成统一的格式，树深最大不超过3.<br>导出的数据以parquet格式存储到hdfs中，同时建立外部的hive表。 </p>
<figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE <span class="keyword">EXTERNAL</span> TABLE activity(</span><br><span class="line">  sequence_id` <span class="keyword">string</span>,</span><br><span class="line">  occur_time bigint,</span><br><span class="line">  activity_map map&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">  ...</span><br><span class="line">partitioned by(indice <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">stored</span> as parquet</span><br><span class="line">location <span class="string">'/user/lab/activity'</span>;</span><br></pre></td></tr></table></figure>
<h3 id="1-spark-shell">1.spark-shell</h3><p>从mavenrepository.com下载elasticsearch-hadoop的jar包, 将该jar包放到 $SPARK_HOME/lib 目录</p>
<p>启动spark-shell:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --jars lib/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,lib/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,lib/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,lib/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,\</span><br><span class="line">  lib/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.0</span>.Beta4.jar --conf spark.es.nodes=<span class="number">192.168</span><span class="number">.47</span><span class="number">.11</span>:<span class="number">9200</span> --master local[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">scp elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar json4s-* <span class="number">192.168</span><span class="number">.47</span><span class="number">.211</span>:~/</span><br><span class="line">cd /usr/install/spark-<span class="number">1.4</span><span class="number">.1</span>-bin-hadoop2<span class="number">.4</span></span><br><span class="line"></span><br><span class="line">bin/spark-shell --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">  --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">30</span> --driver-memory <span class="number">8</span>g \</span><br><span class="line">  --conf spark.es.nodes=<span class="number">192.168</span><span class="number">.47</span><span class="number">.155</span>:<span class="number">9200</span> \</span><br><span class="line">  --conf spark.es.field.read.empty.as.null=<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>参数说明: 1) spark.es.nodes 指定elasticsearch的地址<br>2) –jars必须在同一行内, 不能用\分开  </p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  repository  cp org/json4s/json4s-ast_2<span class="number">.10</span>/<span class="number">3.2</span><span class="number">.11</span>/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar ~/lib</span><br><span class="line">➜  repository  cp org/json4s/json4s-core_2<span class="number">.10</span>/<span class="number">3.2</span><span class="number">.11</span>/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar ~/lib</span><br><span class="line">➜  repository  cp org/json4s/json4s-jackson_2<span class="number">.10</span>/<span class="number">3.2</span><span class="number">.11</span>/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar ~/lib</span><br><span class="line">➜  repository  cp org/json4s/json4s-native_2<span class="number">.10</span>/<span class="number">3.2</span><span class="number">.11</span>/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar ~/lib</span><br><span class="line">➜  repository  cp org/elasticsearch/elasticsearch-hadoop/<span class="number">2.1</span><span class="number">.1</span>/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar ~/lib</span><br></pre></td></tr></table></figure>
<p>在spark-shell中, 只需要import需要的jar包:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import org<span class="class">.elasticsearch</span><span class="class">.spark</span>._</span><br><span class="line">import org<span class="class">.json4s</span><span class="class">.NoTypeHints</span></span><br><span class="line">import org<span class="class">.json4s</span><span class="class">.native</span><span class="class">.Serialization</span></span><br><span class="line">import org<span class="class">.json4s</span><span class="class">.native</span><span class="class">.Serialization</span>._</span><br><span class="line"></span><br><span class="line">import sqlContext<span class="class">.implicits</span>._</span><br></pre></td></tr></table></figure>
<p>测试连接ES,并读取ES数据:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.<span class="function"><span class="title">esRDD</span><span class="params">(s<span class="string">"$exportDB/activity"</span>)</span></span></span><br><span class="line">rdd1.first</span><br></pre></td></tr></table></figure>
<h3 id="2-spark-submit_+_dependency_jars">2.spark-submit + dependency jars</h3><p>打包需要把所有的依赖包打包下来: <code>TODO: 只把需要的jar包打进去, 像spark等直接使用provided方式. 给pom.xml添加插件</code>  </p>
<figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">&lt;plugin&gt;</span></span><br><span class="line">    <span class="variable">&lt;artifactId&gt;</span>maven-assembly-plugin<span class="variable">&lt;/artifactId&gt;</span></span><br><span class="line">    <span class="variable">&lt;configuration&gt;</span></span><br><span class="line">        <span class="variable">&lt;descriptorRefs&gt;</span></span><br><span class="line">            <span class="variable">&lt;descriptorRef&gt;</span>jar-with-dependencies<span class="variable">&lt;/descriptorRef&gt;</span></span><br><span class="line">        <span class="variable">&lt;/descriptorRefs&gt;</span></span><br><span class="line">        <span class="variable">&lt;archive&gt;</span></span><br><span class="line">            <span class="variable">&lt;manifest&gt;</span></span><br><span class="line">                <span class="variable">&lt;mainClass&gt;</span><span class="variable">&lt;/mainClass&gt;</span></span><br><span class="line">            <span class="variable">&lt;/manifest&gt;</span></span><br><span class="line">        <span class="variable">&lt;/archive&gt;</span></span><br><span class="line">    <span class="variable">&lt;/configuration&gt;</span></span><br><span class="line">    <span class="variable">&lt;executions&gt;</span></span><br><span class="line">        <span class="variable">&lt;execution&gt;</span></span><br><span class="line">            <span class="variable">&lt;id&gt;</span>make-assembly<span class="variable">&lt;/id&gt;</span></span><br><span class="line">            <span class="variable">&lt;phase&gt;</span>package<span class="variable">&lt;/phase&gt;</span></span><br><span class="line">            <span class="variable">&lt;goals&gt;</span></span><br><span class="line">                <span class="variable">&lt;goal&gt;</span>single<span class="variable">&lt;/goal&gt;</span></span><br><span class="line">            <span class="variable">&lt;/goals&gt;</span></span><br><span class="line">        <span class="variable">&lt;/execution&gt;</span></span><br><span class="line">    <span class="variable">&lt;/executions&gt;</span></span><br><span class="line"><span class="variable">&lt;/plugin&gt;</span></span><br><span class="line"></span><br><span class="line">cd tongdun-app</span><br><span class="line">rm -rf target</span><br><span class="line">mvn package -D<span class="keyword">skip</span>Tests</span><br><span class="line">cd target</span><br><span class="line">scp tongdun-app-<span class="number">1.0</span>-SNAPSHOT-jar-with-dependencies.jar <span class="number">192.168</span>.<span class="number">47.211</span>:~/</span><br></pre></td></tr></table></figure>
<p>这样的坏处是打包时间有点长, 文件有点大,传输到跳板机上时间也很慢.</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark:<span class="comment">//192.168.47.213:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --executor-<span class="keyword">memory</span> 8g --<span class="keyword">total</span>-executor-cores 24 --driver-<span class="keyword">memory</span> 8g \</span><br><span class="line">  --<span class="keyword">class</span> cn.fraudmetrix.dataplatform.<span class="keyword">lab</span>.ES2HDFSExporter \</span><br><span class="line">  /home/qihuang.zheng/tongdun-<span class="keyword">app</span>-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure>
<h3 id="3-spark-submit_+_third_party_jars">3.spark-submit + third party jars</h3><p>一种改进的方式是: 打成jar包, 但是没有把依赖包打进去, spark-submit的时候使用–jars.<br>这样就不需要上面的maven-assembly-plugin了. 添加–jars如果jar文件很多,可以用脚本的方式:<br><a href="http://stackoverflow.com/questions/24855368/spark-throws-classnotfoundexception-when-using-jars-option" target="_blank" rel="external">http://stackoverflow.com/questions/24855368/spark-throws-classnotfoundexception-when-using-jars-option</a></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">/usr/install/hadoop/bin/hadoop fs -rmr /user/tongdun/activity_hist/year=<span class="number">2015</span>/month=<span class="number">9</span>/day=<span class="number">17</span></span><br><span class="line"></span><br><span class="line">cd /usr/install/spark-<span class="number">1.4</span><span class="number">.1</span>-bin-hadoop2<span class="number">.4</span></span><br><span class="line"></span><br><span class="line">nohup bin/spark-submit --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">30</span> --driver-memory <span class="number">12</span>g \</span><br><span class="line">  --<span class="keyword">class</span> cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter \</span><br><span class="line">  --conf spark.es.nodes=<span class="number">192.168</span><span class="number">.47</span><span class="number">.155</span>:<span class="number">9200</span> --conf spark.es.field.read.empty.as.null=<span class="literal">true</span> \</span><br><span class="line">  --conf spark.speculation=<span class="literal">false</span> \</span><br><span class="line">  /home/qihuang.zheng/tongdun-app-<span class="number">1.0</span>-SNAPSHOT.jar <span class="number">20150917</span> &amp;</span><br><span class="line"></span><br><span class="line">配置信息写死在代码里.</span><br><span class="line">nohup bin/spark-submit --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">30</span> --driver-memory <span class="number">12</span>g \</span><br><span class="line">  --<span class="keyword">class</span> cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter \</span><br><span class="line">  /home/qihuang.zheng/tongdun-app-<span class="number">1.0</span>-SNAPSHOT.jar <span class="number">20150917</span> &amp;</span><br></pre></td></tr></table></figure>
<h3 id="Question_&amp;_Solution">Question &amp; Solution</h3><h4 id="1-连接不到ES">1.连接不到ES</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">org<span class="class">.elasticsearch</span><span class="class">.hadoop</span><span class="class">.rest</span><span class="class">.EsHadoopTransportException</span>: java<span class="class">.net</span><span class="class">.NoRouteToHostException</span>: 没有到主机的路由</span><br><span class="line">发现这个错误不是连接ES的问题, 是因为当时Spark的master正在迁移导致的. 重启下spark-shell即可.</span><br></pre></td></tr></table></figure>
<h4 id="2-es的空值">2.es的空值</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">org<span class="class">.elasticsearch</span><span class="class">.hadoop</span><span class="class">.EsHadoopIllegalArgumentException</span>:</span><br><span class="line">Index [forseti-<span class="number">201509</span>*/activity] missing and settings [es<span class="class">.field</span><span class="class">.read</span><span class="class">.empty</span><span class="class">.as</span><span class="class">.null</span>] is set to false</span><br><span class="line">添加--conf spark<span class="class">.es</span><span class="class">.field</span><span class="class">.read</span><span class="class">.empty</span><span class="class">.as</span><span class="class">.null</span>=true</span><br></pre></td></tr></table></figure>
<h4 id="3-任务运行较慢,开启了推测执行">3.任务运行较慢,开启了推测执行</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">56</span> INFO TaskSetManager: Finished task <span class="number">10.0</span> in stage <span class="number">0.0</span> (TID <span class="number">10</span>) in <span class="number">6388</span> ms on <span class="number">192.168</span><span class="number">.47</span><span class="number">.212</span> (<span class="number">14</span>/<span class="number">17</span>)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">58</span> INFO TaskSetManager: Marking task <span class="number">5</span> in stage <span class="number">0.0</span> (on <span class="number">192.168</span><span class="number">.47</span><span class="number">.217</span>) as speculatable because it ran more than <span class="number">8751</span> ms</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">58</span> INFO TaskSetManager: Starting task <span class="number">5.1</span> in stage <span class="number">0.0</span> (TID <span class="number">17</span>, <span class="number">192.168</span><span class="number">.47</span><span class="number">.212</span>, ANY, <span class="number">30707</span> bytes)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">58</span> INFO TaskSetManager: Marking task <span class="number">16</span> in stage <span class="number">0.0</span> (on <span class="number">192.168</span><span class="number">.47</span><span class="number">.223</span>) as speculatable because it ran more than <span class="number">8751</span> ms</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">58</span> INFO TaskSetManager: Marking task <span class="number">9</span> in stage <span class="number">0.0</span> (on <span class="number">192.168</span><span class="number">.47</span><span class="number">.216</span>) as speculatable because it ran more than <span class="number">8751</span> ms</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">58</span> INFO TaskSetManager: Starting task <span class="number">9.1</span> in stage <span class="number">0.0</span> (TID <span class="number">18</span>, <span class="number">192.168</span><span class="number">.47</span><span class="number">.223</span>, ANY, <span class="number">30706</span> bytes)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">26</span>:<span class="number">58</span> INFO TaskSetManager: Starting task <span class="number">16.1</span> in stage <span class="number">0.0</span> (TID <span class="number">19</span>, <span class="number">192.168</span><span class="number">.47</span><span class="number">.217</span>, ANY, <span class="number">30707</span> bytes)</span><br><span class="line"></span><br><span class="line">在<span class="number">14</span>个Task的时候报错:</span><br><span class="line"></span><br><span class="line">Caused by: org.apache.spark.SparkException: A master URL must be <span class="built_in">set</span> in your configuration</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">23</span> <span class="number">18</span>:<span class="number">00</span>:<span class="number">02</span> WARN TaskSetManager: Lost task <span class="number">9.4</span> in stage <span class="number">0.0</span> (TID <span class="number">26</span>, <span class="number">192.168</span><span class="number">.47</span><span class="number">.212</span>):</span><br><span class="line">  org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException):</span><br><span class="line">  No lease on /user/tongdun/activity_hist/year=<span class="number">2015</span>/month=<span class="number">9</span>/day=<span class="number">17</span>/_temporary/<span class="number">0</span>/_temporary/attempt_201509231800_0000_m_000009_4/part-r-<span class="number">00009</span>-<span class="number">04</span>acba62-<span class="number">4</span>ec8-<span class="number">4212</span>-<span class="number">8185</span>-<span class="number">0999</span>ae3078e1.gz.parquet:</span><br><span class="line">  File does not exist. Holder DFSClient_attempt_201509231759_0000_m_000012_0_2042692825_48 does not have any open files.</span><br><span class="line"></span><br><span class="line">在代码中手动指定--master:</span><br><span class="line">Caused by: java.util.NoSuchElementException: spark.es.nodes</span><br><span class="line">    at org.apache.spark.SparkConf$$anonfun$get$<span class="number">1.</span>apply(SparkConf.scala:<span class="number">172</span>)</span><br><span class="line">    at org.apache.spark.SparkConf$$anonfun$get$<span class="number">1.</span>apply(SparkConf.scala:<span class="number">172</span>)</span><br><span class="line">    at scala.Option.getOrElse(Option.scala:<span class="number">120</span>)</span><br><span class="line">    at org.apache.spark.SparkConf.get(SparkConf.scala:<span class="number">172</span>)</span><br><span class="line">    at cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter$.&lt;init&gt;(ES2HDFSExporter.scala:<span class="number">24</span>)</span><br><span class="line">    at cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter$.&lt;clinit&gt;(ES2HDFSExporter.scala)</span><br><span class="line"></span><br><span class="line">在代码中再次手动设置spark.es.nodes:</span><br><span class="line">nohup bin/spark-submit --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">  --executor-memory <span class="number">8</span>g --total-executor-cores <span class="number">24</span> --driver-memory <span class="number">12</span>g \</span><br><span class="line">  --<span class="keyword">class</span> cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter \</span><br><span class="line">  /home/qihuang.zheng/tongdun-app-<span class="number">1.0</span>-SNAPSHOT.jar <span class="number">20150917</span> <span class="number">1</span> <span class="number">50</span> &amp;</span><br></pre></td></tr></table></figure>
<p>没有报错, 但是因为任务运行较慢,开启了推测机制, 会在后台突然启动了多个应用. 停掉spark-submit进程后,所有ES2HDFS也都停掉了.  </p>
<p><img src="http://img.blog.csdn.net/20150924082457899" alt="spark-speculation"></p>
<p>关闭推测执行: spark.speculation=false. 但是实际上推测执行默认就是关闭的了!!</p>
<h4 id="4-数据文件找不到??">4.数据文件找不到??</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">24</span> <span class="number">12</span>:<span class="number">01</span>:<span class="number">49</span> WARN DFSClient: DFSOutputStream ResponseProcessor exception  <span class="keyword">for</span> block BP-<span class="number">1007412381</span>-<span class="number">192.168</span><span class="number">.47</span><span class="number">.213</span>-<span class="number">1425269150800</span>:blk_1075697042_1956779</span><br><span class="line">java.io.EOFException: Premature EOF: no length prefix available</span><br><span class="line">    at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:<span class="number">1987</span>)</span><br><span class="line">    at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:<span class="number">176</span>)</span><br><span class="line">    at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:<span class="number">796</span>)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">24</span> <span class="number">12</span>:<span class="number">01</span>:<span class="number">49</span> WARN DFSClient: Error Recovery <span class="keyword">for</span> block BP-<span class="number">1007412381</span>-<span class="number">192.168</span><span class="number">.47</span><span class="number">.213</span>-<span class="number">1425269150800</span>:blk_1075697042_1956779</span><br><span class="line">in pipeline <span class="number">192.168</span><span class="number">.47</span><span class="number">.211</span>:<span class="number">50010</span>, <span class="number">192.168</span><span class="number">.47</span><span class="number">.218</span>:<span class="number">50010</span>, <span class="number">192.168</span><span class="number">.47</span><span class="number">.208</span>:<span class="number">50010</span>: bad datanode <span class="number">192.168</span><span class="number">.47</span><span class="number">.211</span>:<span class="number">50010</span></span><br></pre></td></tr></table></figure>
<h4 id="5-ES没有暴露_count">5.ES没有暴露_count</h4><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="atom">val</span> <span class="atom">esRDD</span> = <span class="atom">sc</span>.<span class="atom">esRDD</span>(<span class="atom">s</span><span class="string">"$exportDB/activity"</span>, <span class="atom">query</span>) //<span class="name">RDD</span>[(<span class="name">String</span>, <span class="name">Map</span>[<span class="name">String</span>, <span class="name">AnyRef</span>])]</span><br><span class="line"><span class="atom">println</span>(<span class="string">"COUNT:"</span>+<span class="atom">esRDD</span>.<span class="atom">count</span>())</span><br><span class="line"></span><br><span class="line"><span class="atom">org</span>.<span class="atom">elasticsearch</span>.<span class="atom">hadoop</span>.<span class="atom">rest</span>.<span class="name">EsHadoopInvalidRequest</span>: [<span class="name">GET</span>] <span class="atom">on</span> [<span class="atom">forseti</span>-<span class="number">201509</span>*/<span class="atom">activity</span>/<span class="name">_count</span>] <span class="atom">failed</span>; <span class="atom">server</span>[<span class="atom">null</span>] <span class="atom">returned</span> [<span class="number">404</span>|<span class="name">Not</span> <span class="name">Found</span>:]</span><br></pre></td></tr></table></figure>
<h4 id="6-减少查询粒度">6.减少查询粒度</h4><p>将query的查询时间改成一个小时. 发现任务也是停留在14/17, 所以跟ES的数据量大小没有关系(因为一个小时的数据量是很小的,而且是凌晨1点的).</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">02</span> INFO <span class="string">TaskSchedulerImpl:</span> Adding task set <span class="number">1.0</span> with <span class="number">17</span> tasks</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">02</span> INFO <span class="string">TaskSetManager:</span> Starting task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">1</span>, spark047242, ANY, <span class="number">30647</span> bytes)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">02</span> INFO <span class="string">TaskSetManager:</span> Starting task <span class="number">1.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">2</span>, spark047217, ANY, <span class="number">30647</span> bytes)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">02</span> INFO <span class="string">TaskSetManager:</span> Starting task <span class="number">2.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">3</span>, spark047217, ANY, <span class="number">30647</span> bytes)</span><br><span class="line">....</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">02</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047242:</span><span class="number">45400</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047241:</span><span class="number">33811</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047223:</span><span class="number">60695</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047243:</span><span class="number">53908</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Removed broadcast_0_piece0 on <span class="number">192.168</span><span class="number">.47</span><span class="number">.211</span>:<span class="number">59995</span> <span class="keyword">in</span> memory (<span class="string">size:</span> <span class="number">987.0</span> B, <span class="string">free:</span> <span class="number">4.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047243:</span><span class="number">50652</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Removed broadcast_0_piece0 on <span class="string">spark047242:</span><span class="number">45400</span> <span class="keyword">in</span> memory (<span class="string">size:</span> <span class="number">987.0</span> B, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047217:</span><span class="number">51802</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047216:</span><span class="number">38219</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">05</span> INFO <span class="string">BlockManagerInfo:</span> Added broadcast_1_piece0 <span class="keyword">in</span> memory on <span class="string">spark047216:</span><span class="number">45353</span> (<span class="string">size:</span> <span class="number">4.6</span> KB, <span class="string">free:</span> <span class="number">2.1</span> GB)</span><br><span class="line">....</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">06</span> INFO <span class="string">TaskSetManager:</span> Finished task <span class="number">8.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">9</span>) <span class="keyword">in</span> <span class="number">4162</span> ms on spark047241 (<span class="number">1</span>/<span class="number">17</span>)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">07</span> INFO <span class="string">TaskSetManager:</span> Finished task <span class="number">10.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">11</span>) <span class="keyword">in</span> <span class="number">4831</span> ms on spark047216 (<span class="number">2</span>/<span class="number">17</span>)</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">08</span> INFO <span class="string">TaskSetManager:</span> Finished task <span class="number">14.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">15</span>) <span class="keyword">in</span> <span class="number">6181</span> ms on spark047244 (<span class="number">3</span>/<span class="number">17</span>)</span><br><span class="line">....</span><br><span class="line"><span class="number">15</span><span class="regexp">/09/</span><span class="number">24</span> <span class="number">13</span>:<span class="number">44</span>:<span class="number">16</span> INFO <span class="string">TaskSetManager:</span> Finished task <span class="number">2.0</span> <span class="keyword">in</span> stage <span class="number">1.0</span> (TID <span class="number">3</span>) <span class="keyword">in</span> <span class="number">14051</span> ms on spark047217 (<span class="number">14</span>/<span class="number">17</span>)</span><br><span class="line">这里开始没有反应了....</span><br></pre></td></tr></table></figure>
<h4 id="7-使用DF读取ES数据">7.使用DF读取ES数据</h4><p>由于REST服务无法读取具体的index, 报错:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot find mapping <span class="keyword">for</span> forseti-<span class="number">201509</span>*/activity - one is required before <span class="keyword">using</span> Spark SQL</span><br><span class="line"></span><br><span class="line">curl <span class="number">192.168</span><span class="number">.47</span><span class="number">.155</span>:<span class="number">9201</span>/_cat/indices/forseti-<span class="number">20150909</span>: <span class="number">404</span> Not Found</span><br><span class="line"></span><br><span class="line">https:<span class="comment">//github.com/elastic/elasticsearch-hadoop/blob/master/spark/sql-12/src/main/scala/org/elasticsearch/spark/sql/SchemaUtils.scala</span></span><br><span class="line"></span><br><span class="line">可以看到下面是使用REST服务查询index, 如果不存在, 则报上面的错误!</span><br><span class="line">val repo = <span class="keyword">new</span> RestRepository(cfg)</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (repo.indexExists(<span class="literal">true</span>)) &#123;</span><br><span class="line">  ....</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> EsHadoopIllegalArgumentException(s<span class="string">"Cannot find mapping for $&#123;cfg.getResourceRead&#125; - one is required before using Spark SQL"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">后来发现是连错了端口, 应该使用<span class="number">9200</span>, 而不是<span class="number">9201</span>!😢</span><br></pre></td></tr></table></figure>
<p>DataFrame的结构见es_spark_mapping.jspn:</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- activity: struct (nullable = <span class="keyword">true</span>)</span><br><span class="line">|    |-- accountAddress: string (nullable = <span class="keyword">true</span>)</span><br><span class="line">|    |-- accountAddressCity: string (nullable = <span class="keyword">true</span>)</span><br><span class="line">|-- audit: struct (nullable = <span class="keyword">true</span>)</span><br><span class="line">|    |-- needAudit: <span class="keyword">boolean</span> (nullable = <span class="keyword">true</span>)</span><br><span class="line">|    |-- operateResult: string (nullable = <span class="keyword">true</span>)</span><br></pre></td></tr></table></figure>
<h4 id="8-ElastichSearch-Hive">8.ElastichSearch-Hive</h4><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">add jar /home/qihuang<span class="built_in">.</span>zheng/elasticsearch<span class="attribute">-hadoop</span>-<span class="number">2.1</span><span class="number">.1</span><span class="built_in">.</span>jar;</span><br><span class="line"></span><br><span class="line">CREATE EXTERNAL TABLE activity_20150917 (</span><br><span class="line">    activity <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    audit    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    browser    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    device    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    event_result    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    geo    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    policy    <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">    policyString2    <span class="built_in">string</span></span><br><span class="line">) STORED <span class="keyword">BY</span> <span class="string">'org.elasticsearch.hadoop.hive.EsStorageHandler'</span></span><br><span class="line">TBLPROPERTIES(</span><br><span class="line">    <span class="string">'es.resource'</span> = <span class="string">'forseti-201509*/activity'</span>,</span><br><span class="line">    <span class="string">'es.nodes'</span> = <span class="string">'192.168.47.155'</span>,</span><br><span class="line">    <span class="string">'es.port'</span> = <span class="string">'9200'</span>,</span><br><span class="line">    <span class="string">'es.query'</span> = <span class="string">'?q=_id:1442419200*'</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">hive&gt; <span class="keyword">select</span> * from activity_20150917;</span><br><span class="line">OK</span><br><span class="line">Failed <span class="keyword">with</span> exception java<span class="built_in">.</span>io<span class="built_in">.</span>IOException:org<span class="built_in">.</span>apache<span class="built_in">.</span>hadoop<span class="built_in">.</span>hive<span class="built_in">.</span>ql<span class="built_in">.</span>metadata<span class="built_in">.</span>HiveException:</span><br><span class="line">java<span class="built_in">.</span>lang<span class="built_in">.</span>ClassCastException: org<span class="built_in">.</span>apache<span class="built_in">.</span>hadoop<span class="built_in">.</span>io<span class="built_in">.</span>BooleanWritable can<span class="subst">not</span> be cast <span class="keyword">to</span> org<span class="built_in">.</span>apache<span class="built_in">.</span>hadoop<span class="built_in">.</span>io<span class="built_in">.</span>Text</span><br><span class="line">Time taken: <span class="number">0.711</span> seconds</span><br><span class="line">可见建表的时候指定为<span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;并不合适,因为ES的value不一定是<span class="built_in">string</span><span class="subst">!</span><span class="subst">!</span></span><br><span class="line"></span><br><span class="line">注意不能指定: stored as parquet location <span class="string">'/user/qihuang.zheng/20150926'</span></span><br><span class="line">因为数据源还是在ES中, 每次查询表的时候还是会去查询ES<span class="built_in">. </span>而不能保存到HDFS中<span class="built_in">. </span>这种方式对于ES2HDFS也不适用<span class="built_in">.</span><br><span class="line"></span>这种方式相当于提供了类似SQL的方式查询ES的数据, 查询SQL的任务放在了hive中去做<span class="built_in">. </span>不过底层还是查的ES<span class="subst">!</span> PASS</span><br></pre></td></tr></table></figure>
<h4 id="9-Hive_Load_Map_Data">9.Hive Load Map Data</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> activity_20150917 (</span><br><span class="line">    activity <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    <span class="keyword">audit</span>    <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    browser    <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    device    <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    event_result    <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    geo    <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    <span class="keyword">policy</span>    <span class="keyword">map</span>&lt;<span class="keyword">string</span>,<span class="keyword">string</span>&gt;,</span><br><span class="line">    policyString2    <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line">COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span></span><br><span class="line"><span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">':'</span>;</span></span><br><span class="line"></span><br><span class="line">字段之间适用\t分隔符, 即多个map之间用\t;  Map的key,value适用:组成.</span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> <span class="keyword">LOCAL</span> INPATH <span class="string">'/home/qihuang.zheng/activity_20150917.txt'</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> activity_20150917;</span></span><br></pre></td></tr></table></figure>
<h4 id="10-ES_dump_json">10.ES dump json</h4><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">30分钟的数据</span><br><span class="line">date -j -f <span class="string">"%Y-%m-%d %H:%M:%S"</span> <span class="string">"2015-09-17 00:30:00"</span> <span class="string">"+%s"</span></span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"query"</span>: &#123;<span class="string">"range"</span>: &#123;<span class="string">"activity.eventOccurTime"</span>: &#123;<span class="string">"from"</span> : 1442419200000,<span class="string">"to"</span> : 1442421000000&#125;&#125;&#125;&#125;</span><br><span class="line"></span><br><span class="line">这里没有指定是哪张表, 那是查询全部表吗?</span><br><span class="line">elasticdump \</span><br><span class="line">  --<span class="keyword">input</span>=http:<span class="comment">//localhost:9200/forseti-201509* \</span></span><br><span class="line">  --output=<span class="keyword">query</span>.json \</span><br><span class="line">  --searchBody '&#123;<span class="string">"query"</span>: &#123;<span class="string">"range"</span>: &#123;<span class="string">"activity.eventOccurTime"</span>: &#123;<span class="string">"from"</span> : 1442419200000,<span class="string">"to"</span> : 1442421000000&#125;&#125;&#125;&#125;'</span><br><span class="line"></span><br><span class="line">curl http:<span class="comment">//localhost:9200/_cat/indices | grep forseti-201509</span></span><br><span class="line"></span><br><span class="line">下面这个有问题, 按照文档里面, 实际上不需要/api/<span class="keyword">search</span>的</span><br><span class="line">elasticdump \</span><br><span class="line">  --<span class="keyword">input</span>=http:<span class="comment">//localhost:9200/api/search \</span></span><br><span class="line">  --<span class="keyword">input</span>-index=forseti-20150916/activity \</span><br><span class="line">  --output=201509170030.json \</span><br><span class="line">  --searchBody '&#123;<span class="string">"query"</span>: &#123;<span class="string">"range"</span>: &#123;<span class="string">"activity.eventOccurTime"</span>: &#123;<span class="string">"from"</span> : 1442419200000,<span class="string">"to"</span> : 1442421000000&#125;&#125;&#125;&#125;'</span><br><span class="line"></span><br><span class="line">指定index和<span class="keyword">type</span>, 注意必须精确地指定index名称, 不能用*吗??</span><br><span class="line">elasticdump \</span><br><span class="line">  --<span class="keyword">input</span>=http:<span class="comment">//localhost:9200 \</span></span><br><span class="line">  --<span class="keyword">input</span>-index=forseti-20150916/activity \</span><br><span class="line">  --output=201509170030.json \</span><br><span class="line">  --searchBody '&#123;<span class="string">"query"</span>: &#123;<span class="string">"range"</span>: &#123;<span class="string">"activity.eventOccurTime"</span>: &#123;<span class="string">"from"</span> : 1442419200000,<span class="string">"to"</span> : 1442421000000&#125;&#125;&#125;&#125;'</span><br><span class="line"></span><br><span class="line">查询30分钟的数据, 耗时44-32=12</span><br><span class="line">Sat, 26 Sep 2015 08:44:56 GMT | got 30 objects from source elasticsearch (offset: 508579)</span><br><span class="line">Sat, 26 Sep 2015 08:44:56 GMT | sent 30 objects to destination <span class="keyword">file</span>, wrote 30</span><br><span class="line">Sat, 26 Sep 2015 08:44:56 GMT | got 0 objects from source elasticsearch (offset: 508609)</span><br><span class="line">Sat, 26 Sep 2015 08:44:56 GMT | sent 0 objects to destination <span class="keyword">file</span>, wrote 0</span><br><span class="line">Sat, 26 Sep 2015 08:44:56 GMT | <span class="keyword">Total</span> Writes: 508609</span><br><span class="line">Sat, 26 Sep 2015 08:44:56 GMT | dump complete</span><br><span class="line"></span><br><span class="line">不过文件大小也太大了吧! 30分钟1G, 1天的数据得有50G! 不过压缩后, 文件只有132M.</span><br><span class="line">-rw-r--r--    1 zhengqh  staff   1.0G  9 26 16:44 201509170030.json</span><br><span class="line">-rw-r--r--    1 zhengqh  staff   132M  9 26 17:04 td_activity_201509170030.json.tar.gz</span><br><span class="line"></span><br><span class="line">记录的格式其中_source才是真正的内容:</span><br><span class="line">[</span><br><span class="line">&#123;<span class="string">"_index"</span>:<span class="string">"forseti-20150916"</span>,<span class="string">"_type"</span>:<span class="string">"activity"</span>,<span class="string">"_id"</span>:<span class="string">"1442419220735-70792643"</span>,<span class="string">"_score"</span>:0,<span class="string">"_source"</span>:&#123;&#125;</span><br><span class="line">&#123;<span class="string">"_index"</span>:<span class="string">"forseti-20150916"</span>,<span class="string">"_type"</span>:<span class="string">"activity"</span>,<span class="string">"_id"</span>:<span class="string">"1442419220735-70792643"</span>,<span class="string">"_score"</span>:0,<span class="string">"_source"</span>:&#123;&#125;</span><br><span class="line">]</span><br><span class="line">所以如果要以这种json文件加载到hive中,还要对json文件处理才行!</span><br></pre></td></tr></table></figure>
<h4 id="11-Python_Dump_json">11.Python Dump json</h4><figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> elasticsearch</span><br><span class="line"><span class="keyword">from</span> elasticsearch <span class="keyword">import</span> helpers</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    es = elasticsearch.<span class="type">Elasticsearch</span>(['localhost:<span class="number">9200</span>'])</span><br><span class="line">    query = &#123;</span><br><span class="line">        <span class="string">"query"</span>: &#123;</span><br><span class="line">            <span class="string">"filtered"</span>: &#123;</span><br><span class="line">                <span class="string">"filter"</span>: &#123;</span><br><span class="line">                    <span class="string">"range"</span>: &#123;</span><br><span class="line">                        <span class="string">"activity.eventOccurTime"</span>: &#123;</span><br><span class="line">                            <span class="string">"from"</span>: parse_time('<span class="number">2015</span>-<span class="number">09</span>-<span class="number">17</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>'),</span><br><span class="line">                            <span class="string">"to"</span>: parse_time('<span class="number">2015</span>-<span class="number">09</span>-<span class="number">17</span> <span class="number">00</span>:<span class="number">30</span>:<span class="number">00</span>')</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    r = helpers.scan(es, index='forseti-r-*', doc_type='activity', scroll='<span class="number">1</span>m', query=query, request_timeout=<span class="number">999999</span>)</span><br><span class="line">    f = open(sys.argv[<span class="number">1</span>], 'w')</span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> r:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            source = e['_source']</span><br><span class="line">            f.write(json.dumps(source))</span><br><span class="line">        <span class="keyword">except</span> <span class="type">Exception</span> <span class="keyword">as</span> ex:</span><br><span class="line">            print(ex)</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">def parse_time(t):</span><br><span class="line">    <span class="literal">result</span> = datetime.datetime.strptime(t, <span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line">    <span class="literal">result</span> = <span class="type">int</span>((<span class="literal">result</span> - datetime.datetime(<span class="number">1970</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">8</span>)).total_seconds()*<span class="number">1000</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">result</span></span><br><span class="line"></span><br><span class="line">main()</span><br><span class="line"></span><br><span class="line">执行python文件, 传入json参数. 时间太慢, <span class="type">PASS</span></span><br><span class="line">python es_export_seq.py ~/data/<span class="number">20150917</span>.json</span><br></pre></td></tr></table></figure>
<h4 id="12-Storm-ElasticSearch">12.Storm-ElasticSearch</h4><p>在scala代码中运行EStorm发现都不能输出! 于是放在storm-elasticsearch工程中, 就可以运行. 但是读取ES会超时.<br>这里我们读取的是47.155的ES,通过端口转发到本地localhost:9200. 这也是elasticsearch-storm的默认ES地址.</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">12019</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  org.elasticsearch.hadoop.util.Version - Elasticsearch Hadoop v2.1.1 <span class="list">[<span class="keyword">a1fc48457b</span>]</span><br><span class="line"><span class="number">12025</span> <span class="list">[<span class="keyword">Thread-17-__system</span>] INFO  backtype.storm.daemon.executor - Preparing bolt __system:<span class="list">(<span class="keyword">-1</span>)</span></span><br><span class="line"><span class="number">12035</span> <span class="list">[<span class="keyword">Thread-17-__system</span>] INFO  backtype.storm.daemon.executor - Prepared bolt __system:<span class="list">(<span class="keyword">-1</span>)</span></span><br><span class="line"><span class="number">13596</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  org.elasticsearch.storm.EsSpout - Reading from <span class="list">[<span class="keyword">forseti-20150916/activity</span>]</span><br><span class="line"><span class="number">13706</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  org.elasticsearch.storm.EsSpout - Discovered mapping &#123;forseti-20150916=<span class="list">[<span class="keyword">mappings=</span><span class="list">[<span class="keyword">activity=</span><span class="list">[<span class="keyword">activity=</span><span class="list">[<span class="keyword">accountAddress=STRING</span>,  event_result=<span class="list">[<span class="keyword">accountLogin=STRING</span>, ], geo=<span class="list">[<span class="keyword">ipAddress=STRING</span>, ipCity=STRING, ], policy=<span class="list">[<span class="keyword">hitRules=</span><span class="list">[<span class="keyword">decision=STRING</span>, ]], policyString2=STRING]]]&#125; for <span class="list">[<span class="keyword">forseti-20150916/activity</span>]</span><br><span class="line"><span class="number">13773</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  backtype.storm.daemon.executor - Opened spout es-spout:<span class="list">(<span class="keyword">2</span>)</span></span><br><span class="line"><span class="number">13785</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  backtype.storm.daemon.executor - Activating spout es-spout:<span class="list">(<span class="keyword">2</span>)</span></span><br><span class="line"><span class="number">71947</span> <span class="list">[<span class="keyword">Thread-19-__acker</span>] INFO  backtype.storm.daemon.executor - Processing received message source: __system:-1, stream: __metrics_tick, id: &#123;&#125;, <span class="list">[<span class="keyword">60</span>]</span><br><span class="line"><span class="number">72067</span> <span class="list">[<span class="keyword">Thread-17-__system</span>] INFO  backtype.storm.daemon.executor - Processing received message source: __system:-1, stream: __metrics_tick, id: &#123;&#125;, <span class="list">[<span class="keyword">60</span>]</span><br><span class="line"><span class="number">72068</span> <span class="list">[<span class="keyword">Thread-15-print</span>] INFO  backtype.storm.daemon.executor - Processing received message source: __system:-1, stream: __metrics_tick, id: &#123;&#125;, <span class="list">[<span class="keyword">60</span>]</span><br><span class="line"><span class="number">72069</span> <span class="list">[<span class="keyword">Thread-19-__acker</span>] INFO  backtype.storm.daemon.task - Emitting: __acker __metrics <span class="list">[#&lt;TaskInfo backtype.storm.metric.api.IMetricsConsumer$TaskInfo@6fef5713&gt; <span class="list">[#&lt;DataPoint <span class="list">[<span class="keyword">__emit-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__process-latency</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__receive</span> = &#123;read_pos=0, write_pos=1, capacity=1024, population=1&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__ack-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__transfer-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__execute-latency</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__fail-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__sendqueue</span> = &#123;read_pos=-1, write_pos=-1, capacity=1024, population=0&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__execute-count</span> = &#123;&#125;]&gt;]]</span><br><span class="line"><span class="number">72069</span> <span class="list">[<span class="keyword">Thread-15-print</span>] INFO  backtype.storm.daemon.task - Emitting: print __metrics <span class="list">[#&lt;TaskInfo backtype.storm.metric.api.IMetricsConsumer$TaskInfo@45690684&gt; <span class="list">[#&lt;DataPoint <span class="list">[<span class="keyword">__emit-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__process-latency</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__receive</span> = &#123;read_pos=0, write_pos=1, capacity=1024, population=1&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__ack-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__transfer-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__execute-latency</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__fail-count</span> = &#123;&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__sendqueue</span> = &#123;read_pos=-1, write_pos=-1, capacity=1024, population=0&#125;]&gt; #&lt;DataPoint <span class="list">[<span class="keyword">__execute-count</span> = &#123;&#125;]&gt;]]</span><br><span class="line"><span class="number">88782</span> <span class="list">[<span class="keyword">Thread-13-es-spout</span>] INFO  org.apache.commons.httpclient.HttpMethodDirector - I/O exception <span class="list">(<span class="keyword">java.net.ConnectException</span>)</span> caught when processing request: Operation timed out</span><br><span class="line"><span class="number">88782</span> <span class="list">[<span class="keyword">Thread-11-es-spout</span>] INFO  org.apache.commons.httpclient.HttpMethodDirector - I/O exception <span class="list">(<span class="keyword">java.net.ConnectException</span>)</span> caught when processing request: Operation timed out</span><br><span class="line"><span class="number">88782</span> <span class="list">[<span class="keyword">Thread-11-es-spout</span>] INFO  org.apache.commons.httpclient.HttpMethodDirector - Retrying request</span><br><span class="line"><span class="number">88782</span> <span class="list">[<span class="keyword">Thread-13-es-spout</span>] INFO  org.apache.commons.httpclient.HttpMethodDirector - Retrying request</span><br><span class="line"><span class="number">89120</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  org.apache.commons.httpclient.HttpMethodDirector - I/O exception <span class="list">(<span class="keyword">java.net.ConnectException</span>)</span> caught when processing request: Operation timed out</span><br><span class="line"><span class="number">89120</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] INFO  org.apache.commons.httpclient.HttpMethodDirector - Retrying request</span><br><span class="line"></span><br><span class="line">原因是: 使用localhost代理线上的47.155. 但是查询es会转发到ES的其他节点, 而在本机是无法连接47网段的.</span><br><span class="line"></span><br><span class="line"><span class="number">315605</span> <span class="list">[<span class="keyword">Thread-11-es-spout</span>] ERROR org.elasticsearch.hadoop.rest.NetworkClient - Node <span class="list">[<span class="keyword">Operation</span> timed out] failed <span class="list">(<span class="keyword">192.168.47.82:9200</span>)</span><span class="comment">; selected next node [172.31.238.19:9200]</span></span><br><span class="line"><span class="number">315605</span> <span class="list">[<span class="keyword">Thread-9-es-spout</span>] ERROR org.elasticsearch.hadoop.rest.NetworkClient - Node <span class="list">[<span class="keyword">Operation</span> timed out] failed <span class="list">(<span class="keyword">172.31.238.26:9200</span>)</span><span class="comment">; selected next node [192.168.47.28:9200]</span></span><br><span class="line"><span class="number">315647</span> <span class="list">[<span class="keyword">Thread-13-es-spout</span>] ERROR org.elasticsearch.hadoop.rest.NetworkClient - Node <span class="list">[<span class="keyword">Operation</span> timed out] failed <span class="list">(<span class="keyword">172.31.238.19:9200</span>)</span><span class="comment">; selected next node [192.168.47.82:9200]</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><br></pre></td></tr></table></figure>
<h4 id="13-每半个小时处理一次">13.每半个小时处理一次</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(partition &gt; <span class="number">0</span>)</span><br><span class="line">  activityDF.repartition(partition).write.parquet(s<span class="string">"/user/tongdun/activity_hist/year=$year/month=$month/day=$day"</span>)</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  activityDF.write.parquet(s<span class="string">"/user/tongdun/activity_hist/year=$year/month=$month/day=$day"</span>)</span><br><span class="line"></span><br><span class="line">原先查询一天, 改成半个小时写一次, 最后复制到activity中:</span><br><span class="line"></span><br><span class="line">                   start           end</span><br><span class="line"><span class="number">0</span>:  <span class="number">0</span>min  -- <span class="number">30</span>m   start           start+halfMs</span><br><span class="line"><span class="number">1</span>:  <span class="number">30</span>min -- <span class="number">1</span>h    start+halfMs    start+halfMs*<span class="number">2</span></span><br><span class="line"><span class="number">2</span>:  <span class="number">1</span>h    -- <span class="number">1.5</span>h  start+halfMs*<span class="number">2</span>  start+halfMs*<span class="number">3</span></span><br><span class="line"><span class="number">3</span>:  <span class="number">1.5</span>h  -- <span class="number">2</span>h</span><br><span class="line">..</span><br><span class="line"><span class="number">46</span>: <span class="number">23</span>h   -- <span class="number">23.5</span>h</span><br><span class="line"><span class="number">47</span>: <span class="number">23.5</span>h -- <span class="number">24</span>h</span><br><span class="line"></span><br><span class="line">change the startHour, endHour=startHour+halfMs</span><br><span class="line">startHour = startOfDay + halfMs*hourIndex.  hourIndex: <span class="number">0</span> until <span class="number">48</span></span><br><span class="line"></span><br><span class="line">copy file to hist. If exist, <span class="keyword">delete</span> first scala执行外部shell命令,返回值<span class="number">0</span>表示执行成功,<span class="number">1</span>表示失败</span><br><span class="line"></span><br><span class="line">import scala.sys.process._</span><br><span class="line"></span><br><span class="line">val createDirDemo = <span class="string">"/usr/install/hadoop/bin/hadoop fs -mkdir /user/tongdun/activity_hist/year=2015/month=9/day=17"</span> !</span><br><span class="line">val removeDirDmoe = <span class="string">"/usr/install/hadoop/bin/hadoop fs -rmr /user/tongdun/activity_hist/year=2015/month=9/day=17"</span> !</span><br><span class="line">val testRm = <span class="string">"/usr/install/hadoop/bin/hadoop fs -rmr /user/qihuang.zheng/test/20150917"</span> !</span><br><span class="line">val create = <span class="string">"/usr/install/hadoop/bin/hadoop fs -mkdir /user/qihuang.zheng/test/20150917"</span> !</span><br><span class="line">val cpDemo = <span class="string">"/usr/install/hadoop/bin/hadoop fs -cp /user/qihuang.zheng/test2/20150917/0/*.parquet /user/qihuang.zheng/test/20150917"</span> !</span><br><span class="line"></span><br><span class="line">删除文件夹,创建文件夹,拷贝文件,刷新分区:</span><br><span class="line"></span><br><span class="line"><span class="string">"/usr/install/hadoop/bin/hadoop fs -mkdir /user/tongdun/activity_hist/year=2015/month=9/day=17"</span> !</span><br><span class="line"></span><br><span class="line">(<span class="number">0</span> until <span class="number">48</span>).foreach(halfHourIndex=&gt;&#123;</span><br><span class="line">  s<span class="string">"/usr/install/hadoop/bin/hadoop fs -cp /user/qihuang.zheng/test2/20150917/$halfHourIndex/*.parquet /user/tongdun/activity_hist/year=2015/month=9/day=17"</span> !</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="string">"/usr/install/apache-hive-0.13.1-bin/bin/hive -e 'msck repair table activity'"</span> !</span><br><span class="line"></span><br><span class="line">如果导入的天数比较多, 可以用多个spark-submit多次执行. 每个spark-submit执行不同的导入时间. 传递的参数示例:</span><br><span class="line"></span><br><span class="line"><span class="number">20150917</span></span><br><span class="line"><span class="number">20150819</span> <span class="number">40</span></span><br><span class="line"><span class="number">20150819</span> <span class="number">40</span> <span class="number">20150917</span>,<span class="number">20150924</span>,<span class="number">20150925</span>,<span class="number">20150926</span>        -- 正式</span><br><span class="line"><span class="number">20150819</span> <span class="number">40</span> <span class="number">20150917</span>,<span class="number">20150924</span>,<span class="number">20150925</span>,<span class="number">20150926</span> debug  -- 测试</span><br><span class="line"></span><br><span class="line">nohup bin/spark-submit --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">4</span> --driver-memory <span class="number">4</span>g \</span><br><span class="line">  --<span class="keyword">class</span> cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter \</span><br><span class="line">  /home/qihuang.zheng/tongdun-app-<span class="number">1.0</span>-SNAPSHOT.jar <span class="number">20150819</span> <span class="number">40</span> <span class="number">20150917</span>,<span class="number">20150924</span>,<span class="number">20150925</span>,<span class="number">20150926</span> &amp;</span><br><span class="line"></span><br><span class="line">/usr/install/es2hdfs.sh</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">#!/bin/sh</span></span><br><span class="line">datebeg=<span class="string">"20150819"</span></span><br><span class="line">dateend=<span class="string">"20150927"</span></span><br><span class="line">beg_s=`date -d <span class="string">"$datebeg"</span> +%s`</span><br><span class="line">end_s=`date -d <span class="string">"$dateend"</span> +%s`</span><br><span class="line">excludes=<span class="string">"20150917 20150924 20150925 20150926"</span></span><br><span class="line">cd /usr/install/spark-<span class="number">1.4</span><span class="number">.1</span>-bin-hadoop2<span class="number">.4</span>/</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> [ <span class="string">"$beg_s"</span> -le <span class="string">"$end_s"</span> ];<span class="keyword">do</span></span><br><span class="line">     day=`date -d @$beg_s +<span class="string">"%Y%m%d"</span>`;</span><br><span class="line">     beg_s=$((beg_s+<span class="number">86400</span>));</span><br><span class="line">     flag=<span class="literal">false</span></span><br><span class="line">     <span class="keyword">for</span> item in $excludes</span><br><span class="line">     <span class="keyword">do</span></span><br><span class="line">        <span class="keyword">if</span> [ <span class="string">"$day"</span> == <span class="string">"$item"</span> ]; then</span><br><span class="line">          echo <span class="string">"$day In the list, skip"</span></span><br><span class="line">          flag=<span class="literal">true</span></span><br><span class="line">        fi</span><br><span class="line">     done</span><br><span class="line"></span><br><span class="line">     <span class="keyword">if</span> [ $flag == <span class="literal">false</span> ]; then</span><br><span class="line">       echo $day</span><br><span class="line">       <span class="preprocessor">#do something here....</span></span><br><span class="line"></span><br><span class="line">       bin/spark-submit --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">         --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">         --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">4</span> --driver-memory <span class="number">4</span>g \</span><br><span class="line">         --<span class="keyword">class</span> cn.fraudmetrix.dataplatform.lab.ES2HDFSExporter \</span><br><span class="line">         /home/qihuang.zheng/tongdun-app-<span class="number">1.0</span>-SNAPSHOT.jar $day</span><br><span class="line">     fi</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<h4 id="14-shell可以,submit不行">14.shell可以,submit不行</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">测试一天的数据, 使用submit会报空指针错误. 但是同样的代码使用spark-shell则可以成功运行.</span><br><span class="line">bin/spark-submit --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">         --jars /home/qihuang.zheng/json4s-ast_2.<span class="number">10</span>-<span class="number">3.2</span>.<span class="number">11</span><span class="class">.jar</span>,/home/qihuang.zheng/json4s-core_2.<span class="number">10</span>-<span class="number">3.2</span>.<span class="number">11</span><span class="class">.jar</span>,/home/qihuang.zheng/json4s-jackson_2.<span class="number">10</span>-<span class="number">3.2</span>.<span class="number">11</span><span class="class">.jar</span>,/home/qihuang.zheng/json4s-native_2.<span class="number">10</span>-<span class="number">3.2</span>.<span class="number">11</span><span class="class">.jar</span>,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span>.<span class="number">1</span><span class="class">.jar</span> \</span><br><span class="line">         --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">4</span> --driver-memory <span class="number">4</span>g \</span><br><span class="line">         --class cn<span class="class">.fraudmetrix</span><span class="class">.dataplatform</span><span class="class">.lab</span><span class="class">.ES2HDFSExporter</span> \</span><br><span class="line">         /home/qihuang.zheng/tongdun-app-<span class="number">1.0</span>-SNAPSHOT<span class="class">.jar</span> <span class="number">20150927</span></span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">19</span> INFO TaskSchedulerImpl: Adding task set <span class="number">0.0</span> with <span class="number">3</span> tasks</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">19</span> INFO TaskSetManager: Starting task <span class="number">0.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">0</span>, spark047218, ANY, <span class="number">28706</span> bytes)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">19</span> INFO TaskSetManager: Starting task <span class="number">1.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">1</span>, spark047242, ANY, <span class="number">28705</span> bytes)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">19</span> INFO TaskSetManager: Starting task <span class="number">2.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">2</span>, spark047216, ANY, <span class="number">28705</span> bytes)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">20</span> INFO BlockManagerInfo: Added broadcast_0_piece0 <span class="keyword">in</span> memory on spark047242:<span class="number">58232</span> (size: <span class="number">21.6</span> KB, free: <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">20</span> INFO BlockManagerInfo: Added broadcast_0_piece0 <span class="keyword">in</span> memory on spark047218:<span class="number">59207</span> (size: <span class="number">21.6</span> KB, free: <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">44</span>:<span class="number">22</span> INFO BlockManagerInfo: Added broadcast_0_piece0 <span class="keyword">in</span> memory on spark047216:<span class="number">48575</span> (size: <span class="number">21.6</span> KB, free: <span class="number">2.1</span> GB)</span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">15</span>:<span class="number">49</span>:<span class="number">45</span> WARN TaskSetManager: Lost task <span class="number">2.0</span> <span class="keyword">in</span> stage <span class="number">0.0</span> (TID <span class="number">2</span>, spark047216): java<span class="class">.lang</span><span class="class">.NullPointerException</span></span><br><span class="line">    at parquet<span class="class">.hadoop</span><span class="class">.InternalParquetRecordWriter</span><span class="class">.flushRowGroupToStore</span>(InternalParquetRecordWriter<span class="class">.java</span>:<span class="number">146</span>)</span><br><span class="line">    at parquet<span class="class">.hadoop</span><span class="class">.InternalParquetRecordWriter</span><span class="class">.close</span>(InternalParquetRecordWriter<span class="class">.java</span>:<span class="number">112</span>)</span><br><span class="line">    at parquet<span class="class">.hadoop</span><span class="class">.ParquetRecordWriter</span><span class="class">.close</span>(ParquetRecordWriter<span class="class">.java</span>:<span class="number">73</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.parquet</span><span class="class">.ParquetOutputWriter</span><span class="class">.close</span>(newParquet<span class="class">.scala</span>:<span class="number">88</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.sources</span><span class="class">.DefaultWriterContainer</span><span class="class">.abortTask</span>(commands<span class="class">.scala</span>:<span class="number">491</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.sources</span><span class="class">.InsertIntoHadoopFsRelation</span><span class="class">.org</span><span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$sql</span><span class="variable">$sources</span><span class="variable">$InsertIntoHadoopFsRelation</span>$<span class="variable">$writeRows</span>$<span class="number">1</span>(commands<span class="class">.scala</span>:<span class="number">190</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.sources</span><span class="class">.InsertIntoHadoopFsRelation</span>$<span class="variable">$anonfun</span><span class="variable">$insert</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(commands.scala:<span class="number">160</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.sources</span><span class="class">.InsertIntoHadoopFsRelation</span>$<span class="variable">$anonfun</span><span class="variable">$insert</span>$<span class="number">1</span>.<span class="function"><span class="title">apply</span><span class="params">(commands.scala:<span class="number">160</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.scheduler</span><span class="class">.ResultTask</span><span class="class">.runTask</span>(ResultTask<span class="class">.scala</span>:<span class="number">63</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.scheduler</span><span class="class">.Task</span><span class="class">.run</span>(Task<span class="class">.scala</span>:<span class="number">70</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.executor</span><span class="class">.Executor</span><span class="variable">$TaskRunner</span>.<span class="function"><span class="title">run</span><span class="params">(Executor.scala:<span class="number">213</span>)</span></span></span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.ThreadPoolExecutor</span><span class="class">.runWorker</span>(ThreadPoolExecutor<span class="class">.java</span>:<span class="number">1145</span>)</span><br><span class="line">    at java<span class="class">.util</span><span class="class">.concurrent</span><span class="class">.ThreadPoolExecutor</span><span class="variable">$Worker</span>.<span class="function"><span class="title">run</span><span class="params">(ThreadPoolExecutor.java:<span class="number">615</span>)</span></span></span><br><span class="line">    at java<span class="class">.lang</span><span class="class">.Thread</span><span class="class">.run</span>(Thread<span class="class">.java</span>:<span class="number">744</span>)</span><br></pre></td></tr></table></figure>
<h4 id="15-spark-shell+script_file">15.spark-shell+script file</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/install/ES2HDFS.scala <span class="number">20150917</span> | spark-<span class="number">1.4</span><span class="number">.1</span>-bin-hadoop2<span class="number">.4</span>/bin/spark-shell --jars /home/qihuang.zheng/json4s-ast_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-core_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-jackson_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/json4s-native_2<span class="number">.10</span>-<span class="number">3.2</span><span class="number">.11</span>.jar,/home/qihuang.zheng/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.1</span>.jar \</span><br><span class="line">  --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">4</span> --driver-memory <span class="number">4</span>g \</span><br><span class="line">  --conf spark.es.nodes=<span class="number">192.168</span><span class="number">.47</span><span class="number">.155</span>:<span class="number">9200</span> \</span><br><span class="line">  --conf spark.es.field.read.empty.as.null=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line">spark-shell &lt; ES2HDFSExporter.scala <span class="number">20150927</span></span><br><span class="line"></span><br><span class="line">bin/spark-shell -i ES2HDFSExporter.scala <span class="number">20150927</span></span><br></pre></td></tr></table></figure>
<p>参数怎么传递:</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">http:<span class="comment">//stackoverflow.com/questions/29928999/passing-command-line-arguments-to-spark-shell</span></span><br><span class="line"></span><br><span class="line">./spark-script.<span class="keyword">sh</span> your_file.<span class="keyword">scala</span> first_arg second_arg third_arg</span><br><span class="line"></span><br><span class="line">#!/bin/<span class="keyword">sh</span></span><br><span class="line">scala_file=<span class="label">$1</span></span><br><span class="line">shift 1</span><br><span class="line">arguments=$@</span><br><span class="line">#<span class="keyword">set</span> +o posix  # to enable process substitution when not running <span class="keyword">on</span> bash</span><br><span class="line"><span class="keyword">cd</span> /usr/install/spark-1.4.1-bin-hadoop2.4/bin</span><br><span class="line">spark-<span class="keyword">shell</span>  --jars /home/qihuang.zheng/json4s-ast_2.10-3.2.11.jar,/home/qihuang.zheng/json4s-core_2.10-3.2.11.jar,/home/qihuang.zheng/json4s-jackson_2.10-3.2.11.jar,/home/qihuang.zheng/json4s-native_2.10-3.2.11.jar,/home/qihuang.zheng/elasticsearch-hadoop-2.1.1.jar \</span><br><span class="line">               --master spark:<span class="comment">//192.168.47.209:7077,192.168.47.216:7077 \</span></span><br><span class="line">               --executor-<span class="keyword">memory</span> 4g --<span class="keyword">total</span>-executor-cores 4 --driver-<span class="keyword">memory</span> 4g \</span><br><span class="line">               --<span class="keyword">conf</span> spark.es.nodes=192.168.47.155:9200 \</span><br><span class="line">               --<span class="keyword">conf</span> spark.es.field.<span class="keyword">read</span>.empty.<span class="keyword">as</span>.null=true \</span><br><span class="line">               -i &lt;(echo 'val <span class="keyword">args</span> = <span class="string">"'$arguments'"</span>.<span class="keyword">split</span>(<span class="string">"\\s+"</span>)' ; <span class="keyword">cat</span> <span class="label">$scala_file</span>)</span><br><span class="line"></span><br><span class="line">将es2hdfs.<span class="keyword">sh</span>中间部分换成:</span><br><span class="line">       <span class="keyword">sh</span> spark-script.<span class="keyword">sh</span> /usr/install/ES2HDFS.<span class="keyword">scala</span> <span class="label">$day</span></span><br><span class="line"></span><br><span class="line">./spark-script.<span class="keyword">sh</span>: <span class="keyword">line</span> 16: <span class="keyword">syntax</span> <span class="keyword">error</span> near unexpected <span class="keyword">token</span> `('</span><br><span class="line">./spark-script.<span class="keyword">sh</span>: <span class="keyword">line</span> 16: `               -i &lt;(echo 'val <span class="keyword">args</span> = 20150917' ; <span class="keyword">cat</span> <span class="label">$scala_file</span>)'</span><br></pre></td></tr></table></figure>
<h4 id="16-JSON解析异常">16.JSON解析异常</h4><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Caused <span class="keyword">by</span>: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException:</span><br><span class="line">  org.codehaus.jackson.JsonParseException: Illegal unquoted <span class="keyword">character</span> ((CTRL-CHAR, code <span class="number">4</span>)):</span><br><span class="line">  has <span class="built_in">to</span> be escaped <span class="keyword">using</span> <span class="constant">backslash</span> <span class="built_in">to</span> be included <span class="operator">in</span> <span class="keyword">string</span> <span class="built_in">value</span></span><br><span class="line"> <span class="keyword">at</span> [Source: org.apache.commons.httpclient.AutoCloseInputStream@b1c8cbf; <span class="built_in">line</span>: <span class="number">1</span>, column: <span class="number">29762</span>]</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">16</span>:<span class="number">31</span>:<span class="number">44</span> WARN TaskSetManager: Lost task <span class="number">0.0</span> <span class="operator">in</span> stage <span class="number">17.0</span> (TID <span class="number">51</span>, spark047216):</span><br><span class="line">  org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException):</span><br><span class="line">  No lease <span class="command"><span class="keyword">on</span> /<span class="title">user</span>/<span class="title">qihuang</span>.<span class="title">zheng</span>/<span class="title">es_hdfs_back</span>/<span class="title">20150917</span>/<span class="title">17</span>/<span class="title">_temporary</span>/<span class="title">0</span>/<span class="title">_temporary</span>/<span class="title">attempt_201509291628_0017_m_000000_0</span>/</span></span><br><span class="line">  part-r-<span class="number">00000</span>-b701f6b9-f4d2-<span class="number">439</span>b-<span class="number">8</span>fe9-<span class="number">5334331e6</span>e06.gz.parquet: File does <span class="operator">not</span> exist.</span><br><span class="line">  Holder DFSClient_attempt_201509291525_0000_m_000002_0_-<span class="number">1846407483</span>_48 does <span class="operator">not</span> have <span class="keyword">any</span> <span class="built_in">open</span> <span class="built_in">files</span>.</span><br></pre></td></tr></table></figure>
<p>这个错误并不是总是出现.  </p>
<h3 id="关于ES的mapping和Spark的类型解析">关于ES的mapping和Spark的类型解析</h3><p>esRDD读取出来的map: Map[String, Any]. 下面是ES的一个示例.</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="collection">&#123;</span><br><span class="line">    <span class="string">"policyString2"</span>: <span class="string">"&#123;....&#125;"</span></span><br><span class="line">    <span class="string">"geo"</span>: <span class="collection">&#123;</span><br><span class="line">        <span class="string">"ipCountry"</span>: <span class="string">"*"</span>,</span><br><span class="line">        <span class="string">"sequenceId"</span>: <span class="string">"111111111-111111"</span>,</span><br><span class="line">        <span class="string">"ipAddress"</span>: <span class="string">"100.126.1.41"</span></span><br><span class="line">    &#125;</span>,</span><br><span class="line">    <span class="string">"browser"</span>: <span class="collection">&#123;</span><br><span class="line">        <span class="string">"sequenceId"</span>: <span class="string">"111111111-111111"</span></span><br><span class="line">    &#125;</span>,..</span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure>
<p>由于Map的value有String和{}(比如geo的值是一个Map), 所以是Any类型.<br>而读取HDFS中已经存在的parquet文件的schema, 对于map的value类型都是string.  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[sequence_id: <span class="built_in">string</span>, occur_time: bigint,</span><br><span class="line">  activity_map: <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">  browser_map: <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;,</span><br><span class="line">  ......</span><br><span class="line">  year: <span class="keyword">int</span>, month: <span class="keyword">int</span>, day: <span class="keyword">int</span>]</span><br></pre></td></tr></table></figure>
<p>esRDD读取的是ES的mapping策略. 业务系统中activity的age字段为LONG  </p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"age"</span>: &#123;</span><br><span class="line">  <span class="string">"type"</span>: <span class="string">"long"</span></span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure>
<p>对应es_spark_mapping.json中的也是LONG类型:</p>
<figure class="highlight fix"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">age</span>=<span class="string">LONG,</span></span><br></pre></td></tr></table></figure>
<p>如果是使用esDF查询, 类型是long(es_spark_mapping.json):</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">|    |-- age: <span class="keyword">long</span> (nullable = <span class="keyword">true</span>)</span><br></pre></td></tr></table></figure>
<p>所以parsing方法要具体处理每一种类型.<br>TODO: 但是处理每一种类型, parsing方法在大数据量的时候很慢!</p>
<hr>
<p>parsing要做的是: 要将一个大的Map, 拆分成多个小的Map.</p>
<figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="label">geo_map:</span> &#123;</span><br><span class="line">    <span class="string">"ipCountry"</span>: <span class="string">"*"</span>,</span><br><span class="line">    <span class="string">"sequenceId"</span>: <span class="string">"111111111-111111"</span>,</span><br><span class="line">    <span class="string">"ipAddress"</span>: <span class="string">"100.126.1.41"</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="label">browser_map:</span> &#123;</span><br><span class="line">    <span class="string">"sequenceId"</span>: <span class="string">"111111111-111111"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最终写成parquet文件的表结构:</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">activityDF.printSchema</span><br><span class="line">     root</span><br><span class="line">     |-- <span class="string">sequence_id:</span> string (nullable = <span class="literal">true</span>)</span><br><span class="line">     |-- <span class="string">occur_time:</span> <span class="typename">long</span> (nullable = <span class="literal">false</span>)</span><br><span class="line">     |-- <span class="string">activity_map:</span> map (nullable = <span class="literal">true</span>)</span><br><span class="line">     |    |-- <span class="string">key:</span> string</span><br><span class="line">     |    |-- <span class="string">value:</span> string (valueContainsNull = <span class="literal">true</span>)</span><br><span class="line">     |-- <span class="string">browser_map:</span> map (nullable = <span class="literal">true</span>)</span><br><span class="line">     |    |-- <span class="string">key:</span> string</span><br><span class="line">     |    |-- <span class="string">value:</span> string (valueContainsNull = <span class="literal">true</span>)</span><br><span class="line">     .....</span><br></pre></td></tr></table></figure>
      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/elasticsearch/">elasticsearch</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-06-20-ElasticSearch" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/06/20/2015-06-20-ElasticSearch/" class="article-date">
  	<time datetime="2015-06-19T16:00:00.000Z" itemprop="datePublished">2015-06-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/20/2015-06-20-ElasticSearch/">ElasticSearch入门</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="You_Know,_For_Search">You Know, For Search</h2><p><a href="http://www.elasticsearch.cn/" target="_blank" rel="external">http://www.elasticsearch.cn/</a></p>
<p>启动ES: </p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/elasticsearch</span><br></pre></td></tr></table></figure>
<p>Hello CRUD: </p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//索引. 数据库为twitter,表名为user,主键为kimchy,字段name值为Shay Banon</span></span><br><span class="line">$ curl -XPUT http:<span class="comment">//localhost:9200/twitter/user/kimchy -d '&#123;</span></span><br><span class="line">    <span class="string">"name"</span> : <span class="string">"Shay Banon"</span></span><br><span class="line">&#125;'</span><br><span class="line"></span><br><span class="line"><span class="comment">//索引，多个field. 主键为1,tweet表有三个字段:user,post_date,message</span></span><br><span class="line">$ curl -XPUT http:<span class="comment">//localhost:9200/twitter/tweet/1 -d '&#123;</span></span><br><span class="line">    <span class="string">"user"</span>: <span class="string">"kimchy"</span>,</span><br><span class="line">    <span class="string">"post_date"</span>: <span class="string">"2009-11-15T13:12:00"</span>,</span><br><span class="line">    <span class="string">"message"</span>: <span class="string">"Trying out elasticsearch, so far so good?"</span></span><br><span class="line">&#125;'</span><br><span class="line"></span><br><span class="line"><span class="comment">//索引，注意url里面的id是不一样的哦. 主键为2,另一条记录</span></span><br><span class="line">$ curl -XPUT http:<span class="comment">//localhost:9200/twitter/tweet/2 -d '&#123;</span></span><br><span class="line">    <span class="string">"user"</span>: <span class="string">"kimchy"</span>,</span><br><span class="line">    <span class="string">"post_date"</span>: <span class="string">"2009-11-15T14:12:12"</span>,</span><br><span class="line">    <span class="string">"message"</span>: <span class="string">"You know, for Search"</span></span><br><span class="line">&#125;'</span><br><span class="line"></span><br><span class="line"><span class="comment">//索引, 同一个id再次put相当于更新,version+1</span></span><br><span class="line">$ curl -XPUT http:<span class="comment">//localhost:9200/twitter/tweet/2 -d '&#123;</span></span><br><span class="line">    <span class="string">"user"</span>: <span class="string">"kimchy"</span>,</span><br><span class="line">    <span class="string">"post_date"</span>: <span class="string">"2009-11-15T14:12:12"</span>,</span><br><span class="line">    <span class="string">"message"</span>: <span class="string">"You know, for Search"</span></span><br><span class="line">&#125;'</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取: 获取twitter数据库中tweet表主键为2的记录</span></span><br><span class="line">$ curl -XGET http:<span class="comment">//localhost:9200/twitter/tweet/2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//lucene语法方式的查询, 查询tweet表中user字段=kimchy的记录</span></span><br><span class="line">$ curl -XGET http:<span class="comment">//localhost:9200/twitter/tweet/_search?q=user:kimchy</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//query DSL方式查询, 同上</span></span><br><span class="line">$ curl -XGET http:<span class="comment">//localhost:9200/twitter/tweet/_search -d '&#123;</span></span><br><span class="line">    <span class="string">"query"</span> : &#123;</span><br><span class="line">        <span class="string">"term"</span> : &#123; <span class="string">"user"</span>: <span class="string">"kimchy"</span> &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;'</span><br><span class="line"></span><br><span class="line"><span class="comment">//query DSL方式查询, 查询post_date在指定范围内的记录集</span></span><br><span class="line">$ curl -XGET http:<span class="comment">//localhost:9200/twitter/_search?pretty=true -d '&#123;</span></span><br><span class="line">    <span class="string">"query"</span> : &#123;</span><br><span class="line">        <span class="string">"range"</span> : &#123;</span><br><span class="line">            <span class="string">"post_date"</span> : &#123;</span><br><span class="line">                <span class="string">"from"</span> : <span class="string">"2009-11-15T13:00:00"</span>,</span><br><span class="line">                <span class="string">"to"</span> : <span class="string">"2009-11-15T14:30:00"</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;'</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>Exampel</th>
<th>ElasticSearch</th>
<th>DataBases</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 twitter</td>
<td>Index 👉</td>
<td>DataBase</td>
</tr>
<tr>
<td>2 tweet</td>
<td>Type 👉</td>
<td>Table</td>
</tr>
<tr>
<td>3 user,post_date,message</td>
<td>-</td>
<td>Field</td>
</tr>
</tbody>
</table>
<p>对应的结构Mapping: <strong> <a href="http://localhost:9200/_all?pretty" target="_blank" rel="external">http://localhost:9200/_all?pretty</a> </strong></p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"twitter"</span> : &#123;							    <span class="number">1</span></span><br><span class="line">    <span class="string">"aliases"</span> : &#123; &#125;,</span><br><span class="line">    <span class="string">"mappings"</span> : &#123;</span><br><span class="line">      <span class="string">"user"</span> : &#123;						  <span class="number">2</span></span><br><span class="line">        <span class="string">"properties"</span> : &#123;</span><br><span class="line">          <span class="string">"name"</span> : &#123;					<span class="number">3</span>️</span><br><span class="line">            <span class="string">"type"</span> : <span class="string">"string"</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"tweet"</span> : &#123;					    <span class="number">2</span></span><br><span class="line">        <span class="string">"properties"</span> : &#123;</span><br><span class="line">          <span class="string">"message"</span> : &#123;				<span class="number">3</span></span><br><span class="line">            <span class="string">"type"</span> : <span class="string">"string"</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="string">"post_date"</span> : &#123;			<span class="number">3</span></span><br><span class="line">            <span class="string">"type"</span> : <span class="string">"date"</span>,</span><br><span class="line">            <span class="string">"format"</span> : <span class="string">"dateOptionalTime"</span></span><br><span class="line">          &#125;,</span><br><span class="line">          <span class="string">"user"</span> : &#123;					<span class="number">3</span></span><br><span class="line">            <span class="string">"type"</span> : <span class="string">"string"</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"settings"</span> : &#123;</span><br><span class="line">      <span class="string">"index"</span> : &#123;</span><br><span class="line">        <span class="string">"creation_date"</span> : <span class="string">"1435222918809"</span>,</span><br><span class="line">        <span class="string">"number_of_shards"</span> : <span class="string">"5"</span>,</span><br><span class="line">        <span class="string">"number_of_replicas"</span> : <span class="string">"1"</span>,</span><br><span class="line">        <span class="string">"version"</span> : &#123;</span><br><span class="line">          <span class="string">"created"</span> : <span class="string">"1060099"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"uuid"</span> : <span class="string">"jk1_iEyvTaibjDyLymZO5g"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"warmers"</span> : &#123; &#125;</span><br><span class="line">  &#125;,</span><br></pre></td></tr></table></figure>
<p>日志信息</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">➜  elasticsearch-1.6.0  bin/elasticsearch</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:05,297</span>][<span class="link_reference">INFO </span>][<span class="link_label">node                     </span>] [Y'Garon] version[1.6.0], pid[5619], build[cdd3ac4/2015-06-09T13:36:34Z]</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:05,298</span>][<span class="link_reference">INFO </span>][<span class="link_label">node                     </span>] [Y'Garon] initializing ...</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:05,313</span>][<span class="link_reference">INFO </span>][<span class="link_label">plugins                  </span>] [Y'Garon] loaded [], sites []</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:05,517</span>][<span class="link_reference">INFO </span>][<span class="link_label">env                      </span>] [Y'Garon] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable<span class="emphasis">_space [61.3gb], net total_</span>space [111.8gb], types [hfs]</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:13,075</span>][<span class="link_reference">INFO </span>][<span class="link_label">node                     </span>] [Y'Garon] initialized</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:13,078</span>][<span class="link_reference">INFO </span>][<span class="link_label">node                     </span>] [Y'Garon] starting ...</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:13,457</span>][<span class="link_reference">INFO </span>][<span class="link_label">transport                </span>] [Y'Garon] bound<span class="emphasis">_address &#123;inet[/0:0:0:0:0:0:0:0:9300]&#125;, publish_</span>address &#123;inet[/192.168.7.130:9300]&#125;</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:13,532</span>][<span class="link_reference">INFO </span>][<span class="link_label">discovery                </span>] [Y'Garon] elasticsearch/5K_ytFi9QfGnt0aPACBo-A</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:17,368</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.service          </span>] [<span class="link_label">Y'Garon</span>] new<span class="emphasis">_master [Y'Garon][5K_</span>ytFi9QfGnt0aPACBo-A][<span class="link_label">zqhmac</span>][<span class="link_reference">inet[/192.168.7.130:9300</span>]], reason: zen-disco-join (elected<span class="emphasis">_as_</span>master)</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:17,399</span>][<span class="link_reference">INFO </span>][<span class="link_label">http                     </span>] [Y'Garon] bound<span class="emphasis">_address &#123;inet[/0:0:0:0:0:0:0:0:9200]&#125;, publish_</span>address &#123;inet[/192.168.7.130:9200]&#125;</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:17,399</span>][<span class="link_reference">INFO </span>][<span class="link_label">node                     </span>] [Y'Garon] started</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:17,419</span>][<span class="link_reference">INFO </span>][<span class="link_label">gateway                  </span>] [Y'Garon] recovered [0] indices into cluster_state</span><br><span class="line">[<span class="link_label">2015-06-25 17:01:59,932</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [twitter] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [tweet]</span><br><span class="line">[<span class="link_label">2015-06-25 17:02:00,497</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [twitter] update_mapping [tweet] (dynamic)</span><br><span class="line">[<span class="link_label">2015-06-25 17:05:01,428</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [spark] creating index, cause [api], templates [], shards [5]/[1], mappings []</span><br><span class="line">[<span class="link_label">2015-06-25 17:05:02,010</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [spark] update_mapping [docs] (dynamic)</span><br><span class="line">[<span class="link_label">2015-06-25 17:05:03,284</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [spark] update_mapping [docs] (dynamic)</span><br><span class="line">[<span class="link_label">2015-06-25 17:05:03,402</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [spark] update_mapping [json-trips] (dynamic)</span><br><span class="line">[<span class="link_label">2015-06-25 17:17:11,309</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [twitter] update_mapping [user] (dynamic)</span><br><span class="line">[<span class="link_label">2015-06-25 17:25:32,611</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [my-collection] creating index, cause [auto(bulk api)], templates [], shards [5]/[1], mappings [game, music, book]</span><br><span class="line">[<span class="link_label">2015-06-25 17:25:32,800</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [my-collection] update_mapping [music] (dynamic)</span><br><span class="line">[<span class="link_label">2015-06-25 17:25:32,811</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [my-collection] update_mapping [book] (dynamic)</span><br><span class="line">[<span class="link_label">2015-06-25 17:25:32,819</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [my-collection] update_mapping [game] (dynamic)</span><br><span class="line">[<span class="link_label">2015-06-25 18:04:25,860</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [airports] creating index, cause [api], templates [], shards [5]/[1], mappings []</span><br><span class="line">[<span class="link_label">2015-06-25 18:04:26,148</span>][<span class="link_reference">INFO </span>][<span class="link_label">cluster.metadata         </span>] [Y'Garon] [airports] update_mapping [2015] (dynamic)</span><br></pre></td></tr></table></figure>
<h3 id="基本查询API">基本查询API</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line">👉 <span class="number">1.</span>查询所有数据库所有的记录数</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/_count</span></span><br><span class="line">&#123;<span class="string">"count"</span>:<span class="number">9</span>,<span class="string">"_shards"</span>:&#123;<span class="string">"total"</span>:<span class="number">10</span>,<span class="string">"successful"</span>:<span class="number">10</span>,<span class="string">"failed"</span>:<span class="number">0</span>&#125;&#125;%</span><br><span class="line"></span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/_cat/count</span></span><br><span class="line"><span class="number">1435224149</span> <span class="number">17</span>:<span class="number">22</span>:<span class="number">29</span> <span class="number">9</span></span><br><span class="line"></span><br><span class="line">👉 <span class="number">2.</span>查询根路径下都有哪些REST服务</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/_cat</span></span><br><span class="line">=^.^=</span><br><span class="line">/_cat/allocation</span><br><span class="line">/_cat/shards</span><br><span class="line">/_cat/shards/&#123;index&#125;</span><br><span class="line">/_cat/master</span><br><span class="line">/_cat/nodes</span><br><span class="line">/_cat/indices</span><br><span class="line">/_cat/indices/&#123;index&#125;</span><br><span class="line">/_cat/segments</span><br><span class="line">/_cat/segments/&#123;index&#125;</span><br><span class="line">/_cat/count</span><br><span class="line">/_cat/count/&#123;index&#125;</span><br><span class="line">/_cat/recovery</span><br><span class="line">/_cat/recovery/&#123;index&#125;</span><br><span class="line">/_cat/health</span><br><span class="line">/_cat/pending_tasks</span><br><span class="line">/_cat/aliases</span><br><span class="line">/_cat/aliases/&#123;alias&#125;</span><br><span class="line">/_cat/thread_pool</span><br><span class="line">/_cat/plugins</span><br><span class="line">/_cat/fielddata</span><br><span class="line">/_cat/fielddata/&#123;fields&#125;</span><br><span class="line"></span><br><span class="line">👉 <span class="number">3.</span>ES所有信息</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/_all</span></span><br><span class="line"></span><br><span class="line">👉 <span class="number">4.</span>查询都有哪些数据库. 其中index对应了DB</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/_cat/indices</span></span><br><span class="line">yellow open spark   <span class="number">5</span> <span class="number">1</span> <span class="number">6</span> <span class="number">0</span>  <span class="number">16</span>kb  <span class="number">16</span>kb</span><br><span class="line">yellow open twitter <span class="number">5</span> <span class="number">1</span> <span class="number">3</span> <span class="number">0</span> <span class="number">8.7</span>kb <span class="number">8.7</span>kb</span><br><span class="line"></span><br><span class="line">👉 <span class="number">5.</span>查询数据库下有哪些表</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/spark</span></span><br><span class="line"></span><br><span class="line">👉 <span class="number">6.</span>查询数据库的记录数</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/spark/_count</span></span><br><span class="line">&#123;<span class="string">"count"</span>:<span class="number">6</span>,<span class="string">"_shards"</span>:&#123;<span class="string">"total"</span>:<span class="number">5</span>,<span class="string">"successful"</span>:<span class="number">5</span>,<span class="string">"failed"</span>:<span class="number">0</span>&#125;&#125;</span><br><span class="line"></span><br><span class="line">👉 <span class="number">7.</span>查询数据库下某张表的记录数, 表的信息从<span class="number">5.</span>中得到</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/spark/people/_count</span></span><br><span class="line"></span><br><span class="line">👉 <span class="number">8.</span>查询数据库某张表的所有记录</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/spark/people/_search?q=*&amp;pretty</span></span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/spark/people/_search?petty</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"took"</span> : <span class="number">9</span>,</span><br><span class="line">  <span class="string">"timed_out"</span> : <span class="literal">false</span>,</span><br><span class="line">  <span class="string">"_shards"</span> : &#123;</span><br><span class="line">    <span class="string">"total"</span> : <span class="number">5</span>,</span><br><span class="line">    <span class="string">"successful"</span> : <span class="number">5</span>,</span><br><span class="line">    <span class="string">"failed"</span> : <span class="number">0</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"hits"</span> : &#123;</span><br><span class="line">    <span class="string">"total"</span> : <span class="number">3</span>,</span><br><span class="line">    <span class="string">"max_score"</span> : <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"hits"</span> : [ &#123;</span><br><span class="line">      <span class="string">"_index"</span> : <span class="string">"spark"</span>,</span><br><span class="line">      <span class="string">"_type"</span> : <span class="string">"people"</span>,</span><br><span class="line">      <span class="string">"_id"</span> : <span class="string">"AU4tjNnbu9a9LJalW5X9"</span>,</span><br><span class="line">      <span class="string">"_score"</span> : <span class="number">1.0</span>,</span><br><span class="line">      <span class="string">"_source"</span>:&#123;<span class="string">"name"</span>:<span class="string">"Andy"</span>,<span class="string">"surname"</span>:<span class="string">"Feng"</span>,<span class="string">"age"</span>:<span class="number">30</span>&#125;</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">      <span class="string">"_index"</span> : <span class="string">"spark"</span>,</span><br><span class="line">      <span class="string">"_type"</span> : <span class="string">"people"</span>,</span><br><span class="line">      <span class="string">"_id"</span> : <span class="string">"AU4tjNnbu9a9LJalW5X8"</span>,</span><br><span class="line">      <span class="string">"_score"</span> : <span class="number">1.0</span>,</span><br><span class="line">      <span class="string">"_source"</span>:&#123;<span class="string">"name"</span>:<span class="string">"Michael"</span>,<span class="string">"surname"</span>:<span class="string">"Jackson"</span>,<span class="string">"age"</span>:<span class="number">29</span>&#125;</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">      <span class="string">"_index"</span> : <span class="string">"spark"</span>,</span><br><span class="line">      <span class="string">"_type"</span> : <span class="string">"people"</span>,</span><br><span class="line">      <span class="string">"_id"</span> : <span class="string">"AU4tjNnbu9a9LJalW5X-"</span>,</span><br><span class="line">      <span class="string">"_score"</span> : <span class="number">1.0</span>,</span><br><span class="line">      <span class="string">"_source"</span>:&#123;<span class="string">"name"</span>:<span class="string">"Justin"</span>,<span class="string">"surname"</span>:<span class="string">"Beeper"</span>,<span class="string">"age"</span>:<span class="number">19</span>&#125;</span><br><span class="line">    &#125; ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">👉 查询某张表所有字段的某个值</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/spark/people/_search?q=*s*</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"took"</span> : <span class="number">12</span>,</span><br><span class="line">  <span class="string">"timed_out"</span> : <span class="literal">false</span>,</span><br><span class="line">  <span class="string">"_shards"</span> : &#123;</span><br><span class="line">    <span class="string">"total"</span> : <span class="number">5</span>,</span><br><span class="line">    <span class="string">"successful"</span> : <span class="number">5</span>,</span><br><span class="line">    <span class="string">"failed"</span> : <span class="number">0</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"hits"</span> : &#123;</span><br><span class="line">    <span class="string">"total"</span> : <span class="number">2</span>,</span><br><span class="line">    <span class="string">"max_score"</span> : <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"hits"</span> : [ &#123;</span><br><span class="line">      <span class="string">"_index"</span> : <span class="string">"spark"</span>,</span><br><span class="line">      <span class="string">"_type"</span> : <span class="string">"people"</span>,</span><br><span class="line">      <span class="string">"_id"</span> : <span class="string">"AU4tjNnbu9a9LJalW5X8"</span>,</span><br><span class="line">      <span class="string">"_score"</span> : <span class="number">1.0</span>,</span><br><span class="line">      <span class="string">"_source"</span>:&#123;<span class="string">"name"</span>:<span class="string">"Michael"</span>,<span class="string">"surname"</span>:<span class="string">"Jackson"</span>,<span class="string">"age"</span>:<span class="number">29</span>&#125;</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">      <span class="string">"_index"</span> : <span class="string">"spark"</span>,</span><br><span class="line">      <span class="string">"_type"</span> : <span class="string">"people"</span>,</span><br><span class="line">      <span class="string">"_id"</span> : <span class="string">"AU4tjNnbu9a9LJalW5X-"</span>,</span><br><span class="line">      <span class="string">"_score"</span> : <span class="number">1.0</span>,</span><br><span class="line">      <span class="string">"_source"</span>:&#123;<span class="string">"name"</span>:<span class="string">"Justin"</span>,<span class="string">"surname"</span>:<span class="string">"Beeper"</span>,<span class="string">"age"</span>:<span class="number">19</span>&#125;</span><br><span class="line">    &#125; ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">👉 查询某张表某个字段的值</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/spark/people/_search?q=name:*ch*&amp;pretty</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"took"</span> : <span class="number">14</span>,</span><br><span class="line">  <span class="string">"timed_out"</span> : <span class="literal">false</span>,</span><br><span class="line">  <span class="string">"_shards"</span> : &#123;</span><br><span class="line">    <span class="string">"total"</span> : <span class="number">5</span>,</span><br><span class="line">    <span class="string">"successful"</span> : <span class="number">5</span>,</span><br><span class="line">    <span class="string">"failed"</span> : <span class="number">0</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"hits"</span> : &#123;</span><br><span class="line">    <span class="string">"total"</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="string">"max_score"</span> : <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">"hits"</span> : [ &#123;</span><br><span class="line">      <span class="string">"_index"</span> : <span class="string">"spark"</span>,</span><br><span class="line">      <span class="string">"_type"</span> : <span class="string">"people"</span>,</span><br><span class="line">      <span class="string">"_id"</span> : <span class="string">"AU4tjNnbu9a9LJalW5X8"</span>,</span><br><span class="line">      <span class="string">"_score"</span> : <span class="number">1.0</span>,</span><br><span class="line">      <span class="string">"_source"</span>:&#123;<span class="string">"name"</span>:<span class="string">"Michael"</span>,<span class="string">"surname"</span>:<span class="string">"Jackson"</span>,<span class="string">"age"</span>:<span class="number">29</span>&#125;</span><br><span class="line">    &#125; ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>URL</th>
<th>Tips</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://localhost:9200" target="_blank" rel="external">http://localhost:9200</a></td>
<td>验证安装成功</td>
</tr>
<tr>
<td><a href="http://localhost:9200/_count" target="_blank" rel="external">http://localhost:9200/_count</a></td>
<td>ES所有记录数</td>
</tr>
<tr>
<td><a href="http://localhost:9200/_cat" target="_blank" rel="external">http://localhost:9200/_cat</a></td>
<td>REST服务</td>
</tr>
<tr>
<td><a href="http://localhost:9200/_cat/indices" target="_blank" rel="external">http://localhost:9200/_cat/indices</a></td>
<td>ES所有数据库概览</td>
</tr>
<tr>
<td><a href="http://localhost:9200/spark" target="_blank" rel="external">http://localhost:9200/spark</a></td>
<td>指定数据库spark</td>
</tr>
<tr>
<td><a href="http://localhost:9200/spark/_count" target="_blank" rel="external">http://localhost:9200/spark/_count</a></td>
<td>指定数据库spark的记录数</td>
</tr>
<tr>
<td><a href="http://localhost:9200/spark/people/_count" target="_blank" rel="external">http://localhost:9200/spark/people/_count</a></td>
<td>指定数据库spark指定表people的记录数</td>
</tr>
<tr>
<td><a href="http://localhost:9200/spark/people/_search" target="_blank" rel="external">http://localhost:9200/spark/people/_search</a></td>
<td>查询指定数据库指定表的所有记录</td>
</tr>
<tr>
<td><a href="http://localhost:9200/spark/people/_search/q=*" target="_blank" rel="external">http://localhost:9200/spark/people/_search/q=*</a></td>
<td>查询指定数据库指定表的所有记录</td>
</tr>
<tr>
<td><a href="http://localhost:9200/spark/people/_search/q=name:*a*" target="_blank" rel="external">http://localhost:9200/spark/people/_search/q=name:*a*</a></td>
<td>查询某张表name字段匹配<em>a</em>的记录</td>
</tr>
</tbody>
</table>
<h2 id="spark-es-hdfs">spark-es-hdfs</h2><p><a href="http://chenlinux.com/2014/09/04/spark-to-elasticsearch/" target="_blank" rel="external">http://chenlinux.com/2014/09/04/spark-to-elasticsearch/</a></p>
<h3 id="1-_How_to_Get_the_Schema_when_only_get_the_ES_IP">1. How to Get the Schema when only get the ES IP</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">查询都有哪些数据库</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/_cat/indices</span></span><br><span class="line">green forseti-<span class="number">20140526</span> <span class="number">2</span> <span class="number">1</span>    <span class="number">30056</span>      <span class="number">6</span> <span class="number">125.6</span>mb  <span class="number">62.7</span>mb</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">查询总的记录数</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/_count</span></span><br><span class="line">&#123;<span class="string">"count"</span>:<span class="number">631463275</span>,<span class="string">"_shards"</span>:&#123;<span class="string">"total"</span>:<span class="number">201</span>,<span class="string">"successful"</span>:<span class="number">201</span>,<span class="string">"failed"</span>:<span class="number">0</span>&#125;&#125;%</span><br><span class="line"></span><br><span class="line">选出其中一个数据库比如forseti-<span class="number">20140417</span></span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/forseti-20140407</span></span><br><span class="line"></span><br><span class="line">获取这个数据库表activity的记录数,表名可以从上一步得到</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/forseti-20140407/activity/_count</span></span><br><span class="line">&#123;<span class="string">"count"</span>:<span class="number">118</span>,<span class="string">"_shards"</span>:&#123;<span class="string">"total"</span>:<span class="number">2</span>,<span class="string">"successful"</span>:<span class="number">2</span>,<span class="string">"failed"</span>:<span class="number">0</span>&#125;&#125;%</span><br><span class="line"></span><br><span class="line">查询这个数据库的全部数据, 只关心hits.hits.source, 就是文档的表结构</span><br><span class="line">➜  ~  curl http:<span class="comment">//localhost:9200/forseti-20140407/activity/_search\?pretty</span></span><br><span class="line"><span class="string">"_source"</span>: &#123;</span><br><span class="line">  <span class="string">"geo"</span>: &#123;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"device"</span>: &#123;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Python_ES">Python ES</h3><p>1.Build an Elasticsearch Index with Python:<br><a href="http://blog.qbox.io/building-an-elasticsearch-index-with-python" target="_blank" rel="external">http://blog.qbox.io/building-an-elasticsearch-index-with-python</a>  </p>
<p>2.Elasticsearch in Apache Spark with Python:<br><a href="http://blog.qbox.io/elasticsearch-in-apache-spark-python" target="_blank" rel="external">http://blog.qbox.io/elasticsearch-in-apache-spark-python</a>  </p>
<p>3.Deploying Elasticsearch and Apache Spark to the Cloud:<br><a href="http://blog.qbox.io/deploy-elasticsearch-and-apache-spark-to-the-cloud" target="_blank" rel="external">http://blog.qbox.io/deploy-elasticsearch-and-apache-spark-to-the-cloud</a>  </p>
<p>4.Sparse Matrix Multiplication with Elasticsearch and Apache Spark:<br><a href="http://blog.qbox.io/sparse-matrix-multiplication-elasticsearch-apache-spark" target="_blank" rel="external">http://blog.qbox.io/sparse-matrix-multiplication-elasticsearch-apache-spark</a>  </p>
<p>5.Rectangular Matrix Multiplication with Elasticsearch and Apache Spark:<br><a href="http://blog.qbox.io/rectangular-matrix-multiplication-elasticsearch-apache-spark" target="_blank" rel="external">http://blog.qbox.io/rectangular-matrix-multiplication-elasticsearch-apache-spark</a>  </p>
<p>6.Running Asynchronous Apache Spark Jobs from a Web App with Flask, Celery, &amp; Elasticsearch:<br><a href="http://blog.qbox.io/asynchronous-apache-spark-flask-celery-elasticsearch" target="_blank" rel="external">http://blog.qbox.io/asynchronous-apache-spark-flask-celery-elasticsearch</a> </p>
<p>导入JSON数据:  </p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~  curl -XPOST <span class="string">'http://192.168.6.140:9200/forseti-201501/activity/'</span> -d @<span class="regexp">/Users/</span>zhengqh<span class="regexp">/data/</span>koudai_201501.json</span><br><span class="line">&#123;<span class="string">"_index"</span>:<span class="string">"forseti-201501"</span>,<span class="string">"_type"</span>:<span class="string">"activity"</span>,<span class="string">"_id"</span>:<span class="string">"AU4"</span>,<span class="string">"_version"</span>:<span class="number">1</span>,<span class="string">"created"</span>:<span class="keyword">true</span>&#125;%                                                                                             </span><br><span class="line">➜  ~  curl -XDELETE <span class="string">'http://192.168.6.140:9200/forseti-201501/activity/'</span></span><br><span class="line">&#123;<span class="string">"acknowledged"</span>:<span class="keyword">true</span>&#125;%</span><br></pre></td></tr></table></figure>
<h3 id="2-_Spark-ES_Demo">2. Spark-ES Demo</h3><p>spark-shell方式:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>  bin/spark-shell --master local[<span class="number">2</span>] \</span><br><span class="line">   --jars /Users/zhengqh/Downloads/bigdata/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.0</span>.rc1.jar</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">29</span> <span class="number">11</span>:<span class="number">17</span>:<span class="number">16</span> INFO spark.SparkContext: Added JAR file:/Users/zhengqh/Downloads/bigdata/elasticsearch-hadoop-<span class="number">2.1</span><span class="number">.0</span>.rc1.jar at http:<span class="comment">//192.168.6.140:52359/jars/elasticsearch-hadoop-2.1.0.rc1.jar with timestamp 1435547836823</span></span><br></pre></td></tr></table></figure>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/elasticsearch/">elasticsearch</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
    <article id="post-2015-06-20-Apache-Drill" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/06/20/2015-06-20-Apache-Drill/" class="article-date">
  	<time datetime="2015-06-19T16:00:00.000Z" itemprop="datePublished">2015-06-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/06/20/2015-06-20-Apache-Drill/">Apache Drill入门</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="单机模式">单机模式</h2><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng<span class="comment">@dp0653 ~]$ cd apache-drill-1.0.0</span></span><br><span class="line">[qihuang.zheng<span class="comment">@dp0653 apache-drill-1.0.0]$ bin/drill-embedded</span></span><br><span class="line">apache drill 1.0.0</span><br><span class="line"><span class="string">"json ain't no thang"</span></span><br><span class="line">0: jdbc:drill:zk&gt; select <span class="keyword">*</span> from cp.`employee.json` limit 2;</span><br><span class="line">+--------------+------------------+-------------+------------+--------------+---------------------+-----------+----------------+-------------+------------------------+----------+----------------+------------------+-----------------+---------+--------------------+</span><br><span class="line">|<span class="string"> employee_id  </span>|<span class="string">    full_name     </span>|<span class="string"> first_name  </span>|<span class="string"> last_name  </span>|<span class="string"> position_id  </span>|<span class="string">   position_title    </span>|<span class="string"> store_id  </span>|<span class="string"> department_id  </span>|<span class="string"> birth_date  </span>|<span class="string">       hire_date        </span>|<span class="string">  salary  </span>|<span class="string"> supervisor_id  </span>|<span class="string"> education_level  </span>|<span class="string"> marital_status  </span>|<span class="string"> gender  </span>|<span class="string">  management_role   </span>|</span><br><span class="line">+--------------+------------------+-------------+------------+--------------+---------------------+-----------+----------------+-------------+------------------------+----------+----------------+------------------+-----------------+---------+--------------------+</span><br><span class="line">|<span class="string"> 1            </span>|<span class="string"> Sheri Nowmer     </span>|<span class="string"> Sheri       </span>|<span class="string"> Nowmer     </span>|<span class="string"> 1            </span>|<span class="string"> President           </span>|<span class="string"> 0         </span>|<span class="string"> 1              </span>|<span class="string"> 1961-08-26  </span>|<span class="string"> 1994-12-01 00:00:00.0  </span>|<span class="string"> 80000.0  </span>|<span class="string"> 0              </span>|<span class="string"> Graduate Degree  </span>|<span class="string"> S               </span>|<span class="string"> F       </span>|<span class="string"> Senior Management  </span>|</span><br><span class="line">|<span class="string"> 2            </span>|<span class="string"> Derrick Whelply  </span>|<span class="string"> Derrick     </span>|<span class="string"> Whelply    </span>|<span class="string"> 2            </span>|<span class="string"> VP Country Manager  </span>|<span class="string"> 0         </span>|<span class="string"> 1              </span>|<span class="string"> 1915-07-03  </span>|<span class="string"> 1994-12-01 00:00:00.0  </span>|<span class="string"> 40000.0  </span>|<span class="string"> 1              </span>|<span class="string"> Graduate Degree  </span>|<span class="string"> M               </span>|<span class="string"> M       </span>|<span class="string"> Senior Management  </span>|</span><br><span class="line">+--------------+------------------+-------------+------------+--------------+---------------------+-----------+----------------+-------------+------------------------+----------+----------------+------------------+-----------------+---------+--------------------+</span><br><span class="line">2 rows selected (1.247 seconds)</span><br></pre></td></tr></table></figure>
<p>drill使用zookeeper进行集群. 其中local表示使用本机的zk.</p>
<p>也可以使用sqlline启动:</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng<span class="annotation">@dp</span>0653 apache-drill-<span class="number">1.0</span><span class="number">.0</span>]$ bin/sqlline -u <span class="string">jdbc:</span><span class="string">drill:</span>zk=local</span><br><span class="line"><span class="string">log4j:</span>WARN No appenders could be found <span class="keyword">for</span> logger (DataNucleus.General).</span><br><span class="line"><span class="string">log4j:</span>WARN Please initialize the log4j system properly.</span><br><span class="line"><span class="string">log4j:</span>WARN See <span class="string">http:</span><span class="comment">//logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span></span><br><span class="line">六月 <span class="number">15</span>, <span class="number">2015</span> <span class="number">11</span>:<span class="number">11</span>:<span class="number">18</span> 上午 org.glassfish.jersey.server.ApplicationHandler initialize</span><br><span class="line">信息: Initiating Jersey application, version <span class="string">Jersey:</span> <span class="number">2.8</span> <span class="number">2014</span>-<span class="number">04</span>-<span class="number">29</span> <span class="number">01</span>:<span class="number">25</span>:<span class="number">26.</span>..</span><br><span class="line">apache drill <span class="number">1.0</span><span class="number">.0</span></span><br><span class="line"><span class="string">"a drill in the hand is better than two in the bush"</span></span><br><span class="line"><span class="number">0</span>: <span class="string">jdbc:</span><span class="string">drill:</span>zk=local&gt;</span><br></pre></td></tr></table></figure>
<p>退出drill的方式:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span>: jdbc:drill:zk=local&gt; !quit</span><br><span class="line">Closing: org<span class="class">.apache</span><span class="class">.drill</span><span class="class">.jdbc</span><span class="class">.DrillJdbc41Factory</span><span class="variable">$DrillJdbc41Connection</span></span><br></pre></td></tr></table></figure>
<p>使用后台进程的方式启动:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 apache-drill-1.0.0]$ bin/drillbit.sh <span class="operator"><span class="keyword">start</span></span><br><span class="line"><span class="keyword">starting</span> drillbit, <span class="keyword">logging</span> <span class="keyword">to</span> /home/qihuang.zheng/apache-drill-<span class="number">1.0</span><span class="number">.0</span>/<span class="keyword">log</span>/drillbit.<span class="keyword">out</span></span></span><br></pre></td></tr></table></figure>
<p>查看drill进程</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 apache-drill-<span class="number">1.0</span>.<span class="number">0</span>]$ jps -lm</span><br><span class="line"><span class="number">2788</span> org<span class="class">.apache</span><span class="class">.drill</span><span class="class">.exec</span><span class="class">.server</span><span class="class">.Drillbit</span></span><br><span class="line"><span class="number">3045</span> sqlline<span class="class">.SqlLine</span> -d org<span class="class">.apache</span><span class="class">.drill</span><span class="class">.jdbc</span><span class="class">.Driver</span> --maxWidth=<span class="number">10000</span> --<span class="attribute">color</span>=true -u jdbc:drill:zk=local</span><br></pre></td></tr></table></figure>
<p>第一个是drillbit的后台进程, 第二个是使用sqlline或者dril-embbed启动的客户端进程</p>
<h2 id="Storage_Plugin">Storage Plugin</h2><p>cp是classpath storage plugin, drill的web ui: <a href="http://192.168.6.53:8047/storage" target="_blank" rel="external">http://192.168.6.53:8047/storage</a><br>Drill支持不同的存储介质, 并且可以从不同的存储介质中使用SQL查询数据.</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/QQ20150616-2@2x.png" alt="Storage Plugin"></p>
<p>默认只有cp和dfs是enable的. 在Diabled Storage Plugins中点击某个插件的Enable, 就可以使用这个存储插件了</p>
<h3 id="添加HDFS插件">添加HDFS插件</h3><p>默认没有hdfs, 可以在New Storage Plugin中输入hdfs, 点击Create, 在Configuration中输入hdfs的存储插件配置信息:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "<span class="attribute">type</span>": <span class="value"><span class="string">"file"</span></span>,</span><br><span class="line">  "<span class="attribute">enabled</span>": <span class="value"><span class="literal">true</span></span>,</span><br><span class="line">  "<span class="attribute">connection</span>": <span class="value"><span class="string">"hdfs://192.168.6.53:9000/"</span></span>,</span><br><span class="line">  "<span class="attribute">workspaces</span>": <span class="value">&#123;</span><br><span class="line">    "<span class="attribute">root</span>": <span class="value">&#123;</span><br><span class="line">      "<span class="attribute">location</span>": <span class="value"><span class="string">"/"</span></span>,</span><br><span class="line">      "<span class="attribute">writable</span>": <span class="value"><span class="literal">true</span></span>,</span><br><span class="line">      "<span class="attribute">defaultInputFormat</span>": <span class="value"><span class="literal">null</span></span><br><span class="line">    </span>&#125;</span><br><span class="line">  </span>&#125;</span>,</span><br><span class="line">  "<span class="attribute">formats</span>": <span class="value">&#123;</span><br><span class="line">    "<span class="attribute">csv</span>": <span class="value">&#123;</span><br><span class="line">      "<span class="attribute">type</span>": <span class="value"><span class="string">"text"</span></span>,</span><br><span class="line">      "<span class="attribute">extensions</span>": <span class="value">[</span><br><span class="line">        <span class="string">"csv"</span></span><br><span class="line">      ]</span>,</span><br><span class="line">      "<span class="attribute">delimiter</span>": <span class="value"><span class="string">","</span></span><br><span class="line">    </span>&#125;</span>,</span><br><span class="line">    "<span class="attribute">tsv</span>": <span class="value">&#123;</span><br><span class="line">      "<span class="attribute">type</span>": <span class="value"><span class="string">"text"</span></span>,</span><br><span class="line">      "<span class="attribute">extensions</span>": <span class="value">[</span><br><span class="line">        <span class="string">"tsv"</span></span><br><span class="line">      ]</span>,</span><br><span class="line">      "<span class="attribute">delimiter</span>": <span class="value"><span class="string">"\t"</span></span><br><span class="line">    </span>&#125;</span>,</span><br><span class="line">    "<span class="attribute">parquet</span>": <span class="value">&#123;</span><br><span class="line">      "<span class="attribute">type</span>": <span class="value"><span class="string">"parquet"</span></span><br><span class="line">    </span>&#125;</span><br><span class="line">  </span>&#125;</span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>如果是HDFS HA的模式, 也可以支持: <code>hdfs://tdhdfs</code>, 还可以添加workspaces</p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"tmp"</span>: &#123;</span><br><span class="line">  <span class="string">"location"</span>: <span class="string">"/user/qihuang.zheng"</span>,</span><br><span class="line">  <span class="string">"writable"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="string">"defaultInputFormat"</span>: <span class="literal">null</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="路径(dfs和hdfs)">路径(dfs和hdfs)</h3><p>设置dfs插件的工作目录: 点击dfs插件的Update, 添加work目录, 然后点击Update</p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"connection"</span>: <span class="string">"file:///"</span>,</span><br><span class="line"><span class="string">"workspaces"</span>: &#123;</span><br><span class="line">  <span class="string">"root"</span>: &#123;</span><br><span class="line">    <span class="string">"location"</span>: <span class="string">"/"</span>,</span><br><span class="line">    <span class="string">"writable"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"defaultInputFormat"</span>: <span class="literal">null</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"tmp"</span>: &#123;</span><br><span class="line">    <span class="string">"location"</span>: <span class="string">"/tmp"</span>,</span><br><span class="line">    <span class="string">"writable"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"defaultInputFormat"</span>: <span class="literal">null</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"work"</span>: &#123;</span><br><span class="line">    <span class="string">"location"</span>: <span class="string">"/home/qihuang.zheng/apache-drill-1.0.0/"</span>,</span><br><span class="line">    <span class="string">"writable"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"defaultInputFormat"</span>: <span class="literal">null</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure>
<p>对于hdfs也可以自定义一个自己的工作空间比如work=/user/zhengqh. 则定位到/user/zhengqh下,直接使用hfs.work进行查询</p>
<h3 id="DFS文件">DFS文件</h3><p>下面测试了使用不同的路径查询drill安装目录下sample-data下的parquet文件</p>
<ul>
<li>没有使用定义好的work工作目录,导致无法找到文件</li>
<li>使用了自定义的work目录(注意work的使用方式: dfs.work.``), 使用相对路径也能找到文件</li>
<li>绝对路径</li>
</ul>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:drill:zk&gt; select * from dfs.<span class="code">`sample-data/region.parquet`</span> limit 2;</span><br><span class="line">Error: PARSE ERROR: From line 1, column 15 to line 1, column 17: Table <span class="emphasis">'dfs.sample-data/region.parquet'</span> not found</span><br><span class="line">[Error Id: a1e53ed6-cc07-4799-9e9f-a7b112bb4e36 on dp0657:31010] (state=,code=0)</span><br><span class="line"></span><br><span class="line"><span class="header">0: jdbc:drill:zk&gt; select * from dfs.work.`sample-data/region.parquet` limit 2;</span><br><span class="line">+--------------+----------+-----------------------+</span></span><br><span class="line"><span class="header">| R_REGIONKEY  |  R_NAME  |       R_COMMENT       |</span><br><span class="line">+--------------+----------+-----------------------+</span></span><br><span class="line">| 0            | AFRICA   | lar deposits. blithe  |</span><br><span class="line"><span class="header">| 1            | AMERICA  | hs use ironic, even   |</span><br><span class="line">+--------------+----------+-----------------------+</span></span><br><span class="line">2 rows selected (0.338 seconds)</span><br><span class="line"></span><br><span class="line"><span class="header">0: jdbc:drill:zk&gt; select * from dfs.`/home/qihuang.zheng/apache-drill-1.0.0/sample-data/region.parquet` limit 2;</span><br><span class="line">+--------------+----------+-----------------------+</span></span><br><span class="line"><span class="header">| R_REGIONKEY  |  R_NAME  |       R_COMMENT       |</span><br><span class="line">+--------------+----------+-----------------------+</span></span><br><span class="line">| 0            | AFRICA   | lar deposits. blithe  |</span><br><span class="line"><span class="header">| 1            | AMERICA  | hs use ironic, even   |</span><br><span class="line">+--------------+----------+-----------------------+</span></span><br><span class="line">2 rows selected (0.235 seconds)</span><br></pre></td></tr></table></figure>
<h3 id="HDFS文件">HDFS文件</h3><ul>
<li>第一个查询直接使用了相对路径, 因为默认的hdfs插件的root指向的是/, 而它的connection配置路径是: hdfs://192.168.6.53:9000/.</li>
<li>第二个查询使用了绝对路径</li>
</ul>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="header">0: jdbc:drill:zk&gt; select count(*) from hdfs.`user/admin/evidence`;</span><br><span class="line">+----------+</span></span><br><span class="line"><span class="header">|  EXPR$0  |</span><br><span class="line">+----------+</span></span><br><span class="line"><span class="header">| 4003278  |</span><br><span class="line">+----------+</span></span><br><span class="line">1 row selected (0.586 seconds)</span><br><span class="line"><span class="header">0: jdbc:drill:zk&gt; select count(*) from hdfs.`hdfs://tdhdfs/user/admin/evidence`;</span><br><span class="line">+----------+</span></span><br><span class="line"><span class="header">|  EXPR$0  |</span><br><span class="line">+----------+</span></span><br><span class="line"><span class="header">| 4003278  |</span><br><span class="line">+----------+</span></span><br><span class="line">1 row selected (0.278 seconds)</span><br></pre></td></tr></table></figure>
<p>这里还有一个知识点: 可以直接查询文件夹下的所有文件. 也可以是文件夹下的子文件夹都可以</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 ~]$ /usr/install/hadoop/bin/hadoop fs -ls /user/admin/evidence</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">17</span> <span class="number">08</span>:<span class="number">25</span>:<span class="number">37</span> WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... <span class="keyword">using</span> builtin-java classes where applicable</span><br><span class="line">Found <span class="number">4</span> items</span><br><span class="line">-rw-r--r--   <span class="number">3</span> shuoyi.zhao supergroup   <span class="number">67561800</span> <span class="number">2015</span>-<span class="number">05</span>-<span class="number">21</span> <span class="number">10</span>:<span class="number">49</span> /user/admin/evidence/<span class="number">4</span>b75f114-<span class="number">7f</span>64-<span class="number">40</span>df-<span class="number">9f</span>f6-<span class="number">11</span>a1e75637a7.parquet</span><br><span class="line">-rw-r--r--   <span class="number">3</span> shuoyi.zhao supergroup   <span class="number">96528887</span> <span class="number">2015</span>-<span class="number">05</span>-<span class="number">21</span> <span class="number">10</span>:<span class="number">49</span> /user/admin/evidence/bdb0bdb4-fa04-<span class="number">402f</span>-af05-b2aea02728ed.parquet</span><br><span class="line">-rw-r--r--   <span class="number">3</span> shuoyi.zhao supergroup   <span class="number">80968799</span> <span class="number">2015</span>-<span class="number">05</span>-<span class="number">21</span> <span class="number">10</span>:<span class="number">49</span> /user/admin/evidence/da1439fc-<span class="number">0</span>c32-<span class="number">4</span>cd8-<span class="number">90f</span>2-<span class="number">67</span>d24dbaa6cc.parquet</span><br><span class="line">-rw-r--r--   <span class="number">3</span> shuoyi.zhao supergroup  <span class="number">136852232</span> <span class="number">2015</span>-<span class="number">05</span>-<span class="number">21</span> <span class="number">10</span>:<span class="number">50</span> /user/admin/evidence/f0954a8f-<span class="number">583</span>b-<span class="number">4173</span>-<span class="number">9</span>b89-<span class="number">55</span>ed3107daf1.parquet</span><br></pre></td></tr></table></figure>
<h3 id="复杂SQL查询">复杂SQL查询</h3><ol>
<li>两表join查询</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> nations.<span class="keyword">name</span>, regions.<span class="keyword">name</span> <span class="keyword">FROM</span> (</span><br><span class="line">  <span class="keyword">SELECT</span> N_REGIONKEY <span class="keyword">as</span> regionKey, N_NAME <span class="keyword">as</span> <span class="keyword">name</span></span><br><span class="line">  <span class="keyword">FROM</span> dfs.<span class="keyword">work</span>.<span class="string">`sample-data/nation.parquet`</span></span><br><span class="line">) nations <span class="keyword">join</span> (</span><br><span class="line">  <span class="keyword">SELECT</span> R_REGIONKEY <span class="keyword">as</span> regionKey, R_NAME <span class="keyword">as</span> <span class="keyword">name</span></span><br><span class="line">  <span class="keyword">FROM</span> dfs.<span class="keyword">work</span>.<span class="string">`sample-data/region.parquet`</span></span><br><span class="line">) regions</span><br><span class="line">  <span class="keyword">on</span> nations.regionKey = regions.regionKey</span><br><span class="line">  <span class="keyword">order</span> <span class="keyword">by</span> nations.<span class="keyword">name</span>;</span></span><br><span class="line"></span><br><span class="line">+<span class="comment">-----------------+--------------+</span></span><br><span class="line">|      name       |    name0     |</span><br><span class="line">+<span class="comment">-----------------+--------------+</span></span><br><span class="line">| ALGERIA         | AFRICA       |</span><br><span class="line">| ARGENTINA       | AMERICA      |</span><br><span class="line">| BRAZIL          | AMERICA      |</span><br><span class="line">| CANADA          | AMERICA      |</span><br><span class="line">| CHINA           | ASIA         |</span><br><span class="line">...</span><br><span class="line">+<span class="comment">-----------------+--------------+</span></span><br><span class="line">25 rows selected (1.038 seconds)</span><br></pre></td></tr></table></figure>
<ol>
<li>子查询in</li>
</ol>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">SELECT N<span class="emphasis">_REGIONKEY as regionKey, N_</span>NAME as name  </span><br><span class="line">FROM dfs.work.<span class="code">`sample-data/nation.parquet`</span></span><br><span class="line">WHERE cast(N<span class="emphasis">_NAME as varchar(10)) IN ('INDIA', 'CHINA');</span><br><span class="line"></span><br><span class="line"></span><span class="code">+------------+</span>--------+</span><br><span class="line"><span class="header">| regionKey  |  name  |</span><br><span class="line">+------------+--------+</span></span><br><span class="line">| 2          | INDIA  |</span><br><span class="line"><span class="header">| 2          | CHINA  |</span><br><span class="line">+------------+--------+</span></span><br></pre></td></tr></table></figure>
<h2 id="Drill连接Hive">Drill连接Hive</h2><h3 id="HIVE使用(本机环境:_cdh542)">HIVE使用(本机环境: cdh542)</h3><p>drill中有一个默认的hive配置项:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "<span class="attribute">type</span>": <span class="value"><span class="string">"hive"</span></span>,</span><br><span class="line">  "<span class="attribute">enabled</span>": <span class="value"><span class="literal">false</span></span>,</span><br><span class="line">  "<span class="attribute">configProps</span>": <span class="value">&#123;</span><br><span class="line">    "<span class="attribute">hive.metastore.uris</span>": <span class="value"><span class="string">""</span></span>,</span><br><span class="line">    "<span class="attribute">javax.jdo.option.ConnectionURL</span>": <span class="value"><span class="string">"jdbc:derby:;databaseName=../sample-data/drill_hive_db;create=true"</span></span>,</span><br><span class="line">    "<span class="attribute">hive.metastore.warehouse.dir</span>": <span class="value"><span class="string">"/tmp/drill_hive_wh"</span></span>,</span><br><span class="line">    "<span class="attribute">fs.default.name</span>": <span class="value"><span class="string">"file:///"</span></span>,</span><br><span class="line">    "<span class="attribute">hive.metastore.sasl.enabled</span>": <span class="value"><span class="string">"false"</span></span><br><span class="line">  </span>&#125;</span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>我们修改成使用hive-site.xml中的配置项:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "<span class="attribute">type</span>": <span class="value"><span class="string">"hive"</span></span>,</span><br><span class="line">  "<span class="attribute">enabled</span>": <span class="value"><span class="literal">true</span></span>,</span><br><span class="line">  "<span class="attribute">configProps</span>": <span class="value">&#123;</span><br><span class="line">    "<span class="attribute">hive.metastore.uris</span>": <span class="value"><span class="string">"thrift://localhost:9083"</span></span>,</span><br><span class="line">    "<span class="attribute">hive.metastore.sasl.enabled</span>": <span class="value"><span class="string">"false"</span></span><br><span class="line">  </span>&#125;</span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>启动hadoop: start-all.sh</li>
<li>启动hive: hive –service metastore和hiveserver2</li>
<li>启动drill:  bin/drill-embedded</li>
<li>进入drill的命令行中, 和hive的一些语法类似, 比如下面列出已经存在的数据库show databases, 定位到某个数据库use xxx…</li>
</ul>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="header">0: jdbc:drill:zk=local&gt; show databases;</span><br><span class="line">+---------------------+</span></span><br><span class="line"><span class="header">|     SCHEMA_NAME     |</span><br><span class="line">+---------------------+</span></span><br><span class="line">| INFORMATION<span class="emphasis">_SCHEMA  |</span><br><span class="line">| cp.default          |</span><br><span class="line">| dfs.default         |</span><br><span class="line">| dfs.root            |</span><br><span class="line">| dfs.tmp             |</span><br><span class="line">| hive.default        |</span><br><span class="line">| hive.wiki           |</span><br><span class="line">| sys                 |</span><br><span class="line">+---------------------+</span><br><span class="line">8 rows selected (0.627 seconds)</span><br><span class="line">0: jdbc:drill:zk=local&gt; use hive.wiki;</span><br><span class="line">+-------+----------------------------------------+</span><br><span class="line">|  ok   |                summary                 |</span><br><span class="line">+-------+----------------------------------------+</span><br><span class="line">| true  | Default schema changed to [hive.wiki]  |</span><br><span class="line">+-------+----------------------------------------+</span><br><span class="line">1 row selected (0.156 seconds)</span><br><span class="line">0: jdbc:drill:zk=local&gt; show tables;</span><br><span class="line">+---------------+-------------+</span><br><span class="line">| TABLE_</span>SCHEMA  | TABLE<span class="emphasis">_NAME  |</span><br><span class="line">+---------------+-------------+</span><br><span class="line">| hive.wiki     | invites     |</span><br><span class="line">| hive.wiki     | pokes       |</span><br><span class="line">| hive.wiki     | u_</span>data      |</span><br><span class="line"><span class="header">| hive.wiki     | u_data_new  |</span><br><span class="line">+---------------+-------------+</span></span><br><span class="line">4 rows selected (1.194 seconds)</span><br><span class="line"><span class="header">0: jdbc:drill:zk=local&gt; select count(*) from invites;</span><br><span class="line">+---------+</span></span><br><span class="line"><span class="header">| EXPR$0  |</span><br><span class="line">+---------+</span></span><br><span class="line"><span class="header">| 525     |</span><br><span class="line">+---------+</span></span><br><span class="line">1 row selected (4.204 seconds)</span><br></pre></td></tr></table></figure>
<h3 id="HIVE测试环境(hive-1-2-0)">HIVE测试环境(hive-1.2.0)</h3><p>修改hive的配置信息:</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "<span class="attribute">type</span>": <span class="value"><span class="string">"hive"</span></span>,</span><br><span class="line">  "<span class="attribute">enabled</span>": <span class="value"><span class="literal">true</span></span>,</span><br><span class="line">  "<span class="attribute">configProps</span>": <span class="value">&#123;</span><br><span class="line">    "<span class="attribute">hive.metastore.uris</span>": <span class="value"><span class="string">"thrift://192.168.6.53:9083"</span></span>,</span><br><span class="line">    "<span class="attribute">javax.jdo.option.ConnectionURL</span>": <span class="value"><span class="string">"jdbc:mysql://192.168.6.53:3306/hive?characterEncoding=UTF-8"</span></span>,</span><br><span class="line">    "<span class="attribute">hive.metastore.warehouse.dir</span>": <span class="value"><span class="string">"/user/hive/warehouse"</span></span>,</span><br><span class="line">    "<span class="attribute">fs.default.name</span>": <span class="value"><span class="string">"hdfs://tdhdfs"</span></span>,</span><br><span class="line">    "<span class="attribute">hive.metastore.sasl.enabled</span>": <span class="value"><span class="string">"false"</span></span><br><span class="line">  </span>&#125;</span><br><span class="line"></span>&#125;</span><br></pre></td></tr></table></figure>
<p>注: 上面绿色部分除了fs.default.name都不是必须的.</p>
<p><strong><code>问题: 无法查询hive表数据</code></strong><br>在测试环境遇到一个问题: 死活查不出来hive中的表(但是show databases, show tables, describe xx都是正常)<br>比如select count(*) from employee; 后就一直不动了. 观察web ui显示pending状态</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/QQ20150616-17@2x.png" alt="pending job"></p>
<p>用Control+C取消后, 执行其他之前正常的命令都无法执行了, 使用!quit也无法正常退出. 只能通过kill -9 pid杀死sqlline进程!</p>
<p><strong><code>问题追踪</code></strong><br>将conf下的logback.xml的日志级别改成debug. 这样执行每一条命令都会打印出日志信息<br>前面的语句都没有问题, 当执行查询hive表数据的时候, 报错连的是另外一个地址: tdhdfs/220.250.64.20:8020</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/QQ20150616-9@2x.png" alt="strange address"></p>
<p><strong><code>问题思考</code></strong><br>搜了一番hadoop /etc/hosts以及dns;  hdfs ha dns之后无果.<br>然后想到220.250.64.20:8020其中8020端口根本就是默认的.<br>而我们的测试集群使用的是hdfs ha, 并且用的是9000端口.  </p>
<p>说明drill根本没有找到hadoop的配置! 即使在hive的配置页面指定了hdfs.default.name为hdfs://tdhdfs<br>正因为没有drill没有找到hadoop的配置文件, 那么我们就要手动让drill知道hadoop的配置文件位置!  </p>
<p><strong><code>其他问题</code></strong><br>启动drill-embedded的时候有一个报错:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span>:<span class="number">43</span>:<span class="number">54.789</span> [main] DEBUG org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.util</span><span class="class">.Shell</span> - Failed to detect <span class="tag">a</span> valid hadoop home directory</span><br><span class="line">java<span class="class">.io</span><span class="class">.IOException</span>: HADOOP_HOME or hadoop<span class="class">.home</span><span class="class">.dir</span> are not set.</span><br></pre></td></tr></table></figure>
<p>虽然没有影响drill的启动. 但还是修改下: <code>vi ~/.bashrc</code>  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=<span class="string">"/usr/install/hadoop"</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span></span><br><span class="line"><span class="built_in">export</span> YARN_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_YARN_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> HDFS_CONF_DIR=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> YARN_CONF_DIR=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin"</span></span><br></pre></td></tr></table></figure>
<p>并在drill-env.sh中添加  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=<span class="string">"/usr/install/hadoop"</span></span><br></pre></td></tr></table></figure>
<p>上面增加的配置虽然启动时不再报错, 但是并不能解决我们之前遇到的问题.</p>
<p><strong><code>问题解决</code></strong><br>拷贝hadoop安装目录下的core-site.xml, mapred-site.xml, hdfs-site.xml, yarn-site.xml到DRILL/conf下!<br>重启bin/drill-embedded. (发现重启后, 原先的hive和hdfs配置都不见了, 所以要重新update)</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 conf]$ ll -rt</span><br><span class="line">-rw-r--r--. <span class="number">1</span> qihuang.zheng users <span class="number">3835</span> <span class="number">5</span>月  <span class="number">16</span> <span class="number">10</span>:<span class="number">35</span> drill-override-example.conf</span><br><span class="line">-rwxr-xr-x. <span class="number">1</span> qihuang.zheng users <span class="number">1276</span> <span class="number">6</span>月  <span class="number">16</span> <span class="number">10</span>:<span class="number">51</span> drill-env.sh</span><br><span class="line">-rw-r--r--. <span class="number">1</span> qihuang.zheng users <span class="number">2354</span> <span class="number">6</span>月  <span class="number">16</span> <span class="number">15</span>:<span class="number">12</span> core-site.xml</span><br><span class="line">-rw-r--r--. <span class="number">1</span> qihuang.zheng users <span class="number">3257</span> <span class="number">6</span>月  <span class="number">16</span> <span class="number">15</span>:<span class="number">12</span> hdfs-site.xml</span><br><span class="line">-rw-r--r--. <span class="number">1</span> qihuang.zheng users <span class="number">2111</span> <span class="number">6</span>月  <span class="number">16</span> <span class="number">15</span>:<span class="number">12</span> mapred-site.xml</span><br><span class="line">-rw-r--r--. <span class="number">1</span> qihuang.zheng users <span class="number">8382</span> <span class="number">6</span>月  <span class="number">16</span> <span class="number">15</span>:<span class="number">12</span> yarn-site.xml</span><br><span class="line">-rw-r--r--. <span class="number">1</span> qihuang.zheng users <span class="number">3119</span> <span class="number">6</span>月  <span class="number">16</span> <span class="number">15</span>:<span class="number">25</span> logback.xml</span><br><span class="line">-rw-r--r--. <span class="number">1</span> qihuang.zheng users <span class="number">1237</span> <span class="number">6</span>月  <span class="number">16</span> <span class="number">15</span>:<span class="number">35</span> drill-override.conf</span><br></pre></td></tr></table></figure>
<h3 id="Hive的数据类型">Hive的数据类型</h3><p>目前Drill并<code>不支持Hive一些复杂的结构类型, 比如LIST, MAP, STRUCT, UNION</code></p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="header">0: jdbc:drill:zk&gt; describe koudai;</span><br><span class="line">+-------------------+---------------------------------------+--------------+</span></span><br><span class="line"><span class="header">|    COLUMN_NAME    |               DATA_TYPE               | IS_NULLABLE  |</span><br><span class="line">+-------------------+---------------------------------------+--------------+</span></span><br><span class="line">| sequence<span class="emphasis">_id       | VARCHAR                               | YES          |</span><br><span class="line">| occur_</span>time        | BIGINT                                | YES          |</span><br><span class="line">| activity<span class="emphasis">_map      | (VARCHAR(65535), VARCHAR(65535)) MAP  | NO           |</span><br><span class="line">| device_</span>map        | (VARCHAR(65535), VARCHAR(65535)) MAP  | NO           |</span><br><span class="line">| event<span class="emphasis">_result_</span>map  | (VARCHAR(65535), VARCHAR(65535)) MAP  | NO           |</span><br><span class="line">| geo<span class="emphasis">_map           | (VARCHAR(65535), VARCHAR(65535)) MAP  | NO           |</span><br><span class="line">| policy_</span>map        | (VARCHAR(65535), VARCHAR(65535)) MAP  | NO           |</span><br><span class="line"><span class="header">| indice            | VARCHAR                               | YES          |</span><br><span class="line">+-------------------+---------------------------------------+--------------+</span></span><br><span class="line">8 rows selected (0.504 seconds)</span><br><span class="line">0: jdbc:drill:zk&gt; select count(<span class="strong">*) from koudai;</span><br><span class="line">Error: SYSTEM ERROR: java.lang.RuntimeException: Unsupported Hive data type MAP.</span><br><span class="line">Following Hive data types are supported in Drill for querying: BOOLEAN, BYTE, SHORT, INT, LONG, FLOAT, DOUBLE, DATE, TIMESTAMP, BINARY, DECIMAL, STRING, and VARCHAR</span><br><span class="line"></span><br><span class="line"></span>Fragment 1:0</span><br><span class="line"></span><br><span class="line">[Error Id: 7c5dd0d4-1e18-4dbc-b470-1eb6ca6a3b36 on dp0653:31010] (state=,code=0)</span><br><span class="line"><span class="header">0: jdbc:drill:zk&gt; describe int_table;</span><br><span class="line">+--------------+------------+--------------+</span></span><br><span class="line"><span class="header">| COLUMN_NAME  | DATA_TYPE  | IS_NULLABLE  |</span><br><span class="line">+--------------+------------+--------------+</span></span><br><span class="line"><span class="header">| id           | INTEGER    | YES          |</span><br><span class="line">+--------------+------------+--------------+</span></span><br><span class="line">1 row selected (0.334 seconds)</span><br><span class="line"><span class="header">0: jdbc:drill:zk&gt; select count(*) from int_table;</span><br><span class="line">+---------+</span></span><br><span class="line"><span class="header">| EXPR$0  |</span><br><span class="line">+---------+</span></span><br><span class="line"><span class="header">| 90      |</span><br><span class="line">+---------+</span></span><br><span class="line">1 row selected (0.654 seconds)</span><br></pre></td></tr></table></figure>
<p>So What Can We do when we want to query Hive Table which has map type?</p>
<p>在drill的mail-list上看到这样的一个回复:<br><a href="http://mail-archives.apache.org/mod_mbox/drill-dev/201504.mbox/browser" target="_blank" rel="external">http://mail-archives.apache.org/mod_mbox/drill-dev/201504.mbox/browser</a></p>
<blockquote>
<p>We haven’t yet added support for Hive’s Map type.  Can we work together on<br>adding this?  Drill doesn’t distinguish between maps and structs given its<br>support for schemaless data.  If you could post a small example piece of<br>data, maybe we could figure out the best way to work together to add this<br>functionality.  As I said, it is mostly just a metadata mapping exercise<br>since Drill already has complex type support in the execution engine.  You<br>can see how it works by looking at the JSONReader complex Parquet reader.</p>
<p>大致的意思是我们现在不支持hive的map类型. 为什么呢? 因为drill支持无模式的数据, 所以map类型还是结构类型对于drill而言都是一样的.<br>Drill的执行引擎中已经支持了复杂的类型. 你可以看看怎么读JSON或者Parquet格式的文件是怎么做的.</p>
</blockquote>
<p>然后想到hive包含有map类型的表结构虽然drill不支持. 但是drill可以使用hive的数据啊.<br>既然hive的表结构是有一定schema的. 那么它的数据格式也一定是有格式的.<br>所以这里虽然drill可以和hive公用表结构, 如果我们直接用hive的表数据, 相当于还是使用hdfs插件了.  </p>
<h2 id="Drill分布式模式">Drill分布式模式</h2><ul>
<li>上面在单机上的配置项, 将drill文件夹复制到集群中. 注意修改drill下conf的drill-override.conf  </li>
</ul>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">drill</span><span class="class">.exec</span>: <span class="rules">&#123;</span><br><span class="line">  <span class="rule"><span class="attribute">cluster-id</span>:<span class="value"> <span class="string">"drillbits1"</span>,</span><br><span class="line">  zk.connect: <span class="string">"192.168.6.55:2181,192.168.6.56:2181,192.168.6.57:2181"</span></span><br><span class="line"></span></span></span>&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>每台节点的cluster-id都是一样的. 保证了所有的节点组成一个集群.<br>这和ElasticSearch集群的安装一样. 它的好处是随时可以扩展节点, 而不需要更改原先的任何配置.</p>
</blockquote>
<ul>
<li>然后在每台机器上都启动bin/drillbit.sh start  </li>
<li>随便访问任意一台机器的8047端口, 都可以列出集群中的所有drill服务  </li>
</ul>
<p><a href="http://192.168.6.52:8047/" target="_blank" rel="external">http://192.168.6.52:8047/</a><br><a href="http://192.168.6.53:8047/" target="_blank" rel="external">http://192.168.6.53:8047/</a><br><a href="http://192.168.6.54:8047/" target="_blank" rel="external">http://192.168.6.54:8047/</a><br><a href="http://192.168.6.56:8047/" target="_blank" rel="external">http://192.168.6.56:8047/</a><br><a href="http://192.168.6.57:8047/" target="_blank" rel="external">http://192.168.6.57:8047/</a>  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/QQ20150616-15@2x.png" alt="distribute mode"></p>
<h2 id="客户端连接">客户端连接</h2><p>Drill提供了一些工具, 包括第三方工具也提供了访问Drill数据的方法.<br>主要是Drill和其他SQL DB一样提供了一个ODBC Driver.  参考: <a href="https://drill.apache.org/docs/interfaces-introduction/" target="_blank" rel="external">https://drill.apache.org/docs/interfaces-introduction/</a></p>
<h3 id="sqlline">sqlline</h3><p><strong>连接本地ZK</strong></p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/sqlline -u <span class="string">jdbc:</span><span class="string">drill:</span>zk=local</span><br></pre></td></tr></table></figure>
<p><strong>连接ZK集群</strong>  </p>
<ul>
<li>手动指定ZK</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 apache-drill-<span class="number">1.0</span><span class="number">.0</span>]$ bin/sqlline -u jdbc:drill:zk=<span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>,<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">2181</span></span><br></pre></td></tr></table></figure>
<ul>
<li>如果是集群模式, 也可以不跟上zk地址:   <strong>bin/sqlline -u jdbc:drill:zk</strong> 会自动读取drill-override.conf的配置</li>
</ul>
<blockquote>
<p>注意: 当指定的zk是一个全新的ZK, 之前如果使用zk=local在本次新的zk会话中Storage-Plugin的信息都丢失.<br>因为我们指定的zookeeper集群是全新的. 所以drill还没有往里面写入任何数据.<br>这是因为在web ui上对Storage Plugin进行update或者create的数据都会写入到对应的zookeeper节点上!<br>当我们在界面上update hive, 并且enable后, 通过show databases就可以看到hive里的表了</p>
</blockquote>
<h3 id="iodbc">iodbc</h3><p><strong>iodbc data source manager</strong></p>
<p>选择一个已有的Driver, 修改连接类型, 如果是ZooKeeper,要指定ZK集群和clusterId<br>如果是Direct, 则直接指定要连接的Drill的host和port  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/QQ20150617-5@2x.png" alt="iodbc"></p>
<p>测试成功后, 新建一个SQL查询  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/QQ20150617-6@2x.png" alt="iodbc query"></p>
<p>点击OK后, 会返回查询结果  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/QQ20150617-7@2x.png" alt="iodbc result"></p>
<p><strong>iodbc terminal</strong></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ iodbctest</span><br><span class="line">iODBC Demonstration program</span><br><span class="line">This program shows an interactive SQL processor</span><br><span class="line">Driver Manager: 03.52.0607.1008</span><br><span class="line"></span><br><span class="line">Enter ODBC connect string (? shows list): ?</span><br><span class="line"><span class="header">DSN                              | Driver</span><br><span class="line">------------------------------------------------------------------------------</span></span><br><span class="line">Sample MapR Drill DSN            | MapR Drill ODBC Driver</span><br><span class="line"></span><br><span class="line">Enter ODBC connect string (? shows list): DRIVER=MapR Drill ODBC Driver;AdvancedProperties= &#123;HandshakeTimeout=0;QueryTimeout=0; TimestampTZDisplayTimezone=utc;ExcludedSchemas=sys, INFORMATION_SCHEMA;&#125;;Catalog=DRILL;Schema=; ConnectionType=Direct;Host=192.168.6.53;Port=31010</span><br><span class="line">1: SQLDriverConnect = [<span class="link_label">iODBC</span>][<span class="link_reference">Driver Manager</span>]dlopen(MapR Drill ODBC Driver, 6): image not found (0) SQLSTATE=00000</span><br><span class="line">2: SQLDriverConnect = [<span class="link_label">iODBC</span>][<span class="link_reference">Driver Manager</span>]Specified driver could not be loaded (0) SQLSTATE=IM003</span><br><span class="line"></span><br><span class="line">Enter ODBC connect string (? shows list): DSN=Sample MapR Drill DSN;ConnectionType=Direct;Host=192.168.6.53;Port=31010</span><br><span class="line">Driver: 1.0.0.1001 (MapR Drill ODBC Driver)</span><br><span class="line"></span><br><span class="line">SQL&gt;select count(*) from cp.<span class="code">`employee.json`</span></span><br><span class="line"><span class="header">EXPR$0</span><br><span class="line">--------------------</span></span><br><span class="line">1155</span><br><span class="line"> result set 1 returned 1 rows.</span><br><span class="line">SQL&gt;</span><br></pre></td></tr></table></figure>
<p>注意上面输入ODBC的连接字符串, 按照官方文档有些地方写的是:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DRIVER=MapR Drill ODBC Driver;AdvancedProperties= &#123;HandshakeTimeout=<span class="number">0</span>;QueryTimeout=<span class="number">0</span>; TimestampTZDisplayTimezone=utc;ExcludedSchemas=sys, INFORMATION_SCHEMA;&#125;;Catalog=DRILL;Schema=; ConnectionType=Direct;Host=<span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>;Port=<span class="number">31010</span></span><br></pre></td></tr></table></figure>
<p>会报错说image not found. 正确的格式应该是:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DSN=Sample MapR Drill DSN;ConnectionType=Direct;Host=<span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>;Port=<span class="number">31010</span></span><br></pre></td></tr></table></figure>
<h3 id="Drill_Explorer">Drill Explorer</h3><p>Drill Expoloer连接Drill的字符串格式和上面一样, 在Advance中输入  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/QQ20150618-1@2x.png" alt="explorer connect"></p>
<p>在Drill Explorer中可以浏览数据, 并且可以建立一些视图  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/QQ20150617-4@2x.png" alt="expoloer"></p>
<h2 id="性能测试">性能测试</h2><h3 id="HDFS的Parquet文件查询(单机和分布式模式对比)">HDFS的Parquet文件查询(单机和分布式模式对比)</h3><p>单机模式:</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="header">0: jdbc:drill:zk=local&gt; select count(*) from hdfs.`/user/admin/evidence`;</span><br><span class="line">+----------+</span></span><br><span class="line"><span class="header">|  EXPR$0  |</span><br><span class="line">+----------+</span></span><br><span class="line"><span class="header">| 4003278  |</span><br><span class="line">+----------+</span></span><br><span class="line">1 row selected (0.854 seconds)</span><br><span class="line"></span><br><span class="line"><span class="header">0: jdbc:drill:zk=local&gt; select fraud_type,count(*) from hdfs.`/user/admin/evidence` group by fraud_type order by count(*) desc;</span><br><span class="line">+--------------------+----------+</span></span><br><span class="line"><span class="header">|     fraud_type     |  EXPR$1  |</span><br><span class="line">+--------------------+----------+</span></span><br><span class="line">| fakeMobile         | 1941589  |</span><br><span class="line"><span class="bullet">...</span><br><span class="line"></span><span class="header">| clickFraud,        | 18       |</span><br><span class="line">+--------------------+----------+</span></span><br><span class="line">14 rows selected (4.451 seconds)</span><br></pre></td></tr></table></figure>
<p>五台机器的分布式模式:</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="header">0: jdbc:drill:zk=192.168.6.55,192.168.6.56,19&gt; select count(*) from hdfs.`/user/admin/evidence`;</span><br><span class="line">+----------+</span></span><br><span class="line"><span class="header">|  EXPR$0  |</span><br><span class="line">+----------+</span></span><br><span class="line"><span class="header">| 4003278  |</span><br><span class="line">+----------+</span></span><br><span class="line">1 row selected (0.394 seconds)</span><br><span class="line"></span><br><span class="line"><span class="header">0: jdbc:drill:zk=192.168.6.55,192.168.6.56,19&gt; select fraud_type,count(*) from hdfs.`/user/admin/evidence` group by fraud_type order by count(*) desc;</span><br><span class="line">+--------------------+----------+</span></span><br><span class="line"><span class="header">|     fraud_type     |  EXPR$1  |</span><br><span class="line">+--------------------+----------+</span></span><br><span class="line">| fakeMobile         | 1941589  |</span><br><span class="line"><span class="bullet">...</span><br><span class="line"></span><span class="header">| clickFraud,        | 18       |</span><br><span class="line">+--------------------+----------+</span></span><br><span class="line">14 rows selected (1.744 seconds)</span><br></pre></td></tr></table></figure>
<p><code>实验现象1: Foreman不固定</code><br>在每台机器的8047端口的Profile中看到并不一定每台机器都回显示Queries.<br>比如在dp0655上运行时, 其他几台机器都没有 只有dp0656上才有.<br>而且即使是在相同的客户端, 不同的会话也会在不同的Foreman上运行.</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/QQ20150616-16@2x.png" alt="foreman"></p>
<p>A: Foreman只是类似Facade, 是最终返回查询结果给客户端的节点. 只需要一个即可.<br>Drill分布式计算会由Forman决定如何派发数据给不同的Drillbit节点.</p>
<p>如何验证: 查看Profiles下某个Query, 通常第一个Major Fragment就是Forman节点.<br>其余的Major Fragment会分发到不同的节点.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/QQ20150617-1@2x.png" alt="profile"></p>
<p>其实从Drill的架构图也可以看出Forman只有一个  </p>
<p><img src="http://drill.apache.org/docs/img/query-flow-client.png" alt="query-flow-client"></p>
<p><img src="http://drill.apache.org/docs/img/leaf-frag.png" alt="leaf-frag"></p>
<p><code>实验现象2: 第一次查询慢</code><br>有些查询在第一次执行时较慢. 后面同样的语句会快一倍多.<br>但最后会稳定下来比如上面的group by order by测试结果(400万条,分组后排序)<br>横坐标表示依次在这些机器上执行, 纵坐标表示在这台机器上执行了多次同样的SQL语句.</p>
<table>
<thead>
<tr>
<th>Round</th>
<th>dp0653</th>
<th>dp0652</th>
<th>dp0655</th>
<th>dp0657</th>
<th>dp0656</th>
<th>dp0653</th>
</tr>
</thead>
<tbody>
<tr>
<td>Round1</td>
<td>7.871</td>
<td>5.079</td>
<td>4.8299</td>
<td>1.764</td>
<td>1.557</td>
<td>4.305</td>
</tr>
<tr>
<td>Round2</td>
<td>2.549</td>
<td>2.103</td>
<td>2.106</td>
<td>1.66</td>
<td>1.418</td>
<td>1.854</td>
</tr>
<tr>
<td>Round3</td>
<td>1.888</td>
<td>1.893</td>
<td>1.779</td>
<td>1.534</td>
<td>1.512</td>
<td>1.955</td>
</tr>
<tr>
<td>Round4</td>
<td>1.841</td>
<td>1.641</td>
<td>1.703</td>
</tr>
<tr>
<td>Round5</td>
<td>1.744</td>
<td>1.714</td>
<td>1.9</td>
</tr>
<tr>
<td>Round6</td>
<td>1.763</td>
<td>1.572</td>
<td>1.53</td>
</tr>
</tbody>
</table>
<h2 id="Drill其他知识点">Drill其他知识点</h2><p>1.QueryUI</p>
<p>在<a href="http://192.168.6.53:8047/query" target="_blank" rel="external">http://192.168.6.53:8047/query</a>页面输入查询条件. 注意下面的hdfs.tmp.by_yr<br>其中hdfs.tmp类似于在sqline中先执行了<code>use hdfs.tmp</code>, by_yr是表名  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-query.png" alt=""></p>
<p>点击submit, 可以得出查询结果</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/drill-ui.png" alt=""></p>
<p>2.REST服务</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">curl  \</span><br><span class="line">  <span class="comment">--header "Content-type: application/json" \</span></span><br><span class="line">  <span class="comment">--request POST \</span></span><br><span class="line">  <span class="comment">--data '&#123;</span></span><br><span class="line">    "queryType" : "SQL",</span><br><span class="line">    "query" : "<span class="operator"><span class="keyword">select</span> yr,<span class="keyword">count</span>(*) <span class="keyword">from</span> hdfs.tmp.by_yr <span class="keyword">group</span> <span class="keyword">by</span> yr <span class="keyword">having</span> <span class="keyword">count</span>(*) &gt; <span class="number">30000</span> <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">count</span>(*) <span class="keyword">desc</span><span class="string">"</span><br><span class="line">&#125;' \</span><br><span class="line">http://192.168.6.52:8047/query.json</span></span></span><br></pre></td></tr></table></figure>
<p>返回结果:</p>
<figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="collection">&#123;</span><br><span class="line">  <span class="string">"columns"</span> : <span class="collection">[ <span class="string">"yr"</span>, <span class="string">"EXPR$1"</span> ]</span>,</span><br><span class="line">  <span class="string">"rows"</span> : <span class="collection">[ <span class="collection">&#123;</span><br><span class="line">    <span class="string">"yr"</span> : <span class="string">"2008"</span>,</span><br><span class="line">    <span class="string">"EXPR$1"</span> : <span class="string">"38425"</span></span><br><span class="line">  &#125;</span>, <span class="collection">&#123;</span><br><span class="line">    <span class="string">"yr"</span> : <span class="string">"2004"</span>,</span><br><span class="line">    <span class="string">"EXPR$1"</span> : <span class="string">"38252"</span></span><br><span class="line">  &#125;</span>, <span class="collection">&#123;</span><br><span class="line">    <span class="string">"yr"</span> : <span class="string">"2007"</span>,</span><br><span class="line">    <span class="string">"EXPR$1"</span> : <span class="string">"38069"</span></span><br><span class="line">  &#125;</span>, <span class="collection">&#123;</span><br><span class="line">    <span class="string">"yr"</span> : <span class="string">"2003"</span>,</span><br><span class="line">    <span class="string">"EXPR$1"</span> : <span class="string">"37050"</span></span><br><span class="line">  &#125;</span>, <span class="collection">&#123;</span><br><span class="line">  ...</span><br><span class="line">  &#125;</span>, <span class="collection">&#123;</span><br><span class="line">    <span class="string">"yr"</span> : <span class="string">"1990"</span>,</span><br><span class="line">    <span class="string">"EXPR$1"</span> : <span class="string">"30368"</span></span><br><span class="line">  &#125;</span> ]</span></span></span><br></pre></td></tr></table></figure>
<h2 id="Q&amp;A">Q&amp;A</h2><ul>
<li><p>[ ] Q: 为什么第一次执行会比较慢?</p>
</li>
<li><p>[x] Q: 既然是分布式的, 为什么每次执行时, 只派发给一个Foreman?  </p>
<pre><code><span class="label">A:</span> Forman只是最终返回给客户端的节点, 只需要一个即可.  
但是具体的查询Foreman会分发给多个几点!
</code></pre></li>
</ul>
<h2 id="TODO">TODO</h2><ul>
<li>测试环境hive中没什么表, 而且koudai表的类型是map, drill不支持map类型无法查询<br>准备导入一些测试数据集进来测下</li>
<li>Drill + Tableau  </li>
</ul>
<h2 id="参考文档">参考文档</h2><p><a href="http://drill.apache.org/docs/" target="_blank" rel="external">Drill官网</a><br><a href="http://www.yankay.com/google-dremel-rationale/" target="_blank" rel="external">Google Dremel 原理 - 如何能3秒分析1PB</a></p>
<p><a href="http://duguyiren3476.iteye.com/blog/2203055" target="_blank" rel="external">apache drill 0.8.0 单机/分布式安装测试</a><br><a href="http://xn--jlq582ax31c.xn--fiqs8s/post/31" target="_blank" rel="external">部署分布式Drill集群</a><br><a href="http://blog.chinaunix.net/uid-20593827-id-4042244.html" target="_blank" rel="external">Apache Drill环境搭建及连接hdfs</a>  </p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


      
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/drill/">drill</a></li></ul>
	</div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>











    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        

  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2015 zqhxuyuan
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的静态博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减双栏 Hexo 博客主题">Yelee</a> by MOxFIVE
        </div>
    </div>
    <div class="visit">
      <span id="busuanzi_container_site_pv" style='display:none'>
        <span id="site-visit" >本站到访数: 
        <span id="busuanzi_value_site_uv"></span>
        </span>
      </span>
      <span id="busuanzi_container_page_pv" style='display:none'>
        <span id="page-visit">, 本页阅读量: 
        <span id="busuanzi_value_page_pv"></span>
        </span>
      </span>
    </div>
  </div>
</footer>
    </div>
    
	<script>
		var yiliaConfig = {
			fancybox: false,
			mathjax: true,
			animate: true,
			isHome: true,
			isPost: false,
			isArchive: false,
			isTag: false,
			isCategory: false,
			open_in_new: false
		}
	</script>


<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>

<script>
  var backgroundnum = 5;
  var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));

  $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
</script>




<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
<a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
<a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>