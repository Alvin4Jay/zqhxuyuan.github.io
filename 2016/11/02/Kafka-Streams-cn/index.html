<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Kafka Streams中文翻译 | zqhxuyuan</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Confluent Kafka Streams Documentation 中文翻译：  http://docs.confluent.io/3.0.1/streams/introduction.html">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka Streams中文翻译">
<meta property="og:url" content="http://github.com/zqhxuyuan/2016/11/02/Kafka-Streams-cn/index.html">
<meta property="og:site_name" content="zqhxuyuan">
<meta property="og:description" content="Confluent Kafka Streams Documentation 中文翻译：  http://docs.confluent.io/3.0.1/streams/introduction.html">
<meta property="og:image" content="http://img.blog.csdn.net/20161103092411029">
<meta property="og:image" content="http://img.blog.csdn.net/20161103101744946">
<meta property="og:image" content="http://img.blog.csdn.net/20161103130945109">
<meta property="og:image" content="http://img.blog.csdn.net/20161103131309314">
<meta property="og:image" content="http://img.blog.csdn.net/20161103180445363">
<meta property="og:image" content="http://img.blog.csdn.net/20161103180512627">
<meta property="og:image" content="http://img.blog.csdn.net/20161103181154902">
<meta property="og:image" content="http://img.blog.csdn.net/20161103181203678">
<meta property="og:image" content="http://img.blog.csdn.net/20161103181630440">
<meta property="og:image" content="http://img.blog.csdn.net/20161104091711617">
<meta property="og:image" content="https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/02/52/ASAQL-1.png">
<meta property="og:image" content="https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/02/52/ASAQL-2.png">
<meta property="og:image" content="https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/02/52/ASAQL-3.png">
<meta property="og:image" content="http://img.blog.csdn.net/20161105125241586">
<meta property="og:image" content="http://img.blog.csdn.net/20161105125255977">
<meta property="og:updated_time" content="2016-11-05T08:43:37.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Kafka Streams中文翻译">
<meta name="twitter:description" content="Confluent Kafka Streams Documentation 中文翻译：  http://docs.confluent.io/3.0.1/streams/introduction.html">
  
    <link rel="alternative" href="/atom.xml" title="zqhxuyuan" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://avatars1.githubusercontent.com/u/1088525?v=3&amp;s=180" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">任何忧伤,都抵不过世界的美丽</a></h1>
		</hgroup>

		
				


		
			<div id="switch-btn" class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
						<li>关于我</li>
						
					</ul>
				</div>
			</div>
		

		<div id="switch-area" class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives/">归档</a></li>
				        
							<li><a href="/tags/">标签</a></li>
				        
							<li><a href="/about/">关于</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<ul class="social">
							
								<li id="新浪微博"><a class="新浪微博" target="_blank" href="http://weibo.com/xuyuantree" title="新浪微博"></a></li>
					        
								<li id="GitHub"><a class="GitHub" target="_blank" href="http://github.com/zqhxuyuan" title="GitHub"></a></li>
					        
						</ul>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/apex/" style="font-size: 10px;">apex</a> <a href="/tags/cassandra/" style="font-size: 18.75px;">cassandra</a> <a href="/tags/clojure/" style="font-size: 10px;">clojure</a> <a href="/tags/drill/" style="font-size: 16.25px;">drill</a> <a href="/tags/druid/" style="font-size: 13.75px;">druid</a> <a href="/tags/elasticsearch/" style="font-size: 10px;">elasticsearch</a> <a href="/tags/geode/" style="font-size: 10px;">geode</a> <a href="/tags/graph/" style="font-size: 13.75px;">graph</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 15px;">hbase</a> <a href="/tags/ignite/" style="font-size: 10px;">ignite</a> <a href="/tags/jvm/" style="font-size: 10px;">jvm</a> <a href="/tags/kafka/" style="font-size: 20px;">kafka</a> <a href="/tags/ops/" style="font-size: 13.75px;">ops</a> <a href="/tags/redis/" style="font-size: 11.25px;">redis</a> <a href="/tags/scala/" style="font-size: 12.5px;">scala</a> <a href="/tags/spark/" style="font-size: 13.75px;">spark</a> <a href="/tags/storm/" style="font-size: 17.5px;">storm</a> <a href="/tags/timeseries/" style="font-size: 12.5px;">timeseries</a> <a href="/tags/work/" style="font-size: 13.75px;">work</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">BIG(DATA)</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">任何忧伤,都抵不过世界的美丽</a></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<a href="/" class="profilepic">
				<img lazy-src="https://avatars1.githubusercontent.com/u/1088525?v=3&amp;s=180" class="js-avatar">
			</a>
			<hgroup>
			  <h1 class="header-author"><a href="/" title="回到主页">任何忧伤,都抵不过世界的美丽</a></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives/">归档</a></li>
		        
					<li><a href="/tags/">标签</a></li>
		        
					<li><a href="/about/">关于</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
						<ul class="social">
							
								<li id="新浪微博"><a class="新浪微博" target="_blank" href="http://weibo.com/xuyuantree" title="新浪微博"></a></li>
					        
								<li id="GitHub"><a class="GitHub" target="_blank" href="http://github.com/zqhxuyuan" title="GitHub"></a></li>
					        
						</ul>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-Kafka-Streams-cn" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2016/11/02/Kafka-Streams-cn/" class="article-date">
  	<time datetime="2016-11-01T16:00:00.000Z" itemprop="datePublished">2016-11-02</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Kafka Streams中文翻译
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/kafka/">kafka</a>
	</div>


        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/">kafka</a></li></ul>
	</div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <p>Confluent Kafka Streams Documentation 中文翻译：  <a href="http://docs.confluent.io/3.0.1/streams/introduction.html" target="_blank" rel="external">http://docs.confluent.io/3.0.1/streams/introduction.html</a><br><a id="more"></a></p>
<h1 id="介绍">介绍</h1><h2 id="Kafka_Streams">Kafka Streams</h2><p>Kafka Streams, a component of open source Apache Kafka, is a powerful, easy-to-use library for building highly scalable, fault-tolerant, distributed stream processing applications on top of Apache Kafka. It builds upon important concepts for stream processing such as properly distinguishing between event-time and processing-time, handling of late-arriving data, and efficient management of application state.</p>
<blockquote>
<p>Kafka Streams是构建在Apache Kafka的一个组件，它是一个功能强大的、对于构建高可用、故障容错、分布式流处理应用程序都很容易使用的库。它构建在流处理的重要概念之上，比如正确地区分事件时间（event-time）和处理时间（process-time），处理延时数据，高效的应用程序状态管理。  </p>
</blockquote>
<p>One of the mantras(祷文) of Kafka Streams is to “Build apps, not clusters!”, which means to bring stream processing out of the Big Data niche into the world of mainstream application development. Using the Kafka Streams library you can implement standard Java applications to solve your stream processing needs – whether at small or at large scale – and then run these applications on client machines at the perimeter(边界) of your Kafka cluster. Deployment-wise you are free to chose from any technology that can deploy Java applications, including but not limited to Puppet, Chef, Ansible, Docker, Mesos, YARN, Kubernetes, and so on. This lightweight and integrative(综合) approach of Kafka Streams is in stark(完全、突出) contrast(对比) to other stream processing tools that require you to install and operate separate stream processing clusters and similar heavy-weight infrastructure that come with their own special set of rules on how to use and interact with them.</p>
<blockquote>
<p>Kafka Streams的一个思想是“构建应用程序，不要集群”，这意味着将流处理从大数据生态圈中解放出来，而专注于主流的应用程序开发。使用Kafka Streams客户端库，你可以用标准的Java应用程序（main方法）来解决你的流处理需求（不管是小规模还是大规模的数据），然后可以在你的Kafka集群之外的客户端机器执行这些应用程序。你可以选择任何可以部署Java应用的技术来部署Kafka Streams，包括但不限于Puppet、Chef、Ansible、Docker、Mesos、YARN、Kubernetes等等。Kafka Streams的轻量级以及综合能力使得它和其他流处理工具形成了鲜明的对比，后者需要你单独安装并维护一个流处理集群，需要依赖重量级的基础架构设施。</p>
</blockquote>
<p>The following list highlights several key capabilities and aspects of Kafka Streams that make it a compelling(引人注目) choice for use cases such as stream processing applications, event-driven systems, continuous queries and transformations, reactive applications, and microservices.</p>
<blockquote>
<p>下面列出了Kafka Streams的几个重要的功能，对于这些用例都是个吸引人的选择：流处理应用程序、事件驱动系统、持续查询和转换、响应式应用程序、微服务。</p>
</blockquote>
<p><strong>Powerful</strong>功能强大</p>
<ul>
<li>Highly scalable, elastic, fault-tolerant 高可用、可扩展性、故障容错</li>
<li>Stateful and stateless processing 有状态和无状态的处理</li>
<li>Event-time processing with windowing, joins, aggregations 针对事件的窗口函数、联合操作、聚合操作</li>
</ul>
<p><strong>Lightweight</strong>轻量级</p>
<ul>
<li>No dedicated cluster required 不需要专用的集群</li>
<li>No external dependencies 不需要外部的依赖</li>
<li>“It’s a library, not a framework.” 它是一个客户端库，不是一个框架</li>
</ul>
<p><strong>Fully integrated</strong>完全完整的</p>
<ul>
<li>100% compatible with Kafka 0.10.0.x 和Kafka完全兼容</li>
<li>Easy to integrate into existing applications 和已有应用程序容易集成</li>
<li>No artificial rules for deploying applications 对部署方式没有严格的规则限制</li>
</ul>
<p><strong>Real-time</strong>实时的</p>
<ul>
<li>Millisecond processing latency 微秒级别的处理延迟</li>
<li>Does not micro-batch messages 不是micro-batch处理</li>
<li>Windowing with out-of-order data 对无序数据的窗口操作</li>
<li>Allows for arrival of late data 允许延迟的数据</li>
</ul>
<h2 id="A_closer_look">A closer look</h2><p>Before we dive into the details such as the concepts and architecture of Kafka Streams or getting our feet wet by following the Kafka Streams quickstart guide, let us provide more context to the previous list of capabilities.</p>
<p>在深入研究Kafka Streams的概念和架构细节之前，你应该先看下<a href="http://docs.confluent.io/3.0.1/streams/quickstart.html#streams-quickstart" target="_blank" rel="external">快速指南</a>，现在我们为上面的那些特点提供更多的上下文信息（背景知识）。</p>
<p>1.Stream Processing Made Simple: Designed as a lightweight library in Apache Kafka, much like the Kafka producer and consumer client libraries. You can easily embed and integrate Kafka Streams into your own applications, which is a significant departure from framework-based stream processing tools that dictate many requirements upon you such as how you must package and “submit” processing jobs to their cluster.</p>
<blockquote>
<p>流处理更加简单：被设计为一个轻量级的库，就像Kafka的生产者和消费者客户端库一样。你可以很方便地将Kafka Streams集成到你自己的应用程序中，这是和其他基于框架的流处理工具的主要区别，它们对你会有很多要求，比如你必须打包然后把流处理作业提交到集群上执行。</p>
</blockquote>
<p>Has no external dependencies on systems other than Apache Kafka and can be used in any Java application. Read: You do not need to deploy and operate a separate cluster for your stream processing needs. Your Operations and Info Sec teams, among others, will surely be happy to hear this.</p>
<blockquote>
<p>在应用程序中除了Apache Kafka之外没有别的依赖：你不需要为你的流处理需求部署或维护一个单独的集群。你们的运维和安全团队肯定听到这个信息肯定很happy吧。</p>
</blockquote>
<p>2.Leverages Kafka as its internal messaging layer instead of (re)implementing a custom messaging layer like many other stream processing tools. Notably, Kafka Streams uses Kafka’s partitioning model to horizontally scale processing while maintaining strong ordering guarantees. This ensures high performance, scalability, and operational simplicity for production environments. A key benefit of this design decision is that you do not have to understand and tune two different messaging layers – one for moving data streams at scale (Kafka) plus a separate one for your stream processing tool. Similarly, any performance and reliability improvements of Kafka will automatically be available to Kafka Streams, too, thus tapping into the momentum of Kafka’s strong developer community.</p>
<blockquote>
<p>利用Kafka作为它的内部消息层而不像其他流处理工具一样重新造轮子。特别是，Kafka Streams使用Kafka的分区模型在维护强一致性的同时也具备了线性的处理能力，这种设计的优点是：你不需要理解或者调整两种消息模型（一种是线性地移动数据流，另外一种是流处理的消息）。同样，任何针对Kafka的性能和可靠性的提升，Kafka Streams都会自动具备，这也促使了Kafka开发者社区的动力。</p>
</blockquote>
<p>3.Is agnostic(不可知论) to resource management and configuration tools, so it integrates much more seamlessly(无缝) into the existing development, packaging, deployment, and operational practices of your organization. You are free to use your favorite tools such as Java application servers, Puppet, Ansible, Mesos, YARN, Docker – or even to run your application manually on a single machine for proof-of-concept scenarios.</p>
<blockquote>
<p>Kafka Streams不需要依赖资源管理和配置工具，所以它可以和已有的开发环境、打包、部署等工具无缝集成。可以运行在Java应用服务器，甚至在单机环境下做原型验证（POC）。</p>
</blockquote>
<p>4.Supports fault-tolerant local state, which enables very fast and efficient stateful operations like joins and windowed aggregations. Local state is replicated to Kafka so that, in case of a machine failure, another machine can automatically restore the local state and resume the processing from the point of failure.</p>
<blockquote>
<p>支持本地状态的故障容错，这使得有状态的操作（比如联合、窗口聚合）更快速和高效。由于本地状态本身通过Kafka进行复制，所以当一个机器宕机时，其他机器可以自动恢复本地状态，并且从故障出错的那个点继续处理。</p>
</blockquote>
<p>5.Employs one-record-at-a-time processing to achieve low processing latency, which is crucial(重要，决定性) for a variety of use cases such as fraud detection. This makes Kafka Streams different from micro-batch based stream processing tools.</p>
<blockquote>
<p>一次处理一条记录的流处理模型，所以处理延迟很低，对于像欺诈检测等场景来说非常重要。这也是Kafka Streams有别于基于micro-batch的流处理工具的区别。</p>
</blockquote>
<p>Furthermore, Kafka Streams has a strong focus on usability(可用性) and a great developer experience. It offers all the necessary stream processing primitives to allow applications to read data from Kafka as streams, process the data, and then either write the resulting data back to Kafka or send the final output to an external system. Developers can choose between a high-level DSL with commonly used operations like filter, map, join, as well as a low-level API for developers who need maximum control and flexibility.</p>
<blockquote>
<p>另外，Kafka Streams对开发者是易用和友好的。它提供了所有必要的流处理算子，允许应用程序将从Kafka读取出来的数据作为一个流，然后处理数据，最后可以将处理结果写回到Kafka或者发送给外部系统。开发者可以使用高级DSL（提供很多常用的操作比如filter、map、join）或者低级API两种方式（当需要更好地控制和灵活性时）。</p>
</blockquote>
<p>Finally, Kafka Streams helps with scaling developers, too – yes, the human side – because it has a low barrier(接线) to entry and a smooth path to scale from development to production: You can quickly write and run a small-scale proof-of-concept on a single machine because you don’t need to install or understand a distributed stream processing cluster; and you only need to run additional instances of your application on multiple machines to scale up to high-volume production workloads. Kafka Streams transparently(透明地) handles the load balancing of multiple instances of the same application by leveraging Kafka’s parallelism model.</p>
<blockquote>
<p>最后，Kafka Streams对于开发者也是扩展的。是的，从程序员的视角来看的话，它从开发环境到生产环境几乎没有界线：你可以在一台机器上运行一个很小批量的POC，因为你不需要安装或者理解一个分布式的流处理集群是怎么样（不需要知道程序在分布式环境下会有什么不同）；在多台机器上时，你只需要多运行几个应用程序实例就可以扩展到大规模的生产负载（生产环境下负载很高，只需多启动几个新的实例）。Kafka Streams会利用Kafka的并行模型透明底在相同应用程序多个实例之间处理负载均衡。</p>
</blockquote>
<p>In summary, Kafka Streams is a compelling choice for building stream processing applications. Give it a try and run your first Hello World Streams application! The next sections in this documentation will get you started.</p>
<blockquote>
<p>总之Kafka Streams对于构建流处理应用程序是一个非常不错的选择。快来运行一个<a href="http://docs.confluent.io/3.0.1/streams/quickstart.html#streams-quickstart" target="_blank" rel="external">Hello World的流处理应用吧</a>。</p>
</blockquote>
<h1 id="快速开始">快速开始</h1><h2 id="本节目标">本节目标</h2><p>The goal of this quickstart guide is to provide you with a first hands-on look at Kafka Streams. We will demonstrate how to run your first Java application that uses the Kafka Streams library by showcasing a simple end-to-end data pipeline powered by Kafka.</p>
<p>It is worth noting that this quickstart will only scratch the surface of Kafka Streams. More details are provided in the remainder of the Kafka Streams documentation, and we will include pointers throughout the quickstart to give you directions.</p>
<blockquote>
<p>本节的目标是让你亲自看看Kafka Streams是如何实现的。我们会向你展示使用Kafka完成的一个端到端的数据流管道，以及运行你的第一个使用Kafka库的Java应用程序。注意这里仅仅会涉及到Kafka Streams的表层，后续的部分会深入一些细节。</p>
</blockquote>
<h2 id="我们要做什么">我们要做什么</h2><p>下面是使用Java8实现的WordCount示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Serializers/deserializers (serde) for String and Long types</span></span><br><span class="line"><span class="keyword">final</span> Serde&lt;String&gt; stringSerde = Serdes.String();</span><br><span class="line"><span class="keyword">final</span> Serde&lt;Long&gt; longSerde = Serdes.Long();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Construct a `KStream` from the input topic ""streams-file-input", where message values</span></span><br><span class="line"><span class="comment">// represent lines of text (for the sake of this example, we ignore whatever may be stored in the message keys).</span></span><br><span class="line">KStream&lt;String, String&gt; textLines = builder.stream(stringSerde, stringSerde, <span class="string">"streams-file-input"</span>);</span><br><span class="line"></span><br><span class="line">KStream&lt;String, Long&gt; wordCounts = textLines</span><br><span class="line">    <span class="comment">// Split each text line, by whitespace, into words.  The text lines are the message</span></span><br><span class="line">    <span class="comment">// values, i.e. we can ignore whatever data is in the message keys and thus invoke</span></span><br><span class="line">    <span class="comment">// `flatMapValues` instead of the more generic `flatMap`.</span></span><br><span class="line">    .flatMapValues(value -&gt; Arrays.asList(value.toLowerCase().split(<span class="string">"\\W+"</span>)))</span><br><span class="line">    <span class="comment">// We will subsequently invoke `countByKey` to count the occurrences of words, so we use</span></span><br><span class="line">    <span class="comment">// `map` to ensure the words are available as message keys, too.</span></span><br><span class="line">    .map((key, value) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(value, value))</span><br><span class="line">    <span class="comment">// Count the occurrences of each word (message key).</span></span><br><span class="line">    <span class="comment">// This will change the stream type from `KStream&lt;String, String&gt;` to</span></span><br><span class="line">    <span class="comment">// `KTable&lt;String, Long&gt;` (word -&gt; count), hence we must provide serdes for `String` and `Long`.</span></span><br><span class="line">    .countByKey(stringSerde, <span class="string">"Counts"</span>)</span><br><span class="line">    <span class="comment">// Convert the `KTable&lt;String, Long&gt;` into a `KStream&lt;String, Long&gt;`.</span></span><br><span class="line">    .toStream();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Write the `KStream&lt;String, Long&gt;` to the output topic.</span></span><br><span class="line">wordCounts.to(stringSerde, longSerde, <span class="string">"streams-wordcount-output"</span>);</span><br></pre></td></tr></table></figure>
<p>然后，我们会执行如下步骤来完成第一个流应用程序：</p>
<ol>
<li>在一台机器上启动一个Kafka集群</li>
<li>使用Kafka内置的控制台生产者模拟往一个Kafka主题中写入一些示例数据</li>
<li>使用Kafka Streams库处理输入的数据，处理程序就是上面的wordcount示例</li>
<li>使用Kafka内置的控制台消费者检查应用程序的输出</li>
<li>停止Kafka集群</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">wget http://packages.confluent.io/archive/3.0.1/confluent-3.0.1-2.11.zip</span><br><span class="line">unzip confluent-3.0.1-2.11.zip</span><br><span class="line">cd confluent-3.0.1/</span><br><span class="line">bin/zookeeper-server-<span class="operator"><span class="keyword">start</span> ./etc/kafka/zookeeper.properties</span><br><span class="line"><span class="keyword">bin</span>/kafka-<span class="keyword">server</span>-<span class="keyword">start</span> ./etc/kafka/<span class="keyword">server</span>.properties</span><br><span class="line"></span><br><span class="line"><span class="keyword">bin</span>/kafka-topics <span class="comment">--create \</span></span><br><span class="line">          <span class="comment">--zookeeper localhost:2181 \</span></span><br><span class="line">          <span class="comment">--replication-factor 1 \</span></span><br><span class="line">          <span class="comment">--partitions 1 \</span></span><br><span class="line">          <span class="comment">--topic streams-file-input</span></span><br><span class="line"></span><br><span class="line">echo -<span class="keyword">e</span> <span class="string">"all streams lead to kafka\nhello kafka streams\njoin kafka summit"</span> &gt; /tmp/<span class="keyword">file</span>-<span class="keyword">input</span>.txt</span><br><span class="line"></span><br><span class="line">cat /tmp/<span class="keyword">file</span>-<span class="keyword">input</span>.txt | ./<span class="keyword">bin</span>/kafka-console-producer <span class="comment">--broker-list localhost:9092 --topic streams-file-input</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">bin</span>/kafka-run-<span class="keyword">class</span> org.apache.kafka.streams.examples.wordcount.WordCountDemo</span></span><br></pre></td></tr></table></figure>
<p>Similar to the bounded variant, it is a stateful algorithm that tracks and updates the counts of words. However, since it must assume potentially unbounded input data, it will periodically output its current state and results while continuing to process more data because it cannot know when it has processed “all” the input data. This is a typical difference between the class of algorithms that operate on unbounded streams of data and, say, batch processing algorithms such as Hadoop MapReduce. It will be easier to understand this difference once we inspect the actual output data later on.</p>
<blockquote>
<p>WordCount程序会计算输入单词出现次数的直方图，和之前看到的其他应用程序在有界数据集上不同的是，本例是在一个无限的、无界的数据流上操作。和有界操作相同的是，它也是一个有状态的算法（跟踪和更新单词的次数）。不过，由于它必须假设无限的输入数据，它会定时地输出当前状态和结果，并且持续地处理更多的数据，因为它不知道什么时候它已经处理完了所有的输入数据。这和在有界流数据上的算法是不同的比如Hadoop的MapReduce。在我们检查了实际的输出结果后，你就会更加容易地理解这里的不同点。</p>
</blockquote>
<p>The WordCount demo application will read from the input topic streams-file-input, perform the computations of the WordCount algorithm on the input data, and continuously write its current results to the output topic streams-wordcount-output (the names of its input and output topics are hardcoded). The demo will run for a few seconds and then, unlike typical stream processing applications, terminate automatically.</p>
<blockquote>
<p>这个WordCount示例会从Kafka的输入主题“streams-file-input”中读取数据，在输入数据上执行WorldCount算法，并且持续地将当前结果写入到输出主题“streams-wordcount-output”。不过和其他流处理程序不同的是，这里为了实验，仅仅运行几秒钟后就会退出，通常实际运行的流应用程序是永远不会停止的。</p>
</blockquote>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer --zookeeper localhost:<span class="number">2181</span> \</span><br><span class="line">          --topic streams-wordcount-output \</span><br><span class="line">          --from-beginning \</span><br><span class="line">          --formatter kafka<span class="class">.tools</span><span class="class">.DefaultMessageFormatter</span> \</span><br><span class="line">          --property print.key=true \</span><br><span class="line">          --property key.deserializer=org<span class="class">.apache</span><span class="class">.kafka</span><span class="class">.common</span><span class="class">.serialization</span><span class="class">.StringDeserializer</span> \</span><br><span class="line">          --property value.deserializer=org<span class="class">.apache</span><span class="class">.kafka</span><span class="class">.common</span><span class="class">.serialization</span><span class="class">.LongDeserializer</span></span><br></pre></td></tr></table></figure>
<p>打印信息如下，这里第一列是Kafka消息的键（字符串格式），第二列是消息的值（Long类型）。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">all     <span class="number">1</span></span><br><span class="line">streams <span class="number">1</span></span><br><span class="line">lead    <span class="number">1</span></span><br><span class="line">to      <span class="number">1</span></span><br><span class="line">kafka   <span class="number">1</span></span><br><span class="line">hello   <span class="number">1</span></span><br><span class="line">kafka   <span class="number">2</span></span><br><span class="line">streams <span class="number">2</span></span><br><span class="line">join    <span class="number">1</span></span><br><span class="line">kafka   <span class="number">3</span></span><br><span class="line">summit  <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>等等，输出结果看起来有点奇怪，为什么有重复的条目比如”streams”出现了两次，”kafka”出现了三次，难道不应该是下面这样的吗：  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor">#为什么不是这样，你可能会有疑问</span></span><br><span class="line">all     <span class="number">1</span></span><br><span class="line">lead    <span class="number">1</span></span><br><span class="line">to      <span class="number">1</span></span><br><span class="line">hello   <span class="number">1</span></span><br><span class="line">streams <span class="number">2</span></span><br><span class="line">join    <span class="number">1</span></span><br><span class="line">kafka   <span class="number">3</span></span><br><span class="line">summit  <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>The explanation is that the output of the WordCount application is actually a continuous stream of updates, where each data record (i.e. each line in the original output above) is an updated count of a single word aka record key such as “kafka”. For multiple records with the same key, each later record is an update of the previous one.</p>
<blockquote>
<p>合理的解释是：WordCount应用程序的输出实际上是一个持续不断的”更新流”，每条数据记录（上面示例中每一行的输出结果）都是一个单词（记录的key，比如”kafka”）的更新次数。对于相同key的多条记录，后面的记录都是对前面记录的更新。</p>
</blockquote>
<p>The two diagrams below illustrate what is essentially(本质) happening behind the scenes. The first column shows the evolution of the current state of the KTable<string, long=""> that is counting word occurrences for countByKey. The second column shows the change records that result from state updates to the KTable and that eventually, once converted to a KStream</string,></p>
<blockquote>
<p>下面的两幅图展示了发生在背后的本质，第一列表示<code>KTable&lt;String, Long&gt;</code>的当前状态的进化，通过<code>countByKey</code>计算单词的出现次数。第二列的结果显示了从状态改变到KTable的变更记录，最终被转换为一个KStream。</p>
</blockquote>
<p>First the text line “all streams lead to kafka” is being processed. The KTable is being built up as each new word results in a new table entry (highlighted with a green background), and a corresponding change record is sent to the downstream KStream.</p>
<p>When the second text line “hello kafka streams” is processed, we observe, for the first time, that existing entries in the KTable are being updated (here: for the words “kafka” and for “streams”). And again, change records are being sent to the KStream.</p>
<blockquote>
<p>当第一次处理文本行“all streams lead to kafka”时，KTable会在每个表的条目中构建一个新的单词结果（绿色高亮），并且<strong>把对应的变更记录发送给下游的KStream</strong>。<br>当处理第二个文本行“hello kafka streams”时，我们注意到，和第一次不同的是，存在于KTable的条目会被更新（比如这里的”kafka”和”streams”），并且同样的，变更记录也会被发送到KStream。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161103092411029" alt="10-1 ktable kstream"></p>
<p>And so on (we skip the illustration of how the third line is being processed). This explains why the output topic has the contents we showed above, because it contains the full record of changes, i.e. the information shown in the second column for KStream above:</p>
<blockquote>
<p>第三行的处理也是类似的，这里就不再累述。这就解释了为什么上面输出的主题内容是我们看到的那样，因为它包含了所有完整的变更记录，即上面第二列KStream的内容：</p>
</blockquote>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">all     <span class="number">1</span></span><br><span class="line">streams <span class="number">1</span></span><br><span class="line">lead    <span class="number">1</span></span><br><span class="line"><span class="built_in">to</span>      <span class="number">1</span></span><br><span class="line">kafka   <span class="number">1</span>  &lt;- <span class="keyword">first</span> <span class="built_in">line</span> <span class="function"><span class="keyword">end</span></span><br><span class="line"><span class="title">hello</span>   <span class="title">1</span></span></span><br><span class="line">kafka   <span class="number">2</span></span><br><span class="line">streams <span class="number">2</span>  &lt;- <span class="keyword">second</span> <span class="built_in">line</span> <span class="function"><span class="keyword">end</span></span><br><span class="line"><span class="title">join</span>    <span class="title">1</span></span></span><br><span class="line">kafka   <span class="number">3</span></span><br><span class="line">summit  <span class="number">1</span>  &lt;- <span class="keyword">third</span> <span class="built_in">line</span> <span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>为什么不输出KTable，这是因为KTable每次处理一条记录，都会发送变更记录给下游的KStream，即KTable每次处理一条记录，产生一条变更记录。而KTable本身是有状态的，可以看到在处理第一个单词时，KTable有一条记录，在处理第二个不同的单词时，KTable有两条记录，这个状态是一直保存的，如果说把KTable作为输出，那么就会有重复的问题，比如下面这样的输出肯定不是我们希望看到的：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">all     <span class="number">1</span>  &lt;-处理第一个单词后的KTable</span><br><span class="line">all     <span class="number">1</span></span><br><span class="line">streams <span class="number">1</span>  &lt;-处理第二个单词后的KTable</span><br><span class="line">all     <span class="number">1</span></span><br><span class="line">streams <span class="number">1</span></span><br><span class="line">lead    <span class="number">1</span>  &lt;-处理第三个单词后的KTable</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>Looking beyond the scope of this concrete example, what Kafka Streams is doing here is to leverage the duality(二元性，对偶) between a table and a changelog stream (here: table = the KTable, changelog stream = the downstream KStream): you can publish every change of the table to a stream, and if you consume the entire changelog stream from beginning to end, you can reconstruct the contents of the table.</p>
<blockquote>
<p>Kafka Streams这里做的工作利用了一张表和一个变更流的二元性（表指的是KTable，变更流指的是下游的KStream）：你可以将表的每个变更记录发布给一个流，如果你从整个变更流的最开始消费到最后，你就可以重新构造出表的内容。</p>
</blockquote>
<h1 id="概念">概念</h1><h2 id="Kafka_101">Kafka 101</h2><p>Kafka Streams is, by deliberate(深思熟虑) design, tightly integrated with Apache Kafka: it uses Kafka as its internal messaging layer. As such it is important to familiarize yourself with the key concepts of Kafka, too, notably the sections 1. Getting Started and 4. Design in the Kafka documentation. In particular you should understand:</p>
<p>Kafka Streams是经过深思熟虑的设计，它和Apache Kafka仅仅地集成：它使用Kafka作为内部的消息层。所以理解Kafka的关键概念非常重要，如果不熟悉，可以看Kafka的文档。</p>
<ul>
<li>The who’s who: Kafka distinguishes producers, consumers, and brokers. In short, producers publish data to Kafka brokers, and consumers read published data from Kafka brokers. Producers and consumers are totally decoupled. A Kafka cluster consists of one or more brokers.</li>
<li>The data: Data is stored in topics. The topic is the most important abstraction provided by Kafka: it is a category or feed name to which data is published by producers. Every topic in Kafka is split into one or more partitions, which are replicated across Kafka brokers for fault tolerance.</li>
<li>Parallelism: Partitions of Kafka topics, and especially their number for a given topic, are also the main factor that determines the parallelism of Kafka with regards to reading and writing data. Because of their tight integration the parallelism of Kafka Streams is heavily influenced by and depending on Kafka’s parallelism.</li>
</ul>
<ol>
<li>Kafka分成生产者、消费者、Brokers。生产者发布数据给Kafka的Brokers，消费者从Kafka的Brokers读取发布过的数据。生产者和消费者完全解耦。一个Kafka集群包括一个或多个Broekrs节点。</li>
<li>数据以主题的形式存储。主题是Kafka提供的最重要的一个抽象：它是生产者发布数据的一种分类（相同类型的消息应该发布到相同的主题）。每个主题会分成一个或多个分区，并且为了故障容错，每个分区都会在Kafka的Brokers中进行复制。</li>
<li>Kafka主题的分区数量决定了读取或写入数据的并行度。因为Kafka Streams和Kafka结合的很紧，所以Kafka Streams也依赖于Kafka的并行度。</li>
</ol>
<h2 id="流、流处理、拓扑、算子">流、流处理、拓扑、算子</h2><p>A stream is the most important abstraction provided by Kafka Streams: it represents an unbounded, continuously updating data set, where unbounded means “of unknown or of unlimited size”. A stream is an ordered, replayable, and fault-tolerant sequence of immutable data records, where a data record is defined as a key-value pair.</p>
<blockquote>
<p>流是Kafka Streams提供的最重要的抽象：它代表了一根无界的、持续更新的数据集。流是一个有序的、可重放的、容错的不可变数据记录序列，其中每个数据记录被定位成一个key-value键值对</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161103101744946" alt="stream-record">  </p>
<p>A stream processing application is any program that makes use of the Kafka Streams library. In practice, this means it is probably “your” Java application. It may define its computational logic through one or more processor topologies (see next section).</p>
<blockquote>
<p>流处理应用程序是任何使用了Kafka Streams库进行开发的应用程序，它会通过一个或多个处理拓扑定义计算逻辑。</p>
</blockquote>
<p>A processor topology or simply topology defines the computational logic of the data processing that needs to be performed by a stream processing application. A topology is a graph of stream processors (nodes) that are connected by streams (edges). Developers can define topologies either via the low-level Processor API or via the Kafka Streams DSL, which builds on top of the former.</p>
<p>处理拓扑或者叫拓扑定义了流处理应用程序对数据处理的计算逻辑。拓扑是一张由流处理算子和相连接的流组成的DAG图，其中算子是图的节点，流是图的边。开发者可以通过低级的Processor API或者高级的Kafka Streams DSL定义拓扑，其中后者实际上是构建在前者之上的。</p>
<p>A stream processor is a node in the processor topology(as shown in the diagram of section Processor Topology). It represents a processing step in a topology, i.e. it is used to transform data in streams. Standard operations such as map, filter, join, and aggregations are examples of stream processors that are available in Kafka Streams out of the box. A stream processor receives one input record at a time from its upstream processors in the topology, applies its operation to it, and may subsequently produce one or more output records to its downstream processors.</p>
<blockquote>
<p>流算子是处理拓扑中的节点，它代表了在拓扑中的处理步骤，比如转换算子会在流中转换数据。标准的算子包括map/filter/join/aggregation，这些都是流算子的示例，并且内置在Kafka Streams中开箱即用。一个流算子从它在拓扑中的上游算子一次接收一条输入记录，将操作运用到记录，并且可能会产生一条或多条输出记录给下游的算子。Kafka Streams提供了两种方式来定义算子：</p>
</blockquote>
<ul>
<li>The Kafka Streams DSL provides the most common data transformation operations such as map and filter so you don’t have to implement these stream processors from scratch.</li>
<li>The low-level Processor API allows developers to define and connect custom processors as well as to interact with state stores.</li>
</ul>
<ol>
<li>Kafka Streams DSL提供了最通用的数据转换操作，比如map、filter，这样你不需要自己实现这些算子</li>
<li>低级的Processor API，允许开发者定义和连接定制的算子，并且还可以和状态存储交互</li>
</ol>
<h2 id="时间">时间</h2><p>A critical aspect in stream processing is the the notion of time, and how it is modeled and integrated. For example, some operations such as Windowing are defined based on time boundaries.</p>
<blockquote>
<p>流处理的一个重要概念是时间，如何对时间进行建模和整合非常重要，因为有些操作比如窗口函数会基于时间的边界来定义。有几种类型的时间表示方式：</p>
</blockquote>
<ul>
<li>Event-time: The point in time when an event or data record occurred, i.e. was originally created “by the source”. Achieving event-time semantics typically requires embedding timestamps in the data records at the time a data record is being produced. Example: If the event is a geo-location change reported by a GPS sensor in a car, then the associated event-time would be the time when the GPS sensor captured the location change.</li>
<li>Processing-time: The point in time when the event or data record happens to be processed by the stream processing application, i.e. when the record is being consumed. The processing-time may be milliseconds, hours, or days etc. later than the original event-time. Example: Imagine an analytics application that reads and processes the geo-location data reported from car sensors to present it to a fleet management dashboard. Here, processing-time in the analytics application might be milliseconds or seconds (e.g. for real-time pipelines based on Apache Kafka and Kafka Streams) or hours (e.g. for batch pipelines based on Apache Hadoop or Apache Spark) after event-time.</li>
<li>Ingestion-time: The point in time when an event or data record is stored in a topic partition by a Kafka broker. Ingestion-time is similar to event-time, as a timestamp gets embedded in the data record itself. The difference is, that the timestamp is generated when the record is appended to the target topic by the Kafka broker, not when the record is created “at the source”. Ingestion-time may approximate event-time reasonably well if we assume that the time difference between creation of the record and its ingestion into Kafka is sufficiently small, where “sufficiently” depends on the specific use case. Thus, ingestion-time may be a reasonable alternative for use cases where event-time semantics are not possible, e.g. because the data producers don’t embed timestamps (e.g. older versions of Kafka’s Java producer client) or the producer cannot assign timestamps directly (e.g., it does not have access to a local clock).</li>
</ul>
<ol>
<li>事件时间：事件或数据记录发生的时间点，它是由事件源创建的。实现事件时间予以通常需要在记录中有内置的时间撮字段，表示这条记录在什么时候产生。比如一条事件是车辆传感器报告的地理位置变更，那么对应的事件时间表示GPS传感器捕获位置变更的时间点。</li>
<li>处理时间：事件被流处理应用程序处理的时间点，比如就被消费的时候，处理时间会比原始的事件时间要晚。举例一个分析应用程序读取并处理车辆上传的地理位置，并且呈现到一个dashboard上。这里分析程序的处理时间可能比事件的时间晚几毫米、几秒、甚至几个小时。</li>
<li>摄取时间：事件存储到Kafka Brokers的主题分区中的时间点。摄取时间和时间时间类似，它也是作为数据记录本身的一个内置字段，不同的是<strong>摄取时间是在追加到Kafka中时自动生成的，而不是数据源创建的时间</strong>。如果我们假设记录的创建时间和摄取到Kafka的时间间隔足够短的话，可以认为摄取时间近似于事件时间，当然足够短这个时间跟具体的用例有关。什么场景下采用摄取时间比较合理呢？比如数据源没有内置的事件时间（比如旧版本的Java生产者客户端在消息中不会带有时间撮，新版本则有），或者说生产者不能直接分配时间撮（无法获取到本地时钟）。</li>
</ol>
<p>The choice between event-time and ingestion-time is actually done through the configuration of Kafka (not Kafka Streams): From Kafka 0.10.x onwards, timestamps are automatically embedded into Kafka messages. Depending on Kafka’s configuration these timestamps represent event-time or ingestion-time. The respective Kafka configuration setting can be specified on the broker level or per topic. The default timestamp extractor in Kafka Streams will retrieve these embedded timestamps as-is. Hence, the effective time semantics of your application depend on the effective Kafka configuration for these embedded timestamps. See :ref:Developer Guide <timestamp extractor=""> for detailed information.</timestamp></p>
<blockquote>
<p>选择事件时间还是摄取时间，是通过Kafka的配置文件（不是Kafka Streams的配置），在0.10版本之后，时间撮会自动内嵌到Kafka的消息中。根据Kafka的配置，时间撮可以指定为事件时间还是摄取时间，这个配置可以设置到Broker级别，也可以是每个Topic。默认的Kafka Streams时间撮抽取方式会取出内置的时间撮字段。所以应用程序的有效时间语义依赖于Kafka的内置时间撮。</p>
</blockquote>
<p>Kafka Streams assigns a timestamp to every data record via so-called timestamp extractors. These per-record timestamps describe the progress of a stream with regards to time (although records may be out-of-order within the stream) and are leveraged by time-dependent operations such as joins. They are also used to synchronize multiple input streams within the same application.</p>
<blockquote>
<p>Kafka Streams会通过时间撮抽取器把一个时间撮分配给每条记录。每条记录的时间撮描述了一条流关于时间的进度（尽管流中的记录可能没有顺序），这个时间撮会被时间相关的操作比如join所使用。同时它们也会被用来在同一个应用程序中多个输入流的同步。</p>
</blockquote>
<p>Concrete implementations of timestamp extractors may retrieve or compute timestamps based on the actual contents of data records such as an embedded timestamp field to provide event-time or ingestion-time semantics, or use any other approach such as returning the current wall-clock time at the time of processing, thereby yielding processing-time semantics to stream processing applications. Developers can thus enforce(执行，强制) different notions/semantics of time depending on their business needs.</p>
<blockquote>
<p>具体的时间撮抽取器实现，基于数据记录实际内容的时间撮，可能会读取或者计算，比如（数据记录中）提供的事件时间或者摄取时间语义的时间撮字段，或者使用其他的方式，比如返回当前的处理时间，即流处理应用程序的处理时间语义。开发者可以根据他们的业务需求使用不同的时间语义。</p>
</blockquote>
<p>Be aware that ingestion-time in Kafka Streams is used slightly different as in other stream processing systems. Ingestion-time could mean the time when a record is fetched by a stream processing application’s source operator. In Kafka Streams, ingestion-time refers to the time when a record was appended to a Kafka topic partition.</p>
<blockquote>
<p>注意Kafka Streams的摄取时间可能和其他流处理系统的使用方式有点不同。摄取时间可以表示为被流处理的源算子获取的时间点。而在Kafka Streams中，摄取时间指的是当一条记录被追加到Kafka主题分区的那个时间点（即Producer写入分区的时间）。</p>
</blockquote>
<h2 id="有状态的流处理">有状态的流处理</h2><p>Some stream processing applications don’t require state, which means the processing of a message is independent from the processing of all other messages. If you only need to transform one message at a time, or filter out messages based on some condition, the topology defined in your stream processing application can be simple.</p>
<blockquote>
<p>有些流处理应用程序并不需要状态，这意味着一条消息的处理和其他所有消息的处理都是独立的。如果你只需要在一个时间点转换一条消息，或者基于某些条件对消息进行过滤，你的流计算应用层序的拓扑可以非常简单。</p>
</blockquote>
<p>However, being able to maintain state opens up many possibilities for sophisticated(复杂) stream processing applications: you can join input streams, or group and aggregate data records. Many such stateful operators are provided by the Kafka Streams DSL.</p>
<blockquote>
<p>不过，对于复杂的流处理应用程序，为了能够维护状态，会有很多可能性：联合不同的输入流，对数据记录进行分组和聚合。Kafka Streams的DSL提供了很多有状态的操作算子。</p>
</blockquote>
<h2 id="Streams和Tables的二元性">Streams和Tables的二元性</h2><p>Before we discuss concepts such as aggregations in Kafka Streams we must first introduce tables, and most importantly the relationship between tables and streams: the so-called stream-table duality. Essentially, this duality means that a stream can be viewed as a table, and vice versa. Kafka’s log compaction feature, for example, exploits(功绩，利用，开发) this duality.</p>
<p>在介绍Kafka Streams的概念之前（比如聚合），我们必须先介绍tables，以及tables和streams的关系（所谓的stream-table二元性）。从本质上来说，二元性意味着一个流可以被看做是一张表，反过来也是成立的。Kafka的日志压缩特性，可以实现这样的二元转换。一张表，简单来说就是一系列的键值对，或者被叫做字典、关联数组。</p>
<p><img src="http://img.blog.csdn.net/20161103130945109" alt="k-table"></p>
<p>stream-table二元性描述了两者的紧密关系：</p>
<ul>
<li>Stream as Table: A stream can be considered a changelog of a table, where each data record in the stream captures a state change of the table. A stream is thus a table in disguise(假装), and it can be easily turned into a “real” table by replaying the changelog from beginning to end to reconstruct the table. Similarly, in a more general analogy(类比), aggregating data records in a stream – such as computing the total number of pageviews by user from a stream of pageview events – will return a table (here with the key and the value being the user and its corresponding pageview count, respectively).</li>
<li>Table as Stream: A table can be considered a snapshot, at a point in time, of the latest value for each key in a stream (a stream’s data records are key-value pairs). A table is thus a stream in disguise, and it can be easily turned into a “real” stream by iterating over each key-value entry in the table.</li>
</ul>
<ol>
<li>将流作为表：一个流可以被认为是一张表的变更记录(changelog)，流中的每条数据记录捕获了表的每个变更状态。一个流可以<em>假装</em>是一张表，也可以通过从头到尾重放变更日志来重构表，很容易地变成<em>真正</em>的表。同样地，在流中聚合记录（比如从PV事件流中计算用户的PV数）会返回一张表（这里键值分别是用户，以及对应的PV数）。</li>
<li>将表作为流：一张表可以认为是在某个时间点的一份快照，是流的每个key对应的最近的值。一张表因此也可以假装是一个流，也可以通过迭代表中所有的键值条目转换成一个真实的流。</li>
</ol>
<p>Let’s illustrate this with an example. Imagine a table that tracks the total number of pageviews by user (first column of diagram below). Over time, whenever a new pageview event is processed, the state of the table is updated accordingly. Here, the state changes between different points in time – and different revisions(修正) of the table – can be represented as a changelog stream (second column).</p>
<p>Interestingly, because of the stream-table duality, the same stream can be used to reconstruct the original table (third column):</p>
<p>举例，一张表会跟踪用户的PV总数（左图第一列），当一条新的pageview事件被处理的时候，表的状态会相应地被更新。这里不同时间点的状态变更（针对表的不同修改），可以作为一个变更日志流（左图第二列）。有趣的是，由于stream-table的二元性，相同的流可以被用来构造出原始的表（右图第三列）。</p>
<p><img src="http://img.blog.csdn.net/20161103131309314" alt="k stream table durable"></p>
<p>The same mechanism(机制) is used, for example, to replicate databases via change data capture (CDC) and, within Kafka Streams, to replicate its so-called state stores across machines for fault-tolerance. The stream-table duality is such an important concept that Kafka Streams models it explicitly(明确地) via the KStream and KTable interfaces, which we describe in the next sections.</p>
<blockquote>
<p>这种类似的机制也被用在其他系统中，比如通过CDC复制数据库。在Kafka Streams中，为了容错处理，会将它的状态存储复制到多台机器上。stream-table的二元性是很重要的概念，Kafka Streams通过KStream和KTable接口对它们进行建模。</p>
</blockquote>
<h2 id="KStream（记录流_record_stream）">KStream（记录流 record stream）</h2><p>A KStream is an abstraction of a record stream, where each data record represents a self-contained datum(基准，资料) in the unbounded data set. Using the table analogy(类比), data records in a record stream are always interpreted as “inserts” – think: append-only ledger(分类) – because no record replaces an existing row with the same key. Examples are a credit card transaction, a page view event, or a server log entry.</p>
<blockquote>
<p>一个KStream是对记录流的抽象，每条数据记录能够表示在无限数据集中自包含的数据。用传统数据库中的表这个概念来类比，记录流中的数据可以理解为“插入”（只有追加），因为不会有记录会替换已有的相同key的行。比如信用卡交易、访问时间、服务端日志条目。举例有两条记录发送到流中：  </p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">"alice"</span>, <span class="number">1</span>) --&gt; (<span class="string">"alice"</span>, <span class="number">3</span>)  <span class="comment">//这两条记录依次发送到流中</span></span><br></pre></td></tr></table></figure>
<p>If your stream processing application were to sum the values per user, it would return 4 for alice. Why? Because the second data record would not be considered an update of the previous record. Compare this behavior of KStream to KTable below, which would return 3 for alice.</p>
<blockquote>
<p>如果你的流处理应用程序是为每个用户求和（记录的value含义不是很明确，但是我们只是要对value值求和），那么alice用户的返回结果是4。因为第二条记录不会被认为是对前一条记录的更新（第一条记录和第二条记录是同时存在的）。如果将其和下面的KTable对比，KTable中alice用户的返回结果是3。</p>
</blockquote>
<h2 id="KTable（变更流_changelog_stream）">KTable（变更流 changelog stream）</h2><p>A KTable is an abstraction of a changelog stream, where each data record represents an update. More precisely, the value in a data record is considered to be an update of the last value for the same record key, if any (if a corresponding key doesn’t exist yet, the update will be considered a create). Using the table analogy, a data record in a changelog stream is interpreted as an update because any existing row with the same key is overwritten.</p>
<blockquote>
<p>一个KTable是对变更日志流的抽象，每条数据记录代表的是一个更新。更准确的说，数据记录中的值被认为是对已有相同记录的key的值更新（如果存在key则更新，如果key不存在，更新操作会被认为是创建）。用传统数据库中的表这个概念来类比，变更流中的数据可以理解为“更新”，因为任何已经存在相同key的行都会被覆盖。</p>
</blockquote>
<p>If your stream processing application were to sum the values per user, it would return 3 for alice. Why? Because the second data record would be considered an update of the previous record. Compare this behavior of KTable with the illustration for KStream above, which would return 4 for alice.</p>
<blockquote>
<p>还是以上面的两条记录发送到流中为例，如果也是为每个用户求和，那么alice用户的返回结果是3。因为第二条记录会被认为是对前一条记录的更新（那么第一条记录实际上就不存在了）。如果将其和上面的KStream对比，KStream中alice用户的返回结果是4。</p>
</blockquote>
<p>Effects of Kafka’s log compaction: Another way of thinking about KStream and KTable is as follows: If you were to store a KTable into a Kafka topic, you’d probably want to enable Kafka’s log compaction feature, e.g. to save storage space.</p>
<blockquote>
<p>理解KStream和KTable的另外一种思路是：如果将KTable存储到Kafka主题中，你应该开启Kafka的日志压缩功能。</p>
</blockquote>
<p>However, it would not be safe to enable log compaction in the case of a KStream because, as soon as log compaction would begin purging older data records of the same key, it would break the semantics of the data. To pick up the illustration example again, you’d suddenly get a 3 for alice instead of a 4 because log compaction would have removed the (“alice”, 1) data record. Hence log compaction is perfectly safe for a KTable (changelog stream) but it is a mistake for a KStream (record stream).</p>
<blockquote>
<p>如果是KStream，开启日志压缩不是一个安全的做法，因为日志压缩会清除相同key的不同数据，这会破坏数据的语义。举例，你可能会突然看到用户alice的结果为3而不是4，因为日志压缩会删除(“alice”, 1)这条记录。所以日志压缩对于KTable是安全的，而对KSteram则是错误的用法。</p>
</blockquote>
<p>We have already seen an example of a changelog stream in the section Duality of Streams and Tables. Another example are change data capture (CDC) records in the changelog of a relational database, representing which row in a database table was inserted, updated, or deleted.</p>
<blockquote>
<p>在stream-table二元性中，我们已经看到了一个变更日志流的示例。另一个示例是关系型数据库中的CDC变更日志，表示数据库中哪一行执行了插入，更新、删除动作。</p>
</blockquote>
<p>KTable also provides an ability to look up current values of data records by keys. This table-lookup functionality is available through join operations (see also Joining Streams in the Developer Guide).</p>
<blockquote>
<p>KTable也支持根据记录的key查询当前的value，这种特性会在join操作时使用。</p>
</blockquote>
<h2 id="窗口操作">窗口操作</h2><p>A stream processor may need to divide data records into time buckets, i.e. to window the stream by time. This is usually needed for for join and aggregation operations, etc. Windowed stream buckets can be maintained in the processor’s local state.</p>
<blockquote>
<p>一个流算子可能需要将数据记录分成多个时间段，比如对流按照时间做成一个个窗口。通常在联合和聚合操作时需要这么做。在算子的本地状态中会维护窗口流。</p>
</blockquote>
<p>Windowing operations are available in the Kafka Streams DSL, where users can specify a retention period for the window. This allows Kafka Streams to retain old window buckets for a period of time in order to wait for the late arrival of records whose timestamps fall within the window interval. If a record arrives after the retention period has passed, the record cannot be processed and is dropped.</p>
<blockquote>
<p>窗口算子在Kafka Stream DSL中可以使用，用户可以指定窗口的保留时间。这样允许Kafka Streams会在一段时间内保留旧的窗口段，目的是等待迟来的记录，这些记录的时间撮落在窗口间隔内（虽然不一定是当前窗口，但可能是旧的窗口，如果没有保留旧窗口的话，迟来的记录就会被直接丢弃了，因为当前窗口不能存放旧记录）。如果一条记录在保留时间过去之后才到达，这条记录就不会被处理，只能被丢弃了。</p>
</blockquote>
<p>Late-arriving records are always possible in real-time data streams. However, it depends on the effective time semantics how late records are handled. Using processing-time, the semantics are “when the data is being processed”, which means that the notion of late records is not applicable as, by definition, no record can be late. Hence, late-arriving records only really can be considered as such (i.e. as arriving “late”) for event-time or ingestion-time semantics. In both cases, Kafka Streams is able to properly handle late-arriving records.</p>
<blockquote>
<p>迟到的记录在实时数据流中总是可能发生的。不过，它取决于记录到底有多晚才被处理的有效时间语义。使用处理时间，语义是“当数据正在被处理”，这就意味着迟来的记录是不适合的，也就是说不会有记录迟到的。所以迟到的记录只能针对事件时间或者摄取时间这两种语义。这两种情况下，Kafka Streams都可以很好地处理迟到的记录。</p>
</blockquote>
<h2 id="联合操作">联合操作</h2><p>A join operation merges two streams based on the keys of their data records, and yields a new stream. A join over record streams usually needs to be performed on a windowing basis because otherwise the number of records that must be maintained for performing the join may grow indefinitely.</p>
<blockquote>
<p>联合操作会合并两个流，基于他们的数据记录的keys，并产生一个新的流。在记录流上的join操作通常需要在窗口的基础上执行，否则为了执行join而需要维护的记录数量会无限膨胀（在无限的记录集上无法做join操作，因为你不知道什么时候结束，就无法join，联合操作必须是在有限的记录集上，而窗口正好是有限的记录集）。</p>
</blockquote>
<p>The join operations available in the Kafka Streams DSL differ based on which kinds of streams are being joined (e.g. KStream-KStream join versus KStream-KTable join).</p>
<blockquote>
<p>Kafka Streams DSL中的join操作跟流的类型有关，比如KStream-KStream进行join，或者KStream-KTable进行join。</p>
</blockquote>
<h2 id="聚合操作">聚合操作</h2><p>An aggregation operation takes one input stream, and yields a new stream by combining multiple input records into a single output record. Examples of aggregations are computing counts or sum. An aggregation over record streams usually needs to be performed on a windowing basis because otherwise the number of records that must be maintained for performing the aggregation may grow indefinitely.</p>
<blockquote>
<p>一个聚合操作会接受一个输入流，然后通过合并多条输入记录产生一个新的流，最终生成一个单一的输出记录，聚合算子的示例比如计算次数或求和。和join操作一样，聚合操作也需要在窗口的基础上执行。</p>
</blockquote>
<p>In the Kafka Streams DSL, an input stream of an aggregation can be a KStream or a KTable, but the output stream will always be a KTable. This allows Kafka Streams to update an aggregate value upon the late arrival of further records after the value was produced and emitted. When such late arrival happens, the aggregating KStream or KTable simply emits a new aggregate value. Because the output is a KTable, the new value is considered to overwrite the old value with the same key in subsequent processing steps.</p>
<blockquote>
<p>在Kafka Streams DSL中，<strong>聚合操作的输入流可以是一个KStream或者是一个KTable，但是输出流只能是一个KTable</strong>。这就允许Kafka Streams在value被产生并发送出去之后，即使迟到的记录到来时，也可以更新聚合结果（第一次产生的结果是在当前窗口，然后把结果发送出去，第二次产生的结果已经不在当前窗口，它属于旧的窗口，也会更新对应的聚合结果，然后再把最新的结果发送出去）。当这样的迟到记录到来时，聚合的KStream或者KTable仅仅简单地发送新的聚合结果。由于输出是一个KTable，相同key下，在后续的处理步骤中，新的值会覆盖旧的值。</p>
</blockquote>
<h1 id="架构">架构</h1><p>Kafka Streams simplifies application development by building on the Kafka producer and consumer libraries and leveraging the native capabilities of Kafka to offer data parallelism, distributed coordination, fault tolerance, and operational simplicity. In this section, we describe how Kafka Streams works underneath the covers. Below is Logical view of a Kafka Streams application that contains multiple stream threads, each of which in turn containing multiple stream tasks.</p>
<p>Kafka Streams是构建在Kafka生产者和消费者的库，并且利用了Kafka本身的特性提供了数据的并行、分布式协调、容错，简化了应用程序的开发。下图是Kafka Streams应用程序的逻辑视图，包括了多个流线程，每个线程包括多个流任务。</p>
<p><img src="http://img.blog.csdn.net/20161103180445363" alt="kstream arch overview"></p>
<h2 id="拓扑">拓扑</h2><p>A processor topology or simply topology defines the stream processing computational logic for your application, i.e., how input data is transformed into output data. A topology is a graph of stream processors (nodes) that are connected by streams (edges). There are two special processors in the topology:</p>
<blockquote>
<p>拓扑定义了流处理应用程序的计算逻辑，比如输入数据怎么转换成输出数据。拓扑是由流处理算子和相连的流组成的一张图。在拓扑中有两种特殊类型的流算子：</p>
</blockquote>
<ul>
<li>Source Processor: A source processor is a special type of stream processor that does not have any upstream processors. It produces an input stream to its topology from one or multiple Kafka topics by consuming records from these topics and forward them to its down-stream processors.</li>
<li>Sink Processor: A sink processor is a special type of stream processor that does not have down-stream processors. It sends any received records from its up-stream processors to a specified Kafka topic.</li>
</ul>
<ol>
<li>源算子：没有任何上游算子，它从一个或多个Kafka主题中消费记录，然后产生一个到拓扑的输入流，并且转发到下游的算子</li>
<li>目标算子：没有任何的下游算子，它会把从上游算子接收到的任何记录，发送给指定的Kafka主题</li>
</ol>
<p><img src="http://img.blog.csdn.net/20161103180512627" alt="kstream topo"></p>
<p>A stream processing application – i.e., your application – may define one or more such topologies, though typically it defines only one. Developers can define topologies either via the low-level Processor API or via the Kafka Streams DSL, which builds on top of the former.</p>
<p>A processor topology is merely a logical abstraction for your stream processing code. At runtime, the logical topology is instantiated and replicated inside the application for parallel processing (see Parallelism Model).</p>
<p>一个流处理应用程序可以定义一个或多个拓扑，尽管通常你只会定义一个。开发者可以通过低级的Processor API或者高级的DSL方式定义拓扑。一个拓扑仅仅是流处理代码的逻辑抽象，在运行时，逻辑拓扑会被实例化，并且在应用程序中进行复制以获得并行处理的能力。</p>
<h2 id="并行模型">并行模型</h2><h3 id="Stream_Partitions_and_Tasks">Stream Partitions and Tasks</h3><p>Kafka Streams uses the concepts of partitions and tasks as logical units of its parallelism model. There are close links between Kafka Streams and Kafka in the context of parallelism:</p>
<p>Kafka Streams使用分区和任务的概念作为它的并行模型的逻辑单元。Kafka Streams和Kafka在并行这个上下文上有紧密的联系：  </p>
<ul>
<li>Each stream partition is a totally ordered sequence of data records and maps to a Kafka topic partition.</li>
<li>A data record in the stream maps to a Kafka message from that topic.</li>
<li>The keys of data records determine the partitioning of data in both Kafka and Kafka Streams, i.e., how data is routed to specific partitions within topics.</li>
</ul>
<ol>
<li>每个分区流完全是一个有序的数据记录序列，映射到Kafka的主题分区</li>
<li>流中的一条数据记录，对应了Kafka主题中的一条消息</li>
<li>数据记录的Key决定了它在Kafka和Kafka Streams中的分区方式，比如数据怎么路由到主题的指定分区</li>
</ol>
<p>An application’s processor topology is scaled by breaking it into multiple tasks. More specifically, Kafka Streams creates a fixed number of tasks based on the input stream partitions for the application, with each task assigned a list of partitions from the input streams (i.e., Kafka topics). The assignment of partitions to tasks never changes so that each task is a fixed unit of parallelism of the application. Tasks can then instantiate their own processor topology based on the assigned partitions; they also maintain a buffer for each of its assigned partitions and process messages one-at-a-time from these record buffers. As a result stream tasks can be processed independently and in parallel without manual intervention.</p>
<blockquote>
<p>应用程序的处理拓扑会被分成多个任务来进行扩展。更具体来说，Kafka Streams会基于输入流的分区创建固定数量的任务，每个任务会从输入流（Kafka的主题）分配到多个分区。每个任务分配的分区永远不会改变，这样每个任务作为应用程序固定的并行单元。任务可以基于分配给它们的分区实例化它们自己的处理拓扑；它们也会为每个分配的分区维护一个缓冲区，并且从这些记录的缓冲区中一次只处理一条消息。这样的好处是所有的流任务都各自独立地并行处理，并且需要人工干预。</p>
</blockquote>
<p>Sub-topologies aka topology sub-graphs: If there are multiple processor topologies specified in a Kafka Streams application, each task will only instantiate one of the topologies for processing. In addition, a single processor topology may be decomposed(分离分解) into independent sub-topologies (sub-graphs) as long as sub-topologies are not connected by any streams in the topology; here, each task may instantiate only one such sub-topology for processing. This further scales out the computational workload to multiple tasks.</p>
<blockquote>
<p>子拓扑或者叫拓扑子图：如果在Kafka Streams应用程序中指定了多个处理拓扑，每个任务只会实例化其中的一个拓扑并处理。另外，一个拓扑也可能分成多个独立的子拓扑，只要子拓扑不和拓扑中的任何流存在连接。这里每个任务可能只会实例化一个子拓扑并处理。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161103181154902" alt="kstream task"></p>
<p>It is important to understand that Kafka Streams is not a resource manager, but a library that “runs” anywhere its stream processing application runs. Multiple instances of the application are executed either on the same machine, or spread across multiple machines and tasks can be distributed automatically by the library to those running application instances. The assignment of partitions to tasks never changes; if an application instance fails, all its assigned tasks will be restarted on other instances and continue to consume from the same stream partitions.</p>
<blockquote>
<p>注意Kafka Streams不是一个资源管理器，而是一个可以在和流处理应用程序一起运行在任何地方的客户端库。你可以在一台机器上运行应用程序的多个实例；或者分散在多台机器上，任务就会自动分布式地运行这些应用程序实例。注意分配给任务的分区永远不会改变（和Kafka消费者有点不同，消费者分配的分区是可以改变的）；如果一个应用程序的实例失败了，它的所有任务会在其他实例上重新启动，并且从相同的流分区继续消费。总结下：一个流处理应用程序实例（进程）有多个Task，每个Task分配多个固定的分区，如果进程挂了，其上的所有Task都会在其他进程上执行。而不会说把分区重新分配给剩下的Task。由于Task的分区固定，实际上Task的数量也是固定的，Task会分布式地在多个进程上执行。</p>
</blockquote>
<h3 id="Threading_Model">Threading Model</h3><p>Kafka Streams allows the user to configure the number of threads that the library can use to parallelize processing within an application instance. Each thread can execute one or more tasks with their processor topologies independently.</p>
<blockquote>
<p>Kafka Streams允许用户配置线程的数量，这样Kafka Streams库可以用来决定在一个应用程序实例中的处理并行粒度。每个线程可以执行一个或多个任务，它们的拓扑也都是独立的。</p>
</blockquote>
<p><img src="http://img.blog.csdn.net/20161103181203678" alt="kstream thread"></p>
<p>Starting more stream threads or more instances of the application merely amounts to replicating the topology and having it process a different subset of Kafka partitions, effectively parallelizing processing. It is worth noting that there is no shared state amongst the threads, so no inter-thread coordination is necessary. This makes it very simple to run topologies in parallel across the application instances and threads. The assignment of Kafka topic partitions amongst the various stream threads is transparently handled by Kafka Streams leveraging Kafka’s server-side coordination functionality.</p>
<blockquote>
<p>开启更多的流线程或者更多的应用程序实例仅仅相当于复制拓扑，并且处理不同的Kafka分区子集，有效地并行化处理。注意线程之间不会共享状态，所以不需要内部的线程进行协调。这使得在多个应用程序或者线程之间并行地运行拓扑变得非常简单。不同线程分配到的Kafka主题分区会被Kafka Streams透明地处理，利用的是Kafka服务端的协调者特性。</p>
</blockquote>
<p>As we described above, scaling your stream processing application with Kafka Streams is easy: you merely need to start additional instances of your application, and Kafka Streams takes care of distributing partitions amongst tasks that run in the application instances. You can start as many threads of the application as there are input Kafka topic partitions so that, across all running instances of an application, every thread (or rather, the tasks it runs) has at least one input partition to process.</p>
<blockquote>
<p>正如上面所描述的，使用Kafka Streams扩展你的流处理应用程序非常简单：你只需要为你的应用程序启动额外的实例，然后Kafka Streams就会自动帮你将分区分布在任务之间，任务会运行在应用程序实例中。你可以启动和Kafka的输入主题分区相同数量的应用程序线程，这样在一个应用程序的所有运行实例中，每个线程（更精确地说，是运行的任务）至少都会处理一个输入分区。</p>
</blockquote>
<h3 id="Example">Example</h3><p>To understand the parallelism model that Kafka Streams offers, let’s walk through an example.</p>
<p>Imagine a Kafka Streams application that consumes from two topics, A and B, with each having 3 partitions. If we now start the application on a single machine with the number of threads configured to 2, we end up with two stream threads instance1-thread1 and instance1-thread2. Kafka Streams will break this topology by default into three tasks because the maximum number of partitions across the input topics A and B is max(3, 3) == 3, and then distribute the six input topic partitions evenly across these three tasks; in this case, each task will consume from one partition of each input topic, for a total of two input partitions per task. Finally, these three tasks will be spread evenly – to the extent this is possible – across the two available threads, which in this example means that the first thread will run 2 tasks (consuming from 4 partitions) and the second thread will run 1 task (consuming from 2 partitions).</p>
<p>为了理解Kafka Streams提供的并行度模型，我们来看一个示例。假设有一个Kafka Streams应用程序会消费两个主题：A和B，每个主题都有3个分区。如果我们在一台机器上启动了一个应用程序，配置的线程数量为2，最终我们会有两个流线程：instance1-thread1和instance1-thread2。Kafka Streams会默认将拓扑分成三个任务，因为所有输入主题A和B的最大分区数是max(3,3)=3，然后会将6个输入分区平均分配到这三个任务上。这种情况下，每个任务都会消费每个输入主题的一个分区，即每个任务分配到了总共两个分区。最后，这三个任务会被均匀地分散到两个可用的线程中，这里因为有两个线程，这就意味着第一个线程会运行两个任务（消费了4个分区），第二个线程会运行一个任务（消费了2个分区）。</p>
<p>Now imagine we want to scale out this application later on, perhaps because the data volume has increased significantly. We decide to start running the same application but with only a single thread on another, different machine. A new thread instance2-thread1 will be created, and input partitions will be re-assigned similar to:</p>
<p>现在假设我们要扩展应用程序，可能是因为数据量增长的很明显。我们决定在其他机器上运行相同的应用程序，不过只配置了一个线程。那么一个新的线程instance2-thread1就会被创建，输入分区会被重新分配成下面右图那样。</p>
<p><img src="http://img.blog.csdn.net/20161103181630440" alt="kstream example"></p>
<p>When the re-assignment occurs, some partitions – and hence their corresponding tasks including any local state stores – will be “migrated” from the existing threads to the newly added threads (here: from instance1-thread1 on the first machine to instance2-thread1 on the second machine). As a result, Kafka Streams has effectively rebalanced the workload among instances of the application at the granularity of Kafka topic partitions.</p>
<blockquote>
<p>当重新分配发生时，一些分区，以及它们对应的任务，包括本地存储的状态，都会从已有的线程迁移到新添加的线程。比如这里第一台机器的instance1-thread1线程会迁移到第二台机器的instance2-thread1线程。最终，Kafka Streamsh会在所有应用程序实例中有效地平衡负载，而且是以Kafka主题分区的粒度进行负载均衡。</p>
</blockquote>
<p>What if we wanted to add even more instances of the same application? We can do so until a certain point, which is when the number of running instances is equal to the number of available input partitions to read from. At this point, before it would make sense to start further application instances, we would first need to increase the number of partitions for topics A and B; otherwise, we would over-provision the application, ending up with idle instances that are waiting for partitions to be assigned to them, which may never happen.</p>
<p>如果想要添加相同应用程序的更多实例呢？我们可以像上面那样做，直到运行实例的数量等于读取的可用分区数量（所有主题）。在这之后，如果想要启动更多的应用程序实例变得有意义，我们需要先为主题A和B增加分区；否则会存在空闲的应用程序实例，它们会等待有可用的分区分配给它们，但这<strong>可能</strong>永远都不会发生（虽然应用程序的实例比分区多，导致有些应用程序实例是空闲的，但是如果有应用程序挂掉了，那些空闲的应用程序就有可能分配到分区，而不再空闲。就像Kafka的消费者一样，如果消费者数量比分区数要多，空闲的消费者也会得不到分区，但如果有消费者挂掉了，空闲的消费者也是有机会得到分区的。不过我们无法保证空闲的应用程序实例或者消费者就一定有机会得到分区）。</p>
<h2 id="状态">状态</h2><p>Kafka Streams provides so-called state stores, which can be used by stream processing applications to store and query data, which is an important capability when implementing stateful operations. Every task in Kafka Streams embeds one or more state stores that can be accessed via APIs to store and query data required for processing. These state stores can either be a RocksDB database, an in-memory hash map, or another convenient data structure. Kafka Streams offers fault-tolerance and automatic recovery for local state stores.</p>
<p>Kafka Streams提供了所谓的状态存储，可以被流处理应用程序用来存储和查询数据，这在实现有状态的操作时是一个非常重要的功能。Kafka Streams中的每个任务内置了一个或多个状态存储，并且可以在流处理时通过API的方式存储或者查询状态存储中的数据。这些状态存储可以是RocksDB数据库、内存的hash map、或者其他的数据结构。Kafka Streams为本地状态存储提供了容错和自动恢复机制。</p>
<p><img src="http://img.blog.csdn.net/20161104091711617" alt="kstream state"></p>
<h2 id="容错">容错</h2><p>Kafka Streams builds on fault-tolerance capabilities integrated natively within Kafka. Kafka partitions are highly available and replicated; so when stream data is persisted to Kafka it is available even if the application fails and needs to re-process it. Tasks in Kafka Streams leverage the fault-tolerance capability offered by the Kafka consumer client to handle failures. If a task runs on a machine that fails, Kafka Streams automatically restarts the task in one of the remaining running instances of the application.</p>
<blockquote>
<p>Kafka Streams的容错能力基于原生的Kafka，Kafka的分区是高可用和复制的；所以当流数据持久化到Kafka中，即使应用程序失败了或者需要重新处理，数据也还是可用的。Kafka Streams的任务利用了Kafka消费者客户端提供的容错机制来处理故障。如果运行在一台机器上的一个任务失败了，Kafka Streams会在剩余的应用程序实例选择一个自动重启任务。</p>
</blockquote>
<p>In addition, Kafka Streams makes sure that the local state stores are robust to failures, too. It follows a similar approach as Apache Samza and, for each state store, maintains a replicated changelog Kafka topic in which it tracks any state updates. These changelog topics are partitioned as well so that each local state store instance, and hence the task accessing the store, has its own dedicated changelog topic partition. Log compaction is enabled on the changelog topics so that old data can be purged safely to prevent the topics from growing indefinitely. If tasks run on a machine that fails and are restarted on another machine, Kafka Streams guarantees to restore their associated state stores to the content before the failure by replaying the corresponding changelog topics prior to resuming the processing on the newly started tasks. As a result, failure handling is completely transparent to the end user.</p>
<blockquote>
<p>另外，Kafka Streams确保了本地状态的存储对于故障是鲁棒性的。它采用了和Apache Samza类似的方法，每个状态存储，都维护了具有复制的变更日志（Kafka主题），变更日志（changelog）会跟踪每次状态的更新。这些变更日志主题（change topic）会进行分区，每个本地状态存储的实例（local state store instance），都可以被任务获取，任务都有自己专属的变更日志分区（changelog topic partition）。在变更日志主题上会开启日志压缩，来安全地删除旧数据，防止旧数据无限膨胀。如果在一台机器上的任务运行失败，会在其他机器上重新启动，Kafka Streams可以保证恢复故障发生之前相关的状态存储。这是通过在新启动的任务上恢复处理之前，重放对应的变更日志主题来做到的。最终，故障处理对终端用户而言是透明的。</p>
</blockquote>
<p>Optimization: In order to minimize the time for the state restoration and hence the cost of task (re)initialization, users can configure their applications to have shadow copies of local states. When a task migration happens, Kafka Streams then attempts to assign a task to where a standby replica exists in order to minimize the task initialization cost. See setting num.standby.replicas at Optional configuration parameters in the Developer Guide.</p>
<blockquote>
<p>优化点：为了最小化恢复状态的时间以及任务重新初始化的代价，用户可以为应用程序配置一个本地状态的shadow副本。当一个任务迁移发生时，Kafka Streams会尝试将任务分配到备用副本所在的节点，以尽可能最小化任务初始化的代价。</p>
</blockquote>
<h2 id="流处理的保证">流处理的保证</h2><p>Kafka Streams currently supports at-least-once processing guarantees in the presence of failure. This means that if your stream processing application fails, no data records are lost and fail to be processed, but some data records may be re-read and therefore re-processed.</p>
<p>It depends on the specific use case whether at-least-once processing guarantees are acceptable or whether you may need exactly-once processing.</p>
<blockquote>
<p>Kafka Streams目前支持在错误场景下至少一次的处理语义。这意味着如果你的流处理应用程序失败了，数据不会丢失，也不会被漏掉处理，但是有些数据可能会被重复读取，并被重复处理。根据不同的用例，用户自己决定是否可以接受至少处理一次的保证，还是需要正好一次的处理。</p>
</blockquote>
<p>For many processing use cases, at-least-once processing turns out to be perfectly acceptable: Generally, as long as the effect of processing a data record is idempotent, it is safe for the same data record to be processed more than once. Also, some use cases can tolerate processing data records more than once even if the processing is not idempotent. For example, imagine you are counting hits by IP address to auto-generate blacklists that help with mitigating DDoS attacks against your infrastructure; here, some overcounting is tolerable because hits from malicious IP addresses involved(涉及) in an attack(攻击) will vastly(极大地) outnumber hits from benign(良性的) IP addresses anyway.</p>
<blockquote>
<p>对于很多处理场景，至少一次的处理被证明是可接受的：通常而言，只要处理一条记录的影响是幂等的，那么多次处理同一条记录就是安全的。同时，有些用例也允许容忍多次处理，即使处理的影响不是幂等的。比如，想象下你要根据IP地址计算命中次数，来生成帮你你与DDOS攻击的黑名单；这里，（重复处理导致）过高的计数也是允许的，因为来自恶意IP地址的计数参与的攻击相比良性的IP地址数量上会更多。</p>
</blockquote>
<p>In general however, for non-idempotent operations such as counting, at-least-once processing guarantees may yield incorrect results. If a Kafka Streams application fails and restarts, it may double-count some data records that were processed shortly before the failure. We are planning to address this limitation and will support stronger guarantees and exactly-once processing semantics in a future release of Kafka Streams.</p>
<blockquote>
<p>不过非幂等操作比如计数，在至少一次的处理语义下有可能得到错误的结果。如果流应用程序失败或重启，那么在错误发生前一小段时间内，相同的记录可能会被重复计数。我们正在考虑解决这种限制，并且尝试支持更强的消息处理保证。</p>
</blockquote>
<h2 id="流控">流控</h2><p>Kafka Streams regulates(控制) the progress of streams by the timestamps of data records by attempting to synchronize all source streams in terms of time. By default, Kafka Streams will provide your application with event-time processing semantics. This is important especially when an application is processing multiple streams (i.e., Kafka topics) with a large amount of historical data. For example, a user may want to re-process past data in case the business logic of an application was changed significantly, e.g. to fix a bug in an analytics algorithm. Now it is easy to retrieve a large amount of past data from Kafka; however, without proper flow control, the processing of the data across topic partitions may become out-of-sync and produce incorrect results.</p>
<blockquote>
<p>Kafka Streams通过数据记录的时间撮控制流的进度，它会尝试根据时间来同步所有数据源产生的流。默认Kafka Streams为应用程序提供事件时间的处理语义。对于应用程序处理多个具有大量历史数据的流这种场景是特别重要的。举例应用程序的业务逻辑变化很显著时，用户可能想要重新处理过去的数据，比如在一个分析型的算法中修复一个错误。现在，我们可以很容易地从Kafka中接收大量的历史数据，不过如果没有做恰当的流控，在Kafka主题分区之间的数据处理可能变得不同步，并且产生错误的结果。</p>
</blockquote>
<p>As mentioned in the Concepts section, each data record in Kafka Streams is associated with a timestamp. Based on the timestamps of the records in its stream record buffer, stream tasks determine the next assigned partition to process among all its input streams. However, Kafka Streams does not reorder records within a single stream for processing since reordering would break the delivery semantics of Kafka and make it difficult to recover in the face of failure. This flow control is of course best-effort(尽最大努力) because it is not always possible to strictly enforce execution order across streams by record timestamp; in fact, in order to enforce strict execution ordering, one must either wait until the system has received all the records from all streams (which may be quite infeasible in practice) or inject additional information about timestamp boundaries or heuristic estimates(启发式的预估) such as MillWheel’s watermarks.</p>
<blockquote>
<p>Kafka Streams中的每条数据记录都关联了一个时间撮。基于<strong>流记录缓冲区</strong>（stream record buffer）中每条记录的时间撮，流任务会在所有输入流中决定下一个需要处理的分配分区。不过Kafka Streams不会在处理一个单一的流时对记录重新排序，因为重新排序会破坏Kafka的消息传递语义，并且在故障发生时不容易恢复（消息的顺序）。这种流控当然会尽最大努力，因为在流中并不可能总是按照记录的时间撮严格限制执行的顺序。实际上，如果要实现严格的执行顺序，一个流要么需要等待，直到系统（流处理应用程序）从所有的流中接收到所有的记录（这在实际中是不可实行的），或者注入关于时间边界的额外信息，或者使用类似MillWheel的水位概念做一些启发式的预估。</p>
</blockquote>
<h2 id="背压">背压</h2><p>Kafka Streams does not use a backpressure mechanism because it does not need one. Using a depth-first processing strategy, each record consumed from Kafka will go through the whole processor (sub-)topology for processing and for (possibly) being written back to Kafka before the next record will be processed. As a result, no records are being buffered in-memory between two connected stream processors. Also, Kafka Streams leverages Kafka’s consumer client behind the scenes, which works with a pull-based messaging model that allows downstream processors to control the pace(步伐) at which incoming data records are being read.</p>
<blockquote>
<p>Kafka Streams不适用背压机制，因为它并不需要。使用深入优先的处理策略，从Kafka中消费的每条记录在处理时会流经整个处理拓扑，并且有可能会在处理下一条记录之前回写到Kafka。结果就是：在两个链接的流处理算子中不会有记录被缓存在内存中。同时Kafka Streams利用了Kafka中基于消息拉取模型的消费者客户端，允许下游处理算子控制读取的输入数据的消费速度。</p>
</blockquote>
<p>The same applies to the case of a processor topology that contains multiple independent sub-topologies, which will be processed independently from each other (cf. Parallelism Model). For example, the following code defines a topology with two independent sub-topologies:</p>
<blockquote>
<p>同样的方式也运用在包含多个独立子拓扑的处理拓扑，每个子拓扑都会各自独立地处理，比如下面的代码定义了一个拓扑，具有两个独立的子拓扑：</p>
</blockquote>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">strea<span class="title">m1.</span>to<span class="comment">("my-topic")</span>;</span><br><span class="line">strea<span class="title">m2</span> = builder.stream<span class="comment">("my-topic")</span>;</span><br></pre></td></tr></table></figure>
<p>Any data exchange between sub-topologies will happen through Kafka, i.e. there is no direct data exchange (in the example above, data would be exchanged through the topic “my-topic”). For this reason there is no need for a backpressure mechanism in this scenario, too.</p>
<p>在子拓扑中的任何数据交换都会经过Kafka，在上面的示例中，并没有直接的数据交换，而是通过”my-topic”进行数据交换。基于这些原因，这种场景下也不需要一个背压机制。</p>
<h1 id="开发者指南">开发者指南</h1><h2 id="Kafka_Streams配置">Kafka Streams配置</h2><p>Kafka Streams的配置通过一个StreamsConfig实例完成。其中下面三个是必须要有的配置项：</p>
<ol>
<li>application.id：流处理应用程序的编号，在Kafka集群中必须是唯一的</li>
<li>bootstrap.servers：建立和Kafka集群的初始连接</li>
<li>zookeeper.connect：管理Kafka主题的ZooKeeper</li>
</ol>
<p>每个流处理应用程序的编号必须是唯一的，相同的应用程序编号会给应用程序所有的实例，编号作为资源隔离的标识，用在下面几个地方</p>
<ol>
<li>作为默认的Kafka生产者、消费者的client.id前缀</li>
<li>作为Kafka消费者的group.id，会用来协调工作</li>
<li>作为状态目录（state.dir）的子目录名称</li>
<li>作为内部Kafka主题名称的前缀</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"></span><br><span class="line">Properties settings = <span class="keyword">new</span> Properties();</span><br><span class="line"><span class="comment">// Set a few key parameters</span></span><br><span class="line">settings.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"my-first-streams-application"</span>);</span><br><span class="line">settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"kafka-broker1:9092"</span>);</span><br><span class="line">settings.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, <span class="string">"zookeeper1:2181"</span>);</span><br><span class="line"><span class="comment">// Any further settings</span></span><br><span class="line">settings.put(... , ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create an instance of StreamsConfig from the Properties instance</span></span><br><span class="line">StreamsConfig config = <span class="keyword">new</span> StreamsConfig(settings);</span><br></pre></td></tr></table></figure>
<p><strong>Ser-/Deserialization (key.serde, value.serde)</strong>: Serialization and deserialization in Kafka Streams happens whenever data needs to be materialized, i.e.,:</p>
<ul>
<li>Whenever data is read from or written to a Kafka topic (e.g., via the KStreamBuilder#stream() and KStream#to() methods).</li>
<li>Whenever data is read from or written to a state store.</li>
</ul>
<p>当数据需要物化时，在Kafka Streams中会发生序列化和反序列化：  </p>
<ol>
<li>当从Kafka主题中读取数据，或者写入数据到Kafka主题（比如通过KStreamBuilder.stream()或者KStream.to()方法）</li>
<li>当从状态存储中读取数据，或者写入数据到状态存储</li>
</ol>
<p><strong>Number of Standby Replicas (num.standby.replicas)</strong>: This specifies the number of standby replicas. Standby replicas are shadow copies of local state stores. Kafka Streams attempts to create the specified number of replicas and keep them up to date as long as there are enough instances running. Standby replicas are used to minimize the latency of task failover. A task that was previously running on a failed instance is preferred(优先) to restart on an instance that has standby replicas so that the local state store restoration process from its changelog can be minimized. Details about how Kafka Streams makes use of the standby replicas to minimize the cost of resuming tasks on failover can be found in the State section.</p>
<p>指定备用副本的数量，备用副本是本地状态存储的shadow复制。Kafka Streams会尝试创建指定数量的副本，并且使这些副本一直保持最新的状态，只要有足够的实例在运行的话。备用副本会在任务发生故障切换时最小化延迟。运行在失败实例上的任务会优先在含有备用副本的实例上重启任务，这样可以最小化从变更日志中恢复本地的状态存储。</p>
<p><strong>Number of Stream Threads (num.stream.threads)</strong>: This specifies the number of stream threads in an instance of the Kafka Streams application. The stream processing code runs in these threads. Details about Kafka Streams threading model can be found in section Threading Model.</p>
<p>指定一个Kafka Streams应用程序实例的流线程数量。流处理代码运行在这些线程上。</p>
<p><strong>Replication Factor of Internal Topics (replication.factor)</strong>: This specifies the replication factor of internal topics that Kafka Streams creates when local states are used or a stream is repartitioned for aggregation. Replication is important for fault tolerance. Without replication even a single broker failure may prevent progress of the stream processing application. It is recommended to use a similar replication factor as source topics.</p>
<p>指定内部主题的副本因子，在使用本地状态或者流在聚合需要重新分区时，Kafka Streams会创建内部主题。副本对于故障容错非常重要。如果没有副本机制，即使一个Broker挂掉后，也会阻止流处理应用程序的正常进行。推荐设置为和源主题相同的副本因子。</p>
<p><strong>State Directory (state.dir)</strong>: Kafka Streams persists local states under the state directory. Each application has a subdirectory on its hosting machine, whose name is the application id, directly under the state directory. The state stores associated with the application are created under this subdirectory.</p>
<p>Kafka Streams会在状态目录下持久化本地状态。每个应用程序在它的物理机的状态目录下都有一个子目录，名称是应用程序的编号。和应用程序关联的状态存储都会在这个子目录下创建。</p>
<p><strong>Timestamp Extractor (timestamp.extractor)</strong>: A timestamp extractor extracts a timestamp from an instance of ConsumerRecord. Timestamps are used to control the progress of streams.</p>
<p>The default extractor is ConsumerRecordTimestampExtractor. This extractor retrieves built-in timestamps that are automatically embedded into Kafka messages by the Kafka producer client (introduced in Kafka 0.10.0.0, see KIP-32: Add timestamps to Kafka message). Depending on the setting of Kafka’s log.message.timestamp.type parameter, this extractor will provide you with:</p>
<ul>
<li>event-time processing semantics if log.message.timestamp.type is set to CreateTime aka “producer time” (which is the default). This represents the time when the Kafka producer sent the original message.</li>
<li>ingestion-time processing semantics if log.message.timestamp.type is set to LogAppendTime aka “broker time”. This represents the time when the Kafka broker received the original message.</li>
</ul>
<p>从一个ConsumerRecord实例解析出时间撮的解析器，时间撮会用来控制流的进度。默认的时间撮解析器是<code>ConsumerRecordTimestampExtractor</code>，这个解析器会获取被自动嵌入到Kafka消息中的内置时间撮（KIP-32：生产者产生消息时，会嵌入一个时间段到消息中）。根据<code>log.message.timestamp.type</code>的设置，有两种类型的解析器：  </p>
<ol>
<li>设置类型为<code>CreateTime</code>，即事件时间的处理语义。也是Producer的时间，作为默认值。表示Kafka生产者发送原始消息的时间点</li>
<li>设置类型为<code>LogAppendTime</code>，即摄取时间的处理语义。也是Broker的时间。表示Kafka Broker接收原始消息的时间点</li>
</ol>
<p>Another built-in extractor is WallclockTimestampExtractor. This extractor does not actually “extract” a timestamp from the consumed record but rather returns the current time in milliseconds from the system clock, which effectively means Streams will operate on the basis(基础) of the so-called processing-time of events.</p>
<p>You can also provide your own timestamp extractors, for instance to retrieve timestamps embedded in the payload of messages. Here is an example of a custom TimestampExtractor implementation:</p>
<p>另一个内置的解析器是<code>WallclockTimestampExtractor</code>，这个解析器并不会从消费记录中解析出一个时间撮，而是返回当前的系统时钟。你也可以提供自定义的时间撮解析器，比如从消息的内容（payload）中获取时间撮（通常是数据源自带的时间，而不是摄取时间），下面是一个自定义的TimestampExtractor实现类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.TimestampExtractor;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Extracts the embedded timestamp of a record (giving you "event time" semantics).</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyEventTimeExtractor</span> <span class="keyword">implements</span> <span class="title">TimestampExtractor</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extract</span><span class="params">(ConsumerRecord&lt;Object, Object&gt; record)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// `Foo` is your own custom class, which we assume has a method that returns</span></span><br><span class="line">    <span class="comment">// the embedded timestamp (in milliseconds).</span></span><br><span class="line">    Foo myPojo = (Foo) record.value();</span><br><span class="line">    <span class="keyword">if</span> (myPojo != <span class="keyword">null</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> myPojo.getTimestampInMillis();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Kafka allows `null` as message value.  How to handle such message values</span></span><br><span class="line">      <span class="comment">// depends on your use case.  In this example, we decide to fallback to</span></span><br><span class="line">      <span class="comment">// wall-clock time (= processing-time).</span></span><br><span class="line">      <span class="keyword">return</span> System.currentTimeMillis();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>You would then define the custom timestamp extractor in your Streams configuration as follows:<br>然后你需要在Streams配置中指定自定义的时间撮解析器（就像自定义序列化和反序列化器一样，都需要在配置文件中明确指定）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"></span><br><span class="line">Properties settings = <span class="keyword">new</span> Properties();</span><br><span class="line">settings.put(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MyEventTimeExtractor.class.getName());</span><br></pre></td></tr></table></figure>
<p><strong>Partition Grouper (partition.grouper)</strong>: A partition grouper is used to create a list of stream tasks given the partitions of source topics, where each created task is assigned with a group of source topic partitions. The default implementation provided by Kafka Streams is DefaultPartitionGrouper, which assigns each task with at most one partition for each of the source topic partitions; therefore, the generated number of tasks is equal to the largest number of partitions among the input topics. Usually an application does not need to customize the partition grouper.</p>
<p>分区分组用来在给定源主题的分区下创建流任务列表，每个创建的任务都会分配一组源主题的分区。默认的实现是<code>DefaultPartitionGrouper</code>，每个任务<strong>至多</strong>分配到每个源主题分区的一个分区。因此生成的任务数量等于输入主题中的最大分区数量（假设主题A有3个分区，主题B有4个分区，任务数量就等于max(3,4)=4）。通常应用程序不需要自定义分区分组方式。</p>
<p>Apart from Kafka Streams’ own configuration parameters (see previous sections) you can also specify parameters for the Kafka consumers and producers that are used internally, depending on the needs of your application. Similar to the Streams settings you define any such consumer and/or producer settings via StreamsConfig:</p>
<p>除了Kafka Streams自己的配置，你也可以根据你自己应用程序的需求设置内部的Kafka消费者和生产者的配置。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Properties streamsSettings = <span class="keyword">new</span> Properties();</span><br><span class="line"><span class="comment">// Example of a "normal" setting for Kafka Streams</span></span><br><span class="line">streamsSettings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"kafka-broker-01:9092"</span>);</span><br><span class="line"><span class="comment">// Customize the Kafka consumer settings of your Streams application</span></span><br><span class="line">streamsSettings.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, <span class="number">60000</span>);</span><br><span class="line">StreamsConfig config = <span class="keyword">new</span> StreamsConfig(streamsSettings);</span><br></pre></td></tr></table></figure>
<h2 id="编写一个流处理应用程序">编写一个流处理应用程序</h2><p>Any Java application that makes use of the Kafka Streams library is considered a Kafka Streams application. The computational logic of a Kafka Streams application is defined as a processor topology, which is a graph of stream processors (nodes) and streams (edges).</p>
<p>Currently Kafka Streams provides two sets of APIs to define the processor topology:</p>
<ul>
<li>A low-level Processor API that lets you add and connect processors as well as interact directly with state stores.</li>
<li>A high-level Kafka Streams DSL that provides common data transformation operations in a functional programming style such as map and filter operations. The DSL is the recommended starting point for developers new to Kafka Streams, and should cover many use cases and stream processing needs.</li>
</ul>
<p>任何使用Kafka Streams客户端库的Java应用程序被认为是一个Kafka Streams应用程序。Kafka Streams应用程序的计算逻辑被定义为一个处理拓扑，它是流处理算子和流的一张图。目前支持两种API定义处理拓扑：  </p>
<ol>
<li>低级Processor API：允许你添加和连接Processor，以及和状态存储直接交互</li>
<li>高级DSL：以函数式变成的风格提供通用的数据转换算子</li>
</ol>
<p>首先需要在pom.xml中定义Kafka Streams的依赖：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-streams<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.10.0.1<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>0.10.0.1<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Dependencies below are required/recommended only when using Apache Avro. --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>io.confluent<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>kafka-avro-serializer<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>3.0.1<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.avro<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>avro<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.avro<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>avro-maven-plugin<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>1.7.7<span class="tag">&lt;/<span class="title">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>你可以在应用程序代码的任何地方调用Kafka Streams库，通常是在main方法中。首先你需要创建一个KafkaStreams实例。KafkaStream构造器的第一个参数接收一个用来定义拓扑的builder（Kafka STreams DSL使用KStreamBuilder，Processor API使用TopologyBuilder）；第二个参数是StreamsConfig实例，定义了这个指定拓扑的配置。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.KafkaStreams;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KStreamBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.TopologyBuilder;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the builders to define the actual processing topology, e.g. to specify</span></span><br><span class="line"><span class="comment">// from which input topics to read, which stream operations (filter, map, etc.)</span></span><br><span class="line"><span class="comment">// should be called, and so on.</span></span><br><span class="line"></span><br><span class="line">KStreamBuilder builder = ...;  <span class="comment">// when using the Kafka Streams DSL</span></span><br><span class="line"><span class="comment">// OR</span></span><br><span class="line">TopologyBuilder builder = ...; <span class="comment">// when using the Processor API</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Use the configuration to tell your application where the Kafka cluster is,</span></span><br><span class="line"><span class="comment">// which serializers/deserializers to use by default, to specify security settings,</span></span><br><span class="line"><span class="comment">// and so on.</span></span><br><span class="line">StreamsConfig config = ...;</span><br><span class="line"></span><br><span class="line">KafkaStreams streams = <span class="keyword">new</span> KafkaStreams(builder, config);</span><br></pre></td></tr></table></figure>
<p>现在内部结果已经初始化完毕，不过处理工作还没有开始。你需要手动调用start()方法显示地启动Kafka Streams的线程：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Start the Kafka Streams threads</span></span><br><span class="line">streams.start();</span><br></pre></td></tr></table></figure>
<p>如果这个流处理应用程序还有其他实例运行在其他地方，Kafka Streams针对用户会透明地将任务从已经存在的实例<strong>重新分配</strong>到你刚刚启动的新实例上。为了捕获一些非预期的异常，你需要在启动应用程序之前设置一个<code>UncaughtExceptionHandler</code>异常处理器，这个处理器会在无论什么时候流处理线程因为非预期的异常而终结的时候被调用。</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">streams<span class="built_in">.</span>setUncaughtExceptionHandler(<span class="literal">new</span> <span class="keyword">Thread</span><span class="built_in">.</span>UncaughtExceptionHandler() &#123;</span><br><span class="line">    <span class="keyword">public</span> uncaughtException(<span class="keyword">Thread</span> t, throwable e) &#123;</span><br><span class="line">        <span class="comment">// here you should examine the exception and perform an appropriate action!</span></span><br><span class="line">    &#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>停止应用程序实例的方式是调用KafkaStreams的close()方法</p>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Stop the Kafka Streams threads</span></span><br><span class="line">streams.close<span class="comment">()</span>;</span><br></pre></td></tr></table></figure>
<p>为了保证你的应用程序在响应SIGTERM时优雅地退出，推荐在关闭钩子中调用KafkaStreams.close()方法，Java8中的关闭钩子使用方式如下：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add shutdown hook to stop the Kafka Streams threads</span></span><br><span class="line">Runtime<span class="built_in">.</span>getRuntime()<span class="built_in">.</span>addShutdownHook(<span class="literal">new</span> <span class="keyword">Thread</span>(streams<span class="tag">::close</span>));</span><br></pre></td></tr></table></figure>
<p>Java7中的而关闭钩子使用方式如下：</p>
<figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add shutdown hook</span></span><br><span class="line">Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="comment">// Stop the Kafka Streams threads</span></span><br><span class="line">      streams.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;));</span><br></pre></td></tr></table></figure>
<p>当一个特定的应用程序实例停止时，Kafka Streams会将运行在这个实例上的任何任务迁移到其他运行的实例上（假设还存在这样的应用程序实例）。下面我们会详细描述两种API的使用方式。</p>
<h3 id="Processor_API">Processor API</h3><p>As mentioned in the Concepts section, a stream processor is a node in the processor topology that represents a single processing step. With the Processor API users can define arbitrary stream processors that processes one received record at a time, and connect these processors with their associated state stores to compose the processor topology.</p>
<p>流处理算子是处理拓扑的一个节点，代表了一个单独的处理步骤。使用Processor API，用户可以定义任意的流处理算子，一次处理一条接收到的记录，并且将这些算子和它们对应的状态存储连接在一起，组成算子拓扑。用户可以实现Processor接口来定义定制的流算子，Processor接口有两个主要的方法：</p>
<p><strong>DEFINING A STREAM PROCESSOR</strong>  </p>
<p>Users can define their customized stream processor by implementing the Processor interface, which provides two main API methods: process() and punctuate().</p>
<ul>
<li>process() is called on each of the received record.</li>
<li>punctuate() is called periodically based on advanced record timestamps. For example, if processing-time is used as the record timestamp, then punctuate() will be triggered every specified period of time.</li>
</ul>
<ol>
<li>process()会在每个接收到的记录上调用</li>
<li>punctuate()会基于记录的时间撮被定时调用</li>
</ol>
<p>The Processor interface also has an init() method, which is called by the Kafka Streams library during task construction phase. Processor instances should perform any required initialization in this method. The init() method passes in a ProcessorContext instance, which provides access to the metadata of the currently processed record, including its source Kafka topic and partition, its corresponding message offset, and further such information . This context instance can also be used to schedule the punctuation period (via ProcessorContext#schedule()) for punctuate(), to forward a new record as a key-value pair to the downstream processors (via ProcessorContext#forward()), and to commit the current processing progress (via ProcessorContext#commit()).</p>
<p>Processor接口也有一个init()方法，它会在Kafka Streams库初始化任务的阶段调用。Processor实例需要在该方法上执行一些必须的初始化工作。init()方法传递了一个ProcessorContext实例，为当前处理的记录提供元数据的访问接口，包括输入源的Kafka主题和分区，对应的消息偏移量，以及更多的一些信息。这个上下文对象也可以被用来调度punctuation()方法的间隔（通过schedule方法），将新的记录作为键值对转发到下游的处理算子（通过forward方法），或者提交当前的处理进度（通过commit方法）。</p>
<p>下面的Processor实现类定义了一个简单的WordCount算法：  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountProcessor</span> <span class="keyword">implements</span> <span class="title">Processor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> ProcessorContext context;</span><br><span class="line">  <span class="keyword">private</span> KeyValueStore&lt;String, Long&gt; kvStore;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessorContext context)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// keep the processor context locally because we need it in punctuate() and commit()</span></span><br><span class="line">      <span class="keyword">this</span>.context = context;</span><br><span class="line">      <span class="comment">// call this processor's punctuate() method every 1000 time units.</span></span><br><span class="line">      <span class="keyword">this</span>.context.schedule(<span class="number">1000</span>);</span><br><span class="line">      <span class="comment">// retrieve the key-value store named "Counts"</span></span><br><span class="line">      kvStore = (KeyValueStore) context.getStateStore(<span class="string">"Counts"</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String dummy, String line)</span> </span>&#123;</span><br><span class="line">      String[] words = line.toLowerCase().split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">          Long oldValue = kvStore.get(word);</span><br><span class="line">          <span class="keyword">if</span> (oldValue == <span class="keyword">null</span>) &#123;</span><br><span class="line">              kvStore.put(word, <span class="number">1L</span>);</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              kvStore.put(word, oldValue + <span class="number">1L</span>);</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">punctuate</span><span class="params">(<span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">      KeyValueIterator&lt;String, Long&gt; iter = <span class="keyword">this</span>.kvStore.all();</span><br><span class="line">      <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">          KeyValue&lt;String, Long&gt; entry = iter.next();</span><br><span class="line">          context.forward(entry.key, entry.value.toString());</span><br><span class="line">      &#125;</span><br><span class="line">      iter.close();</span><br><span class="line">      <span class="comment">// commit the current processing progress</span></span><br><span class="line">      context.commit();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="comment">// close the key-value store</span></span><br><span class="line">      kvStore.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的实现中，执行了下列操作：</p>
<ul>
<li>In the init() method, schedule the punctuation every 1000 time units (the time unit is normally milliseconds, which in this example would translate to punctuation every 1 second) and retrieve the local state store by its name “Counts”.</li>
<li>In the process() method, upon each received record, split the value string into words, and update their counts into the state store (we will talk about this later in this section).</li>
<li>In the punctuate() method, iterate the local state store and send the aggregated counts to the downstream processor (we will talk about downstream processors later in this section), and commit the current stream state.</li>
</ul>
<ol>
<li>init()方法，调度punctuation每隔1000m个时间单元（这个时间单元通常是ms，这里表示每隔一秒调度一次punctuation），并且通过“Counts”名称获取本地的状态存储</li>
<li>process()方法，当接收到每条记录时，将字符串分成多个单词，并且将它们的次数更新到状态存储中</li>
<li>punctuate()方法，迭代本地状态存储，发送聚合次数给下游的处理算子，最后提交当前的流状态</li>
</ol>
<p><strong>DEFINING A STATE STORE 定义一个状态存储</strong>  </p>
<p>Note that the WordCountProcessor defined above can not only access the currently received record in the process() method, but also maintain processing states to keep recently arrived records for stateful processing needs such as aggregations and joins. To take advantage of these states, users can define a state store by implementing the StateStore interface (the Kafka Streams library also has a few extended interfaces such as KeyValueStore); in practice, though, users usually do not need to customize such a state store from scratch but can simply use the Stores factory to define a state store by specifying whether it should be persistent, log-backed, etc.</p>
<p>上面定义的WordCountProcessor不仅可以在process()方法中访问当前接收到的记录，也会维护处理状态，并保持最近到达的记录，可以被用于有状态的处理比如聚合和联合操作。为了利用这些状态的优势，用户可以实现自定义的StateStore接口。不过实际中用户通常不需要从头开始实现一个这样的状态存储，而只需要使用Stores的工厂类来定义一个状态存储。下面的示例中，定义了一个持久化的键值存储，名字叫做“Counts”，并且Key的类型是String，Value的类型是Long。</p>
<figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">StateStoreSupplier</span> countStore = <span class="type">Stores</span>.create(<span class="string">"Counts"</span>)</span><br><span class="line">              .withKeys(<span class="type">Serdes</span>.<span class="type">String</span><span class="literal">()</span>)</span><br><span class="line">              .withValues(<span class="type">Serdes</span>.<span class="type">Long</span><span class="literal">()</span>)</span><br><span class="line">              .persistent<span class="literal">()</span></span><br><span class="line">              .build<span class="literal">()</span>;</span><br></pre></td></tr></table></figure>
<p><strong>CONNECTING PROCESSORS AND STORES 连接处理算子和状态</strong>  </p>
<p>Now that we have defined the processor and the state stores, we can now construct the processor topology by connecting these processors and state stores together by using the TopologyBuilder instance. In addition, users can add source processors with the specified Kafka topics to generate input data streams into the topology, and sink processors with the specified Kafka topics to generate output data streams out of the topology.</p>
<p>现在我们已经定义了处理算子和状态存储，我们可以开始构造处理拓扑：通过使用TopologyBuilder的实例来连接这些处理算子以及状态存储。另外，用户可以添加输入源处理算子（source processors），并且指定特定的Kafka主题，这样就可以（读取Kafka主题）生成输入数据流进入到拓扑中；也可以指定目标处理算子（sink processors），也会指定特定的Kafka主题，这样就可以（写入Kafka主题）生成输出数据流到拓扑之外。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line"></span><br><span class="line"><span class="comment">// add the source processor node that takes Kafka topic "source-topic" as input</span></span><br><span class="line">builder.addSource(<span class="string">"Source"</span>, <span class="string">"source-topic"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add the WordCountProcessor node which takes the source processor as its upstream processor</span></span><br><span class="line">    .addProcessor(<span class="string">"Process"</span>, () -&gt; <span class="keyword">new</span> WordCountProcessor(), <span class="string">"Source"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create the countStore associated with the WordCountProcessor processor</span></span><br><span class="line">    .addStateStore(countStore, <span class="string">"Process"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add the sink processor node that takes Kafka topic "sink-topic" as output</span></span><br><span class="line">    <span class="comment">// and the WordCountProcessor node as its upstream processor</span></span><br><span class="line">    .addSink(<span class="string">"Sink"</span>, <span class="string">"sink-topic"</span>, <span class="string">"Process"</span>);</span><br></pre></td></tr></table></figure>
<p>There are several steps in the above implementation to build the topology, and here is a quick walk through:<br>上面的实现中构造一个拓扑有几个步骤：</p>
<ul>
<li>A source processor node named “Source” is added to the topology using the addSource method, with one Kafka topic “source-topic” fed to it.</li>
<li>A processor node named “Process” with the pre-defined WordCountProcessor logic is then added as the downstream processor of the “Source” node using the addProcessor method.</li>
<li>A predefined persistent key-value state store countStore is created and associated to the “Process” node.</li>
<li>A sink processor node is then added to complete the topology using the addSink method, taking the “Process” node as its upstream processor and writing to a separate “sink-topic” Kafka topic.</li>
</ul>
<ol>
<li>一个叫做“Source”的源算子被添加到拓扑中，使用了addSource方法，并且有一个Kafka的主题“source-topic”会（将数据）喂到这个源算子里</li>
<li>一个叫做“Process”的处理节点，使用了预定义的WordCountProcessor逻辑，然后通过addProcessor方法将其添加到（作为）“Source”节点的下游处理算子</li>
<li>创建了一个预定义的持久化键值存储，保存了countStore，并且关联了“Process”节点</li>
<li>使用addSink方法添加了一个目标处理节点，构成了一个完整的拓扑。将“Process”节点作为它的上游处理算子，并且写到一个Kafka的输出主题</li>
</ol>
<p>In this defined topology, the “Process” stream processor node is considered a downstream processor of the “Source” node, and an upstream processor of the “Sink” node. As a result, whenever the “Source” node forward a newly fetched record from Kafka to its downstream “Process” node, WordCountProcessor#process() method is triggered to process the record and update the associated state store; and whenever context#forward() is called in the WordCountProcessor#punctuate() method, the aggregate key-value pair will be sent via the “Sink” processor node to the Kafka topic “sink-topic”. Note that in the WordCountProcessor implementation, users need to refer with the same store name “Counts” when accessing the key-value store; otherwise an exception will be thrown at runtime indicating that the state store cannot be found; also if the state store itself is not associated with the processor in the TopologyBuilder code, accessing it in the processor’s init() method will also throw an exception at runtime indicating the state store is not accessible from this processor.</p>
<p>在上面定义的拓扑中，“Process”流处理节点被认为是“Source”节点的下游处理算子，也是“Sink”节点的上游处理算子。结果就是：无论什么时候，当”Source“节点从Kafka拉取一条新的记录，并转发给下游的”Process“节点，WordCountProcessor#process()方法就会被触发，并且会处理这条记录，以及更新相应的状态存储；并且无论什么时候当在WordCountProcessor#punctuate()方法中调用context#forward()时，聚合的键值对会通过”Sink“处理节点发送到Kafka的输出主题“sink-topic”中。注意在WordCountProcessor的实现中，用户在获取键值存储时需要参考这里（构造拓扑）相同的状态存储名称“Counts”（即拓扑定义的Counts键值存储名称，在WordCountProcessor中为了获取这个键值存储，两者的名称必须是一致的），否则在运行时会抛出一个异常说状态存储未找到。如果状态存储本身没有和TopologyBuilder代码中的处理节点（Processor节点，只有WordCountProcessor这一个节点）相关联的话，在Processor的init方法中访问状态存储也会抛出运行时的异常（状态存储不能在当前Processor节点访问）。</p>
<p>With the defined processor topology, users can now start running a Kafka Streams application instance. Please read how to run a Kafka Streams application for details.</p>
<p>有了定义好的处理拓扑，用户现在就可以启动一个Kafka Streams应用程序实例了。</p>
<h3 id="Kafka_Streams_DSL">Kafka Streams DSL</h3><p>As mentioned in the Concepts section, a stream is an unbounded, continuously updating data set. With the Kafka Streams DSL users can define the processor topology by concatenating multiple transformation operations where each operation transforming one stream into other stream(s); the resulted topology then takes the input streams from source Kafka topics and generates the final output streams throughout its concatenated transformations. However, different streams may have different semantics in their data records:</p>
<p>一个流是一个无界的，持续更新的数据集。使用Kakfa Streams的DSL，用户可以通过连接多个转换操作的方式来定义处理拓扑，其中每个操作会从一个流转换到新的其他流。最终拓扑会将从Kafka源读取的主题作为输入流，并且在贯穿连接的转换操作中，生成最终的输出流。不过不同的流在它们的数据记录中，可能有不同的语义：</p>
<ul>
<li>In some streams, each record represents a new immutable datum in their unbounded data set; we call these record streams.</li>
<li>In other streams, each record represents a revision (or update) of their unbounded data set in chronological(按时间顺序) order; we call these changelog streams.</li>
</ul>
<ol>
<li>在一些流中，每条记录代表了无界数据集中新的不可变数据，我们叫做记录流（record streams）</li>
<li>在其他流中，每条记录代表了无界数据集中按照时间顺序排序的更新，我们叫做变更日志流（changelog streams）</li>
</ol>
<p>这两种类型的流都可以存储成Kafka的主题。不过，它们的计算语义则截然不同。举例有一个聚合操作，为指定的key计算记录的次数。对于记录流（record streams）而言，每条记录都是来自于Kafka主题中带有key的消息（比如一个页面浏览的数据流会以用户的编号作为key）：</p>
<p>Both of these two types of streams can be stored as Kafka topics. However, their computational semantics can be quite different. Take the example of an aggregation operation that counts the number of records for the given key. For record streams, each record is a keyed message from a Kafka topic (e.g., a page view stream keyed by user ids):</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># Example: a record stream for page view events</span></span><br><span class="line"><span class="preprocessor"># Notation is &lt;record key&gt; =&gt; &lt;record value&gt;</span></span><br><span class="line"><span class="number">1</span> =&gt; &#123;<span class="string">"time"</span>:<span class="number">1440557383335</span>, <span class="string">"user_id"</span>:<span class="number">1</span>, <span class="string">"url"</span>:<span class="string">"/home?user=1"</span>&#125;</span><br><span class="line"><span class="number">5</span> =&gt; &#123;<span class="string">"time"</span>:<span class="number">1440557383345</span>, <span class="string">"user_id"</span>:<span class="number">5</span>, <span class="string">"url"</span>:<span class="string">"/home?user=5"</span>&#125;</span><br><span class="line"><span class="number">2</span> =&gt; &#123;<span class="string">"time"</span>:<span class="number">1440557383456</span>, <span class="string">"user_id"</span>:<span class="number">2</span>, <span class="string">"url"</span>:<span class="string">"/profile?user=2"</span>&#125;</span><br><span class="line"><span class="number">1</span> =&gt; &#123;<span class="string">"time"</span>:<span class="number">1440557385365</span>, <span class="string">"user_id"</span>:<span class="number">1</span>, <span class="string">"url"</span>:<span class="string">"/profile?user=1"</span>&#125;</span><br></pre></td></tr></table></figure>
<p>The counting operation for record streams is trivial to implement: you can maintain a local state store that tracks the latest count for each key, and, upon receiving a new record, update the corresponding key by incrementing its count by one.</p>
<p>对记录流进行计数操作非常容易实现：你可以维护一个本地的状态存储，用来跟踪每个key的最近的次数。并且，当接收到一条新的记录时，更新对应key的值，把它的次数加1。</p>
<p>For changelog streams, on the other hand, each record is an update of the unbounded data set (e.g., a changelog stream for a user profile table, where the user id serves as both the primary key for the table and as the record key for the stream; here, a new record represents a changed row of the table). In practice you would usually store such streams in Kafka topics where log compaction is enabled.</p>
<p>对于变更日志流而言，每条记录都是对于无界数据集的一次更新（比如一个变更日志流是针对用户的个人信息表，用户的编号既作为表的主键，也会作为日志流记录的key，这里一条新记录表示的是表的一行更新）。实际应用中在Kafka的主题存储这样的路，应该开启日志压缩。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="preprocessor"># Example: a changelog stream for a user profile table</span></span><br><span class="line"><span class="number">1</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557383335</span>, <span class="string">"user_id"</span>:<span class="number">1</span>, <span class="string">"email"</span>:<span class="string">"user1@aol.com"</span>&#125;</span><br><span class="line"><span class="number">5</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557383345</span>, <span class="string">"user_id"</span>:<span class="number">5</span>, <span class="string">"email"</span>:<span class="string">"user5@gmail.com"</span>&#125;</span><br><span class="line"><span class="number">2</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557383456</span>, <span class="string">"user_id"</span>:<span class="number">2</span>, <span class="string">"email"</span>:<span class="string">"user2@yahoo.com"</span>&#125;</span><br><span class="line"><span class="number">1</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557385365</span>, <span class="string">"user_id"</span>:<span class="number">1</span>, <span class="string">"email"</span>:<span class="string">"user1-new-email-addr@comcast.com"</span>&#125;</span><br><span class="line"><span class="number">2</span> =&gt; &#123;<span class="string">"last_modified_time"</span>:<span class="number">1440557385395</span>, <span class="string">"user_id"</span>:<span class="number">2</span>, <span class="string">"email"</span>:null&#125;  &lt;-- user has been deleted</span><br></pre></td></tr></table></figure>
<p>As a result the counting operation for changelog streams is no longer monotonically incrementing: you need to also decrement the counts when a delete update record is received on some given key as well. In addition, even for counting aggregations on an record stream, the resulting aggregate is no longer an record stream but a relation / table, which can then be represented as a changelog stream of updates on the table.</p>
<p>对变更日志流的计数操作不再是单调递增的了：当接收到指定key的一个删除更新记录时，你需要减少计数。另外，即使是在记录流上做聚合计数，聚合的结果也不是一个记录流，而是一张表，它代表的是在表上的变更日志流的更新。</p>
<p>The counting operation of a user profile changelog stream is peculiar(罕见的) because it will generate, for a given key, a count of either 0 (meaning the key does not exist or does not exist anymore) or 1 (the key exists) only. Multiple records for the same key are considered duplicate, old information of the most recent record, and thus will not contribute to the count.</p>
<p>For changelog streams developers usually prefer counting a non-primary-key field. We use the example above just for the sake of illustration.</p>
<p>对用户的个人信息变更日志流做计数操作是比较少见的，因为它为指定的key生成的计数要么是0（表示key不存在），要么是1（key存在）。相同key的多个记录被认为是重复的、相对当前最近记录而言是旧的信息，因此不会对计数结果产生影响。对于变更日志流，开发者通常会在非主键字段上计数，上面的示例仅仅是为了演示（有主键）。</p>
<p>One of the key design principles of the Kafka Streams DSL is to understand and distinguish between record streams and changelog streams and to provide operators with the correct semantics for these two different types of streams. More concretely, in the Kafka Streams DSL we use the KStream interface to represent record streams, and we use a separate KTable interface to represent changelog streams. The Kafka Streams DSL is therefore the recommended API to implement a Kafka Streams application. Compared to the lower-level Processor API, its benefits are:</p>
<p>Kafka Streams DSL的一个主要设计原则是理解和区分记录流合变更日志流，并且为这两种类型流的操作提供正确的语义。更具体地来说，在Kafka Streams DSL中，我们使用KStream接口来表示记录流，使用KTable接口代表变更日志流。所以Kafka Streams DSL是用来在实现Kafka Streams应用程序时推荐的API，和低级Processor API比较，它有几个优点：</p>
<ul>
<li>More concise and expressive code, particularly when using Java 8+ with lambda expressions.</li>
<li>Easier to implement stateful transformations such as joins and aggregations.</li>
<li>Understands the semantic differences of record streams and changelog streams, so that transformations such as aggregations work as expected depending on which type of stream they operate against.</li>
</ul>
<ol>
<li>更简洁、更具有表达力的代码，尤其是使用Java8的lambda表达式时</li>
<li>可以很容易地实现一个有状态的操作，比如联合和聚合操作</li>
<li>理解记录流和变更日志流的语义区别，这样转换操作（比如聚合）根据流的类型按照预期的方式工作</li>
</ol>
<h4 id="CREATING_SOURCE_STREAMS_FROM_KAFKA">CREATING SOURCE STREAMS FROM KAFKA</h4><p>Both KStream or KTable objects can be created as a source stream from one or more Kafka topics via KStreamBuilder, an extended class of TopologyBuilder used in the lower-level Processor API (for KTable you can only create the source stream from a single topic).</p>
<p>KStream和KTable对象都可以通过KStreamBuilder创建，作为从一个或多个Kafka主题的输入源流。KStreamBuilder是TopologyBuilder（低级的Processor API）的继承类。对于KTable，你只能从一个Kafka主题中创建一个输入源流。</p>
<table>
<thead>
<tr>
<th>Interface</th>
<th>How to instantiate</th>
</tr>
</thead>
<tbody>
<tr>
<td>KStream<k, v=""></k,></td>
<td>KStreamBuilder#stream(…)</td>
</tr>
<tr>
<td>KTable<k, v=""></k,></td>
<td>KStreamBuilder#table(…)</td>
</tr>
</tbody>
</table>
<p>When creating an instance, you may override the default serdes for record keys (K) and record values (V) used for reading the data from Kafka topics (see Data types and serdes for more details); otherwise the default serdes specified through StreamsConfig will be used.</p>
<p>当创建一个（KStream或KTable）实例时，需要指定输入源的Kafka主题，你可能需要指定读取记录的序列化器，如果不指定时，会使用StreamsConfig中指定的序列化器。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KStreamBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KTable;</span><br><span class="line"></span><br><span class="line">KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line"></span><br><span class="line"><span class="comment">// In this example we assume that the default serdes for keys and values are</span></span><br><span class="line"><span class="comment">// the String serde and the generic Avro serde, respectively.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a stream of page view events from the PageViews topic, where the key of</span></span><br><span class="line"><span class="comment">// a record is assumed to be the user id (String) and the value an Avro GenericRecord</span></span><br><span class="line"><span class="comment">// that represents the full details of the page view event.</span></span><br><span class="line">KStream&lt;String, GenericRecord&gt; pageViews = builder.stream(<span class="string">"PageViews"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a changelog stream for user profiles from the UserProfiles topic,</span></span><br><span class="line"><span class="comment">// where the key of a record is assumed to be the user id (String) and its value</span></span><br><span class="line"><span class="comment">// an Avro GenericRecord.</span></span><br><span class="line">KTable&lt;String, GenericRecord&gt; userProfiles = builder.table(<span class="string">"UserProfiles"</span>);</span><br></pre></td></tr></table></figure>
<h4 id="TRANSFORM_A_STREAM">TRANSFORM A STREAM</h4><p>KStream and KTable support a variety of transformation operations. Each of these operations can be translated into one or more connected processors into the underlying processor topology. Since KStream and KTable are strongly typed, all these transformation operations are defined as generics functions where users could specify the input and output data types.</p>
<p>KStream和KTable支持很多类型的转换操作。每种操作都可以翻译成底层处理拓扑的一个或多个相连起来的处理算子。由于KStream和KTable是强类型的，所有这些转换操作被定位为通用的函数，这样用户可以指定输入和输出的数据类型。</p>
<p>Some KStream transformations may generate one or more KStream objects (e.g., filter and map on KStream generate another KStream, while branch on KStream can generate multiple KStream) while some others may generate a KTable object (e.g., aggregation) interpreted(理解) as the changelog stream to the resulted relation. This allows Kafka Streams to continuously update the computed value upon arrivals of late records after it has already been produced to the downstream transformation operators. As for KTable, all its transformation operations can only generate another KTable (though the Kafka Streams DSL does provide a special function to convert a KTable representation into a KStream, which we will describe later). Nevertheless, all these transformation methods can be chained together to compose a complex processor topology.</p>
<p>有些KStream转换可能产生不止一个KStream对象（在KStream上进行过滤和映射会生成新的KStream，而在KStream上进行分支则会产生多个KStream），而有些可能产生一个KTable对象（比如聚合转换）。这就允许Kafka Streams即使在记录已经发送给下游的转换算子的情况下，在迟到的记录到来时，也可以持续地更新已经计算过的值。对于KTable而言，它的所有转换操作只会生成新的KTable（尽管Kafka Streams DSL提供额外的函数可以将一个KTable转换成KStream）。不仅如此，所有这些转换操作都可以被链接在一起，从而构造出一个复杂的处理拓扑。</p>
<p>We describe these transformation operations in the following subsections, categorizing them as two categories: stateless and stateful transformations.</p>
<p>下面我们会将这些转换操作分成两类：无状态的和有状态的转换。</p>
<h4 id="STATELESS_TRANSFORMATIONS">STATELESS TRANSFORMATIONS</h4><p>Stateless transformations include filter, filterNot, foreach, map, mapValues, selectKey, flatMap, flatMapValues, branch. Most of them can be applied to both KStream and KTable, where users usually pass a customized function to these functions as a parameter; e.g. a Predicate for filter, a KeyValueMapper for map, and so on. Stateless transformations, by definition, do not depend on any state for processing, and hence implementation-wise they do not require a state store associated with the stream processor.</p>
<p>无状态的转换包括：<code>filter, filterNot, foreach, map, mapValues, selectKey, flatMap, flatMapValues, branch</code>。它们中的大部分都可以被同时用在KStream和KTable上，用户通常需要为这些函数传递一个自定义的函数作为参数。比如对于filter算子，需要传递一个Predicate，对于map算子需要传递一个KeyValueMapper等等。无状态的操作，从定义上来说，不依赖于处理的任何状态，因此它们并不需要和流处理算子相关联的状态存储。</p>
<p>下面是mapValues分别使用Java8的lambda，以及Java7的示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Java8</span></span><br><span class="line">KStream&lt;Long, String&gt; uppercased =</span><br><span class="line">    nicknameByUserId.mapValues(nickname -&gt; nickname.toUpperCase());</span><br><span class="line"></span><br><span class="line"><span class="comment">//Java7</span></span><br><span class="line">KStream&lt;Long, String&gt; uppercased =</span><br><span class="line">    nicknameByUserId.mapValues(</span><br><span class="line">        <span class="keyword">new</span> ValueMapper&lt;String&gt;() &#123;</span><br><span class="line">            <span class="annotation">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">apply</span><span class="params">(String nickname)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> nickname.toUpperCase();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    );</span><br></pre></td></tr></table></figure>
<p>The function is applied to each record, and its result will trigger the creation a new record.  </p>
<p>这个函数会作用于每条记录，它的结果会创建一个新的记录。</p>
<h4 id="STATEFUL_TRANSFORMATIONS">STATEFUL TRANSFORMATIONS</h4><p>有状态的操作包括如下几个： </p>
<ul>
<li>joins (KStream/KTable): join, leftJoin, outerJoin</li>
<li>aggregations (KStream): countByKey, reduceByKey, aggregateByKey</li>
<li>aggregations (KTable): groupBy plus count, reduce, aggregate (via KGroupedTable)</li>
<li>general transformations (KStream): process, transform, transformValues</li>
</ul>
<p>Stateful transformations are transformations where the processing logic requires accessing an associated state for processing and producing outputs. For example, in join and aggregation operations, a windowing state is usually used to store all the records received so far within the defined window boundary. The operators can then access accumulated records in the store and compute based on them (see Windowing a Stream for details).</p>
<p>有状态的操作指的是转换的处理逻辑需要访问相关的状态，来做处理操作和产生结果。举例联合和聚合操作，会使用一个窗口状态，存储定义的窗口边界内接收到的所有记录。操作算子就可以从存储中访问收集到的记录，基于这些记录做计算。使用Java8的WordCoun示例：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// We assume message values represent lines of text.  For the sake of this example, we ignore</span></span><br><span class="line"><span class="comment">// whatever may be stored in the message keys.</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; textLines = <span class="attribute">...</span>;</span><br><span class="line"></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, Long&gt; wordCounts = textLines</span><br><span class="line">    <span class="comment">// Split each text line, by whitespace, into words.  The text lines are the message</span></span><br><span class="line">    <span class="comment">// values, i.e. we can ignore whatever data is in the message keys and thus invoke</span></span><br><span class="line">    <span class="comment">// `flatMapValues` instead of the more generic `flatMap`.</span></span><br><span class="line">    <span class="built_in">.</span>flatMapValues(value <span class="subst">-&gt; </span>Arrays<span class="built_in">.</span>asList(value<span class="built_in">.</span>toLowerCase()<span class="built_in">.</span>split(<span class="string">"\\W+"</span>)))</span><br><span class="line">    <span class="comment">// We will subsequently invoke `countByKey` to count the occurrences of words, so we use</span></span><br><span class="line">    <span class="comment">// `map` to ensure the key of each record contains the respective word.</span></span><br><span class="line">    <span class="built_in">.</span><span class="built_in">map</span>((key, word) <span class="subst">-&gt; </span><span class="literal">new</span> KeyValue&lt;&gt;(word, word))</span><br><span class="line">    <span class="comment">// Count the occurrences of each word (record key).</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// This will change the stream type from `KStream&lt;String, String&gt;` to</span></span><br><span class="line">    <span class="comment">// `KTable&lt;String, Long&gt;` (word -&gt; count).  We must provide a name for</span></span><br><span class="line">    <span class="comment">// the resulting KTable, which will be used to name e.g. its associated</span></span><br><span class="line">    <span class="comment">// state store and changelog topic.</span></span><br><span class="line">    <span class="built_in">.</span>countByKey(<span class="string">"Counts"</span>)</span><br><span class="line">    <span class="comment">// Convert the `KTable&lt;String, Long&gt;` into a `KStream&lt;String, Long&gt;`.</span></span><br><span class="line">    <span class="built_in">.</span>toStream();</span><br></pre></td></tr></table></figure>
<p>WordCount使用Java 7:</p>
<figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Code below is equivalent to the previous Java 8+ example above.</span></span><br><span class="line">KStream&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; textLines = ...;</span><br><span class="line"></span><br><span class="line">KStream&lt;<span class="keyword">String</span>, Long&gt; wordCounts = textLines</span><br><span class="line">    .flatMapValues(<span class="keyword">new</span> ValueMapper&lt;<span class="keyword">String</span>, Iterable&lt;<span class="keyword">String</span>&gt;&gt;() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        <span class="keyword">public</span> Iterable&lt;<span class="keyword">String</span>&gt; apply(<span class="keyword">String</span> value) &#123;</span><br><span class="line">            <span class="keyword">return</span> Arrays.asList(value.toLowerCase().<span class="built_in">split</span>(<span class="string">"\\W+"</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .<span class="built_in">map</span>(<span class="keyword">new</span> KeyValueMapper&lt;<span class="keyword">String</span>, <span class="keyword">String</span>, KeyValue&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;&gt;() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        <span class="keyword">public</span> KeyValue&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt; apply(<span class="keyword">String</span> <span class="variable">key</span>, <span class="keyword">String</span> word) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> KeyValue&lt;<span class="keyword">String</span>, <span class="keyword">String</span>&gt;(word, word);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    .countByKey(<span class="string">"Counts"</span>)</span><br><span class="line">    .toStream();</span><br></pre></td></tr></table></figure>
<h4 id="WINDOWING_A_STREAM">WINDOWING A STREAM</h4><p>Windowing is a common prerequisite for stateful transformations which group records in a stream, for example, by their timestamps. A local state store is usually needed for a windowing operation to store recently received records based on the window interval, while old records in the store are purged after the specified window retention period. The retention time can be set via Windows#until().</p>
<p>窗口是有状态转换操作的基本条件，它会在流中对记录进行分组，比如根据时间撮的方式进行分组。一个窗口相关的操作通常需要一个本地状态存储，来保存最近接收到的记录，这些记录是基于窗口间隔，状态存储中旧的记录会在指定的窗口保留时间过去后会被删除。保留时间是通过Windows#until()设置的。Kafka Streams目前支持以下类型的窗口：</p>
<table>
<thead>
<tr>
<th>Window name</th>
<th>Behavior</th>
<th>Short description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tumbling time window（翻转窗口）</td>
<td>Time-based</td>
<td>Fixed-size, non-overlapping, gap-less windows</td>
</tr>
<tr>
<td>Hopping time window（跳跃时间窗口）</td>
<td>Time-based</td>
<td>Fixed-size, overlapping windows</td>
</tr>
<tr>
<td>Sliding time window（滑动时间窗口）</td>
<td>Time-based</td>
<td>Fixed-size, overlapping windows that work on differences between record timestamps</td>
</tr>
</tbody>
</table>
<p><strong>Tumbling time windows</strong> are a special case of hopping time windows and, like the latter, are windows based on time intervals. They model fixed-size, non-overlapping, gap-less windows. A tumbling window is defined by a single property: the window’s size. A tumbling window is a hopping window whose window size is equal to its advance interval. Since tumbling windows never overlap, a data record will belong to one and only one window.</p>
<p>翻转窗口是跳跃窗口的一个特例，和后者一样，所有的窗口都是基于时间间隔。窗口的大小是固定的，窗口之间不会重复，也没有间隙。一个翻转窗口之定义了窗口大小这个简单的属性，默认它的窗口大小和前进间隔相等。由于翻转窗口不会有重叠，所以一条记录指挥属于一个窗口。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A tumbling time window with a size 60 seconds (and, by definition, an implicit</span></span><br><span class="line"><span class="comment">// advance interval of 60 seconds).</span></span><br><span class="line"><span class="comment">// The window's name -- the string parameter -- is used to e.g. name the backing state store.</span></span><br><span class="line"><span class="keyword">long</span> windowSizeMs = <span class="number">60</span> * <span class="number">1000L</span>;</span><br><span class="line">TimeWindows.of(<span class="string">"tumbling-window-example"</span>, windowSizeMs);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The above is equivalent to the following code:</span></span><br><span class="line">TimeWindows.of(<span class="string">"tumbling-window-example"</span>, windowSizeMs).advanceBy(windowSizeMs);</span><br></pre></td></tr></table></figure>
<p><img src="https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/02/52/ASAQL-1.png" alt=""></p>
<p><strong>Hopping time windows</strong> are windows based on time intervals. They model fixed-sized, (possibly) overlapping windows. A hopping window is defined by two properties: the window’s size and its advance interval (aka “hop”). The advance interval specifies by how much a window moves forward relative to the previous one. For example, you can configure a hopping window with a size 5 minutes and an advance interval of 1 minute. Since hopping windows can overlap – and in general they do – a data record may belong to more than one such windows.</p>
<p>跳跃时间窗口是基于时间间隔的窗口，固定大小，但是窗口之间可能有重叠。跳跃窗口定义了两个属性：窗口大小和跳跃间隔（hop的中文意思是跳跃）。跳跃间隔指定了一个窗口相对于前一个窗口的移动大小。比如你可以配置一个跳跃窗口的大小为5分钟，跳跃间隔为1分钟。由于跳跃窗口允许重叠，所以一条记录可能属于不止一个窗口。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A hopping time window with a size of 5 minutes and an advance interval of 1 minute.</span></span><br><span class="line"><span class="comment">// The window's name -- the string parameter -- is used to e.g. name the backing state store.</span></span><br><span class="line"><span class="keyword">long</span> windowSizeMs = <span class="number">5</span> * <span class="number">60</span> * <span class="number">1000L</span>;</span><br><span class="line"><span class="keyword">long</span> advanceMs =    <span class="number">1</span> * <span class="number">60</span> * <span class="number">1000L</span>;</span><br><span class="line">TimeWindows.of(<span class="string">"hopping-window-example"</span>, windowSizeMs).advanceBy(advanceMs);</span><br></pre></td></tr></table></figure>
<p><img src="https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/02/52/ASAQL-2.png" alt=""></p>
<p>Pay attention, that tumbling and hopping time windows are aligned to the epoch and that the lower window time interval bound is inclusive, while the upper bound is exclusive.</p>
<p>Aligned to the epoch means, that the first window starts at timestamp zero. For example, hopping windows with size of 5000ms and advance of 3000ms, have window boundaries [0;5000),[3000;8000),…— and not [1000;6000),[4000;9000),… or even something “random” like [1452;6452),[4452;9452),…, which might be the case if windows get initialized depending on system/application start-up time, introducing non-determinism.</p>
<p>注意，翻转窗口和跳跃窗口是和时间点对齐的，所以窗口时间的下界（lower bound）被包含，而下界（upper bound）不被包含。和时间点对齐表示，第一个窗口从时间点为0开始。比如跳跃窗口的大小=5000ms，跳跃间隔=3000ms，对应的窗口边界分别是<code>[0;5000),[3000;8000),...</code>，而不是<code>[1000;6000),[4000;9000),...</code>，也不是<code>[1452;6452),[4452;9452),...</code>。窗口计数的示例如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">KStream&lt;String, GenericRecord&gt; viewsByUser = ...;</span><br><span class="line"></span><br><span class="line">KTable&lt;Windowed&lt;String&gt;, Long&gt; userCounts =</span><br><span class="line">    <span class="comment">// count users, using hopping windows of size 5 minutes that advance every 1 minute</span></span><br><span class="line">    viewsByUser.countByKey(TimeWindows.of(<span class="string">"GeoPageViewsWindow"</span>, <span class="number">5</span> * <span class="number">60</span> * <span class="number">1000L</span>).advanceBy(<span class="number">60</span> * <span class="number">1000L</span>));</span><br></pre></td></tr></table></figure>
<p>Unlike non-windowed aggregates that we have seen previously, windowed aggregates return a windowed KTable whose key type is Windowed<k>. This is to differentiate aggregate values with the same key from different windows. The corresponding window instance and the embedded key can be retrieved as Windowed#window() and Windowed#key(), respectively.</k></p>
<p>和前面看到的没有窗口的聚合不同的是，窗口聚合操作返回一个带有窗口的KTable，它的key是Windowed<k>，目的是区分不同窗口中存在相同的key（如果没有带窗口，那么相同的key在不同的窗口中就无法区分）。对应的窗口实例以及内置的key分别可以通过<code>Windowed#window()</code>和<code>Windowed#key()</code>获取到。</k></p>
<p><strong>Sliding windows</strong> are actually quite different from hopping and tumbling windows. A sliding window models a fixed-size window that slides continuously over the time axis; here, two data records are said to be included in the same window if the difference of their timestamps is within the window size. Thus, sliding windows are not aligned to the epoch, but on the data record timestamps. Pay attention, that in contrast to hopping and tumbling windows, lower and upper window time interval bounds are both inclusive. In Kafka Streams, sliding windows are used only for join operations, and can be specified through the JoinWindows class.</p>
<p>滑动窗口和跳跃窗口、翻转窗口都不同，它也有固定的窗口大小，不过是在时间轴上持续地滑动。比如有两条记录的时间撮差别是在窗口大小内的，这两条记录就会被包含在同一个窗口中。所以滑动窗口并不是和时间点对齐，而是和记录的时间撮对齐。注意和跳跃窗口、翻转窗口相反的是，滑动窗口会同时包含窗口边界的上界和下界。在Kafka Streams中，滑动窗口仅用于join操作。</p>
<p><img src="https://msdnshared.blob.core.windows.net/media/TNBlogsFS/prod.evol.blogs.technet.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/02/52/ASAQL-3.png" alt=""></p>
<h4 id="JOINING_STREAMS">JOINING STREAMS</h4><p>Many stream processing applications can be coded as stream join operations. For example, applications backing an online shop might need to access multiple, updating database tables (e.g. sales prices, inventory, customer information) when processing a new record. These applications can be implemented such that they work on the tables’ changelog streams directly, i.e. without requiring to make a database query over the network for each record. In this example, the KTable concept in Kafka Streams would enable you to track the latest state (think: snapshot) of each table in a local key-value store, thus greatly reducing the processing latency as well as reducing the load of the upstream databases.</p>
<p>许多流处理应用程序都需要流式的join操作。比如在线商店应用程序在处理一条新记录时可能需要访问或更新多张数据库表（比如销售价格表，库存表，用户信息表）。这些应用程序可以在表的变更日志流（KTable是变更日志流，KStream是记录流）上直接实现，比如对每条记录不需要跨网络的数据库查询。在这个示例中，Kafka Streams的KTable概念使你可以在一个本地键值存储中跟踪每张表的最新状态（快照），因此可以很明显地减少处理延迟，以及减少上游数据库的压力。在Kafka Streams中，有下面两种的join操作：</p>
<ul>
<li>Join a KStream with another KStream or KTable.</li>
<li>Join a KTable with another KTable only.</li>
</ul>
<p>有三种join组合方式：</p>
<ul>
<li>KStream-to-KStream Joins are always windowed joins, since otherwise the join result size might grow infinitely in size. Here, a newly received record from one of the streams is joined with the other stream’s records within the specified window interval to produce one result for each matching pair based on user-provided ValueJoiner. A new KStream instance representing the result stream of the join is returned from this operator.</li>
<li>KTable-to-KTable Joins are join operations designed to be consistent(一致) with the ones in relational databases. Here, both changelog streams are materialized into local state stores to represent the latest snapshot of the their data table duals(双重). When a new record is received from one of the streams, it is joined with the other stream’s materialized state stores to produce one result for each matching pair based on user-provided ValueJoiner. A new KTable instance representing the result stream of the join, which is also a changelog stream of the represented table, is returned from this operator.</li>
<li>KStream-to-KTable Joins allow you to perform table lookups against a changelog stream (KTable) upon receiving a new record from another record stream (KStream). An example use case would be to enrich(使丰富) a stream of user activities (KStream) with the latest user profile information (KTable). Only records received from the record stream will trigger the join and produce results via ValueJoiner, not vice versa (i.e., records received from the changelog stream will be used only to update the materialized state store). A new KStream instance representing the result stream of the join is returned from this operator.</li>
</ul>
<ol>
<li><strong>KStream-to-KStrea Join</strong>操作总是针对窗口的Join，否则Join结果的大小会无限膨胀。从其中一个流接收到的新记录会和另外一个流的记录进行Join，后者的流会指定窗口间隔，最后会基于用户提供的<code>ValueJoiner</code>为每个匹配对产生一个结果。这里返回的结果是一个新的KStream实例，代表了Join操作的结果流。</li>
<li><strong>KTable-to-KTable Join</strong>操作被设计为和关系型数据库类似的操作。两个变更日志流（KTable）都会被物化成本地状态存储，表示数据表的最近快照。当从其中的一个流接收到一条新记录，它会和另外一个流的物化状态存储进行join，并根据用户提供的ValueJoiner产生一个匹配的结果。返回的结果是新的KTable实例，代表Join操作的结果流，它也是一个变更日志流。</li>
<li><strong>KStream-to-KTable Join</strong>操作允许你在记录流（KStream）上接收到一条新记录时，从一个变更日志流（KTable）上执行表（级别的记录）查询。比如对一个用户活动流（KStream）使用最近的用户个人信息（KTable）进行信息增强。只有从记录流接收的记录才会触发join操作，并通过ValueJoiner产生结果，反过来则不行（比如从变更日志流中接收的新记录只会被用来更新物化的状态存储，而不会和KStream记录流进行join）。返回的结果是一个新的KStream，代表了Join操作的结果流。</li>
</ol>
<p>根据操作对象，不同操作类型支持不同的join语义：</p>
<table>
<thead>
<tr>
<th>Join operands</th>
<th>(INNER) JOIN</th>
<th>OUTER JOIN</th>
<th>LEFT JOIN</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>KStream-to-KStream</td>
<td>Supported</td>
<td>Supported</td>
<td>Supported</td>
<td>KStream</td>
</tr>
<tr>
<td>KTable-to-KTable</td>
<td>Supported</td>
<td>Supported</td>
<td>Supported</td>
<td>KTable</td>
</tr>
<tr>
<td>KStream-to-KTable</td>
<td>N/A</td>
<td>N/A</td>
<td>Supported</td>
<td>KStream</td>
</tr>
</tbody>
</table>
<p>Kafka Streams的Join语义类似于关系型数据库的操作符：</p>
<ul>
<li>Inner join produces new joined records when the join operator finds some records with the same key in the other stream / materialized store.</li>
<li>Outer join works like inner join if some records are found in the windowed stream / materialized store. The difference is that outer join still produces a record even when no records are found. It uses null as the value of missing record.</li>
<li>Left join is like outer join except that for KStream-KStream join it is always driven by record arriving from the primary stream; while for KTable-KTable join it is driven by both streams to make the result consistent with the left join of databases while only permits missing records in the secondary stream. In a KStream-KTable left join, a KStream record will only join a KTable record if the KTable record arrived before the KStream record (and is in the KTable). Otherwise, the join result will be null</li>
</ul>
<ol>
<li><strong>Inner join</strong>：当join操作符在两个流或物化存储中都找到相同key的记录，产生新的join记录。</li>
<li><strong>Outer join</strong>：如果在窗口流或物化视图中找到记录，则和inner join类似。不同的是，outer join即使在没有找到记录也会输出一条记录，对于缺失的记录使用null作为value。</li>
<li><strong>Left join</strong>：和outer join类似，不过对于KStream-KSteram的join（KStream left join KStream），它总是在主要流（A left join B，则A是主要的流）的记录到达时驱动的；对于KTable-KTable的join（KTable left join KTable），它是由两个流一起驱动，并且结果和left join左边的流是一致的，只允许右边流的记录缺失；对于KStream-KTable的left join（KStream left join KTable），一条KStream的记录只会和一条KTable的记录join，并且这条KTable的记录必须要在KStream的记录之前到达（当然必须在KTable中），否则join结果为null。</li>
</ol>
<p>Since stream joins are performed over the keys of records, it is required that joining streams are co-partitioned by key, i.e., their corresponding Kafka topics must have the same number of partitions and partitioned on the same key so that records with the same keys are delivered to the same processing thread. This is validated by Kafka Streams library at runtime (we talked about the threading model and data parallelism with more details in the Architecture section).</p>
<p>由于流的join是在记录的key上执行的，这就要求参与join的流能够按照key协同分区。比如它们（流）对应的Kafka主题必须有相等数量的分区，而且在相同的key上进行分区。这样相同key的记录会被发送到相同的处理线程。上面的限制会在Kafka Streams库中在运行时进行验证。</p>
<p>Join示例，使用Java8：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Key is user, value is number of clicks by that user</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, Long&gt; userClicksStream =  <span class="attribute">...</span>;</span><br><span class="line"><span class="comment">// Key is user, value is the geo-region of that user</span></span><br><span class="line">KTable&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; userRegionsTable = <span class="attribute">...</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// KStream-KTable join</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, RegionWithClicks&gt; userClicksWithRegion = userClicksStream</span><br><span class="line">    <span class="comment">// Null values possible: In general, null values are possible for region (i.e. the value of</span></span><br><span class="line">    <span class="comment">// the KTable we are joining against) so we must guard against that (here: by setting the</span></span><br><span class="line">    <span class="comment">// fallback region "UNKNOWN").</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Also, we need to return a tuple of (region, clicks) for each user.  But because Java does</span></span><br><span class="line">    <span class="comment">// not support tuples out-of-the-box, we must use a custom class `RegionWithClicks` to</span></span><br><span class="line">    <span class="comment">// achieve the same effect.  This class two fields -- the region (String) and the number of</span></span><br><span class="line">    <span class="comment">// clicks (Long) for that region -- as well as a matching constructor, which we use here.</span></span><br><span class="line">    <span class="built_in">.</span>leftJoin(userRegionsTable,</span><br><span class="line">      (clicks, region) <span class="subst">-&gt; </span><span class="literal">new</span> RegionWithClicks(region == <span class="built_in">null</span> ? <span class="string">"UNKNOWN"</span> : region, clicks));</span><br></pre></td></tr></table></figure>
<p>Join示例，使用Java7：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Key is user, value is number of clicks by that user</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, Long&gt; userClicksStream =  <span class="attribute">...</span>;</span><br><span class="line"><span class="comment">// Key is user, value is the geo-region of that user</span></span><br><span class="line">KTable&lt;<span class="built_in">String</span>, <span class="built_in">String</span>&gt; userRegionsTable = <span class="attribute">...</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// KStream-KTable join</span></span><br><span class="line">KStream&lt;<span class="built_in">String</span>, RegionWithClicks&gt; userClicksWithRegion = userClicksStream</span><br><span class="line">    <span class="comment">// Null values possible: In general, null values are possible for region (i.e. the value of</span></span><br><span class="line">    <span class="comment">// the KTable we are joining against) so we must guard against that (here: by setting the</span></span><br><span class="line">    <span class="comment">// fallback region "UNKNOWN").</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Also, we need to return a tuple of (region, clicks) for each user.  But because Java does</span></span><br><span class="line">    <span class="comment">// not support tuples out-of-the-box, we must use a custom class `RegionWithClicks` to</span></span><br><span class="line">    <span class="comment">// achieve the same effect.  This class two fields -- the region (String) and the number of</span></span><br><span class="line">    <span class="comment">// clicks (Long) for that region -- as well as a matching constructor, which we use here.</span></span><br><span class="line">    <span class="built_in">.</span>leftJoin(userRegionsTable, <span class="literal">new</span> ValueJoiner&lt;Long, <span class="built_in">String</span>, RegionWithClicks&gt;() &#123;</span><br><span class="line">      @Override</span><br><span class="line">      <span class="keyword">public</span> RegionWithClicks apply(Long clicks, <span class="built_in">String</span> region) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">new</span> RegionWithClicks(region == <span class="built_in">null</span> ? <span class="string">"UNKNOWN"</span> : region, clicks);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br></pre></td></tr></table></figure>
<h4 id="APPLYING_A_CUSTOM_PROCESSOR">APPLYING A CUSTOM PROCESSOR</h4><p>Beyond the provided transformation operators, users can also specify any customized processing logic on their stream data via the KStream#process() method, which takes an implementation of the ProcessorSupplier interface as its parameter. This is essentially equivalent to the addProcessor() method in the Processor API.</p>
<p>The following example shows how to leverage, via the process() method, a custom processor that sends an email notification whenever a page view count reaches a predefined threshold.</p>
<p>除了Kafka Streams提供的转换操作（DSL），用户可以在流数据中通过KStream#process()方法指定定制的处理逻辑：将ProcessorSupplier接口的实现作为参数，这和Processor API的addProcessor()方法是等价的。下面的示例展示了通过process()方法如何使用自定义处理器，这个处理器会在当页面浏览计数到达指定的阈值时发送一个邮件通知。</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Send an email notification when the view count of a page reaches one thousand. </span></span><br><span class="line"><span class="comment">// JAVA8</span></span><br><span class="line">pageViews.countByKey(<span class="string">"PageViewCounts"</span>)</span><br><span class="line">         .filter((PageId pageId, <span class="keyword">Long</span> viewCount) -&gt; viewCount == <span class="number">1000</span>)</span><br><span class="line">         <span class="comment">// PopularPageEmailAlert is your custom processor that implements the</span></span><br><span class="line">         <span class="comment">// `Processor` interface, see further down below.</span></span><br><span class="line">         .process(() -&gt; <span class="keyword">new</span> PopularPageEmailAlert(<span class="string">"alerts@yourcompany.com"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// JAVA7</span></span><br><span class="line">pageViews.countByKey(<span class="string">"PageViewCounts"</span>)</span><br><span class="line">         .filter(</span><br><span class="line">            <span class="keyword">new</span> Predicate&lt;PageId, <span class="keyword">Long</span>&gt;() &#123;</span><br><span class="line">              <span class="keyword">public</span> <span class="keyword">boolean</span> test(PageId pageId, <span class="keyword">Long</span> viewCount) &#123;</span><br><span class="line">                <span class="keyword">return</span> viewCount == <span class="number">1000</span>;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">         .process(</span><br><span class="line">           <span class="keyword">new</span> ProcessorSupplier&lt;PageId, <span class="keyword">Long</span>&gt;() &#123;</span><br><span class="line">             <span class="keyword">public</span> Processor&lt;PageId, <span class="keyword">Long</span>&gt; get() &#123;</span><br><span class="line">               <span class="comment">// PopularPageEmailAlert is your custom processor that implements</span></span><br><span class="line">               <span class="comment">// the `Processor` interface, see further down below.</span></span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">new</span> PopularPageEmailAlert(<span class="string">"alerts@yourcompany.com"</span>);</span><br><span class="line">             &#125;</span><br><span class="line">           &#125;);</span><br></pre></td></tr></table></figure>
<p>上面的示例中PopularPageEmailAlert是一个自定义实现了Processor接口的流处理算子：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A processor that sends an alert message about a popular page to a configurable email address</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PopularPageEmailAlert</span> <span class="keyword">implements</span> <span class="title">Processor</span>&lt;<span class="title">PageId</span>, <span class="title">Long</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> String emailAddress;</span><br><span class="line">  <span class="keyword">private</span> ProcessorContext;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">PopularPageEmailAlert</span><span class="params">(String emailAddress)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.emailAddress = emailAddress;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessorContext context)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.context = context;</span><br><span class="line">    <span class="comment">// Here you would perform any additional initializations</span></span><br><span class="line">    <span class="comment">// such as setting up an email client.</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">(PageId pageId, Long count)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Here would format and send the alert email.</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// In this specific example, you would be able to include information</span></span><br><span class="line">    <span class="comment">// about the page's ID and its view count (because the class implements</span></span><br><span class="line">    <span class="comment">// `Processor&lt;PageId, Long&gt;`).</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">punctuate</span><span class="params">(<span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Stays empty.  In this use case there would be no need for a periodical</span></span><br><span class="line">    <span class="comment">// action of this processor.</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="annotation">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Any code for clean up would go here.</span></span><br><span class="line">    <span class="comment">// This processor instance will not be used again after this call.</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>就像前面提到的，一个流处理算子可以通过调用ProcessorContext#getStateStore()方法访问任何可用的状态存储。可用指的是：这些状态存储的名称在调用KStream#process()方法时指定（注意和Processor#process()方法不同，KStream#process()是在构造拓扑时定义）。</p>
<h4 id="WRITING_STREAMS_BACK_TO_KAFKA">WRITING STREAMS BACK TO KAFKA</h4><p>Any streams may be (continuously) written back to a Kafka topic via KStream#to() and KTable#to().</p>
<p>任何的流都可能会通过KStream#to()和KTable#to()方法持续地（将记录）写到Kafka主题中。</p>
<figure class="highlight fsharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Write the stream userCountByRegion to the output topic 'RegionCountsTopic'</span></span><br><span class="line">userCountByRegion.<span class="keyword">to</span>(<span class="string">"RegionCountsTopic"</span>);</span><br></pre></td></tr></table></figure>
<p>Best practice: It is strongly recommended to manually create output topics ahead of time rather than relying on auto-creation of topics. First, auto-creation of topics may be disabled in your Kafka cluster. Second, auto-creation will always apply the default topic settings such as the replicaton factor, and these default settings might not be what you want for certain output topics (cf. auto.create.topics.enable=true in the Kafka broker configuration).</p>
<blockquote>
<p>最佳实践：推荐手动创建输出主题而不是依赖于自动创建。首先自动创建主题可能会被你的Kafka集群禁用掉；其二，自动创建会运用一些默认的设置，比如副本因子，而这些默认的设置可能在某些输出主题上不是你想要的。</p>
</blockquote>
<p>If your application needs to continue reading and processing the records after they have been written to a topic via to() above, one option is to construct a new stream that reads from the output topic:</p>
<p>如果你的应用程序需要持续读取并处理的记录是通过to()方法写到的输出主题，一种方式是从输出主题中构造新的流：</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Write to a Kafka topic.</span></span><br><span class="line">userCountByRegion<span class="built_in">.</span><span class="keyword">to</span>(<span class="string">"RegionCountsTopic"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Read from the same Kafka topic by constructing a new stream from the</span></span><br><span class="line"><span class="comment">// topic RegionCountsTopic, and then begin processing it (here: via `map`)</span></span><br><span class="line">builder<span class="built_in">.</span>stream(<span class="string">"RegionCountsTopic"</span>)<span class="built_in">.</span><span class="built_in">map</span>(<span class="attribute">...</span>)<span class="attribute">...</span>;</span><br></pre></td></tr></table></figure>
<p>Kafka Streams provides a convenience method called through() that is equivalent to the code above:</p>
<p>Kafka Streams还提供了一种方便的方式：调用through()方法和上面的代码是类似的：</p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// <span class="string">`through`</span> combines <span class="keyword">write</span>-to-Kafka-topic <span class="keyword">and</span> <span class="keyword">read</span>-from-same-Kafka-topic operations</span><br><span class="line">userCountByRegion.through(<span class="string">"RegionCountsTopic"</span>).<span class="keyword">map</span>(...)...;</span><br></pre></td></tr></table></figure>
<p>Whenever data is read from or written to a Kafka topic, Streams must know the serdes to be used for the respective data records. By default the to() and through() methods use the default serdes defined in the Streams configuration. You can override these default serdes by passing explicit serdes to the to() and through() methods.</p>
<p>当数据从一个Kafka主题读取或者写入时，流必须知道记录使用的序列化方式，默认to()和through()方法使用了Streams配置中默认的序列化方式。你可以通过传递明确的序列化器给to()和through()方法来覆写这些默认的配置。</p>
<p>Besides writing the data back to Kafka, users can also apply a custom processor as mentioned above to write to any other external stores, for example, to materialize a data store, as stream sinks at the end of the processing.</p>
<p>除了将数据写回到Kafka，用户也可以像上面提到的那样运用一个自定义的处理器，并写到其他的外部存储介质中，比如物化数据存储，作为流处理的目标。</p>
<h2 id="运行流处理程序">运行流处理程序</h2><p>A Java application that uses the Kafka Streams library can be run just like any other Java application – there is no special magic or requirement on the side of Kafka Streams.</p>
<p>一个使用Kafka Streams客户端库的Java应用程序，它的运行方式和其他普通的Java应用程序一样，没有特殊的魔法，也没有额外的限制。比如你可以将你的Java应用程序打成一个fat jar包，然后启动：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># <span class="operator"><span class="keyword">Start</span> the application <span class="keyword">in</span> <span class="keyword">class</span> <span class="string">`com.example.MyStreamsApp`</span></span><br><span class="line"># <span class="keyword">from</span> the fat jar named <span class="string">`path-to-app-fatjar.jar`</span>.</span><br><span class="line">$ <span class="keyword">java</span> -cp <span class="keyword">path</span>-<span class="keyword">to</span>-app-fatjar.jar com.example.MyStreamsApp</span></span><br></pre></td></tr></table></figure>
<p>It is important to understand that, when starting your application as described above, you are actually launching what Kafka Streams considers to be one instance of your application. More than one instance of your application may be running at a time, and in fact the common scenario is that there are indeed multiple instances of your application running in parallel. See Parallelism Model for further information.</p>
<p>当启动应用程序时，在Kafka Streams看来，是启动了应用程序的一个实例。可能在同一个时间点，你的应用程序会同时运行多个实例，而实际上通常的场景的确是你的多个应用程序实例是并行执行的。</p>
<h3 id="水平扩展你的应用程序">水平扩展你的应用程序</h3><p>Kafka Streams makes your stream processing applications elastic and scalable: you can add and remove processing capacity dynamically during the runtime of your application, and you can do so without any downtime or data loss. This means that, unlike other stream processing technologies, with Kafka Streams you do not have to completely stop your application, recompile/reconfigure, and then restart. This is great not just for intentionally(故意地) adding or removing processing capacity, but also for being resilient(有弹性的) in the face of failures (e.g. machine crashes, network outages) and for allowing maintenance work (e.g. rolling upgrades).</p>
<p>Kafka Streams会让你的流处理应用程序可伸缩和可扩展。你可以在应用程序的运行时动态地添加或删除流处理能力，并且不需要停机维护，也不会丢失数据。这意味着，和其他流处理技术不同，使用Kafka Streams你不需要完全地停止你的应用程序，重新编译，重新配置，然后重启。对于故意地添加或删除处理能力，或者失败时的弹性机制，以及维护工作都是有好处的。</p>
<p>If you are wondering how this elasticity is actually achieved behind the scenes, you may want to read the Architecture chapter, notably the Parallelism Model section. In a nutshell(概括), Kafka Streams leverages existing functionality in Kafka, notably its group management functionality. This group management, which is built right into the Kafka wire protocol, is the foundation that enables the elasticity of Kafka Streams applications: members of a group will coordinate and collaborate(合作) jointly(共同地) on the consumption and processing of data in Kafka. On top of this foundation Kafka Streams provides some additional functionality, e.g. to enable stateful processing and to allow for fault-tolerante state in environment where application instances may come and go at any time.</p>
<p>如果你对如何实现扩展能力的背后机制感兴趣，可以阅读架构章节，尤其是并行模型那一部分。简单来说，Kafka Streams利用了Kafka已有的特性，尤其是组管理协议的功能。组管理协议构建在Kafka的协议之上，是Kafka Streams应用程序具有可伸缩性的基础：组的成员会协调合作，共同消费和处理Kafka中的数据。基于这些基础，Kafka Streams还提供了额外的功能，比如有状态的处理，以及在应用程序实例随时添加和删除的环境中，允许故障容错的状态存储。</p>
<h3 id="增加处理能力（Expand）">增加处理能力（Expand）</h3><p>If you need more processing capacity for your stream processing application, you can simply start another instance of your stream processing application, e.g. on another machine, in order to scale out. The instances of your application will become aware of each other and automatically begin to share the processing work. More specifically, what will be handed over from the existing instances to the new instances is (some of) the stream tasks that have been run by the existing instances. Moving stream tasks from one instance to another results in moving the processing work plus any internal state of these stream tasks (the state of a stream task will be re-created in the target instance by restoring the state from its corresponding changelog topic).</p>
<p>如果你需要为流处理应用程序添加更多的处理能力，你只需要在其他机器上简单地启动新的流处理应用程序实例，来达到扩展的目的。应用程序的所有示例都会彼此感知，并且自动地开始共享处理工作。更具体地说，从已有的实例移交给新实例的工作是在已有实例上运行的流任务。将流任务从一个实例移动到另一个实例的结果是，将这些流任务的处理工作以及内部状态都一起移动（一个流任务的状态会在目标实例中重新创建，从对应的变更日志主题中恢复数据来构造状态）。</p>
<p>The various instances of your application each run in their own JVM process, which means that each instance can leverage all the processing capacity that is available to their respective JVM process (minus the capacity that any non-Kafka-Streams part of your application may be using). This explains why running additional instances will grant your application additional processing capacity. The exact capacity you will be adding by running a new instance depends of course on the environment in which the new instance runs: available CPU cores, available main memory and Java heap space, local storage, network bandwidth, and so on. Similarly, if you stop any of the running instances of your application, then you are removing and freeing up the respective processing capacity.</p>
<p>应用程序的不同实例运行在它们各自的JVM进程中，这意味着每个实例可以利用它们对应的JVM进程的所有处理能力/资源（减去应用程序中不是Kafka Streams部分使用的资源）。这就解释了为什么运行额外的实例可以提升应用程序的处理能力。不过运行一个新实例期望新增加的处理能力当然会和新实例所在的环境有关：比如可用的CPU核，可用的主内存和Java堆空间大小，本地存储，网络带宽等等。同样，如果你停止了运行中的任意一个实例，你就删除并且释放掉相应的处理能力。</p>
<p><img src="http://img.blog.csdn.net/20161105125241586" alt="kstream expand"></p>
<p>Before adding capacity: only a single instance of your Kafka Streams application is running. At this point the corresponding Kafka consumer group of your application contains only a single member (this instance). All data is being read and processed by this single instance.</p>
<p>After adding capacity: now two additional instances of your Kafka Streams application are running, and they have automatically joined the application’s Kafka consumer group for a total of three current members. These three instances are automatically splitting the processing work between each other. The splitting is based on the Kafka topic partitions from which data is being read.</p>
<p>上图左边是添加处理能力之前，只有一个Kafka Streams应用程序实例在运行，这时你的应用程序对应的Kafka消费组只有一个成员（就是这个实例），所有的数据都通过这个唯一的实例读取并处理。右图是添加处理能力之后，现在增加了两个额外的应用程序运行实例，而且它们自动加入到应用程序对应的Kafka消费组中，这个消费组目前总共有3个成员。这三个实例两两之间都会自动地均摊处理工作。分摊是基于Kafka的主题分区，即（每个实例）从不同分区读取不同的数据。</p>
<h3 id="减少处理能力（Shrink）">减少处理能力（Shrink）</h3><p>If you need less processing capacity for your stream processing application, you can simply stop one or more running instances of your stream processing application, e.g. shut down 2 of 4 running instances. The remaining instances of your application will become aware that other instances were stopped and automatically take over the processing work of the stopped instances. More specifically, what will be handed over from the stopped instances to the remaining instances is the stream tasks that were run by the stopped instances. Moving stream tasks from one instance to another results in moving the processing work plus any internal state of these stream tasks (the state of a stream task will be re-created in the target instance by restoring the state from its corresponding changelog topic).</p>
<p>如果你的流处理应用程序需要较少的处理能力，你只需要停止一个或多个运行的流处理应用程序即可，比如将4个运行的实例关闭掉2个。剩余的应用程序实例会感知到其他实例已经被停止了，并且会自动接管这些停掉示例的处理工作。更具体的来说，从停止实例移交给剩余实例的工作是在停止实例上运行的流任务。将流任务从一个实例移动到另一个实例的结果是，将这些流任务的处理工作以及内部状态都一起移动。</p>
<p><img src="http://img.blog.csdn.net/20161105125255977" alt="kstream shrink"></p>
<p>If one of the application instances is stopped (e.g. intentional reduction of capacity, maintenance, machine failure), it will automatically leave the application’s consumer group, which causes the remaining instances to automatically take over the stopped instance’s processing work.</p>
<p>图中如果停止（故意减少容量，维护，或者机器故障都可能停止）了一个应用程序实例，它就会自动离开应用程序的消费组，并导致剩余的示例自动接管这个停止实例的处理工作。</p>
<h3 id="运行多少个应用程序实例">运行多少个应用程序实例</h3><p>How many instances can or should you run for your application? Is there an upper limit for the number of instances and, similarly, for the parallelism of your application? In a nutshell, the parallelism of a Kafka Streams application – similar to the parallelism of Kafka – is primarily determined by the number of partitions of the input topic(s) from which your application is reading. For example, if your application reads from a single topic that has 10 partitions, then you can run up to 10 instances of your applications (note that you can run further instances but these will be idle).</p>
<p>The number of topic partitions is the upper limit for the parallelism of your Kafka Streams application and thus for the number of running instances of your application.</p>
<p>那么到底可以或者应该运行多少个应用程序实例？是否有一个数量的上限，来并行化你的应用程序？简单来说，一个Kafka Streams应用程序的并行度，类似于Kafka的并行度，主要的决定因素是：应用程序所读取的输入主题的分区数量。比如你的应用程序读取的一个主题有10个分区，那么你就可以运行最多10个应用程序实例（虽然你可以运行更多的实例，但是会有一些实例是空闲的）。所以，主题分区的数量是你的流处理应用程序并行度的上限，因此也是你的应用程序运行实例的上限。</p>
<p>How to achieve(获得) a balanced processing workload across application instances to prevent processing hotpots: The balance of the processing work between application instances depends on factors such as how well data messages are balanced between partitions (think: if you have 2 topic partitions, having 1 million messages in each partition is better than having 2 million messages in the first partition and no messages in the second) and how much processing capacity is required to process the messages (think: if the time to process messages varies heavily, then it is better to spread the processing-intensive messages across partitions rather than storing these messages within the same partition).</p>
<p>那么怎么在应用程序实例之前来保证处理的负载是平衡的，防止发生处理热点。工作负载是否平衡的决定因素是分区的数据有多平衡（比如你有两个分区，每个分区有一百万条消息要比一个分区有两百万条消息，而另外一个分区没有一条消息要好的多），以及消息的处理能力（比如处理消息的时间变化很大，那么将处理比较耗时的消息分散在多个分区，要比这些消息都存储在一个分区也要好得多）。</p>
<p>If your data happens to be heavily skewed(倾斜) in the way described above, some application instances may become processing hotspots (say, when most messages would end up being stored in only 1 of 10 partitions, then the application instance that is processing this one partition would be performing most of the work while other instances might be idle). You can minimize the likelihood of such hotspots by ensuring better data balancing across partitions (i.e. minimizing data skew at the point in time when data is being written to the input topics in Kafka) and by over-partitioning the input topics (think: use 50 or 100 partitions instead of just 10), which lowers the probability that a small subset of partitions will end up storing most of the topic’s data.</p>
<p>如果你的数据恰巧倾斜的很严重，有一些应用程序实例就会变成处理热点。你可以通过确保数据在分区之间有更好的平衡来最小化出现这种热点的可能性（比如当大部分的消息都只存储在10个分区中的一个时，那么处理这个分区的应用程序实例就会处理大部分的工作，而其他实例则可能很空闲），或者对输入主题采用更多的分区数（比如在数据写入到Kafka的输入主题是就尽量最小化数据的倾斜）来减少一个很小的分区字节存储了大部分主题数据的这种可能性。</p>
<h2 id="数据类型和序列化">数据类型和序列化</h2><h2 id="应用重置工具">应用重置工具</h2><p>The Application Reset Tool allows you to quickly reset an application in order to reprocess its data from scratch – think: an application “reset” button. Scenarios when would you like to reset an application include:</p>
<p>应用程序重置工具允许你快速地重置一个应用程序，然后重新处理数据，可以认为是应用程序的一个“重置”按钮。需要重置应用程序的场景包括：</p>
<ul>
<li>Development and testing 开发和测试时</li>
<li>Addressing bugs in production 在生产环境定位问题时</li>
<li>Demos 演示</li>
</ul>
<p>However, resetting an application manually is a bit involved. Kafka Streams is designed to hide many details about operator state, fault-tolerance, and internal topic management from the user (especially when using Kafka Streams DSL). In general this hiding of details is very desirable(值得要的，令人满意的) but it also makes manually resetting an application more difficult. The Application Reset Tool fills this gap and allows you to quickly reset an application.</p>
<p>不过，手动方式重置一个应用程序需要做很多工作。Kafka Streams提供了一个重置工具，帮你隐藏了很多细节，比如操作状态，故障容错，内部的主题管理。</p>
<p><strong>用户主题和内部主题</strong>  </p>
<p>在Kafka Streams中，我们会区分用户主题和内部主题，这两种都是普通的Kafka主题，不过对于内部主题，有一些特定的命名约定。</p>
<p>用户主题包括输入主题、输出主题、临时主题。这些主题是用户创建或管理的，包括应用程序的输入和输出主题，以及通过through()方法指定的临时主题，临时主题实际上同时既是输出也是输入主题。</p>
<p>内部主题是由Kafka Streams底层自动创建的。比如针对状态存储的变更日志主题就是一个内部主题。内部主题的命名约定是：<application.id>-<operatorname>-<suffix>。</suffix></operatorname></application.id></p>
<p>应用程序重置工具做的工作有：  </p>
<ol>
<li>对输入主题：重置应用程序所有分区的消费者提交偏移量到0</li>
<li>对临时主题：跳到主题的最后，比如设置应用程序的消费者提交偏移量到每个分区的logSize（实际上就是分区的最后位置）</li>
<li>对内部主题：除了重置偏移量到0，还要删除内部主题</li>
</ol>
<p>应用程序重置工具不做的：</p>
<ol>
<li>不会重置应用程序的输出主题。如果任何的输出主题或者临时主题被下游的应用程序消费，那么调整这些下游应用程序是你自己的责任</li>
<li>不会重置应用程序实例的本地环境。同样删除应用程序实例运行所在机器的本地状态，也是你自己的责任。</li>
</ol>
<p><strong>步骤1：运行重置工具</strong>  </p>
<p>执行<code>bin/kafka-streams-application-reset</code>命令，需要指定如下的参数：</p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Option (* = required)         Description</span><br><span class="line">-<span class="ruby">--------------------         -----------</span><br><span class="line"></span>* --application-id &lt;id&gt;       The Kafka Streams application ID (application.id)</span><br><span class="line">-<span class="ruby">-bootstrap-servers &lt;urls&gt;    <span class="constant">Comma</span>-separated list of broker urls with <span class="symbol">format:</span> <span class="constant">HOST1</span><span class="symbol">:PORT1</span>,<span class="constant">HOST2</span><span class="symbol">:PORT2</span></span><br><span class="line"></span>                                (default: localhost:9092)</span><br><span class="line">-<span class="ruby">-intermediate-topics &lt;list&gt;  <span class="constant">Comma</span>-separated list of intermediate user topics</span><br><span class="line"></span>-<span class="ruby">-input-topics &lt;list&gt;         <span class="constant">Comma</span>-separated list of user input topics</span><br><span class="line"></span>-<span class="ruby">-zookeeper &lt;url&gt;             <span class="constant">Format</span><span class="symbol">:</span> <span class="constant">HOST</span><span class="symbol">:POST</span></span><br><span class="line"></span>                                (default: localhost:2181)</span><br></pre></td></tr></table></figure>
<p>You can combine the parameters of the script as needed. For example, if an application should only restart from an empty internal state but not reprocess previous data, simply omit the parameters –input-topics and –intermediate-topics.</p>
<p>你可以任意组合上面的参数，比如如果应用程序只会从空的内部状态重启，不会重新处理已有的数据，可以忽略输入主题和临时主题这两个参数。</p>
<p>On intermediate topics: In general, we recommend to manually delete and re-create any intermediate topics before running the application reset tool. This allows to free disk space in Kafka brokers early on. It is important to first delete and re-create intermediate topics before running the application reset tool.</p>
<p>Not deleting intermediate topics and only using the application reset tool is preferable:</p>
<ul>
<li>when there are external downstream consumers for the application’s intermediate topics</li>
<li>during development, where manually deleting and re-creating intermediate topics might be cumbersome and often unnecessary</li>
</ul>
<p>关于临时主题：通常我们推荐在运行重置工具之前，手动删除然后重建临时主题。这样可以尽早释放Kafka的集群磁盘空间。什么时候不需要删除临时主题：</p>
<ol>
<li>存在外部的下游消费者订阅了应用程序的临时主题</li>
<li>在开发环境，手动删除并重建主题可能很繁琐，而且通常没有必要这么做</li>
</ol>
<p><strong>步骤2：重置本地环境</strong>  </p>
<p>Running the application reset tool (step 1) ensures that your application’s state – as tracked globally in the application’s configured Kafka cluster – is reset. However, by design the reset tool does not modify or reset the local environment of your application instances, which includes the application’s local state directory.</p>
<p>For a complete application reset you must also delete the application’s local state directory on any machines on which an application instance was run prior to restarting an application instance on the same machines. You can either use the API method KafkaStreams#cleanUp() in your application code or manually delete the corresponding local state directory (default location: /var/lib/kafka-streams/<application.id>, cf. state.dir configuration parameter).</application.id></p>
<p>运行步骤1的应用程序重置工具确保你的应用程序状态被重置（应用程序的状态实际上是在应用程序配置的Kafka集群被全局地跟踪）。不过，重置工具被设计的时候，并不会修改或重置应用程序实例的本地环境，包括应用程序的本地状态目录。</p>
<p>如果要彻底重置应用程序，你必须删除应用程序的本地状态目录，而且任何之前运行过的应用程序实例所在的机器都需要在重启应用程序实例之前删除干净。你可以在应用程序中调用KafkaStreams#cleanUp()方法，或者手动删除对应的本地状态目录（默认的路径是： /var/lib/kafka-streams/<application.id>，即state.dir的配置参数）来清理。</application.id></p>
<p><strong>示例</strong>  </p>
<p>Let’s imagine you are developing and testing an application locally and want to iteratively improve your application via run-reset-modify cycles. You might have code such as the following:</p>
<p>假设你在本地开发和测试一个应用程序，并且通过运行-重置-修改的循环开发模式来不断迭代提升你的应用程序，你可能需要这样的代码：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> ResetDemo &#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> throws Exception </span>&#123;</span><br><span class="line">    <span class="comment">// Kafka Streams configuration</span></span><br><span class="line">    Properties streamsConfiguration = <span class="keyword">new</span> Properties();</span><br><span class="line">    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"my-streams-app"</span>);</span><br><span class="line">    <span class="comment">// ...and so on...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Define the processing topology</span></span><br><span class="line">    KStreamBuilder builder = <span class="keyword">new</span> KStreamBuilder();</span><br><span class="line">    builder.stream(<span class="string">"my-input-topic"</span>)</span><br><span class="line">        .selectKey(...)</span><br><span class="line">        .through(<span class="string">"rekeyed-topic"</span>)</span><br><span class="line">        .countByKey(<span class="string">"global-count"</span>)</span><br><span class="line">        .to(<span class="string">"my-output-topic"</span>);</span><br><span class="line"></span><br><span class="line">    KafkaStreams app = <span class="keyword">new</span> KafkaStreams(builder, streamsConfiguration);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Delete the application's local state.</span></span><br><span class="line">    <span class="comment">// <span class="doctag">Note:</span> In real application you'd call `cleanUp()` only under</span></span><br><span class="line">    <span class="comment">// certain conditions.  See tip on `cleanUp()` below.</span></span><br><span class="line">    app.cleanUp();</span><br><span class="line"></span><br><span class="line">    app.start();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// <span class="doctag">Note:</span> In real applications you would register a shutdown hook</span></span><br><span class="line">    <span class="comment">// that would trigger the call to `app.close()` rather than</span></span><br><span class="line">    <span class="comment">// using the sleep-then-close example we show here.</span></span><br><span class="line">    Thread.sleep(<span class="number">30</span> * <span class="number">1000L</span>);</span><br><span class="line">    app.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Calling cleanUp() is safe but do so judiciously(明智的): It is always safe to call KafkaStreams#cleanUp() because the local state of an application instance can be recovered from the underlying internal changelog topic(s). However, to avoid the corresponding recovery overhead it is recommended to not call cleanUp() unconditionally and every time an application instance is restarted/resumed. A production application would therefore use e.g. command line arguments to enable/disable the cleanUp() call as needed.</p>
<p>调用cleanUp()方法是安全的，不过要谨慎调用：调用KafkaStreams#cleanUp()总是安全的，因为应用程序实例的本队状态可以从底层的内部变更日志主题恢复过来。不过，为了防止由此产生的恢复开销，推荐不要盲目调用cleanUp()，或者说在每次应用程序重启/恢复的时候就调用cleanUp()。生产环境下的应用程序因此会使用命令行的参数来允许或禁止调用cleanUp()。</p>
</blockquote>
<p>然后你就可以执行如下的“run-reset-modify”循环：</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run your application</span></span><br><span class="line">$ bin/kafka-<span class="command">run</span>-<span class="type">class</span> io.confluent.examples.streams.ResetDemo</span><br><span class="line"></span><br><span class="line"><span class="comment"># After stopping all application instances, reset the application</span></span><br><span class="line">$ bin/kafka-streams-<span class="type">application</span>-reset <span class="comment">--application-id my-streams-app \</span></span><br><span class="line">                                      <span class="comment">--input-topics my-input-topic \</span></span><br><span class="line">                                      <span class="comment">--intermediate-topics rekeyed-topic</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Now you can modify/recompile as needed and then re-run the application again.</span></span><br><span class="line"><span class="comment"># You can also experiment, for example, with different input data without</span></span><br><span class="line"><span class="comment"># modifying the application.</span></span><br></pre></td></tr></table></figure>
<p>EOF. 翻译完毕 @2016.11.5</p>
<h2 id="重置流处理应用程序">重置流处理应用程序</h2><p>Confluent关于重置的实现翻译：<a href="https://www.confluent.io/blog/data-reprocessing-with-kafka-streams-resetting-a-streams-application/" target="_blank" rel="external">https://www.confluent.io/blog/data-reprocessing-with-kafka-streams-resetting-a-streams-application/</a>  </p>

      
    </div>
    
  </div>
  
    
<div class="copyright">
  <p><span>本文标题:</span><a href="/2016/11/02/Kafka-Streams-cn/">Kafka Streams中文翻译</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 任何忧伤,都抵不过世界的美丽 的个人博客">任何忧伤,都抵不过世界的美丽</a></p>
  <p><span>发布时间:</span>2016年11月02日 - 00时00分</p>
  <p><span>最后更新:</span>2016年11月05日 - 16时43分</p>
  <p>
    <span>原始链接:</span><a href="/2016/11/02/Kafka-Streams-cn/" title="Kafka Streams中文翻译">http://github.com/zqhxuyuan/2016/11/02/Kafka-Streams-cn/</a>
    <span class="btn" data-clipboard-text="原文: http://github.com/zqhxuyuan/2016/11/02/Kafka-Streams-cn/　　作者: 任何忧伤,都抵不过世界的美丽" title="点击复制文章链接">
        <i class="fa fa-clipboard"></i>
    </span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" title="中国大陆 (CC BY-NC-SA 3.0 CN)">"署名-非商用-相同方式共享 3.0"</a> 转载请保留原文链接及作者。</p>
  <script src="/js/clipboard.min.js"></script>
  <script> var clipboard = new Clipboard('.btn'); </script>
</div>
<style type="text/css">
  .copyright p .btn {
    margin-left: 1em;
  }
  .copyright:hover p .btn::after {
    content: "复制"
  }
  .copyright p .btn:hover {
      color: gray;
      cursor: pointer;
    };
</style>



<nav id="article-nav">
  
    <div id="article-nav-newer" class="article-nav-title">
      <a href="/2016/11/18/Kafka-CQRS-Streams/">
        译：Kafka事件驱动和流处理
      </a>
    </div>
  
  
    <div id="article-nav-older" class="article-nav-title">
      <a href="/2016/10/31/Data-Transform/">
        
      </a>
    </div>
  
</nav>

  
  
    <div class ="post-donate">
    <div id="donate_board" class="donate_bar center">
        <a id="btn_donate" class="btn_donate" href="javascript:;" title="打赏"></a>
        <span class="donate_txt">
           &uarr;<br>
		   赏点酒钱
        </span>
        <br>
    </div>  
	<div id="donate_guide" class="donate_bar center hidden" >
		<img src="/img/zhifubao.png" alt="支付宝打赏"> 
		<img src="/img/weixin.png" alt="微信打赏">  
    </div>
	<script type="text/javascript">
		document.getElementById('btn_donate').onclick = function(){
			$('#donate_board').addClass('hidden');
			$('#donate_guide').removeClass('hidden');
		}
	</script>
</div>
  
</article>

<!-- 默认显示文章目录，在文章---前输入toc: false关闭目录 -->
<!-- Show TOC and tocButton in default, Hide TOC via putting "toc: false" before "---" at [post].md -->
<div id="toc" class="toc-article">
<strong class="toc-title">文章目录</strong>
<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#介绍"><span class="toc-number">1.</span> <span class="toc-text">介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka_Streams"><span class="toc-number">1.1.</span> <span class="toc-text">Kafka Streams</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A_closer_look"><span class="toc-number">1.2.</span> <span class="toc-text">A closer look</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#快速开始"><span class="toc-number">2.</span> <span class="toc-text">快速开始</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#本节目标"><span class="toc-number">2.1.</span> <span class="toc-text">本节目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#我们要做什么"><span class="toc-number">2.2.</span> <span class="toc-text">我们要做什么</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#概念"><span class="toc-number">3.</span> <span class="toc-text">概念</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka_101"><span class="toc-number">3.1.</span> <span class="toc-text">Kafka 101</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#流、流处理、拓扑、算子"><span class="toc-number">3.2.</span> <span class="toc-text">流、流处理、拓扑、算子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#时间"><span class="toc-number">3.3.</span> <span class="toc-text">时间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#有状态的流处理"><span class="toc-number">3.4.</span> <span class="toc-text">有状态的流处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Streams和Tables的二元性"><span class="toc-number">3.5.</span> <span class="toc-text">Streams和Tables的二元性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KStream（记录流_record_stream）"><span class="toc-number">3.6.</span> <span class="toc-text">KStream（记录流 record stream）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KTable（变更流_changelog_stream）"><span class="toc-number">3.7.</span> <span class="toc-text">KTable（变更流 changelog stream）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#窗口操作"><span class="toc-number">3.8.</span> <span class="toc-text">窗口操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#联合操作"><span class="toc-number">3.9.</span> <span class="toc-text">联合操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#聚合操作"><span class="toc-number">3.10.</span> <span class="toc-text">聚合操作</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#架构"><span class="toc-number">4.</span> <span class="toc-text">架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#拓扑"><span class="toc-number">4.1.</span> <span class="toc-text">拓扑</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#并行模型"><span class="toc-number">4.2.</span> <span class="toc-text">并行模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Stream_Partitions_and_Tasks"><span class="toc-number">4.2.1.</span> <span class="toc-text">Stream Partitions and Tasks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Threading_Model"><span class="toc-number">4.2.2.</span> <span class="toc-text">Threading Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Example"><span class="toc-number">4.2.3.</span> <span class="toc-text">Example</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#状态"><span class="toc-number">4.3.</span> <span class="toc-text">状态</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#容错"><span class="toc-number">4.4.</span> <span class="toc-text">容错</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#流处理的保证"><span class="toc-number">4.5.</span> <span class="toc-text">流处理的保证</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#流控"><span class="toc-number">4.6.</span> <span class="toc-text">流控</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#背压"><span class="toc-number">4.7.</span> <span class="toc-text">背压</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#开发者指南"><span class="toc-number">5.</span> <span class="toc-text">开发者指南</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka_Streams配置"><span class="toc-number">5.1.</span> <span class="toc-text">Kafka Streams配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#编写一个流处理应用程序"><span class="toc-number">5.2.</span> <span class="toc-text">编写一个流处理应用程序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Processor_API"><span class="toc-number">5.2.1.</span> <span class="toc-text">Processor API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka_Streams_DSL"><span class="toc-number">5.2.2.</span> <span class="toc-text">Kafka Streams DSL</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#CREATING_SOURCE_STREAMS_FROM_KAFKA"><span class="toc-number">5.2.2.1.</span> <span class="toc-text">CREATING SOURCE STREAMS FROM KAFKA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TRANSFORM_A_STREAM"><span class="toc-number">5.2.2.2.</span> <span class="toc-text">TRANSFORM A STREAM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#STATELESS_TRANSFORMATIONS"><span class="toc-number">5.2.2.3.</span> <span class="toc-text">STATELESS TRANSFORMATIONS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#STATEFUL_TRANSFORMATIONS"><span class="toc-number">5.2.2.4.</span> <span class="toc-text">STATEFUL TRANSFORMATIONS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#WINDOWING_A_STREAM"><span class="toc-number">5.2.2.5.</span> <span class="toc-text">WINDOWING A STREAM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#JOINING_STREAMS"><span class="toc-number">5.2.2.6.</span> <span class="toc-text">JOINING STREAMS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#APPLYING_A_CUSTOM_PROCESSOR"><span class="toc-number">5.2.2.7.</span> <span class="toc-text">APPLYING A CUSTOM PROCESSOR</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#WRITING_STREAMS_BACK_TO_KAFKA"><span class="toc-number">5.2.2.8.</span> <span class="toc-text">WRITING STREAMS BACK TO KAFKA</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#运行流处理程序"><span class="toc-number">5.3.</span> <span class="toc-text">运行流处理程序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#水平扩展你的应用程序"><span class="toc-number">5.3.1.</span> <span class="toc-text">水平扩展你的应用程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#增加处理能力（Expand）"><span class="toc-number">5.3.2.</span> <span class="toc-text">增加处理能力（Expand）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#减少处理能力（Shrink）"><span class="toc-number">5.3.3.</span> <span class="toc-text">减少处理能力（Shrink）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#运行多少个应用程序实例"><span class="toc-number">5.3.4.</span> <span class="toc-text">运行多少个应用程序实例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据类型和序列化"><span class="toc-number">5.4.</span> <span class="toc-text">数据类型和序列化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#应用重置工具"><span class="toc-number">5.5.</span> <span class="toc-text">应用重置工具</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#重置流处理应用程序"><span class="toc-number">5.6.</span> <span class="toc-text">重置流处理应用程序</span></a></li></ol></li></ol>
</div>
<style type="text/css">
  .left-col .switch-btn {
    display: none;
  }
  .left-col .switch-area {
    display: none;
  }
</style>

<input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">
<script type="text/javascript">
  var toc_button= document.getElementById("tocButton");
  var toc_div= document.getElementById("toc");
  /* Show or hide toc when click on tocButton.
  通过点击设置的按钮显示或者隐藏文章目录.*/
  toc_button.onclick=function(){
  if(toc_div.style.display=="none"){
  toc_div.style.display="block";
  toc_button.value="隐藏目录";
  document.getElementById("switch-btn").style.display="none";
  document.getElementById("switch-area").style.display="none";
  }
  else{
  toc_div.style.display="none";
  toc_button.value="显示目录";
  document.getElementById("switch-btn").style.display="block";
  document.getElementById("switch-area").style.display="block";
  }
  }
    if ($(".toc").length < 1) {
        $("#toc").css("display","none");
        $("#tocButton").css("display","none");
        $(".switch-btn").css("display","block");
        $(".switch-area").css("display","block");
    }
</script>


    <style>
        .toc {
            white-space: nowrap;
            overflow-x: hidden;
        }
    </style>

    <script>
        $(document).ready(function() {
            $(".toc li a").mouseover(function() {
                var title = $(this).attr('href');
                $(this).attr("title", title);
            });
        })
    </script>




<div class="share">
	<div class="bdsharebuttonbox">
	<a href="#" class="bds_more" data-cmd="more"></a>
	<a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
	<a href="#" class="bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
	<a href="#" class="bds_copy" data-cmd="copy" title="复制网址"></a>
	<a href="#" class="bds_mail" data-cmd="mail" title="通过邮件分享"></a>
	<a href="#" class="bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
	</div>
	<script>
	window._bd_share_config={
		"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
	</script>
</div>



<div class="duoshuo" id="comments">
	<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="2016/11/02/Kafka-Streams-cn/" data-title="Kafka Streams中文翻译" data-url="http://github.com/zqhxuyuan/2016/11/02/Kafka-Streams-cn/"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"zqhxuyuan"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<!-- 多说公共JS代码 end -->
</div>






    <style type="text/css">
    #scroll {
      display: none;
    }
    </style>
    <div class="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
    </div>


  
  
    
    <div  class="post-nav-button">
    <a href="/2016/11/18/Kafka-CQRS-Streams/" title="上一篇: 译：Kafka事件驱动和流处理">
    <i class="fa fa-angle-left"></i>
    </a>
    <a href="/2016/10/31/Data-Transform/" title="下一篇: ">
    <i class="fa fa-angle-right"></i>
    </a>
    </div>
  



    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2016 任何忧伤,都抵不过世界的美丽
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的静态博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减双栏 Hexo 博客主题">Yelee</a> by MOxFIVE
        </div>
    </div>
    <div class="visit">
      <span id="busuanzi_container_site_pv" style='display:none'>
        <span id="site-visit" >本站到访数: 
        <span id="busuanzi_value_site_uv"></span>
        </span>
      </span>
      <span id="busuanzi_container_page_pv" style='display:none'>
        <span id="page-visit">, 本页阅读量: 
        <span id="busuanzi_value_page_pv"></span>
        </span>
      </span>
    </div>
  </div>
</footer>
    </div>
    

<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>

<script>
  var backgroundnum = 5;
  var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));

  $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
</script>


<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-80646710-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
<a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
<a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>